{"paper_id": 0, "abstract": "In the realm of reinforcement learning and Monte Carlo methods, a formidable challenge looms: the quest to estimate quantities tethered to the stationary distribution of a Markov chain. In the unpredictable landscape of real-world applications, we often find ourselves constrained by a fixed dataset, with no further communion with the environment to glean additional insights. Yet, fear not! We unveil a path forward, demonstrating that consistent estimation is not only feasible but can also yield robust results in critical applications.  Our method hinges on a clever ratio that rectifies the gap between the stationary and empirical distributions, drawing upon the inherent properties of the stationary distribution itself. By deftly employing constraint reformulations grounded in variational divergence minimization, we forge a new algorithm: GenDICE. This approach is not only elegant but also remarkably effective.  We rigorously establish the consistency of our method under broad conditions, accompanied by a comprehensive error analysis that lays bare its strengths. Furthermore, our empirical results shine brightly, showcasing GenDICE's prowess across benchmark tasks, including off-line PageRank and off-policy policy evaluation. In this intricate dance of data and algorithms, we have crafted a tool that promises to illuminate the shadows of uncertainty in estimation.", "introduction": "Estimation of quantities defined by the stationary distribution of a Markov chain lies at the heart of many scientific and engineering problems.Famously, the steady-state distribution of a random walk on the World Wide Web provides the foundation of the PageRank algorithm (Langville & Meyer, 2004).In many areas of machine learning, Markov chain Monte Carlo (MCMC) methods are used to conduct approximate Bayesian inference by considering Markov chains whose equilibrium distribution is a desired posterior (Andrieu et al., 2002).An example from engineering is queueing theory, where the queue lengths and waiting time under the limiting distribution have been extensively studied (Gross et al., 2018).As we will also see below, stationary distribution quantities are of fundamental importance in reinforcement learning (RL) (e.g., Tsitsiklis & Van Roy, 1997).Classical algorithms for estimating stationary distribution quantities rely on the ability to sample next states from the current state by directly interacting with the environment (as in on-line RL or MCMC), or even require the transition probability distribution to be given explicitly (as in PageRank).Unfortunately, these classical approaches are inapplicable when direct access to the environment is not available, which is often the case in practice.There are many practical scenarios where a collection of sampled trajectories is available, having been collected off-line by an external mechanism that chose states and recorded the subsequent next states.Given such data, we still wish to estimate a stationary quantity.One important example is off-policy policy evaluation in RL, where we wish to estimate the value of a policy different from that used to collect experience.Another example is off-line PageRank (OPR), where we seek to estimate the relative importance of webpages given a sample of the web graph.Motivated by the importance of these off-line scenarios, and by the inapplicability of classical methods, we study the problem of off-line estimation of stationary values via a stationary distribution corrector.Instead of having access to the transition probabilities or a next-state sampler, we assume only access to a fixed sample of state transitions, where states have been sampled from an unknown distribution and next-states are sampled according to the Markov chain's transition operator.The off-line setting is indeed more challenging than its more traditional on-line counterpart, given that one must infer an asymptotic quantity from finite data.Nevertheless, we develop techniques that still allow consistent estimation under general conditions, and provide effective estimates in practice.The main contributions of this work are:\u2022 We formalize the problem of off-line estimation of stationary quantities, which captures a wide range of practical applications.\u2022 We propose a novel stationary distribution estimator, GenDICE, for this task.The resulting algorithm is based on a new dual embedding formulation for divergence minimization, with a carefully designed mechanism that explicitly eliminates degenerate solutions.\u2022 We theoretically establish consistency and other statistical properties of GenDICE, and empirically demonstrate that it achieves significant improvements on several behavior-agnostic offpolicy evaluation benchmarks and an off-line version of PageRank.The methods we develop in this paper fundamentally extend recent work in off-policy policy evaluation (Liu et al., 2018;Nachum et al., 2019) by introducing a new formulation that leads to a more general, and as we will show, more effective estimation method."}
{"paper_id": 1, "abstract": "In the ever-evolving realm of computer vision, we stand on the precipice of a remarkable transformation, one that has been fueled by the deep features gleaned from vast, supervised datasets. Yet, while static image analysis has surged forward with astonishing advancements, the domain of video understanding has lagged behind, its progress modest at best. Despite the emergence of new datasets and spatiotemporal models, the tried-and-true frame-by-frame classification methods often continue to hold their ground, proving surprisingly resilient.  We propose that the current landscape of video datasets is marred by implicit biases relating to scene and object structures\u2014biases that can overshadow even the most intricate variations in temporal dynamics. To address this, we introduce CATER, a groundbreaking video dataset meticulously crafted to allow for complete observation and control over object and scene biases. This dataset is not merely a collection of videos; it is a challenge that demands true spatiotemporal understanding to unravel its complexities.  Rendered synthetically using a diverse library of standard 3D objects, CATER tests the limits of our ability to recognize and reason about the movements of these objects over time, requiring a depth of long-term reasoning that has previously been uncharted. Beyond its role as a formidable challenge, CATER also serves as a treasure trove of diagnostic tools, enabling us to dissect and analyze the latest advancements in spatiotemporal video architectures with unprecedented clarity. Through our exploration of CATER, we aim to shed light on the intricacies of state-of-the-art deep video architectures, paving the way for future breakthroughs in this captivating field.", "introduction": "While deep features have revolutionized static image analysis, video descriptors have struggled to outperform classic hand-crafted descriptors (Wang & Schmid, 2013).Though recent works have shown improvements by merging image and video models by inflating 2D models to 3D (Carreira & Zisserman, 2017;Feichtenhofer et al., 2016), simpler 2D models (Wang et al., 2016b) still routinely appear among top performers in video benchmarks such as the Kinetics Challenge at CVPR'17.This raises the natural question: are videos trivially understandable by simply averaging the predictions over a sampled set of frames?At some level, the answer must be no.Reasoning about high-level cognitive concepts such as intentions, goals, and causal relations requires reasoning over long-term temporal structure and order (Shoham, 1987;Bobick, 1997).Consider, for example, the movie clip in Fig. 1 (a), where an actor leaves the table, grabs a firearm from another room, and returns.Even though no gun is visible in the final frames, an observer can easily infer that the actor is surreptitiously carrying the gun.Needless to say, any single frame from the video seems incapable of supporting that inference, and one needs to reason over space and time in order to reach that conclusion.As a simpler instance of the problem, consider the cup-and-balls magic routine 1 , or the gamblingbased shell game 2 , as shown in Fig. 1 (b).In these games, an operator puts a target object (ball) under one of multiple container objects (cups), and moves them about, possibly revealing the target at various times and recursively containing cups within other cups.The task at the end is to tell which of the cups is covering the ball.Even in its simplest instantiation, one can expect any human or computer system that solves this task to require the ability to model state of the world over long temporal horizons, reason about occlusion, understand the spatiotemporal implications of containment, etc.An important aspect of both our motivating examples is the adversarial nature of the task, Figure 1: Real world video understanding.Consider this iconic movie scene from The Godfather in (a), where the protagonist leaves the table, goes to the bathroom to extract a hidden firearm, and returns to the table presumably with the intentions of shooting a person.While the gun itself is visible in only a few frames of the whole clip, it is trivial for us to realize that the protagonist has it in the last frame.An even simpler instantiation of such a reasoning task could be the cup-and-ball shell game in (b), where the task is to determine which of the cups contain the ball at the end of the trick.Can we design similarly hard tasks for computers?where the operator in control is trying to make the observer fail.Needless to say, a frame by frame prediction model would be incapable of solving such tasks.Given these motivating examples, why don't spatiotemporal models dramatically outperform their static counterparts for video understanding?We posit that this is due to limitations of existing video benchmarks.Even though video datasets have evolved from the small regime with tens of labels (Soomro et al., 2012;Kuehne et al., 2011;Schuldt et al., 2004) to large with hundreds of labels (Sigurdsson et al., 2016;Kay et al., 2017), tasks have remained highly correlated to the scene and object context.For example, it is trivial to recognize a swimming action given a swimming pool in the background (He et al., 2016b).This is further reinforced by the fact that state of the art pose-based action recognition models (Yan et al., 2018) are outperformed by simpler frame-level models (Wang et al., 2016b) on the Kinetics (Kay et al., 2017) benchmark, with a difference of nearly 45% in accuracy!Sigurdsson et al. also found similar results for their Charades (Sigurdsson et al., 2016) benchmark, where adding ground truth object information gave the largest boosts to action recognition performance (Sigurdsson et al., 2017).In this work, we take an alternate approach to developing a video understanding dataset.Inspired by the recent CLEVR dataset (Johnson et al., 2017) (that explores spatial reasoning in tabletop scenes) and inspired by the adversarial parlor games above (that require temporal reasoning), we introduce CATER, a diagnostic dataset for Compositional Actions and TEmporal Reasoning in dynamic tabletop scenes.We define three tasks on the dataset, each with an increasingly higher level of complexity, but set up as classification problems in order to be comparable to existing benchmarks for easy transfer of existing models and approaches.Specifically, we consider primitive action recognition, compositional action recognition, and adversarial target tracking under occlusion and containment.However, note that this does not limit the usability of our dataset to these tasks, and we provide full metadata with the rendered videos that can be used for more complex, structured prediction tasks like detection, tracking, forecasting, and so on.Our dataset does not model an operator (or hand) moving the tabletop objects, though this could be simulated as well in future variants, as in (Rogez et al., 2015).Being synthetic, CATER can easily be scaled up in size and complexity.It also allows for detailed model diagnostics by controlling various dataset generation parameters.We use CATER to benchmark state-of-the-art video understanding models (Wang et al., 2018;2016b;Hochreiter & Schmidhuber, 1997), and show even the best models struggle on our dataset.We also uncover some insights into the behavior of these models by changing parameters such as the temporal duration of an occlusion, the degree of camera motion, etc., which are difficult to both tune and label in real-world video data."}
{"paper_id": 2, "abstract": "In the realm of automatic differentiation, a singular focus reigns supreme: the calculation of the average mini-batch gradient. Yet, lurking in the shadows are other quantities\u2014such as the variance of mini-batch gradients and various approximations to the Hessian\u2014that, in theory, could be computed with remarkable efficiency, all while simultaneously deriving the gradient itself. These quantities hold immense value for both researchers and practitioners alike, yet the current landscape of deep learning software falls short, offering no support for their automatic computation.   The burden of manual implementation looms large, often resulting in cumbersome, inefficient code that seldom sees the light of day in shared repositories. This stagnation stifles the advancement of deep learning, constraining research to the narrow corridors of gradient descent and its myriad variants. Furthermore, it complicates replication studies and hinders comparisons between emerging methodologies that rely on these elusive quantities, rendering such efforts nearly impossible.  To illuminate a path forward, we present BackPACK\u2014a robust framework constructed atop PyTorch, designed to extend the backpropagation algorithm. BackPACK deftly extracts additional insights from both first- and second-order derivatives, empowering researchers to glean the information they seek without the burdensome overhead of manual coding. We demonstrate its prowess through benchmark reports that showcase the computation of these additional quantities within deep neural networks, alongside a practical application that evaluates several recent curvature approximations for optimization. In this way, BackPACK not only broadens the horizons of what is possible in deep learning but also rekindles the spirit of collaboration and progress within the field.", "introduction": "The success of deep learning and the applications it fuels can be traced to the popularization of automatic differentiation frameworks.Packages like TENSORFLOW (Abadi et al., 2016), CHAINER (Tokui et al., 2015), MXNET (Chen et al., 2015), and PYTORCH (Paszke et al., 2019) provide efficient implementations of parallel, GPU-based gradient computations to a wide range of users, with elegant syntactic sugar.However, this specialization also has its shortcomings: it assumes the user only wants to compute gradients or, more precisely, the average of gradients across a mini-batch of examples.Other quantities can also be computed with automatic differentiation at a comparable cost or minimal overhead to the gradient backpropagation pass; for example, approximate second-order information or the variance of gradients within the batch.These quantities are valuable to understand the geometry of deep neural networks, for the identification of free parameters, and to push the development of more efficient optimization algorithms.But researchers who want to investigate their use face a chickenand-egg problem: automatic differentiation tools required to go beyond standard gradient methods are not available, but there is no incentive for their implementation in existing deep-learning software as long as no large portion of the users need it.Second-order methods for deep learning have been continuously investigated for decades (e.g., Becker & Le Cun, 1989;Amari, 1998;Bordes et al., 2009;Martens & Grosse, 2015).But still, the standard optimizers used in deep learning remain some variant of stochastic gradient descent (SGD); more complex methods have not found wide-spread, practical use.This is in stark contrast to domains like convex optimization and generalized linear models, where second-order methods are the default.There may of course be good scientific reasons for this difference; maybe second-order methods do not work well in the (non-convex, stochastic) setting of deep learning.And the computational cost associated with the high dimensionality of deep models may offset their benefits.Whether these are the case remains somewhat unclear though, because a much more direct road-block is that these methods are so complex to implement that few practitioners ever try them out.Recent approximate second-order methods such as KFAC (Martens & Grosse, 2015) show promising results, even on hard deep learning problems (Tsuji et al., 2019).Their approach, based on the earlier work of Schraudolph (2002), uses the structure of the network to compute approximate secondorder information in a way that is similar to gradient backpropagation.This work sparked a new line of research to improve the second-order approximation (Grosse & Martens, 2016;Botev et al., 2017;Martens et al., 2018;George et al., 2018).However, all of these methods require low-level applications of automatic differentiation to compute quantities other than the averaged gradient.It is a daunting task to implement them from scratch.Unless users spend significant time familiarizing themselves with the internals of their software tools, the resulting implementation is often inefficient, which also puts the original usability advantage of those packages into question.Even motivated researchers trying to develop new methods, who need not be expert software developers, face this problem.They often end up with methods that cannot compete in runtime, not necessarily because the method is inherently bad, but because the implementation is not efficient.New methods are also frequently not compared to their predecessors and competitors because they are so hard to reproduce.Authors do not want to represent the competition in an unfair light caused by a bad implementation.Another example is offered by a recent string of research to adapt to the stochasticity induced by mini-batch sampling.An empirical estimate of the (marginal) variance of the gradients within the batch has been found to be theoretically and practically useful for adapting hyperparameters like learning rates (Mahsereci & Hennig, 2017) and batch sizes (Balles et al., 2017), or regularize firstorder optimization (Le Roux et al., 2007;Balles & Hennig, 2018;Katharopoulos & Fleuret, 2018).To get such a variance estimate, one simply has to square, then sum, the individual gradients after the backpropagation, but before they are aggregated to form the average gradient.Doing so should have negligible cost in principle, but is programmatically challenging in the standard packages.Members of the community have repeatedly asked for such featuresfoot_0 but the established automatic differentiation frameworks have yet to address such requests, as their focus has been-rightly-on improving their technical backbone.Features like those outlined above are not generally defined for arbitrary functions, but rather emerge from the specific structure of machine learning applications.General automatic differentiation frameworks can not be expected to serve such specialist needs.This does not mean, however, that it is impossible to efficiently realize such features within these frameworks: In essence, backpropagation is a technique to compute multiplications with Jacobians.Methods to extract second-order information (Mizutani & Dreyfus, 2008) or individual gradients from a mini-batch (Goodfellow, 2015) have been known to a small group of specialists; they are just rarely discussed or implemented."}
{"paper_id": 3, "abstract": "In the realm of deep learning, where vast datasets loom like towering mountains, the challenge of training large neural networks can feel insurmountable. Recently, a wave of innovation has surged forth, focusing on the power of large batch stochastic optimization methods to conquer this daunting task. At the forefront of this movement stands LARS, a remarkable algorithm that employs layerwise adaptive learning rates, enabling the swift training of ResNet on ImageNet in mere minutes. Yet, like a hero facing an unexpected foe, LARS falters when confronted with attention models such as BERT, revealing that its triumphs are not universally applicable across all challenges.  In this paper, we embark on a quest to explore a principled layerwise adaptation strategy designed to hasten the training of deep neural networks when wielding large mini-batches. From this exploration, we forge a new optimization technique known as LAMB\u2014Layerwise Adaptive Large Batch optimization. We delve into the convergence properties of both LAMB and LARS, unveiling their paths toward stationary points in the complex landscape of nonconvex settings.  Our empirical findings shine a light on LAMB's exceptional prowess across a variety of tasks, including the formidable training of BERT and ResNet-50, all while requiring minimal hyperparameter tuning. Notably, in the training of BERT, our optimizer empowers the use of colossal batch sizes\u2014up to 32,868\u2014without sacrificing performance. By pushing the boundaries of batch size to the limits of a TPUv3 Pod's memory, we dramatically slash BERT's training time from a grueling three days down to a mere 76 minutes. Thus, we unveil not just a tool, but a powerful ally in the ongoing battle against the computational challenges of deep learning.", "introduction": "With the advent of large scale datasets, training large deep neural networks, even using computationally efficient optimization methods like Stochastic gradient descent (SGD), has become particularly challenging.For instance, training state-of-the-art deep learning models like BERT and ResNet-50 takes 3 days on 16 TPUv3 chips and 29 hours on 8 Tesla P100 gpus respectively (Devlin et al., 2018;He et al., 2016).Thus, there is a growing interest to develop optimization solutions to tackle this critical issue.The goal of this paper is to investigate and develop optimization techniques to accelerate training large deep neural networks, mostly focusing on approaches based on variants of SGD.Methods based on SGD iteratively update the parameters of the model by moving them in a scaled (negative) direction of the gradient calculated on a minibatch.However, SGD's scalability is limited by its inherent sequential nature.Owing to this limitation, traditional approaches to improve SGD training time in the context of deep learning largely resort to distributed asynchronous setup (Dean et al., 2012;Recht et al., 2011).However, the implicit staleness introduced due to the asynchrony limits the parallelization of the approach, often leading to degraded performance.The feasibility of computing gradient on large minibatches in parallel due to recent hardware advances has seen the resurgence of simply using synchronous SGD with large minibatches as an alternative to asynchronous SGD.However, na\u00efvely increasing the batch size typically results in degradation of generalization performance and reduces computational benefits (Goyal et al., 2017).Synchronous SGD on large minibatches benefits from reduced variance of the stochastic gradients used in SGD.This allows one to use much larger learning rates in SGD, typically of the order square root of the minibatch size.Surprisingly, recent works have demonstrated that up to certain minibatch sizes, linear scaling of the learning rate with minibatch size can be used to further speed up the 2018; Smith et al., 2017;Martens & Grosse, 2015;Devarakonda et al., 2017;Mikami et al., 2018;Osawa et al., 2018;You et al., 2019;Yamazaki et al., 2019).To the best of our knowledge, the fastest training result for RESNET-50 on ImageNet is due to Ying et al. (2018), who achieve 76+% top-1 accuracy.By using the LARS optimizer and scaling the batch size to 32K on a TPUv3 Pod, Ying et al. (2018) was able to train RESNET-50 on ImageNet in 2.2 minutes.However, it was empirically observed that none of these performance gains hold in other tasks such as BERT training (see Section 4)."}
{"paper_id": 4, "abstract": "In the realm of artificial intelligence, much of the focus on lifelong learning has revolved around images and games, leaving the intricacies of language largely unexplored. Enter LAMOL\u2014a groundbreaking approach to lifelong language learning (LLL) that harnesses the power of language modeling in a way that is both simple and remarkably effective.   LAMOL operates on a clever premise: by replaying pseudo-samples from previous tasks, it sidesteps the need for additional memory or increased model capacity. This innovative language model not only tackles new challenges but also generates training samples from its past, creating a seamless loop of learning. When faced with a new task, LAMOL conjures up these pseudo-samples, integrating them with fresh data to enrich its training experience.  The results are nothing short of remarkable. LAMOL adeptly wards off the dreaded specter of catastrophic forgetting, exhibiting none of the intransigence often seen in traditional models. With the ability to tackle five distinctly different language tasks in succession\u2014all within a single model\u2014LAMOL sets a new standard. It significantly outperforms previous methodologies and comes tantalizingly close to the multitasking benchmark, falling only 2-3% short of what is typically regarded as the upper limit of LLL performance.  For those eager to delve deeper into this innovative approach, the source code is readily available at https://github.com/jojotenya/LAMOL. Join us on this journey of discovery as we redefine the boundaries of language learning in the age of AI.", "introduction": "The current dominant paradigm for machine learning is to run an algorithm on a given dataset to produce a trained model specifically for a particular purpose; this is isolated learning (Chen & Liu, 2016, p. 150).In isolated learning, the model is unable to retain and accumulate the knowledge it has learned before.When a stream of tasks are joined to be trained sequentially, isolated learning faces catastrophic forgetting (McCloskey & Cohen, 1989) due to a non-stationary data distribution that biases the model (left figure of Figure 1).In contrast, lifelong learning is designed to address a stream of tasks by accumulating interconnected knowledge between learned tasks and retaining the performance of those tasks.A human easily achieves lifelong learning, but this is nontrivial for a machine; thus lifelong learning is a vital step toward artificial general intelligence.In this paper, we focus on lifelong language learning, where a machine achieves lifelong learning on a stream of natural language processing (NLP) tasks.To the best of our knowledge, lifelong language learning has been studied in only a few instances; for sentiment analysis (Chen et al., 2015b;Xia et al., 2017), conversational agents (Lee, 2017), word representation learning (Xu et al., 2018), sentence representation learning (Liu et al., 2019), text classification, and question answering (d'Autume et al., 2019).However, in all previous work, the tasks in the stream are essentially the same task but in different domains.To achieve lifelong language learning on fundamentally different tasks, we propose LAMOL -LAnguage MOdeling for Lifelong language learning.It has been shown that many NLP tasks can be considered question answering (QA) (Bryan McCann & Socher, 2018).Therefore, we address multiple NLP tasks with a single model by training a language model (LM) that generates an answer based on the context and the question.Treating QA as language modeling is beneficial because the LM can be pre-trained on a large number of sentences without any labeling (Radford et al., 2019); however, this does not directly solve the problem of LLL.If we train an LM on a stream of tasks, catastrophic forgetting still occurs.However, as an LM is intrinsically a text generator, we can use it to answer questions while generating pseudo-samples of Figure 1: Left: After learning Task 2, the learner has already forgetten how to solve Task 1.This is \"catastrophic forgetting\".Middle: The basic idea of the data-based LLL approach.A generator is learned to generate examples it has seen before.Using the generator, the learner also learns from examples from the previous task to prevent it from forgetting.Right: A language model that simultaneously takes on the roles of learner and generator.the previous task to be replayed later.LAMOL is inspired by the data-based approach for LLL in which a generator learns to generate samples in previous tasks (middle of Figure 1) (Hanul Shin & Kim, 2017;Kemker & Kanan, 2017).In contrast to previous approaches, LAMOL needs no extra generator (right of Figure 1).LAMOL is also similar to multitask training, but the model itself generates data from previous tasks instead of using real data.Our main contributions in this paper are:\u2022 We present LAMOL, a simple yet effective method for LLL.Our method has the advantages of no requirements in terms of extra memory or model capacity.We also do not need to know how many tasks to train in advance and can always train on additional tasks when needed.\u2022 Experimental results show that our methods outperform baselines and other state-of-the-art methods by a considerable margin and approaches the multitasking upper bound within 2-3%.\u2022 Furthermore, we propose adding task-specific tokens during pseudo-sample generation to evenly split the generated samples among all previous tasks.This extension stabilizes LLL and is particularly useful when training on a large number of tasks.\u2022 We analyze how different amounts of pseudo-samples affect the final performance of LAMOL, considering results both with and without the task-specific tokens.\u2022 We open-source our code to facilitate further LLL research."}
{"paper_id": 5, "abstract": "In the realm of Multi-Agent Reinforcement Learning (MARL), the prevailing focus has been on fostering cooperation among agents to achieve shared objectives. Yet, the tapestry of reality often presents a different narrative\u2014agents driven by self-interest, akin to employees vying for recognition within a company or clubs competing for glory in a league. Herein lies the conundrum for the leader, be it a manager or a league commissioner: effective coordination demands the judicious allocation of bonuses to incentivize followers, a phenomenon we term \"expensive coordination.\"   The challenges of this intricate dance are twofold. First, the leader must navigate the murky waters of long-term consequences, predicting the behaviors of followers while distributing rewards. Second, the complex web of interactions among followers complicates convergence in the training process, particularly as the leader\u2019s strategy evolves over time.   In this work, we embark on a quest to unravel this dilemma through an innovative event-based deep reinforcement learning approach. Our contributions are threefold:   1. We frame the leader's decision-making as a semi-Markov Decision Process, introducing a cutting-edge multi-agent event-based policy gradient to cultivate a long-term strategy for the leader.     2. We leverage a leader-follower consistency scheme to craft a follower-aware module and a follower-specific attention mechanism, enabling precise predictions of follower behavior and tailored responses.  3. We unveil an action abstraction-based policy gradient algorithm, streamlining the decision space for followers and hastening their training journey.  Through rigorous experimentation across domains such as resource collection, navigation, and the predator-prey game, we demonstrate that our approach not only meets but vastly exceeds the performance of existing state-of-the-art methods.", "introduction": "Deep Multi-Agent Reinforcement Learning (MARL) has been widely used in coordinating cooperative agents to jointly complete certain tasks where the agent is assumed to be selfless (fully cooperative), i.e., the agent is willing to sacrifice itself to maximize the team reward.However, in many cases of the real world, the agents are self-interested, such as taxi drivers in a taxi company (fleets) and clubs in a league.For instance, in the example of taxi fleets (Miao et al., 2016), drivers may prefer to stay in the area with high customer demand to gain more reward.It is unfair and not efficient to compel the taxi driver to selflessly contribute to the company, e.g., to stay in the low customer demand area.Forcing the drivers to selflessly contribute may increase the income for the company in a short-term but it will finally causes the low efficient and unsustainable of that company in the long run because the unsatisfied drivers may be demotivated and even leave the company.Another important example is that the government wants some companies to invest on the poverty area to achieve the fairness of the society, which may inevitably reduce the profits of companies.Similar to previous example, the companies may leave when the government forces them to invest.A better way to achieve coordination among followers and achieve the leader's goals is that the manager of the company or the government needs to provide bonuses to followers, like the taxi company pays extra bonuses for serving the customers in rural areas and the government provides subsidies for investing in the poverty areas, which we term as expensive coordination.In this paper, we solve the large-scale sequential expensive coordination problem with a novel RL training scheme.There are several lines of works related to the expensive coordination problem, including mechanism design (Nisan & Ronen, 2001) and the principal-agent model (Laffont & Martimort, 2009).However, these works focus more on static decisions (each agent only makes a single decision).To consider sequential decisions, the leader-follower MDP game (Sabbadin & Viet, 2013;2016) and the RL-based mechanism design (Tang, 2017;Shen et al., 2017) are introduced but most of their works only focus on matrix games or small-scale Markov games, which cannot be applied to the case with the large-scale action or state space.The most related work is M 3 RL (Shu & Tian, 2019) where the leader assigns goals and bonuses by using a simple attention mechanism (summing/averaging the features together) and mind (behaviors) tracking to predict the followers' behaviors and makes response to the followers' behaviors.But they only consider the rule-based followers, i.e., followers with fixed preference, and ignore the followers' behaviors responding to the leader's policy, which significantly simplifies the problem and leads the unreasonability of the model.In the expensive coordination problem, there are two critical issues which should be considered: 1) the leader's long-term decision process where the leader has to consider both the long-term effect of itself and long-term behaviors of the followers when determining his action to incentivise the coordination among followers, which is not considered in (Sabbadin & Viet, 2013;Mguni et al., 2019); and 2) the complex interactions between the leader and followers where the followers will adapt their policies to maximize their own utility given the leader's policy, which makes the training process unstable and hard, if not unable, to converge in large-scale environment, especially when the leader changes his actions frequently, which is ignored by (Tharakunnel & Bhattacharyya, 2007;Shu & Tian, 2019).In this work, we address these two issues in the expensive coordination problem through an abstraction-based deep RL approach.Our main contributions are threefold.(1) We model the leader's decision-making process as a semi-Markov Decision Process (semi-MDP) and propose a novel event-based policy gradient to learn the leader's policy considering the long-term effect (leader takes actions at important points rather than at each step to avoid myopic decisions.)(Section 4.1).(2) A well-performing leader's policy is also highly dependent on how well the leader knows the followers.To predict the followers' behaviors precisely, we show the leader-follower consistency scheme.Based on the scheme, the follower-aware module, the follower-specific attention module, and the sequential decision module are proposed to capture these followers' behaviors and make accurate response to their behaviors (Section 4.2).(3) To accelerate the training process, we propose an action abstraction-based policy gradient algorithm for the followers.This approach is able to reduce followers' decision space and thus simplifies the interaction between the leader and followers as well as accelerates the training process of followers (Section 4.3).Experiments in resource collections, navigation and predatorprey show that our method outperforms the state-of-the-art methods dramatically."}
{"paper_id": 6, "abstract": "In a world woven together by intricate webs of connections, graph-structured data stands as a silent titan, underpinning countless domains. While the brilliance of deep neural networks has been heralded far and wide, their potential in the realm of graph data remains a treasure yet to be unearthed. Enter our groundbreaking network architecture, a creation forged in the crucible of innovation, designed to harness the profound complexities of graph structures.   At the heart of our approach lies the concept of discrete graph curvature\u2014a powerful metric that reveals the hidden relationships nestled within the neighborhoods of interconnected nodes. Imagine the curvature of an edge (x, y) as a map that charts the journey from the friends of x to the companions of y, comparing it to the direct path between them. This perspective transcends the limitations of traditional features, which often cling to mere node attributes or superficial topological details like degree.  Our curvature graph convolution network emerges not merely as a tool, but as a beacon of advancement, eclipsing state-of-the-art methodologies across a spectrum of synthetic and real-world graphs. Its true prowess shines brightest in the larger, denser datasets, where it deftly navigates the complexities of structure and relationship, proving that the depths of graph data are ripe for exploration and mastery.", "introduction": "Despite the huge success of deep neural networks, it remains challenging to fully exploit their power on graph-structured data, i.e., data whose underlying structure is captured by graphs, e.g., social networks, telecommunication networks, biological networks and brain connectomes.Inspired by the power of convolutional neural networks on image data, convolutional networks have been proposed for graph-structured data.Existing works can be roughly divided into two categories, depending on whether convolution is applied to the spectral or spatial domain.Spectral approaches (Bruna et al., 2013;Defferrard et al., 2016;Henaff et al., 2015;Veli\u010dkovi\u0107 et al., 2017) apply convolution to eigen-decomposed graph Laplacians and are generally efficient in both computation and memory.However, the learned convolution filters are graph-specific and cannot generalize to different graphs.Spatial approaches execute \"convolution\" directly on the graph and operate on the neighborhood as defined by the graph topology.A spatial method iteratively updates the representation of each graph node by aggregating representations from its neighbors, i.e., adjacent nodes (Xu et al., 2018).Nonlinear transformations are applied to the representation passed from one node to another, called a message.These transformations have the same input/output dimension, i.e., the dimension of the node representation.They can be shared and learned across different nodes and even different graphs.For spatial approaches, it is important to incorporate local structural information of the graph.Node degree has been used to re-parametrize the non-linear transformation of messages (Monti et al., 2017) or as an additional node feature (Hamilton et al., 2017).However, node degree is fairly limited; there can be different graph topologies with the same degree distribution.The limitation is illustrated in x y(a) Tree: \u03baxy = -0.5x y (b) Grid: \u03baxy = 0x y (c) Clique: \u03baxy = 0.625Figure 1: Illustration of structural information.In all three graphs, the degrees of x and y are the same.However, the Ricci curvature of the edge (x, y) is negative, zero, and positive, respectively.All edges have weight 1. \u03b1 = 0.5 so each node keeps 50% of the probability mass to itself.Figure 1.Nodes x and y have the same degree in three significantly different graphs: a tree, a grid and a clique.To effectively make use of graph structural knowledge, one would need a feature with more discriminative power; one that can distinguish these three scenarios in Figure 1.In this paper, we propose a novel graph neural network that exploits advanced structural information.Notice that node degree only describes the number of neighbors of each node, but does not say how these neighbors are connected among themselves.We seek to use structural information characterizing how neighborhoods of a pair of nodes relate to each other.In Figure 1, the neighborhoods of x and y are well separated in different branches of a tree.In a grid graph, the two neighborhoods are within a parallel shift of each other.In a clique, they completely overlap.To quantify such pairwise structural information, we draw inspiration from recent study of graph curvature (Ollivier, 2009;Lin et al., 2011;Weber et al., 2016).Similar to curvature in the continuous domain, e.g., the Ricci curvature of a Riemannian manifold, the discrete graph curvature measures how the geometry of a pair of neighborhoods deviates from a \"flat\" case, namely, the case of a grid graph.There are several definitions of discrete curvature for graphs.The most notable one is Ollivier's Ricci curvature (Ollivier, 2009).The edges of an infinite grid graph have zero curvature.The curvature of an edge in a tree is negative and is positive in a complete graph.Intuitively, the graph curvature measures how well two neighborhoods are connected and/or overlap with each other.Such information is related to how information propagates in the neighborhood, and should be leveraged by a graph convolutional network.We propose Curvature Graph Network (CurvGN), the first graph convolutional network built on advanced graph curvature information.The use of curvature information allows CurvGN to adapt to different local structural scenarios and filter messages passed between nodes differently.Notice that curvature captures how easily information flows between two nodes.Within a well-connected community, neighborhoods of adjacent nodes have large overlap and many shortcuts.The corresponding curvature is positive and passing information between the nodes is easy.For an edge serving as a bridge of two clusters, its curvature is negative and it is hard for information to pass.The key to our success is that we choose to be agnostic on whether curvature should be used to block or accelerate the messages in graph convolution.We exploit the curvature information in a data-driven manner and learn how to use it to reweigh different channels of the messages.To further investigate how curvature information affects graph convolution, we carried out extensive experiments with various synthetic graphs and real-world graphs.Our synthetic graphs are generated according to various well-established graph models, e.g., stochastic block model (Decelle et al., 2011), Watts-Strogatz network (Watts & Strogatz, 1998), Newman-Watts network (Newman & Watts, 1999) and Kleinberg's navigable small world graph (Kleinberg, 2000).On these data, CurvGN outperforms vanilla graph network and networks using node degree information and self attention, demonstrating the benefit of curvature information in graph convolution.Such benefit is more apparent as the graph size increases.We hypothesize that graph convolution alone can adapt to any graph topology, at the cost of more convolutional layers and more training data.This is corroborated by our experiments on real-world graphs.CurvGN outperforms state-of-the-art graph neural networks, especially on large and dense graphs, which tend to have a large variation of local structures.The success of CurvGN demonstrates how topological intuitions inspire better practical solutions.It encourages us to continue the endeavor in applying theoretical insights in successful deployments of deep learning."}
{"paper_id": 7, "abstract": "In the realm of deep neural networks (DNNs), a troubling truth lingers: conventional training methods often leave these intricate constructs poorly calibrated, casting shadows over their confidence in predictions. To illuminate a path forward, we unveil a groundbreaking approach known as Distance-Based Learning from Errors (DBLE). This method reimagines confidence estimation, anchoring it in the very distances that dwell within the representation space.  At the heart of DBLE lies the adaptation of prototypical learning, a technique that shapes classification models into a realm where the distance between a test sample and its true class center serves as a beacon for calibration. However, a conundrum arises during inference\u2014the absence of ground truth labels leaves these distances obscured. To navigate this challenge, we introduce a dual-training strategy, wherein a confidence model is trained alongside the classification model. This innovative design allows us to infer distances for each test sample, transforming the way we learn from misclassified training instances into a powerful tool for effective learning.  Our experiments across diverse datasets and DNN architectures reveal that DBLE not only surpasses traditional single-model confidence calibration methods but also rivals the performance of computationally-intensive ensemble approaches\u2014yet it does so with a fraction of the computational burden and fewer parameters. In this way, DBLE emerges as a beacon of hope, guiding us toward a future where the confidence of neural networks is as robust as their predictions.", "introduction": "Deep neural networks (DNNs) are being deployed in many important decision-making scenarios (Goodfellow et al., 2016).Making wrong decisions could be very costly in most of them (Brundage et al., 2018) -it could cost human lives in medical diagnosis and autonomous transportation, and it could cost significant business losses in loan categorization and sales forecasting.To prevent these from happening, it is strongly desired for a DNN to output confidence estimations on its decisions.In almost all of the aforementioned scenarios, detrimental consequences could be avoided by refraining from making decisions or consulting human experts, in the cases of decisions with insufficient confidence.In addition, by tracking the confidence in decisions, dataset shifts can be detected and developers can build insights towards improving the model performance.However, confidence estimation (also referred as 'confidence calibration') is still a challenging problem for DNNs.For a 'well-calibrated' model, predictions with higher confidence should be more likely accurate.However, as studied in (Nguyen et al., 2014;Guo et al., 2017), for DNNs with conventional (also referred as 'vanilla') training to minimize the softmax cross-entropy loss, the outputs do not contain sufficient information for well-calibrated confidence estimation.Posterior probability estimates (e.g. the softmax outputs) can be interpreted as confidence estimation, but it calibrates the decision quality poorly (Gal & Ghahramani, 2016) -the confidence tend to be large even when the classification is inaccurate.Therefore, it would be desirable if the training approach is redesigned to make its decision making more transparent, thus providing more information for accurate confidence calibration.In this paper, we propose a novel training method, Distance-based Learning from Errors (DBLE), towards better-calibrated DNNs.DBLE learns a distance-based representation space through classification and exploits distances in the space to yield well-calibrated classification.Our motivation is that a test sample's location in the representation space and its distance to training samples should contain important information about the model's decision-making process, which is useful for guiding confidence estimation.However, in vanilla training, since both training and inference are not based on distances in the representation space, they are not optimized to fulfill this goal.Therefore, in DBLE, we propose to adapt prototypical learning for training and inference, to learn a distance-based representation space through classification.In this space, a test sample's distance to its ground-truth class center can calibrate the model's performance.However, this distance cannot be calculated at inference directly, since the ground truth label is unknown.To this end, we propose to train a separate confidence model in DBLE, jointly with the classification model, to estimate this distance at inference.To train the confidence model, we utilize the mis-classified training samples during the training of the classification model.We demonstrate that on multiple DNN models and datasets, DBLE achieves superior confidence calibration without increasing the computational cost as ensemble methods."}
{"paper_id": 8, "abstract": "In the realm of machine learning and computer vision, determinantal point processes (DPPs) stand as a beacon of diversity, wielding the power to enhance a multitude of tasks. Yet, within the deep learning framework, the optimization of DPPs often succumbs to the treacherous waters of approximation, a process fraught with complications that can undermine the very diversity we seek to promote. To date, the quest for a deep learning paradigm that can directly optimize DPPs has remained elusive, primarily due to the daunting challenge of matrix inversion, a task notorious for its computational instability. This limitation has stymied the broader application of DPPs, particularly in scenarios where they are crucial for measuring feature diversity.  In this paper, we unveil a straightforward yet potent algorithm designed to tackle this conundrum head-on. By optimizing the DPP term directly through an L-ensemble expressed in the spectral domain over the Gram matrix, we transcend the constraints of parametric kernels, offering a more adaptable solution. Moreover, our algorithm cleverly incorporates geometric constraints, allowing it to generate valid sub-gradients for the DPP term, even when faced with the unforgiving reality of a non-invertible Gram matrix. This innovative approach ensures that gradients can be derived, paving the way for seamless integration with a variety of deep learning tasks.  Our experimental results illuminate the efficacy of this algorithm, revealing its potential to deliver compelling performance across real-world learning challenges. With this advancement, we stand on the brink of a new era in the application of DPPs, one where diversity is no longer a distant goal but an achievable reality.", "introduction": "Diversity is desired in multiple machine learning and computer vision tasks (e.g., image hashing (Chen et al., 2017;Carreira-Perpin\u00e1n & Raziperchikolaei, 2016), descriptor learning (Zhang et al., 2017), metric learning (Mishchuk et al., 2017) and video summarization (Sharghi et al., 2018;Liu et al., 2017)), in which sub-sampled points or learned features need to spread out through a specific bounded space.Originated from quantum physics, determinantal point processes (DPP) have shown its power in delivering such properties (Kulesza et al., 2012;Kulesza & Taskar, 2011b).Compared with other diversity-oriented techniques (e.g., entropy (Zadeh et al., 2017) and orthogonality (Zhang et al., 2017)), DPP shows its superiority as it incorporates only one single metric and delivers genuine diversity on any bounded space (Kulesza et al., 2012;Affandi et al., 2013;Gillenwater et al., 2012).Therefore, DPP has been utilized in a large body of diversity-oriented tasks.In general, sample points from a DPP tend to distribute diversely within a bounded space A (Kulesza et al., 2012).Given a positive semi-definite kernel function \u03ba : A \u00d7 A \u2192 R, the probability of a discrete point set X \u2282 A under a DPP with kernel function \u03ba can be characterized as:(1) where L is a |X | \u00d7 |X | matrix with entry L ij = \u03ba(x i , x j ) and x i , x j \u2208 X .L is called L-ensemble.Note that A is a continuous space, whereas X is finite.In the Hilbert space associated with \u03ba, larger determinant implies larger spanned volume, thus the mapped points tend not to be similar or linearly dependent.DPP can be viewed from two perspectives: sampling and learning.A comprehensive introduction to mathematical fundamentals of DPP for sampling from a discrete space can be found in Kulesza et al. (2012).Based on this, a line of works has been proposed (Kulesza & Taskar, 2011a;Kang, 2013;Hennig & Garnett, 2016).In this paper, we concentrate on learning DPPs.In learning of DPP, the term det(L) is typically treated as a singleton diversity measurement and is extended to learning paradigms on continuous space (Chao et al., 2015;Kulesza & Taskar, 2010;Affandi et al., 2014).There are generally two lines of strategies to learn DPPs:Approximation.This type of methods is to convert DPP into a simpler format which can ease and stabilize the computation.low-rank approximation proves powerful in easing the computational burden (Gartrell et al., 2017), in which the gram matrix is factorized as L = BB where B \u2208 n\u00d7m with m n.This decomposition can also reduce the complexity which is originally a cubic time of |L|.Kulesza & Taskar (2011b) explicitly expressed the kernel with \u03ba(x, y) = \u03c3 1 \u03c3 2 \u03b4(x) \u03b4(y), where \u03c3 measures the intrinsic quality of the feature and \u03b4(\u2022) is function mapping input x to a feature space.In this sense, the pairwise similarity is calculated in Euclidean feature space with cosine distance.Elfeki et al. (2019) suggest approximating a given distribution by approximating the eigenvalues of the corresponding DPP.As such, the computation can be eased and become stable.Following this, DPP is also applied on some visual tasks, such as video summarization (Sharghi et al., 2018), ranking (Liu et al., 2017) and image classification (Xie et al., 2017).It can be noted that the approximation is not straightforward for DPP, thus cannot fully deliver the diversity property (e.g.resulting in rank-deficiency).Direct optimization.While the aforementioned methods optimize DPP with specific approximation, a series of efforts also seek to optimize the DPP term directly (Gillenwater et al., 2014;Mariet & Sra, 2015;Bardenet & Titsias, 2015).In this setting, the whole gram matrix L corresponding to the pairwise similarity among features is updated directly, which allows accommodating more flexible feature mapping functions rather than an approximation.Gillenwater et al. (2014) proposed an Expectation-Maximization algorithm to update marginal kernel DPP K = L(L + I) -1 , together with a baseline K-Ascent derived from projected gradient ascent (Levitin & Polyak, 1966).Mariet & Sra (2015) extended DPP from a fixed-point perspective and Bardenet & Titsias (2015) proposed to optimize DPP upon a lower bound in variational inference fashion.A key problem of such line of works is that the computation is not differentiable, making it difficult to be used in deep learning frameworks.To the best of our knowledge, there is no previous method incorporating DPP as a feature-level diversity metric in deep learning.A key difficulty in doing so is that the calculation of the gradient of det(L) involves matrix inversion, which can be unstable and inaccurate in GPUs.Though K-Ascent seems to be a naive rule, it still needs explicit matrix inversion in the first step before the projection procedure.This fact greatly hinders the tight integration of DPP with deep networks.Some alternative methods seek to reach diversity under more constrained settings.For example, Zhang et al. (2017) resorted to a global pairwise orthogonality constraint in hyper-sphere and Zadeh et al. (2017) employed statistical moments to measure the diversity.However, compared with DPP, such measurements are unable to fully characterize diversity in an arbitrary bounded space.In this paper, rather than providing more efficient DPP solvers, we concentrate on delivering a feasible feature-level DPP integration under the deep learning framework.To this end, we revisit the spectral decomposition of DPP and propose a sub-gradient generation method which can be tightly integrated with deep learning.Our method differs from either approximation or direct optimization by introducing a \"differentiable direct optimization\" procedure, thus can produce genuinely diverse features in continuous bounded space.Our method is stable and scalable to the relatively large dataset with a specific mini-batch sampling strategy, which is verified by several experiments on various tasks.Notations: Bold lower case x and bold upper case K represent vector and matrix, respectively.det(\u2022) and Tr(\u2022) calculate the determinant and trace of a matrix, respectively.A \u2297 B is the element-wise product of matrices A and B. |X | and |x| measure the cardinality of a finite set X and the L 2 length of a vector x, respectively.x, y calculates the inner product of the two vectors.x = diag(X) transforms a diagonal matrix X into its vector form x, and vice versa.We refer \"positive semi-definite\" and \"positive definite\" to PSD and PD, respectively.Denote the real numbers."}
{"paper_id": 9, "abstract": "In our latest endeavor, we delve into the intricate realm of univariate time series point forecasting, wielding the power of deep learning as our primary tool. We unveil a novel deep neural architecture, ingeniously designed with both backward and forward residual links, complemented by an impressively deep stack of fully-connected layers. This architecture boasts a suite of desirable traits: it is interpretable, seamlessly adaptable across a broad spectrum of target domains, and remarkably swift to train.  To validate our creation, we put it to the test against several esteemed datasets, including the M3, M4, and the TOURISM competition datasets, each brimming with time series from a myriad of fields. Our results speak volumes, as we achieve state-of-the-art performance with two configurations of N-BEATS across all datasets, enhancing forecast accuracy by an impressive 11% over a traditional statistical benchmark and surpassing last year's M4 competition champion\u2014a meticulously crafted hybrid model that melded neural networks with statistical methodologies\u2014by 3%.  Intriguingly, our first model configuration operates without any time-series-specific components, yet its stellar performance on such diverse datasets challenges the prevailing notion that specialized techniques are essential. Instead, it suggests that fundamental deep learning constructs, like residual blocks, possess the inherent capability to tackle a vast array of forecasting challenges.   Moreover, we reveal an innovative approach to augment our architecture, enabling outputs that maintain interpretability without sacrificing accuracy. In this way, we not only push the boundaries of what deep learning can achieve in time series forecasting but also pave the way for a deeper understanding of the models we create.", "introduction": "Time series (TS) forecasting is an important business problem and a fruitful application area for machine learning (ML).It underlies most aspects of modern business, including such critical areas as inventory control and customer management, as well as business planning going from production and distribution to finance and marketing.As such, it has a considerable financial impact, often ranging in the millions of dollars for every point of forecasting accuracy gained (Jain, 2017;Kahn, 2003).And yet, unlike areas such as computer vision or natural language processing where deep learning (DL) techniques are now well entrenched, there still exists evidence that ML and DL struggle to outperform classical statistical TS forecasting approaches (Makridakis et al., 2018a;b).For instance, the rankings of the six \"pure\" ML methods submitted to M4 competition were 23, 37, 38, 48, 54, and 57 out of a total of 60 entries, and most of the best-ranking methods were ensembles of classical statistical techniques (Makridakis et al., 2018b).On the other hand, the M4 competition winner (Smyl, 2020), was based on a hybrid between neural residual/attention dilated LSTM stack with a classical Holt-Winters statistical model (Holt, 1957;2004;Winters, 1960) with learnable parameters.Since Smyl's approach heavily depends on this Holt-Winters component, Makridakis et al. (2018b) further argue that \"hybrid approaches and combinations of method are the way forward for improving the forecasting accuracy and making forecasting more valuable\".In this work we aspire to challenge this conclusion by exploring the potential of pure DL architectures in the context of the TS forecasting.Moreover, in the context of interpretable DL architecture design, we are interested in answering the following question: can we inject a suitable inductive bias in the model to make its internal operations more interpretable, in the sense of extracting some explainable driving factors combining to produce a given forecast?"}
{"paper_id": 10, "abstract": "In the realm of machine learning, where the quest for efficiency often collides with the limitations of data, meta-learning emerges as a beacon of hope. It seeks to harness the wisdom gleaned from previous tasks, weaving a tapestry of knowledge that can be applied to new challenges. Yet, lurking in the shadows of this promising approach is a formidable adversary: task heterogeneity. Traditional methods, bound by the constraints of a universally shared framework, struggle to navigate the diverse landscapes of varied tasks.  Moreover, the current task-specific meta-learning techniques often falter, either shackled by the rigidity of hand-crafted designs or lacking the finesse required to unveil the intricate relationships that exist between tasks. In this paper, we draw inspiration from the meticulous organization of knowledge within knowledge bases to introduce an innovative framework: Automated Relational Meta-Learning (ARML). This approach deftly extracts and elucidates the cross-task relationships, constructing a dynamic meta-knowledge graph.  When a new task emerges, ARML swiftly identifies the most pertinent structures, tailoring the acquired knowledge to fit the needs of the meta-learner. This not only confronts the specter of task heterogeneity with a robust, learned meta-knowledge graph but also enhances the interpretability of the model, illuminating the pathways of understanding. Through rigorous experimentation on both 2D toy regression and few-shot image classification tasks, our results showcase the formidable prowess of ARML, standing tall against state-of-the-art baselines and carving a new path in the landscape of meta-learning.", "introduction": "Learning quickly is the key characteristic of human intelligence, which remains a daunting problem in machine intelligence.The mechanism of meta-learning is widely used to generalize and transfer prior knowledge learned from previous tasks to improve the effectiveness of learning on new tasks, which has benefited various applications, such as computer vision (Kang et al., 2019;Liu et al., 2019), natural language processing (Gu et al., 2018;Lin et al., 2019) and social good (Zhang et al., 2019;Yao et al., 2019a).Most of existing meta-learning algorithms learn a globally shared meta-learner (e.g., parameter initialization (Finn et al., 2017;2018), meta-optimizer (Ravi & Larochelle, 2016), metric space (Snell et al., 2017;Garcia & Bruna, 2017;Oreshkin et al., 2018)).However, globally shared meta-learners fail to handle tasks lying in different distributions, which is known as task heterogeneity (Vuorio et al., 2018;Yao et al., 2019b).Task heterogeneity has been regarded as one of the most challenging issues in meta-learning, and thus it is desirable to design meta-learning models that effectively optimize each of the heterogeneous tasks.The key challenge to deal with task heterogeneity is how to customize globally shared meta-learner by using task-specific information?Recently, a handful of works try to solve the problem by learning a task-specific representation for tailoring the transferred knowledge to each task (Oreshkin et al., 2018;Vuorio et al., 2018;Lee & Choi, 2018).However, the expressiveness of these methods is limited due to the impaired knowledge generalization between highly related tasks.Recently, learning the underlying structure across tasks provides a more effective way for balancing the customization and generalization.Representatively, Yao et al. propose a hierarchically structured meta-learning method to customize the globally shared knowledge to each cluster (Yao et al., 2019b).Nonetheless, the hierarchical clustering structure completely relies on the handcrafted design which needs to be tuned carefully and may lack the capability to capture complex relationships.Hence, we are motivated to propose a framework to automatically extract underlying relational structures from historical tasks and leverage those relational structures to facilitate knowledge customization on a new task.This inspiration comes from the way of structuring knowledge in knowledge bases (i.e., knowledge graphs).In knowledge bases, the underlying relational structures across text entities are automatically constructed and applied to a new query to improve the searching efficiency.In the meta-learning problem, similarly, we aim at automatically establishing the metaknowledge graph between prior knowledge learned from previous tasks.When a new task arrives, it queries the meta-knowledge graph and quickly attends to the most relevant entities (vertices), and then takes advantage of the relational knowledge structures between them to boost the learning effectiveness with the limited training data.The proposed meta-learning framework is named as Automated Relational Meta-Learning (ARML).Specifically, the ARML automatically builds the meta-knowledge graph from meta-training tasks to memorize and organize learned knowledge from historical tasks, where each vertex represents one type of meta-knowledge (e.g., the common contour between birds and aircrafts).To learn the meta-knowledge graph at meta-training time, for each task, we construct a prototype-based relational graph for each class, where each vertex represents one prototype.The prototype-based relational graph not only captures the underlying relationship behind samples, but alleviates the potential effects of abnormal samples.The meta-knowledge graph is then learned by summarizing the information from the corresponding prototype-based relational graphs of meta-training tasks.After constructing the meta-knowledge graph, when a new task comes in, the prototype-based relational graph of the new task taps into the meta-knowledge graph for acquiring the most relevant knowledge, which further enhances the task representation and facilitates its training process.Our major contributions of the proposed ARML are three-fold: (1) it automatically constructs the meta-knowledge graph to facilitate learning a new task; (2) it empirically outperforms the state-ofthe-art meta-learning algorithms; (3) the meta-knowledge graph well captures the relationship among tasks and improves the interpretability of meta-learning algorithms."}
{"paper_id": 11, "abstract": "In the realm of deep learning, where the shadows of noisy labels can loom large and distort the path to mastery, our journey unveils a beacon of hope: the self-ensemble label filtering method, or SELF. This innovative approach serves to gradually sift through the tumult of erroneous labels that often ensnare deep neural networks (DNNs) during their training odyssey. By harnessing the power of ensemble predictions\u2014those collective whispers of wisdom gathered from the network's outputs across various epochs\u2014we are able to discern the truth hidden amidst the clamor of noise.  As we embark on this quest, SELF empowers the model to focus its learning on the potentially pristine labels, casting aside the tainted ones that threaten to derail its progress. Through this meticulous filtering process, we not only enhance the model's performance but also breathe new life into the training dynamics. The filtered samples, though removed from the direct supervision of the training loss, are not abandoned. Instead, they are cleverly repurposed within a semi-supervised framework, enriching the unsupervised loss and guiding the model\u2019s growth.  Our findings reveal that this method shines brightly across a spectrum of image classification challenges, whether faced with symmetric or asymmetric label noise, and regardless of the noise ratio. In a landscape where previous endeavors faltered, SELF stands tall, outpacing all prior efforts in the realm of noise-aware learning across diverse datasets. Its versatility knows no bounds, seamlessly adapting to a wide array of network architectures. Thus, we forge a new path forward, illuminating the way for future explorations in the intricate dance between noise and clarity in deep learning.", "introduction": "The acquisition of large quantities of a high-quality human annotation is a frequent bottleneck in applying DNNs.There are two cheap but imperfect alternatives to collect annotation at large scale: crowdsourcing from non-experts and web annotations, particularly for image data where the tags and online query keywords are treated as valid labels.Both these alternatives typically introduce noisy (wrong) labels.While Rolnick et al. (2017) empirically demonstrated that DNNs can be surprisingly robust to label noise under certain conditions, Zhang et al. (2017) has shown that DNNs have the capacity to memorize the data and will do so eventually when being confronted with too many noisy labels.Consequently, training DNNs with traditional learning procedures on noisy data strongly deteriorates their ability to generalize -a severe problem.Hence, limiting the influence of label noise is of great practical importance.A common approach to mitigate the negative influence of noisy labels is to eliminate them from the training data and train deep learning models just with the clean labels (Fr\u00e9nay & Verleysen, 2013).Employing semi-supervised learning can even counteract the noisy labels (Laine & Aila, 2016;Luo et al., 2018).However, the decision which labels are noisy and which are not is decisive for learning robust models.Otherwise, unfiltered noisy labels still influence the (supervised) loss and affect the task performance as in these previous works.They use the entire label set to compute the loss and severely lack a mechanism to identify and filter out the erroneous labels from the labels set.In this paper, we propose a self-ensemble label filtering (SELF) framework that identifies potentially noisy labels during training and keeps the network from receiving supervision from the filtered noisy labels.This allows DNNs to gradually focus on learning from undoubtedly correct samples even with an extreme level of noise in the labels (e.g., 80% noise ratio) and leads to improved performance as the supervision become less noisy.The key contribution of our work is progressive filtering, i.e., leverage the knowledge provided in the network's output over different training iterations to form a consensus of predictions (self-ensemble predictions) to progressively identify and filter out the noisy labels from the labeled data.When learning under label noise, the network receives noisy updates and hence fluctuates strongly.Such conduct of training would impede to learn stable neural representations and further mislead the consensus of the predictions.Therefore, it is essential to incorporate a model with stable training behavior to obtain better estimates from the consensus.Concretely, we employ the semi-supervised technique as a backbone to our framework to stabilize the learning process of the model.Correctly, we maintain the running average model, such as proposed by Tarvainen & Valpola (2017), a.k.a. the Mean-Teacher model.This model ensemble learning provides a more stable supervisory signal than the noisy model snapshots and provides a stable ground for progressive filtering to filter out potential noisy labels.Note that this is different from just a mere combination of semi-supervised techniques with a noisy label filtering method.We call our approach self-ensemble label filtering (SELF) -that establishes model ensemble learning as a backbone to form a solid consensus of the self-ensemble predictions to filter out the noisy labels progressively.Our framework allows to compute supervised loss on cleaner subsets rather than the entire noisy labeled data as in previous works.It further leverages the entire dataset, including the filtered out erroneous samples in the unsupervised loss.To best of our knowledge, we are the first to identify and propose self-ensemble as a principled technique against learning under noisy labels.Our motivation stems from the observation that DNNs start to learn from easy samples in initial phases and gradually adapt to hard ones during training.When trained on wrongly labeled data, DNNs learn from clean labels at ease and receive inconsistent error signals from the noisy labels before over-fitting to the dataset.The network's prediction is likely to be consistent on clean samples and inconsistent or oscillates strongly on wrongly labeled samples over different training iterations.Based on this observation, we record the outputs of a single network made on different training epochs and treat them as an ensemble of predictions obtained from different individual networks.We call these ensembles that are evolved from a single network self-ensemble predictions.Subsequently, we identify the correctly labeled samples via the agreement between the provided label set and our running average of self-ensemble predictions.The samples of ensemble predictions that agree with the provided labels are likely to be consistent and treated as clean samples.In summary, our SELF framework stabilizes the training process and improves the generalization ability of DNNs.We evaluate the proposed technique on image classification tasks using CI-FAR10, CIFAR100 & ImageNet.We demonstrate that SELF consistently outperforms the existing approaches on asymmetric and symmetric noise at all noise levels, as shown in Fig. 1.Besides, SELF remains robust towards the choice of the network architecture.Our work is transferable to other tasks without the need to modify the architecture or the primary learning objective.(Tarvainen & Valpola, 2017) to provide a stable learning signal.Also, the model collects a self-ensemble prediction (moving-average) for the subsequent filtering.Once the best model is found, these predictions identify and filter out noisy labels using the original label set L 0 .The model performs this progressive filtering until there is no more better model.For details see Algorithm 1."}
{"paper_id": 12, "abstract": "In the realm of natural question generation (QG), the quest to transform passages and answers into coherent inquiries has long been fraught with challenges. Many previous endeavors have either overlooked the intricate structural nuances embedded within the text, relied heavily on cross-entropy loss\u2014which often leads to pitfalls like exposure bias and discrepancies between training and testing metrics\u2014or failed to harness the full potential of the answer information. To surmount these obstacles, we present an innovative approach: a reinforcement learning (RL) driven graph-to-sequence (Graph2Seq) model tailored for QG. Our architecture features a Graph2Seq generator, powered by a cutting-edge Bidirectional Gated Graph Neural Network encoder that adeptly captures the essence of the passage. Complementing this is a hybrid evaluator that deftly intertwines cross-entropy and RL losses, ensuring that the generated questions are not only syntactically sound but also rich in semantic depth. Additionally, we introduce a robust Deep Alignment Network designed to weave the answer information seamlessly into the passage at both the word and contextual levels. This model is fully end-to-end trainable and sets a new benchmark, achieving remarkable scores that eclipse existing methodologies on the widely recognized SQuAD benchmark.", "introduction": "Natural question generation (QG) has many useful applications such as improving the question answering task (Chen et al., 2017;2019a) by providing more training data (Tang et al., 2017;Yuan et al., 2017), generating practice exercises and assessments for educational purposes (Heilman & Smith, 2010;Danon & Last, 2017), and helping dialog systems to kick-start and continue a conversation with human users (Mostafazadeh et al., 2016).While many existing works focus on QG from images (Fan et al., 2018;Li et al., 2018) or knowledge bases (Serban et al., 2016;Elsahar et al., 2018), in this work, we focus on QG from text.Conventional methods (Mostow & Chen, 2009;Heilman & Smith, 2010;Heilman, 2011) for QG rely on heuristic rules or hand-crafted templates, leading to the issues of low generalizability and scalability.Recent attempts have been focused on exploiting Neural Network (NN) based approaches that do not require manually-designed rules and are end-to-end trainable.Encouraged by the huge success of neural machine translation, these approaches formulate the QG task as a sequence-tosequence (Seq2Seq) learning problem.Specifically, attention-based Seq2Seq models (Bahdanau et al., 2014;Luong et al., 2015) and their enhanced versions with copy (Vinyals et al., 2015;Gu et al., 2016) and coverage (Tu et al., 2016) mechanisms have been widely applied and show promising results on this task (Du et al., 2017;Zhou et al., 2017;Song et al., 2018a;Kumar et al., 2018a).However, these methods typically ignore the hidden structural information associated with a word sequence such as the syntactic parsing tree.Failing to utilize the rich text structure information beyond the simple word sequence may limit the effectiveness of these models for QG.It has been observed that in general, cross-entropy based sequence training has several limitations like exposure bias and inconsistency between train/test measurement (Ranzato et al., 2015;Wu et al., 2016).As a result, they do not always produce the best results on discrete evaluation metrics on sequence generation tasks such as text summarization (Paulus et al., 2017) or question generation (Song et al., 2017).To cope with these issues, some recent QG approaches (Song et al., 2017;Kumar et al., 2018b) directly optimize evaluation metrics using Reinforcement Learning (RL) (Williams, 1992).However, existing approaches usually only employ evaluation metrics like BLEU and ROUGE-L as rewards for RL training.More importantly, they fail to exploit other important metrics such as syntactic and semantic constraints for guiding high-quality text generation.Early works on neural QG did not take into account the answer information when generating a question.Recent works have started to explore various means of utilizing the answer information.When question generation is guided by the semantics of an answer, the resulting questions become more relevant and readable.Conceptually, there are three different ways to incorporate the answer information by simply marking the answer location in the passage (Zhou et al., 2017;Zhao et al., 2018;Liu et al., 2019), or using complex passage-answer matching strategies (Song et al., 2017), or separating answers from passages when applying a Seq2Seq model (Kim et al., 2018;Sun et al., 2018).However, they neglect potential semantic relations between passage words and answer words, and thus fail to explicitly model the global interactions among them in the embedding space.To address these aforementioned issues, in this paper, we present a novel reinforcement learning based generator-evaluator architecture that aims to: i) make full use of rich hidden structure information beyond the simple word sequence; ii) generate syntactically and semantically valid text while maintaining the consistency of train/test measurement; iii) model explicitly the global interactions of semantic relationships between passage and answer at both word-level and contextual-level.In particular, to achieve the first goal, we explore two different means to either construct a syntaxbased static graph or a semantics-aware dynamic graph from the text sequence, as well as its rich hidden structure information.Then, we design a graph-to-sequence (Graph2Seq) model based generator that encodes the graph representation of a text passage and decodes a question sequence using a Recurrent Neural Network (RNN).Our Graph2Seq model is based on a novel bidirectional gated graph neural network, which extends the gated graph neural network (Li et al., 2015) by considering both incoming and outgoing edges, and fusing them during the graph embedding learning.To achieve the second goal, we design a hybrid evaluator which is trained by optimizing a mixed objective function that combines both cross-entropy and RL loss.We use not only discrete evaluation metrics like BLEU, but also semantic metrics like word mover's distance (Kusner et al., 2015) to encourage both syntactically and semantically valid text generation.To achieve the third goal, we propose a novel Deep Alignment Network (DAN) for effectively incorporating answer information into the passage at multiple granularity levels.Our main contributions are as follows:\u2022 We propose a novel RL-based Graph2Seq model for natural question generation.To the best of our knowledge, we are the first to introduce the Graph2Seq architecture for QG.\u2022 We explore both static and dynamic ways of constructing graph from text and are the first to systematically investigate their performance impacts on a GNN encoder.\u2022 The proposed model is end-to-end trainable, achieves new state-of-the-art scores, and outperforms existing methods by a significant margin on the standard SQuAD benchmark for QG.Our human evaluation study also corroborates that the questions generated by our model are more natural (semantically and syntactically) compared to other baselines."}
{"paper_id": 13, "abstract": "In the ever-evolving landscape of Multi-Task Reinforcement Learning, we embark on a quest to unveil the power of shared representations across diverse tasks. Our exploration is grounded in a compelling premise: that when tasks share underlying properties, the wisdom gleaned from one can illuminate the path for others, leading to a richer tapestry of knowledge and more effective feature extraction than when we tread the solitary path of single-task learning.  Imagine a world where the features derived from a collective of tasks not only enhance understanding but also elevate the performance of Reinforcement Learning algorithms. We delve into this notion, providing robust theoretical guarantees that delineate the conditions under which sharing representations becomes not just beneficial, but essential. By extending the well-established finite-time bounds of Approximate Value Iteration to our multi-task realm, we lay a foundation for our claims.  To bring our theories to life, we introduce multi-task adaptations of three prominent Reinforcement Learning algorithms. Through rigorous empirical evaluations on widely recognized benchmarks, we demonstrate that our approach yields substantial improvements in both sample efficiency and overall performance, surpassing the limitations of traditional single-task methods. Join us as we chart a course through the intricate interplay of tasks, revealing the transformative potential of shared knowledge in the realm of deep learning.", "introduction": "Multi-Task Learning (MTL) ambitiously aims to learn multiple tasks jointly instead of learning them separately, leveraging the assumption that the considered tasks have common properties which can be exploited by Machine Learning (ML) models to generalize the learning of each of them.For instance, the features extracted in the hidden layers of a neural network trained on multiple tasks have the advantage of being a general representation of structures common to each other.This translates into an effective way of learning multiple tasks at the same time, but it can also improve the learning of each individual task compared to learning them separately (Caruana, 1997).Furthermore, the learned representation can be used to perform Transfer Learning (TL), i.e. using it as a preliminary knowledge to learn a new similar task resulting in a more effective and faster learning than learning the new task from scratch (Baxter, 2000;Thrun & Pratt, 2012).The same benefits of extraction and exploitation of common features among the tasks achieved in MTL, can be obtained in Multi-Task Reinforcement Learning (MTRL) when training a single agent on multiple Reinforcement Learning (RL) problems with common structures (Taylor & Stone, 2009;Lazaric, 2012).In particular, in MTRL an agent can be trained on multiple tasks in the same domain, e.g.riding a bicycle or cycling while going towards a goal, or on different but similar domains, e.g.balancing a pendulum or balancing a double pendulumfoot_0 .Considering recent advances in Deep Reinforcement Learning (DRL) and the resulting increase in the complexity of experimental benchmarks, the use of Deep Learning (DL) models, e.g.deep neural networks, has become a popular and effective way to extract common features among tasks in MTRL algorithms (Rusu et al., 2015;Liu et al., 2016;Higgins et al., 2017).However, despite the high representational capacity of DL models, the extraction of good features remains challenging.For instance, the performance of the learning process can degrade when unrelated tasks are used together (Caruana, 1997;Baxter, 2000); another detrimental issue may occur when the training of a single model is not balanced properly among multiple tasks (Hessel et al., 2018).Recent developments in MTRL achieve significant results in feature extraction by means of algorithms specifically developed to address these issues.While some of these works rely on a single deep neural network to model the multi-task agent (Liu et al., 2016;Yang et al., 2017;Hessel et al., 2018;Wulfmeier et al., 2019), others use multiple deep neural networks, e.g. one for each task and another for the multi-task agent (Rusu et al., 2015;Parisotto et al., 2015;Higgins et al., 2017;Teh et al., 2017).Intuitively, achieving good results in MTRL with a single deep neural network is more desirable than using many of them, since the training time is likely much less and the whole architecture is easier to implement.In this paper we study the benefits of shared representations among tasks.We theoretically motivate the intuitive effectiveness of our method, deriving theoretical guarantees that exploit the theoretical framework provided by Maurer et al. (2016), in which the authors present upper bounds on the quality of learning in MTL when extracting features for multiple tasks in a single shared representation.The significancy of this result is that the cost of learning the shared representation decreases with a factor O( 1 / \u221a T ), where T is the number of tasks for many function approximator hypothesis classes.The main contribution of this work is twofold.1. We derive upper confidence bounds for Approximate Value-Iteration (AVI) and Approximate Policy-Iteration (API)foot_1 (Farahmand, 2011) in the MTRL setting, and we extend the approximation error bounds in Maurer et al. (2016) to the case of multiple tasks with different dimensionalities.Then, we show how to combine these results resulting in, to the best of our knowledge, the first proposed extension of the finite-time bounds of AVI/API to MTRL.Despite being an extension of previous works, we derive these results to justify our approach showing how the error propagation in AVI/API can theoretically benefit from learning multiple tasks jointly.2. We leverage these results proposing a neural network architecture, for which these bounds hold with minor assumptions, that allow us to learn multiple tasks with a single regressor extracting a common representation.We show an empirical evidence of the consequence of our bounds by means of a variant of Fitted Q-Iteration (FQI) (Ernst et al., 2005), based on our shared network and for which our bounds apply, that we call Multi Fitted Q-Iteration (MFQI).Then, we perform an empirical evaluation in challenging RL problems proposing multitask variants of the Deep Q-Network (DQN) (Mnih et al., 2015) and Deep Deterministic Policy Gradient (DDPG) (Lillicrap et al., 2015) algorithms.These algorithms are practical implementations of the more general AVI/API framework, designed to solve complex problems.In this case, the bounds apply to these algorithms only with some assumptions, e.g.stationary sampling distribution.The outcome of the empirical analysis joins the theoretical results, showing significant performance improvements compared to the singletask version of the algorithms in various RL problems, including several MuJoCo (Todorov et al., 2012) domains."}
{"paper_id": 14, "abstract": "In the ever-evolving landscape of text generation, reinforcement learning (RL) has emerged as a powerful ally, particularly in the realm of machine translation (MT). Techniques like Minimum Risk Training (MRT) and Generative Adversarial Networks (GAN) have been heralded as breakthroughs, yet the depths of their learning processes remain shrouded in mystery. In our exploration, we unveil a startling revelation: one of the most prevalent RL strategies in MT does not actually optimize the expected reward. Moreover, we demonstrate that alternative methods often succumb to the crushing weight of time, taking an unreasonably long journey to reach convergence.  Our investigation reveals a crucial insight: the enhancements attributed to RL in MT are likely to manifest only when the pre-trained parameters are already teetering on the brink of producing accurate translations. This realization casts a shadow over the perceived efficacy of these methods, suggesting that the gains we observe may stem from phenomena unrelated to the training signal itself. Specifically, shifts in the distribution curve's shape may be responsible for these improvements, leading us to question the true nature of progress in this domain.", "introduction": "Reinforcement learning (RL) is an appealing path for advancement in Machine Translation (MT), as it allows training systems to optimize non-differentiable score functions, common in MT evaluation, as well as tackling the \"exposure bias\" (Ranzato et al., 2015) in standard training, namely that the model is not exposed during training to incorrectly generated tokens, and is thus unlikely to recover from generating such tokens at test time.These motivations have led to much interest in RL for text generation in general and MT in particular (see \u00a72).Various policy gradient methods have been used, notably REINFORCE (Williams, 1992) and variants thereof (e.g., Ranzato et al., 2015;Edunov et al., 2018) and Minimum Risk Training (MRT;e.g., Och, 2003;Shen et al., 2016).Another popular use of RL is for training GANs (Yang et al., 2018;Tevet et al., 2018).Nevertheless, despite increasing interest and strong results, little is known about what accounts for these performance gains, and the training dynamics involved.We present the following contributions.First, our theoretical analysis shows that commonly used approximation methods are theoretically ill-founded, and may converge to parameter values that do not minimize the risk, nor are local minima thereof ( \u00a72.2).Second, using both naturalistic experiments and carefully constructed simulations, we show that performance gains observed in the literature likely stem not from making target tokens the most probable, but from unrelated effects, such as increasing the peakiness of the output distribution (i.e., the probability mass of the most probable tokens).We do so by comparing a setting where the reward is informative, vs. one where it is constant.In \u00a74 we discuss this peakiness effect (PKE).Third, we show that promoting the target token to be the mode is likely to take a prohibitively long time.The only case we find, where improvements are likely, is where the target token is among the first 2-3 most probable tokens according to the pretrained model.These findings suggest that REINFORCE ( \u00a75) and CMRT ( \u00a76) are likely to improve over the pre-trained model only under the best possible conditions, i.e., where the pre-trained model is \"nearly\" correct.We conclude by discussing other RL practices in MT which should be avoided for practical and theoretical reasons, and briefly discuss alternative RL approaches that will allow RL to tackle a larger class of errors in pre-trained models ( \u00a77).An MT system generates tokens y = (y 1 , ..., y n ) from a vocabulary V one token at a time.The probability of generating y i given preceding tokens y <i is given by P \u03b8 (\u2022|x, y <i ), where x is the source sentence and \u03b8 are the model parameters.For each generated token y i , we denote with r(y i ; y <i , x, y (ref ) ) the score, or reward, for generating y i given y <i , x, and the reference sentence y (ref ) .For brevity, we omit parameters where they are fixed within context.For simplicity, we assume r does not depend on following tokens y >i .We also assume there is exactly one valid target token, as de facto, training is done against a single reference (Schulz et al., 2018).In practice, either a token-level reward is approximated using Monte-Carlo methods (e.g., Yang et al., 2018), or a sentence-level (sparse) reward is given at the end of the episode (sentence).The latter is equivalent to a uniform token-level reward.r is often the negative log-likelihood, or a standard MT metric, e.g., BLEU (Papineni et al., 2002).RL's goal is to maximize the expected episode reward (denoted with R); i.e., to find(1)"}
{"paper_id": 15, "abstract": "In the realm of graph analysis, the quest for high-level representations is akin to forging a powerful artifact\u2014one that can unlock the secrets hidden within complex structures. While graph convolution has garnered significant attention, the equally vital domain of graph pooling remains shrouded in relative obscurity. Many existing techniques, it seems, overlook a critical element: the inherent structural information of the graph itself.   In this work, we present a groundbreaking approach known as StructPool, which shines a light on this overlooked aspect. We reframe graph pooling as a node clustering challenge, necessitating the development of a cluster assignment matrix that captures the essence of the graph's topology. By treating this as a structured prediction problem, we harness the power of conditional random fields to weave together the intricate relationships among node assignments.  Moreover, we extend our method to integrate topological insights into the design of the Gibbs energy function, enhancing our framework\u2019s robustness. The results of our experiments across various datasets stand testament to the efficacy of StructPool, illuminating a path forward in the exploration of graph pooling techniques.", "introduction": "Graph neural networks have achieved the state-of-the-art results for multiple graph tasks, such as node classification (Veli\u010dkovi\u0107 et al., 2018;Gao & Ji, 2019b;Gao et al., 2018) and link prediction (Zhang & Chen, 2018;Cai & Ji, 2020).These results demonstrate the effectiveness of graph neural networks to learn node representations.However, graph classification tasks also require learning good graph-level representations.Since pooling operations are shown to be effective in many image and NLP tasks, it is natural to investigate pooling techniques for graph data (Yu & Koltun, 2016;Springenberg et al., 2014).Recent work extends the global sum/average pooling operations to graph models by simply summing or averaging all node features (Atwood & Towsley, 2016;Simonovsky & Komodakis, 2017).However, these trivial global pooling operations may lose important features and ignore structural information.Furthermore, global pooling are not hierarchical so that we cannot apply them where multiple pooling operations are required, such as Graph U-Net (Gao & Ji, 2019a).Several advanced graph pooling methods, such as SORTPOOL (Zhang et al., 2018), TOPKPOOL (Gao & Ji, 2019a), DIFFPOOL (Ying et al., 2018), and SAGPOOL (Lee et al., 2019) , are recently proposed and achieve promising performance on graph classification tasks.However, none of them explicitly models the relationships among different nodes and thus may ignore important structural information.We argue that such information is important and should be explicitly captured in graph pooling.In this work, we propose a novel graph pooling technique, known as the STRUCTPOOL, that formulates graph pooling as a structured prediction problem.Following DIFFPOOL (Ying et al., 2018), we consider graph pooling as a node clustering problem, and each cluster corresponds to a node in the new graph after pooling.Intuitively, two nodes with similar features should have a higher probability of being assigned to the same cluster.Hence, the assignment of a given node should depend on both the input node features and the assignments of other nodes.We formulate this as a structured prediction problem and employ conditional random fields (CRFs) (Lafferty et al., 2001) to capture such high-order structural relationships among the assignments of different nodes.In addition, we generalize our method by incorporating the graph topological information so that our method can control the clique set in our CRFs.We employ the mean field approximation to compute the assignments and describe how to incorporate it in graph networks.Then the networks can be trained in an end-to-end fashion.Experiments show that our proposed STRUCTPOOL outperforms existing methods significantly and consistently.We also show that STRUCTPOOL incurs acceptable computational cost given its superior performance."}
{"paper_id": 16, "abstract": "In the realm of graph matching, the quest to establish a precise correspondence between nodes in two disparate graphs stands as a formidable challenge, one steeped in the complexities of combinatorial puzzles and crowned by the NP-completeness of its nature. Yet, in a recent surge of innovation, deep graph matching techniques have begun to harness the power of deep learning, achieving remarkable strides in matching accuracy. In this paper, we unveil two synergistic contributions designed to enhance existing methodologies and serve as versatile plugins for future endeavors.  First, we introduce a groundbreaking strategy for node and edge embedding that draws inspiration from the multi-head mechanism found in attention models. This approach allows for the independent merging of information across various channels, a significant advancement over previous methods that relied solely on node embeddings. Second, we present a sophisticated masking mechanism applied to the loss function, engineered to smooth the learning process for graph matching. Utilizing the Hungarian algorithm, this mechanism dynamically constructs a structured, sparsely connected layer that focuses on the most impactful matching pairs, treating them as hard attention points.  Our method not only demonstrates competitive performance but also holds the potential to elevate state-of-the-art techniques when integrated as a plugin, particularly in terms of matching accuracy across three widely recognized benchmarks.", "introduction": "Without loss of generality, we consider the bijection problem for graph matching: given graph G 1 and G 2 of equal size n, graph matching seeks to find the one-vs-one node correspondence 1 : maxwhere x = vec(X) \u2208 {0, 1} n 2 which is the column-wise vectorized form of the permutation matrix X that encodes the node-to-node correspondence between two graphs, and K \u2208 R n 2 \u00d7n 2 + is the so-called affinity matrix 2 , respectively.Note P is a selection matrix encoding the one-to-one correspondence constraint.This problem is called Lawler's QAP (Lawler, 1963) and has attracted enormous attention for its generally NP-complete (Hartmanis, 1982) challenge, as well as a wide spectrum of applications in computer vision, graphics, machine learning and operational research etc.In particular, Koopmans-Beckmann's QAP (Loiola et al., 2007) with objective tr(X F 1 XF 2 ) is a special case of Eq. ( 1), which can be converted to Lawler's QAP by K = F 2 \u2297 F 1 and F i refers to the weighted adjacency matrix.A series of solvers haven been developed to solve graph matching problem (Leordeanu & Hebert, 2005;Cho et al., 2010;Bernard et al., 2018;Yan et al., 2015;Yu et al., 2018).All these methods are based on deterministic optimization, which are conditioned with pre-defined affinity matrix and no learning paradigm is involved.This fact greatly limits the performance and broad application w.r.t.different problem settings considering its NP-hard nature.separately in local stages (Caetano et al., 2009;Cho et al., 2013).On the other hand, Graph Convolutional Networks (GCN) (Kipf & Welling, 2017) brings about new capability on tasks over graph-like data, as it naturally integrates the intrinsic graph structure in a general updating rule:where \u00c2 is the normalized connectivity matrix.H (l) and W (l) are the features and weights at layer l, respectively.Node embedding is updated by aggregation from 1-neighboring nodes, which is akin to the convolution operator in CNN.By taking advantages of both DGM and GCN, Wang et al. (2019) and Zhang & Lee (2019) incorporate permutation loss instead of displacement loss in (Zanfir & Sminchisescu, 2018), with notable improvement across both synthetic and real data.Note that Eq. ( 1) involves both node and edge information, which exactly correspond to the diagonal and off-diagonal elements in K, respectively.Edges can carry informative multi-dimensional attributes (namely weights) which are fundamental to graph matching.However existing embedding based graph matching methods (Wang et al., 2019;Xu et al., 2019) are focused on the explicit modeling of node level features, whereby the edges are only used as topological node connection for message passing in GCN.Besides, edge attributes are neither well modeled in the embedding-free model (Zanfir & Sminchisescu, 2018) since the edge information is derived from the concatenation of node features.To our best knowledge, there is no deep graph matching method explicitly incorporating edge attributes.In contrast, edge attributes e.g.length and orientation are widely used in traditional graph matching models (Cho et al., 2010;Yan et al., 2015;Yu et al., 2018) for constructing the affinity matrix K.Such a gap shall be filled in the deep graph matching pipeline.Another important consideration refers to the design of loss function.There are mainly two forms in existing deep graph matching works: i) displacement loss (Zanfir & Sminchisescu, 2018) similar to the use in optical flow estimation (Ren et al., 2017); ii) the so-called permutation loss (Wang et al., 2019) involving iterative Sinkhorn procedure followed by a cross-entropy loss.Results in (Wang et al., 2019) show the latter is an effective improvement against the former regression based loss.However, we argue that the continuous Sinkhorn procedure (in training stage) is yet an unnatural approximation to Hungarian sampling (in testing stage) for discretization.If the network is equipped with a continuous loss function (e.g.cross-entropy), we argue that the training process will make a great \"meaningless effort\" to enforce some network output digits of the final matching matrix into binary and neglect the resting digits which might have notable impact on accuracy.This paper strikes an endeavor on the above two gaps and makes the following main contributions:i) We propose a new approach for edge embedding via channel-wise operation, namely channelindependent embedding (CIE).The hope is to effectively explore the edge attribute and simulate the multi-head strategy in attention models (Veli\u010dkovi\u0107 et al., 2018) by decoupling the calculations parallel and orthogonal to channel direction.In fact, edge attribute information has not been considered in existing embedding based graph matching methods (Wang et al., 2019;Xu et al., 2019).ii) We devise a new mechanism to adjust the loss function based on the Hungarian method which is widely used for linear assignment problem, as termed by Hungarian attention.It resorts to dynamically generating sparse matching mask according to Hungarian sampling during training, rather than approximating Hungarian sampling with a differentiable function.As such, the Hungarian attention introduces higher smoothness against traditional loss functions to ease the training.iii) The empirical results on three public benchmarks shows that the two proposed techniques are orthogonal and beneficial to existing techniques.Specifically, on the one hand, our CIE module can effectively boost the accuracy by exploring the edge attributes which otherwise are not considered in state-of-the-art deep graph matching methods; on the other hand, our Hungarian attention mechanism also shows generality and it is complementary to existing graph matching loss."}
{"paper_id": 17, "abstract": "In this study, we embark on a journey into the realm of semi-supervised classification of graph data, where the elusive categories of unlabeled nodes are drawn forth from the wisdom of their labeled counterparts and the intricate tapestry of graph structures. While many recent endeavors have sought to tackle this challenge through sophisticated graph convolution techniques in a traditionally supervised fashion, the fragility of performance becomes apparent when the well of labeled data runs dry.   To counter this, we unveil our Graph Inference Learning (GIL) framework, a beacon of hope designed to elevate node classification by harnessing the power of inference drawn from the very topology of the graph itself. In our quest, we introduce a formal definition of structural relations\u2014an intricate binding of node attributes, paths between nodes, and local topological configurations\u2014allowing for seamless deductions from one node to another.   To refine this inference process, we incorporate meta-optimization of these structural relations, bridging the gap from training nodes to validation nodes. This ensures that the graph inference capabilities we cultivate can adapt and thrive when faced with the challenges posed by test nodes.   Our comprehensive evaluations across four benchmark datasets\u2014Cora, Citeseer, Pubmed, and NELL\u2014reveal the undeniable superiority of our GIL framework, standing tall against other state-of-the-art methods in the semi-supervised node classification arena.", "introduction": "Graph, which comprises a set of vertices/nodes together with connected edges, is a formal structural representation of non-regular data.Due to the strong representation ability, it accommodates many potential applications, e.g., social network (Orsini et al., 2017), world wide data (Page et al., 1999), knowledge graph (Xu et al., 2017), and protein-interaction network (Borgwardt et al., 2007).Among these, semi-supervised node classification on graphs is one of the most interesting also popular topics.Given a graph in which some nodes are labeled, the aim of semi-supervised classification is to infer the categories of those remaining unlabeled nodes by using various priors of the graph.While there have been numerous previous works (Brandes et al., 2008;Zhou et al., 2004;Zhu et al., 2003;Yang et al., 2016;Zhao et al., 2019) devoted to semi-supervised node classification based on explicit graph Laplacian regularizations, it is hard to efficiently boost the performance of label prediction due to the strict assumption that connected nodes are likely to share the same label information.With the progress of deep learning on grid-shaped images/videos (He et al., 2016), a few of graph convolutional neural networks (CNN) based methods, including spectral (Kipf & Welling, 2017) and spatial methods (Niepert et al., 2016;Pan et al., 2018;Yu et al., 2018), have been proposed to learn local convolution filters on graphs in order to extract more discriminative node representations.Although graph CNN based methods have achieved considerable capabilities of graph embedding by optimizing filters, they are limited into a conventionally semi-supervised framework and lack of an efficient inference mechanism on graphs.Especially, in the case of few-shot learning, where a small number of training nodes are labeled, this kind of methods would drastically compromise the performance.For example, the Pubmed graph dataset (Sen et al., 2008) consists Figure 1: The illustration of our proposed GIL framework.For the problem of graph node labeling, the category information of these unlabeled nodes depends on the similarity computation between a query node (e.g., vj) and these labeled reference nodes (e.g., vi).We consider the similarity from three points: node attributes, the consistency of local topological structures (i.e., the circle with dashed line), and the between-node path reachability (i.e., the red wave line from vi to vj).Specifically, the local structures as well as node attributes are encoded as high-level features with graph convolution, while the between-node path reachability is abstracted as reachable probabilities of random walks.To better make the inference generalize to test nodes, we introduce a meta-learning strategy to optimize the structure relations learning from training nodes to validation nodes.of 19,717 nodes and 44,338 edges, but only 0.3% nodes are labeled for the semi-supervised node classification task.These aforementioned works usually boil down to a general classification task, where the model is learnt on a training set and selected by checking a validation set.However, they do not put great efforts on how to learn to infer from one node to another node on a topological graph, especially in the few-shot regime.In this paper, we propose a graph inference learning (GIL) framework to teach the model itself to adaptively infer from reference labeled nodes to those query unlabeled nodes, and finally boost the performance of semi-supervised node classification in the case of a few number of labeled samples.Given an input graph, GIL attempts to infer the unlabeled nodes from those observed nodes by building between-node relations.The between-node relations are structured as the integration of node attributes, connection paths, and graph topological structures.It means that the similarity between two nodes is decided from three aspects: the consistency of node attributes, the consistency of local topological structures, and the between-node path reachability, as shown in Fig. 1.The local structures anchored around each node as well as the attributes of nodes therein are jointly encoded with graph convolution (Defferrard et al., 2016) for the sake of high-level feature extraction.For the between-node path reachability, we adopt the random walk algorithm to obtain the characteristics from a labeled reference node v i to a query unlabeled node v j in a given graph.Based on the computed node representation and between-node reachability, the structure relations can be obtained by computing the similar scores/relationships from reference nodes to unlabeled nodes in a graph.Inspired by the recent meta-learning strategy (Finn et al., 2017), we learn to infer the structure relations from a training set to a validation set, which can benefit the generalization capability of the learned model.In other words, our proposed GIL attempts to learn some transferable knowledge underlying in the structure relations from training samples to validation samples, such that the learned structure relations can be better self-adapted to the new testing stage.We summarize the main contributions of this work as three folds:\u2022 We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.The structure relations are well defined by jointly considering node attributes, between-node paths, and graph topological structures.\u2022 To make the inference model better generalize to test nodes, we introduce a meta-learning procedure to optimize structure relations, which could be the first time for graph node classification to the best of our knowledge.\u2022 Comprehensive evaluations on three citation network datasets (including Cora, Citeseer, and Pubmed) and one knowledge graph data (i.e., NELL) demonstrate the superiority of our proposed GIL in contrast with other state-of-the-art methods on the semi-supervised classification task."}
{"paper_id": 18, "abstract": "In the intricate dance of learning, where the art of imitation meets the challenge of high-dimensional realms and unpredictable dynamics, the path can often seem fraught with peril. Traditional approaches, such as behavioral cloning (BC), find themselves ensnared in the quagmire of distribution shift. Here, the eager agent, in its quest to mirror the actions of its expert, risks straying from the sanctum of demonstrated states, led astray by the accumulation of errors.   Yet, the horizon is not devoid of hope. Enter the realm of reinforcement learning (RL), where innovative strategies like inverse RL and generative adversarial imitation learning (GAIL) rise to the occasion, guiding the agent to harmoniously align with expert demonstrations over extended horizons. However, these methods grapple with the elusive nature of true reward functions, often resorting to convoluted and fragile approximations born of adversarial training.  In response to this complexity, we present a refreshing alternative: a method that embraces the spirit of RL while liberating itself from the burden of learning a reward function. Our approach centers on a simple yet profound principle\u2014providing the agent with a compelling incentive to remain in sync with demonstrated behaviors over the long haul. By bestowing a constant reward of r=+1 for actions taken in demonstrated states, and a neutral r=0 for all other actions, we cultivate a pathway for the agent to return to the realm of demonstrated states when confronted with the unfamiliar.  We call this innovative strategy soft Q imitation learning (SQIL). With just a few judicious tweaks, SQIL can be seamlessly integrated into any standard Q-learning or off-policy actor-critic algorithm. Theoretically, it emerges as a regularized variant of BC, leveraging a sparsity prior that encourages enduring imitation. Empirical evidence reveals that SQIL not only eclipses the performance of traditional BC but also stands shoulder to shoulder with GAIL, showcasing its prowess across a spectrum of image-based and low-dimensional challenges in environments like Box2D, Atari, and MuJoCo.  In essence, this paper serves as a testament to the power of simplicity, illustrating how a straightforward imitation methodology grounded in RL and constant rewards can rival the effectiveness of its more intricate counterparts that rely on learned rewards.", "introduction": "Many sequential decision-making problems can be tackled by imitation learning: an expert demonstrates near-optimal behavior to an agent, and the agent attempts to replicate that behavior in novel situations (Argall et al., 2009).This paper considers the problem of training an agent to imitate an expert, given expert action demonstrations and the ability to interact with the environment.The agent does not observe a reward signal or query the expert, and does not know the state transition dynamics.Standard approaches based on behavioral cloning (BC) use supervised learning to greedily imitate demonstrated actions, without reasoning about the consequences of actions (Pomerleau, 1991).As a result, compounding errors cause the agent to drift away from the demonstrated states (Ross et al., 2011).The problem with BC is that, when the agent drifts and encounters out-of-distribution states, the agent does not know how to return to the demonstrated states.Recent methods based on inverse reinforcement learning (IRL) overcome this issue by training an RL agent not only to imitate demonstrated actions, but also to visit demonstrated states (Ng et al., 2000;Wulfmeier et al., 2015;Finn et al., 2016b;Fu et al., 2017).This is also the core idea behind generative adversarial imitation learning (GAIL) (Ho & Ermon, 2016), which implements IRL using generative adversarial networks (Goodfellow et al., 2014;Finn et al., 2016a).Since the true reward function for the task is unknown, these methods construct a reward signal from the demonstrations through adversarial training, making them difficult to implement and use in practice (Kurach et al., 2018).The main idea in this paper is that the effectiveness of adversarial imitation methods can be achieved by a much simpler approach that does not require adversarial training, or indeed learning a reward function at all.Intuitively, adversarial methods encourage long-horizon imitation by providing the agent with (1) an incentive to imitate the demonstrated actions in demonstrated states, and (2) an incentive to take actions that lead it back to demonstrated states when it encounters new, out-ofdistribution states.One of the reasons why adversarial methods outperform greedy methods, such as BC, is that greedy methods only do (1), while adversarial methods do both (1) and (2).Our approach is intended to do both (1) and ( 2) without adversarial training, by using constant rewards instead of learned rewards.The key idea is that, instead of using a learned reward function to provide a reward signal to the agent, we can simply give the agent a constant reward of r = +1 for matching the demonstrated action in a demonstrated state, and a constant reward of r = 0 for all other behavior.We motivate this approach theoretically, by showing that it implements a regularized variant of BC that learns long-horizon imitation by (a) imposing a sparsity prior on the reward function implied by the imitation policy, and (b) incorporating information about the state transition dynamics into the imitation policy.Intuitively, our method accomplishes (a) by training the agent using an extremely sparse reward function -+1 for demonstrations, 0 everywhere else -and accomplishes (b) by training the agent with RL instead of supervised learning.We instantiate our approach with soft Q-learning (Haarnoja et al., 2017) by initializing the agent's experience replay buffer with expert demonstrations, setting the rewards to a constant r = +1 in the demonstration experiences, and setting rewards to a constant r = 0 in all of the new experiences the agent collects while interacting with the environment.Since soft Q-learning is an off-policy algorithm, the agent does not necessarily have to visit the demonstrated states in order to experience positive rewards.Instead, the agent replays the demonstrations that were initially added to its buffer.Thus, our method can be applied in environments with stochastic dynamics and continuous states, where the demonstrated states are not necessarily reachable by the agent.We call this method soft Q imitation learning (SQIL).The main contribution of this paper is SQIL: a simple and general imitation learning algorithm that is effective in MDPs with high-dimensional, continuous observations and unknown dynamics.We run experiments in four image-based environments -Car Racing, Pong, Breakout, and Space Invadersand three low-dimensional environments -Humanoid, HalfCheetah, and Lunar Lander -to compare SQIL to two prior methods: BC and GAIL.We find that SQIL outperforms BC and achieves competitive results compared to GAIL.Our experiments illustrate two key benefits of SQIL: (1) that it can overcome the state distribution shift problem of BC without adversarial training or learning a reward function, which makes it easier to use, e.g., with images, and (2) that it is simple to implement using existing Q-learning or off-policy actor-critic algorithms."}
{"paper_id": 19, "abstract": "In the ever-evolving realm of machine learning, deep neural networks (DNNs) have emerged as the mighty champions, wielding their power across diverse domains such as vision, natural language processing, and speech. Yet, in the intricate landscape of heterogeneous tabular data, the supremacy of DNNs over their shallower counterparts remains shrouded in uncertainty. The prevailing wisdom suggests that deep learning has yet to forge a path that consistently outshines the stalwart gradient boosting decision trees (GBDT), which have long held the crown for tackling tabular challenges.  In this paper, we unveil a groundbreaking architecture: Neural Oblivious Decision Ensembles (NODE). This innovative design is crafted specifically for the complexities of tabular data. At its core, NODE transcends traditional ensembles of oblivious decision trees, harnessing the dual forces of end-to-end gradient-based optimization and the profound capabilities of multi-layer hierarchical representation learning.  Through rigorous experimentation against the leading GBDT frameworks across a vast array of tabular datasets, we reveal the formidable advantages of NODE. Our findings demonstrate that this architecture not only competes but often prevails, outperforming its rivals in the majority of tasks. To foster collaboration and further exploration, we are excited to open-source the PyTorch implementation of NODE, envisioning it as a universal framework that will redefine the landscape of machine learning for tabular data.", "introduction": "The recent rise of deep neural networks (DNN) resulted in a substantial breakthrough for a large number of machine learning tasks in computer vision, natural language processing, speech recognition, reinforcement learning (Goodfellow et al., 2016).Both gradient-based optimization via backpropagation (Rumelhart et al., 1985) and hierarchical representation learning appear to be crucial in increasing the performance of machine learning for these problems by a large margin.While the superiority of deep architectures in these domains is undoubtful, machine learning for tabular data still did not fully benefit from the DNN power.Namely, the state-of-the-art performance in problems with tabular heterogeneous data is often achieved by \"shallow\" models, such as gradient boosted decision trees (GBDT) (Friedman, 2001;Chen & Guestrin, 2016;Ke et al., 2017;Prokhorenkova et al., 2018).While the importance of deep learning on tabular data is recognized by the ML community, and many works address this problem (Zhou & Feng, 2017;Yang et al., 2018;Miller et al., 2017;Lay et al., 2018;Feng et al., 2018;Ke et al., 2018), the proposed DNN approaches do not consistently outperform the state-of-the-art shallow models by a notable margin.In particular, to the best of our knowledge, there is still no universal DNN approach that was shown to systematically outperform the leading GBDT packages (e.g., XGBoost (Chen & Guestrin, 2016)).As additional evidence, a large number of Kaggle ML competitions with tabular data are still won by the shallow GBDT methods (Harasymiv, 2015).Overall, at the moment, there is no dominant deep learning solution for tabular data problems, and we aim to reduce this gap by our paper.We introduce Neural Oblivious Decision Ensembles (NODE), a new DNN architecture, designed to work with tabular problems.The NODE architecture is partially inspired by the recent CatBoost package (Prokhorenkova et al., 2018), which was shown to provide state-of-the-art performance on a large number of tabular datasets.In a nutshell, CatBoost performs gradient boosting on oblivious decision trees (decision tables) (Kohavi, 1994;Lou & Obukhov, 2017), which makes inference very efficient, and the method is quite resistant to overfitting.In its essence, the proposed NODE architecture generalizes CatBoost, making the splitting feature choice and decision tree routing differentiable.As a result, the NODE architecture is fully differentiable and could be incorporated in any computational graph of existing DL packages, such as TensorFlow or PyTorch.Furthermore, NODE allows constructing multi-layer architectures, which resembles \"deep\" GBDT that is trained end-to-end, which was never proposed before.Besides the usage of oblivious decision tables, another important design choice is the recent entmax transformation (Peters et al., 2019), which effectively performs a \"soft\" splitting feature choice in decision trees inside the NODE architecture.As discussed in the following sections, these design choices are critical to obtain state-of-the-art performance.In a large number of experiments, we compare the proposed approach with the leading GBDT implementations with tuned hyperparameters and demonstrate that NODE outperforms competitors consistently on most of the datasets.Overall, the main contributions of our paper can be summarized as follows:1. We introduce a new DNN architecture for machine learning on tabular data.To the best of our knowledge, our method is the first successful example of deep architectures that substantially outperforms leading GBDT packages on tabular data.2. Via an extensive experimental evaluation on a large number of datasets, we show that the proposed NODE architecture outperforms existing GBDT implementations.3. The PyTorch implementation of NODE is available onlinefoot_0 .The rest of the paper is organized as follows.In Section 2 we review prior work relevant to our method.The proposed Neural Oblivious Decision Ensembles architecture is described in Section 3 and experimentally evaluated in Section 4. Section 5 concludes the paper."}
{"paper_id": 20, "abstract": "In the realm of person re-identification (re-ID), the quest to recognize the same individual across disparate camera feeds presents a formidable challenge. The crux of the issue lies in the domain diversities that exist between various datasets, complicating the seamless adaptation of re-ID models trained in one environment to another. While cutting-edge unsupervised domain adaptation techniques have made strides by leveraging knowledge from a source domain\u2014often employing clustering algorithms to generate pseudo labels for the target domain\u2014these methods overlook a critical flaw: the inherent label noise introduced by clustering. This noise becomes a substantial barrier, stifling the model's potential to enhance feature representations within the target domain.  To address this conundrum, we introduce an innovative unsupervised framework known as Mutual Mean-Teaching (MMT). This approach seeks to refine the noisy pseudo labels in the target domain through a dual strategy\u2014harnessing both offline refined hard pseudo labels and online refined soft pseudo labels, all within an alternating training paradigm. Furthermore, while it is customary to employ a combination of classification loss and triplet loss to optimize performance in person re-ID models, traditional triplet loss struggles to accommodate the nuances of softly refined labels. To bridge this gap, we propose a novel soft softmax-triplet loss, specifically designed to facilitate learning with soft pseudo triplet labels, thereby enhancing domain adaptation efficacy.  Our MMT framework demonstrates remarkable advancements, achieving significant improvements of 14.4%, 18.2%, 13.1%, and 16.4% in mean Average Precision (mAP) across the Market-to-Duke, Duke-to-Market, Market-to-MSMT, and Duke-to-MSMT unsupervised domain adaptation tasks. Through this work, we carve a new path in the journey of person re-ID, illuminating the way forward in the face of domain challenges.", "introduction": "Person re-identification (re-ID) aims at retrieving the same persons' images from images captured by different cameras.In recent years, person re-ID datasets with increasing numbers of images were proposed to facilitate the research along this direction.All the datasets require time-consuming annotations and are keys for re-ID performance improvements.However, even with such large-scale datasets, for person images from a new camera system, the person re-ID models trained on existing datasets generally show evident performance drops because of the domain gaps.Unsupervised Domain Adaptation (UDA) is therefore proposed to adapt the model trained on the source image domain (dataset) with identity labels to the target image domain (dataset) with no identity annotations.State-of-the-art UDA methods (Song et al., 2018;Zhang et al., 2019b;Yang et al., 2019) for person re-ID group unannotated images with clustering algorithms and train the network with clusteringgenerated pseudo labels.Although the pseudo label generation and feature learning with pseudo labels are conducted alternatively to refine the pseudo labels to some extent, the training of the neural network is still substantially hindered by the inevitable label noise.The noise derives from the limited transferability of source-domain features, the unknown number of target-domain identities, and the imperfect results of the clustering algorithm.The refinery of noisy pseudo labels has crucial influences to the final performance, but is mostly ignored by the clustering-based UDA methods.Figure 1: Person image A 1 and A 2 belong to the same identity while B with similar appearance is from another person.However, clustering-generated pseudo labels in state-of-the-art Unsupervised Domain Adaptation (UDA) methods contain much noise that hinders feature learning.We propose pseudo label refinery with on-line refined soft pseudo labels to effectively mitigate the influence of noisy pseudo labels and improve UDA performance on person re-ID.To effectively address the problem of noisy pseudo labels in clustering-based UDA methods (Song et al., 2018;Zhang et al., 2019b;Yang et al., 2019) (Figure 1), we propose an unsupervised Mutual Mean-Teaching (MMT) framework to effectively perform pseudo label refinery by optimizing the neural networks under the joint supervisions of off-line refined hard pseudo labels and on-line refined soft pseudo labels.Specifically, our proposed MMT framework provides robust soft pseudo labels in an on-line peer-teaching manner, which is inspired by the teacher-student approaches (Tarvainen & Valpola, 2017;Zhang et al., 2018b) to simultaneously train two same networks.The networks gradually capture target-domain data distributions and thus refine pseudo labels for better feature learning.To avoid training error amplification, the temporally average model of each network is proposed to produce reliable soft labels for supervising the other network in a collaborative training strategy.By training peer-networks with such on-line soft pseudo labels on the target domain, the learned feature representations can be iteratively improved to provide more accurate soft pseudo labels, which, in turn, further improves the discriminativeness of learned feature representations.The classification and triplet losses are commonly adopted together to achieve state-of-the-art performances in both fully-supervised (Luo et al., 2019) and unsupervised (Zhang et al., 2019b;Yang et al., 2019) person re-ID models.However, the conventional triplet loss (Hermans et al., 2017) cannot work with such refined soft labels.To enable using the triplet loss with soft pseudo labels in our MMT framework, we propose a novel soft softmax-triplet loss so that the network can benefit from softly refined triplet labels.The introduction of such soft softmax-triplet loss is also the key to the superior performance of our proposed framework.Note that the collaborative training strategy on the two networks is only adopted in the training process.Only one network is kept in the inference stage without requiring any additional computational or memory cost.The contributions of this paper could be summarized as three-fold.(1) We propose to tackle the label noise problem in state-of-the-art clustering-based UDA methods for person re-ID, which is mostly ignored by existing methods but is shown to be crucial for achieving superior final performance.The proposed Mutual Mean-Teaching (MMT) framework is designed to provide more reliable soft labels.(2) Conventional triplet loss can only work with hard labels.To enable training with soft triplet labels for mitigating the pseudo label noise, we propose the soft softmax-triplet loss to learn more discriminative person features.(3) The MMT framework shows exceptionally strong performances on all UDA tasks of person re-ID.Compared with state-of-the-art methods, it leads to significant improvements of 14.4%, 18.2%, 13.4%, 16.4% mAP on Market-to-Duke, Duke-to-Market, Market-to-MSMT, Duke-to-MSMT re-ID tasks."}
{"paper_id": 21, "abstract": "In the realm of image classification, we embark on a quest to uncover new classes hidden within a vast collection, armed only with the knowledge of other, labeled classes. This challenge resembles the trials of semi-supervised learning, yet it is fraught with greater peril: the absence of labeled examples for the new classes we seek to unveil. Our mission, then, is to harness the insights gleaned from the labeled images to forge a robust clustering model, one capable of illuminating the shadows where the unlabelled data resides.  In this endeavor, we weave together three innovative threads: First, we assert that the traditional method of bootstrapping image representations solely from labeled data introduces a treacherous bias. Instead, we advocate for the use of self-supervised learning to forge our representations anew, drawing from the collective strength of both labeled and unlabeled data. Second, we employ rank statistics as a bridge, transferring the model's understanding of the labeled classes to the uncharted territory of clustering the unlabeled images. Lastly, we optimize a joint objective function that simultaneously enhances the supervised classification of our labeled data and the clustering of the unlabeled, creating a harmonious balance that elevates both tasks.  Through rigorous evaluation against established classification benchmarks, our approach emerges triumphant, surpassing the current methodologies for novel category discovery by a remarkable margin. In this way, we not only illuminate the path forward but also redefine the boundaries of what is possible in the realm of image classification.", "introduction": "Modern machine learning systems can match or surpass human-level performance in tasks such as image classification (Deng et al., 2009), but at the cost of collecting large quantities of annotated training data.Semi-supervised learning (SSL) (Oliver et al., 2018) can alleviate this issue by mixing labelled with unlabelled data, which is usually much cheaper to obtain.However, these methods still require some annotations for each of the classes that one wishes to learn.We argue this is not always possible in real applications.For instance, consider the task of recognizing products in supermarkets.Thousands of new products are introduced in stores every week, and it would be very expensive to annotate them all.However, new products do not differ drastically from the existing ones, so it should be possible to discover them automatically as they arise in the data.Unfortunately, machines are still unable to effectively learn new classes without manual annotations.In this paper, we thus consider the problem of discovering new visual classes automatically, assuming that a certain number of classes are already known by the model (Hsu et al., 2018;2019;Han et al., 2019).This knowledge comes in the form of a labelled dataset of images for a certain set of classes.Given that this data is labelled, off-the-shelf supervised learning techniques can be used to train a very effective classifier for the known classes, particularly if Convolutional Neural Networks (CNNs) are employed.However, this does not mean that the learned features are useful as a representation of the new classes.Furthermore, even if the representation transfers well, one still has the problem of identifying the new classes in an unlabelled dataset, which is a clustering problem.We tackle these problems by introducing a novel approach that combines three key ideas (section 2 and fig.1).The first idea is to pre-train the image representation (a CNN) using all available images, both labelled and unlabelled, using a self-supervised learning objective.Crucially, this objective does not leverage the known labels, resulting in features that are much less biased towards the labelled The first step of the training consists in learning an unbiased image representation via self-supervision using both labelled and unlabelled data, which learns well the early layers of the representation; in the second step, we fine-tune only the last few layers of the model using supervision on the labelled set; finally, the fine-tuned representation is used, via rank statistics, to induce clusters in the unlabelled data, while maintaining a good representation on the labelled set.classes.Labels are used only after pre-training to learn a classifier specific to the labelled data as well as to fine-tune the deepest layers of the CNN, for which self-supervision is not as effective.The second idea is a new approach to transfer the information contained in the labelled images to the problem of clustering the unlabelled ones.Information is transferred by sharing the same representation between labelled and unlabelled images, motivated by the fact that the new classes are often similar to the known ones.In more detail, pairs of unlabelled images are compared via their representation vectors.The comparison is done using robust rank statistics, by testing if two images share the same subset of k maximally activated representation components.This test is used to decide if two unlabelled images belong to the same (new) class or not, generating a set of noisy pairwise pseudo-labels.The pseudo-labels are then used to learn a similarity function for the unlabelled images.The third and final idea is, after bootstrapping the representation, to optimise the model by minimizing a joint objective function, containing terms for both the labelled and unlabelled subsets, using respectively the given labels and the generated pseudo-labels, thus avoiding the forgetting issue that may arise with a sequential approach.A further boost is obtained by incorporating incremental learning of the discovered classes in the classification task, which allows information to flow between the labelled and unlabelled images.We evaluate our method on several public benchmarks (section 3), outperforming by a large margin all existing techniques (section 4) that can be applied to this problem, demonstrating the effectiveness of our approach.We conclude the paper by summarizing our findings (section 5).Our code can be found at http://www.robots.ox.ac.uk/ \u02dcvgg/research/auto_novel."}
{"paper_id": 22, "abstract": "In the realm of Q-learning, a shadow looms\u2014a persistent overestimation bias that plagues our attempts to harness the full potential of action values. This bias arises from the method we employ, where we approximate the maximum action value through the lens of the highest estimated action value. While numerous algorithms have emerged, each claiming to wrestle this bias into submission, a deeper understanding of how this bias intertwines with our performance remains elusive.   In this paper, we embark on a journey to illuminate the intricate dance between bias and efficiency, revealing that the impact of overestimation is not a universal constant, but rather a chameleon, shifting with the environment. We introduce a novel approach to Q-learning\u2014\\emph{Maxmin Q-learning}\u2014a versatile framework that empowers practitioners with a parameter to deftly modulate bias. Our theoretical explorations unveil a remarkable truth: there exists a sweet spot within our parameter space that enables Maxmin Q-learning to achieve unbiased estimation while boasting a lower approximation variance than its predecessor.  Moreover, we establish the convergence of our algorithm in the tabular case, alongside a proof of convergence for several prior Q-learning variants, all within the elegant structure of a newly conceived Generalized Q-learning framework. Through empirical validation, we demonstrate that our algorithm not only tames the beast of estimation bias in simplified environments but also outshines its competitors across a spectrum of benchmark challenges. Join us as we unravel the complexities of Q-learning, forging a path toward a more precise and powerful understanding of reinforcement learning.", "introduction": "Q-learning (Watkins, 1989) is one of the most popular reinforcement learning algorithms.One of the reasons for this widespread adoption is the simplicity of the update.On each step, the agent updates its action value estimates towards the observed reward and the estimated value of the maximal action in the next state.This target represents the highest value the agent thinks it could obtain from the current state and action, given the observed reward.Unfortunately, this simple update rule has been shown to suffer from overestimation bias (Thrun & Schwartz, 1993;van Hasselt, 2010).The agent updates with the maximum over action values might be large because an action's value actually is high, or it can be misleadingly high simply because of the stochasticity or errors in the estimator.With many actions, there is a higher probability that one of the estimates is large simply due to stochasticity and the agent will overestimate the value.This issue is particularly problematic under function approximation, and can significant impede the quality of the learned policy (Thrun & Schwartz, 1993;Szita & L\u0151rincz, 2008;Strehl et al., 2009) or even lead to failures of Q-learning (Thrun & Schwartz, 1993).More recently, experiments across several domains suggest that this overestimation problem is common (Hado van Hasselt et al., 2016).Double Q-learning (van Hasselt, 2010) is introduced to instead ensure underestimation bias.The idea is to maintain two unbiased independent estimators of the action values.The expected action value of estimator one is selected for the maximal action from estimator two, which is guaranteed not to overestimate the true maximum action value.Double DQN (Hado van Hasselt et al., 2016), the extension of this idea to Q-learning with neural networks, has been shown to significantly improve performance over Q-learning.However, this is not a complete answer to this problem, because trading overestimation bias for underestimation bias is not always desirable, as we show in our experiments.Several other methods have been introduced to reduce overestimation bias, without fully moving towards underestimation.Weighted Double Q-learning (Zhang et al., 2017) uses a weighted combination of the Double Q-learning estimate, which likely has underestimation bias, and the Q-learning estimate, which likely has overestimation bias.Bias-corrected Q-Learning (Lee et al., 2013) reduces the overestimation bias through a bias correction term.Ensemble Q-learning and Averaged Q-learning (Anschel et al., 2017) take averages of multiple action values, to both reduce the overestimation bias and the estimation variance.However, with a finite number of actionvalue functions, the average operation in these two algorithms will never completely remove the overestimation bias, as the average of several overestimation biases is always positive.Further, these strategies do not guide how strongly we should correct for overestimation bias, nor how to determine-or control-the level of bias.The overestimation bias also appears in the actor-critic setting (Fujimoto et al., 2018;Haarnoja et al., 2018).For example, Fujimoto et al. (2018) propose the Twin Delayed Deep Deterministic policy gradient algorithm (TD3) which reduces the overestimation bias by taking the minimum value between two critics.However, they do not provide a rigorous theoretical analysis for the effect of applying the minimum operator.There is also no theoretical guide for choosing the number of estimators such that the overestimation bias can be reduced to 0.In this paper, we study the effects of overestimation and underestimation bias on learning performance, and use them to motivate a generalization of Q-learning called Maxmin Q-learning.Maxmin Q-learning directly mitigates the overestimation bias by using a minimization over multiple action-value estimates.Moreover, it is able to control the estimation bias varying from positive to negative which helps improve learning efficiency as we will show in next sections.We prove that, theoretically, with an appropriate number of action-value estimators, we are able to acquire an unbiased estimator with a lower approximation variance than Q-learning.We empirically verify our claims on several benchmarks.We study the convergence properties of our algorithm within a novel Generalized Q-learning framework, which is suitable for studying several of the recently proposed Q-learning variants.We also combine deep neural networks with Maxmin Q-learning (Maxmin DQN) and demonstrate its effectiveness in several benchmark domains."}
{"paper_id": 23, "abstract": "In the realm of machine learning, where the quest for data privacy and efficiency reigns supreme, federated learning emerges as a beacon of hope, weaving together the disparate threads of mobile phones, IoT devices, and wearables into a tapestry of collaborative intelligence. Yet, even this innovative approach is not without its challenges. The specter of domain shift looms large, a formidable foe that disrupts the harmony between source nodes and target nodes, causing trained models to falter when faced with new, unlabeled data.  In this work, we embark on a journey to confront the intricate problem of federated domain adaptation. Our mission is to harmonize the representations gleaned from various nodes, aligning them with the unique data distribution of the target node. To achieve this, we extend the powerful adversarial adaptation techniques, skillfully crafting them to fit the constraints of the federated landscape. But we do not stop there; we introduce a dynamic attention mechanism, a guiding light that enhances the transfer of knowledge, and employ feature disentanglement to further refine our approach.  Through rigorous experimentation across a diverse array of image and text classification tasks, we unveil promising results in the realm of unsupervised federated domain adaptation. Our findings illuminate a path forward, showcasing the potential of our methods to transcend the barriers posed by domain shifts and foster a new era of collaborative learning.", "introduction": "Data generated by networks of mobile and IoT devices poses unique challenges for training machine learning models.Due to the growing storage/computational power of these devices and concerns about data privacy, it is increasingly attractive to keep data and computation locally on the device (Smith et al., 2017).Federated Learning (FL) (Mohassel & Rindal, 2018;Bonawitz et al., 2017;Mohassel & Zhang, 2017) provides a privacy-preserving mechanism to leverage such decen-tralized data and computation resources to train machine learning models.The main idea behind federated learning is to have each node learn on its own local data and not share either the data or the model parameters.While federated learning promises better privacy and efficiency, existing methods ignore the fact that the data on each node are collected in a non-i.i.d manner, leading to domain shift between nodes (Quionero-Candela et al., 2009).For example, one device may take photos mostly indoors, while another mostly outdoors.In this paper, we address the problem of transferring knowledge from the decentralized nodes to a new node with a different data domain, without requiring any additional supervision from the user.We define this novel problem Unsupervised Federated Domain Adaptation (UFDA), as illustrated in Figure 1(a).There is a large body of existing work on unsupervised domain adaptation (Long et al., 2015;Ganin & Lempitsky, 2015;Tzeng et al., 2017;Zhu et al., 2017;Gong et al., 2012;Long et al., 2018), but the federated setting presents several additional challenges.First, the data are stored locally and cannot be shared, which hampers mainstream domain adaptation methods as they need to access both the labeled source and unlabeled target data (Tzeng et al., 2014;Long et al., 2017;Ghifary et al., 2016;Sun & Saenko, 2016;Ganin & Lempitsky, 2015;Tzeng et al., 2017).Second, the model parameters are trained separately for each node and converge at different speeds, while also offering different contributions to the target node depending on how close the two domains are.Finally, the knowledge learned from source nodes is highly entangled (Bengio et al., 2013), which can possibly lead to negative transfer (Pan & Yang, 2010).In this paper, we propose a solution to the above problems called Federated Adversarial Domain Adaptation (FADA) which aims to tackle domain shift in a federated learning system through adversarial techniques.Our approach preserves data privacy by training one model per source node and updating the target model with the aggregation of source gradients, but does so in a way that reduces domain shift.First, we analyze the federated domain adaptation problem from a theoretical perspective and provide a generalization bound.Inspired by our theoretical results, we propose an efficient adaptation algorithm based on adversarial adaptation and representation disentanglement applied to the federated setting.We also devise a dynamic attention model to cope with the varying convergence rates in the federated learning system.We conduct extensive experiments on real-world datasets, including image recognition and natural language tasks.Compared to baseline methods, we improve adaptation performance on all tasks, demonstrating the effectiveness of our devised model."}
{"paper_id": 24, "abstract": "In the realm of sequence-to-sequence models, where the arcane arts of computation are wielded with precision, a curious limitation emerges: these models, regardless of the complexity of the task at hand, adhere to a fixed number of computations for each input sequence. In this exploration, we delve into the heart of the Transformer architecture, forging a path where predictions can materialize at various stages of the network. Herein, we illuminate novel methods to discern the computational needs of each unique sequence, allowing us to tailor our approach with finesse.  Unlike the dynamic computations of Universal Transformers, which iterate through a uniform set of layers, our method introduces a dynamic tapestry where distinct layers are employed at each juncture. This not only optimizes the computational load but also enhances the model's capacity, adapting to the challenges presented by each sequence. In our trials on the IWSLT German-English translation task, our innovative approach achieves the same level of accuracy as a meticulously tuned baseline Transformer, all while employing less than a quarter of the decoder layers. Thus, we stand on the precipice of a new era in model efficiency, where the power of computation is wielded with both artistry and precision.", "introduction": "The size of modern neural sequence models (Gehring et al., 2017;Vaswani et al., 2017;Devlin et al., 2019) can amount to billions of parameters (Radford et al., 2019).For example, the winning entry of the WMT'19 news machine translation task in English-German used an ensemble totaling two billion parameters (Ng et al., 2019).While large models are required to do better on hard examples, small models are likely to perform as well on easy ones, e.g., the aforementioned ensemble is probably not required to translate a short phrase such as \"Thank you\".However, current models apply the same amount of computation regardless of whether the input is easy or hard.In this paper, we propose Transformers which adapt the number of layers to each input in order to achieve a good speed-accuracy trade off at inference time.We extend Graves (2016;ACT) who introduced dynamic computation to recurrent neural networks in several ways: we apply different layers at each stage, we investigate a range of designs and training targets for the halting module and we explicitly supervise through simple oracles to achieve good performance on large-scale tasks.Universal Transformers (UT) rely on ACT for dynamic computation and repeatedly apply the same layer (Dehghani et al., 2018).Our work considers a variety of mechanisms to estimate the network depth and applies a different layer at each step.Moreover, Dehghani et al. (2018) fix the number of steps for large-scale machine translation whereas we vary the number of steps to demonstrate substantial improvements in speed at no loss in accuracy.UT uses a layer which contains as many weights as an entire standard Transformer and this layer is applied several times which impacts speed.Our approach does not increase the size of individual layers.We also extend the resource efficient object classification work of Huang et al. (2017) and Bolukbasi et al. (2017) to structured prediction where dynamic computation decisions impact future computation.Related work from computer vision includes Teerapittayanon et al. (2016); Figurnov et al. (2017) and Wang et al. (2018) who explored the idea of dynamic routing either by exiting early or by skipping layers.We encode the input sequence using a standard Transformer encoder to generate the output sequence with a varying amount of computation in the decoder network.Dynamic computation poses a challenge for self-attention because omitted layers in prior time-steps may be required in the future.We experiment with two approaches to address this and show that a simple approach works well ( \u00a72).Next, we investigate different mechanisms to control the amount of computation in the decoder network, either for the entire sequence or on a per-token basis.This includes multinomial and binomial classifiers supervised by the model likelihood or whether the argmax is already correct as well as simply thresholding the model score ( \u00a73).Experiments on IWSLT14 German-English translation (Cettolo et al., 2014) as well as WMT'14 English-French translation show that we can match the performance of well tuned baseline models at up to 76% less computation ( \u00a74)."}
{"paper_id": 25, "abstract": "In the quest for crafting neural network models that are both sparse and efficient, researchers have long traversed the landscape of regularization techniques. Many have turned their gaze toward the L1 and L0 regularizers, each with its own strengths and weaknesses in the pursuit of weight sparsity during training. The L0 regularizer, a direct measure of parameter sparsity, stands proudly as scale-invariant, yet it falters in providing useful gradients, often leading to convoluted optimization challenges. In contrast, the L1 regularizer, with its near-universal differentiability, dances gracefully with gradient descent, but it lacks the scale invariance necessary for true efficiency, applying a uniform shrinkage that can stifle the potential for increased sparsity.  Drawing inspiration from the Hoyer measure\u2014an elegant ratio of L1 to L2 norms that has found its place in the realm of traditional compressed sensing\u2014we unveil DeepHoyer. This innovative suite of sparsity-inducing regularizers is designed to be both differentiable almost everywhere and scale-invariant, offering a harmonious balance that was previously elusive. Our experiments reveal that by harnessing the power of DeepHoyer, we can achieve even greater levels of sparsity in neural network models, all while maintaining the same accuracy as prior methods. Furthermore, we demonstrate the versatility of DeepHoyer, showcasing its applicability in both element-wise and structural pruning. In this way, we pave new paths in the realm of neural network optimization, where efficiency and sparsity can coexist in remarkable harmony.", "introduction": "The use of deep neural network (DNN) models has been expanded from handwritten digit recognition (LeCun et al., 1998) to real-world applications, such as large-scale image classification (Simonyan & Zisserman, 2014), self driving (Makantasis et al., 2015) and complex control problems (Mnih et al., 2013).However, a modern DNN model like AlexNet (Krizhevsky et al., 2012) or ResNet (He et al., 2016) often introduces a large number of parameters and computation load, which makes the deployment and real-time processing on embedded and edge devices extremely difficult (Han et al., 2015b;a;Wen et al., 2016).Thus, model compression techniques, especially pruning methods that increase the sparsity of weight matrices, have been extensively studied to reduce the memory consumption and computation cost of DNNs (Han et al., 2015b;a;Wen et al., 2016;Guo et al., 2016;Louizos et al., 2017b;Luo et al., 2017;Zhang et al., 2018;Liu et al., 2015).Most of the previous works utilize some form of sparsity-inducing regularizer in searching for sparse neural networks.The 1 regularizer, originally proposed by Tibshirani (1996), can be easily optimized through gradient descent for its convex and almost everywhere differentiable property.Therefore it is widely used in DNN pruning: Liu et al. (2015) directly apply 1 regularization to all the weights of a DNN to achieve element-wise sparsity; Wen et al. (2016;2017) present structural sparsity via group lasso, which applies an 1 regularization over the 2 norms of different groups of parameters.However, it has been noted that the value of the 1 regularizer is proportional to the scaling of parameters (i.e.||\u03b1W || 1 = |\u03b1|\u2022||W || 1 ), so it \"scales down\" all the elements in the weight matrices with the same speed.This is not efficient in finding sparsity and may sacrifice the flexibility of the trained model.On the other hand, the 0 regularizer directly reflects the real sparsity of weights and is scale invariant (i.e.||\u03b1W || 0 = ||W || 0 , \u2200\u03b1 = 0), yet the 0 norm cannot provide useful gradients.Han et al. (2015b) enforce an element-wise 0 constraint by iterative pruning a fixed percentage of smallest weight elements, which is a heuristic method and therefore can hardly achieve optimal compression rate.Some recent works mitigate the lack of gradient information by integrating 0 regularization with stochastic approximation (Louizos et al., 2017b) or more complex optimization methods (e.g.ADMM) (Zhang et al., 2018).These additional measures brought overheads to the optimization process, making the use of these methods on larger networks difficult.To achieve even sparser neural networks, we argue to move beyond 0 and 1 regularizers and seek for a sparsity-inducing regularizer that is both almost everywhere differentiable (like 1 ) and scale-invariant (like 0 ).Beyond the 1 regularizer, plenty of non-convex sparsity measurements have been used in the field of feature selection and compressed sensing (Hurley & Rickard, 2009;Wen et al., 2018).Some popular regularizers like SCAD (Fan & Li, 2001), MDP (Zhang et al., 2010) and Trimmed 1 (Yun et al., 2019) use a piece-wise formulation to mitigate the proportional scaling problem of 1 .The piece-wise formulation protects larger elements by having zero penalty to elements greater than a predefined threshold.However, it is extremely costly to manually seek for the optimal trimming threshold, so it is hard to obtain optimal result in DNN pruning by using these regularizers.The transformed 1 regularizer formulated as N i=1(a+1)|wi| a+|wi| manages to smoothly interpolate between 1 and 0 by tuning the hyperparameter a (Ma et al., 2019).However, such an approximation is close to 0 only when a approaches infinity, so the practical formulation of the transformed 1 (i.e. a = 1) is still not scale-invariant.Particularly, we are interested in the Hoyer regularizer (Hoyer, 2004), which estimates the sparsity of a vector with the ratio between its 1 and 2 norms.Comparing to other sparsity-inducing regularizers, Hoyer regularizer achieves superior performance in the fields of non-negative matrix factorization (Hoyer, 2004), sparse reconstruction (Esser et al., 2013;Tran et al., 2018) and blend deconvolution (Krishnan et al., 2011;Repetti et al., 2015).We note that Hoyer regularizer is both almost everywhere differentiable and scale invariant, satisfying the desired property of a sparsityinducing regularizer.We therefore propose DeepHoyer, which is the first Hoyer-inspired regularizers for DNN sparsification.Specifically, the contributions of this work include:\u2022 Hoyer-Square (HS) regularizer for element-wise sparsity: We enhance the original Hoyer regularizer to the HS regularizer and achieve element-wise sparsity by applying it in the training of DNNs.The HS regularizer is both almost everywhere differentiable and scale invariant.It has the same range and minima structure as the 0 norm.Thus, the HS regularizer presents the ability of turning small weights to zero while protecting and maintaining those weights that are larger than an induced, gradually adaptive threshold; \u2022 Group-HS regularizer for structural sparsity, which is extended from the HS regularizer;\u2022 Generating sparser DNN models: Our experiments show that the proposed regularizers beat state-of-the-arts in both element-wise and structural weight pruning of modern DNNs."}
{"paper_id": 26, "abstract": "In the realm of deep learning, Neural Architecture Search (NAS) stands as a beacon of innovation, striving to uncover optimal network designs for a myriad of tasks. Traditional methodologies have carved a path through two distinct phases: the exploration of the vast architecture landscape and the subsequent validation of the most promising designs. Yet, in the current landscape, NAS algorithms are predominantly judged by their downstream task performance\u2014a notion that, while seemingly straightforward, overlooks a critical aspect: the efficacy of their search strategies.  In this paper, we venture into uncharted territory by proposing a novel framework to scrutinize the search phase of NAS. Our investigation juxtaposes the quality of solutions yielded by various NAS search policies against the backdrop of random architecture selection. The findings are both illuminating and sobering: (i) On average, the leading NAS algorithms exhibit performance strikingly akin to that of random selection; (ii) the prevalent weight-sharing strategy, while popular, tarnishes the ranking of NAS candidates, obscuring their true potential and ultimately undermining the search process.  We posit that our evaluation framework will serve as a cornerstone for the evolution of NAS strategies, empowering them to consistently unearth architectures that surpass the randomness of chance. With this work, we seek to illuminate the path forward, guiding future endeavors in the intricate dance of architecture discovery.", "introduction": "By automating the design of a neural network for the task at hand, Neural Architecture Search (NAS) has tremendous potential to impact the practicality of deep learning (Zoph & Le, 2017;Liu et al., 2018b;a;Tan et al., 2018;Baker et al., 2016), and has already obtained state-of-the-art performance on many tasks.A typical NAS technique (Zoph & Le, 2017;Pham et al., 2018;Liu et al., 2018a) has two stages: the search phase, which aims to find a good architecture, and the evaluation one, where the best architecture is trained from scratch and validated on the test data.Table 1: Comparison of NAS algorithms with random sampling.We report results on PTB using mean validation perplexity (the lower, the better) and on CIFAR-10 using mean top 1 accuracy.We also provide the p-value of Student's t-tests against random sampling.PTB (PPL) t-test CIFAR-10 (acc.)t-test ENAS 59.88 \u00b1 1.92 0.73 96.79 \u00b1 0.11 0.01 DARTS 60.61 \u00b1 2.54 0.62 96.62 \u00b1 0.23 0.20 NAO 61.99 \u00b1 1.95 0.02 96.86 \u00b1 0.17 0.00 Random 60.13 \u00b1 0.65 -96.44 \u00b1 0.19 -In the literature, NAS algorithms are typically compared based on their results in the evaluation phase.While this may seem intuitive, the search phase of these algorithms often differ in several ways, such as their architecture sampling strategy and the search space they use, and the impact of these individual factors cannot be identified by looking at the downstream task results only.Furthermore, the downstream task results are often reported for a single random seed, which leaves unanswered the question of robustness of the search strategies.Furthermore, even when using such heavy computational resources, vanilla NAS has to restrict the number of trained architectures from a total of 10 9 to 10 4 , and increasing the sampler accuracy can only be achieved by increasing the resources.ENAS (Pham et al., 2018) was the first to propose a training scheme with shared parameters, reducing the resources from thousands of GPU days to one.Instead of being trained from scratch each sampled model inherits the parameters from previously-trained ones.Since then, NAS research has mainly focused on two directions: 1) Replacing the RL sampler with a better search algorithm, such as gradient descent (Liu et al., 2019b), bayesian optimiziation (Zhou et al., 2019) and performance predictors (Luo et al., 2018); 2) Exploiting NAS for other applications, e.g., object detection (Ghiasi et al., 2019;Chen et al., 2019), semantic segmentation (Liu et al., 2019a), and finding compact networks (Cai et al., 2018b;Wu et al., 2018;Chu et al., 2019;Guo et al., 2019).Characterizing the search space.Ying et al. (2019); Dong & Yang (2020) introduced a dataset that contains the ground-truth performance of CNN cells, and Wang et al. (2019) evaluated some traditional search algorithms on it.Similarly, Radosavovic et al. (2019) characterizes many CNN search spaces by computing the statistics of a set of sampled architectures, revealing that, for datasets such as CIFAR-10 or ImageNet, these statistics are similar.While these works support our claim that evaluation of NAS algorithms is crucial, they do not directly evaluate the state-of-the-arts NAS algorithms as we do here.Evaluation of NAS algorithms.Typically, the quality of NAS algorithms is judged based on the results of the final architecture they produce on the downstream task.In other words, the search and robustness of these algorithms are generally not studied, with (Liu et al., 2019b;So et al., 2019) the only exception for robustness, where results obtained with different random seeds were reported.Here, we aim to further the understanding of the mechanisms behind the search phase of NAS algorithms.Specifically, we propose doing so by comparing them with a simple random search policy, which uniformly randomly samples one architecture per run in the same search space as the NAS techniques.While some works have provided partial comparisons to random search, these comparisons unfortunately did not give a fair chance to the random policy.Specifically, (Pham et al., 2018) reports the results of only a single random architecture, and (Liu et al., 2018b) those of an architecture selected among 8 randomly sampled ones as the most promising one after training for 300 epochs only.Here, we show that a fair comparison to the random policy, obtained by training all architectures, i.e., random and NAS ones, for 1000 epochs and averaging over multiple random seeds for robustness, yields a different picture; the state-of-the-art search policies are no better than the random one.The motivation behind this comparison was our observation of only a weak correlation between the performance of the searched architectures and the ones trained from scratch during the evaluation phase.This phenomenon was already noticed by Zela et al. (2018), and concurrently to our work by Li & Talwalkar (2019); Xie et al. (2019); Ying et al. (2019), but the analysis of its impact or its causes went no further.Here, by contrast, we link this difference in performance between the search and evaluation phases to the use of weight sharing.While this may seem to contradict the findings of Bender et al. (2018), which, on CIFAR-10, observed a strong correlation between architectures trained with and without weight sharing when searching a CNN cell, our work differs from (Bender et al., 2018) in two fundamental ways: 1) The training scheme in (Bender et al., 2018), in which the entire model with shared parameters is trained via random path dropping, is fundamentally different from those used by state-of-the-arts weight sharing NAS strategies (Pham et al., 2018;Liu et al., 2019b;Luo et al., 2018); 2) While the correlation in (Bender et al., 2018) was approximated using a small subset of sampled architectures, we make use of a reduced search space where we can perform a complete evaluation of all architectures, thus providing an exact correlation measure in this space."}
{"paper_id": 27, "abstract": "In the intricate dance of future possibilities, where the fates of agents\u2014like pedestrians on a bustling street\u2014intertwine, the art of forecasting becomes crucial, especially for the guardians of safety, such as autonomous vehicles. These machines, tasked with navigating the unpredictable tapestry of human behavior, require not just a glimpse into the most probable outcomes, but a rich array of potential paths, each diverse enough to encompass the myriad of possibilities that could unfold. Relying solely on the most likely trajectories risks ensnaring ourselves in the gravitational pull of a singular outcome, neglecting the vital nuances that lie in the shadows of uncertainty.  Enter the realm of generative models, where variational autoencoders (VAEs) have emerged as formidable allies in the quest to learn the distribution of future trajectories. Yet, even within this powerful framework, the samples drawn from the learned models often cluster around the dominant mode, leaving the vast landscape of potential futures underexplored. To combat this limitation, we introduce a novel concept: the Diversity Sampling Function (DSF). This ingenious mechanism transforms the context of forecasting into a vibrant palette of latent codes, which can then be decoded by a generative model\u2014like a VAE\u2014into a rich tapestry of diverse trajectory samples.  Our approach redefines the challenge of generating this diverse set of trajectories as an optimization problem: the parameter estimation of the DSF itself. By harnessing the mathematical elegance of a determinantal point process (DPP), we craft a diversity loss that guides the learning of the DSF parameters. Through the iterative process of gradient descent, we adeptly navigate the latent space, sculpting the codes to yield an optimal assortment of trajectories that are not only diverse but also grounded in likelihood.  In our exploration, we showcase the remarkable versatility of our method, demonstrating its prowess in generating diverse trajectories across both the simple confines of low-dimensional 2D data and the complex, high-dimensional realm of human motion. Our work stands at the intersection of creativity and precision, illuminating a path forward in the quest for safety and understanding in an unpredictable world.", "introduction": "Forecasting future trajectories of vehicles and human has many useful applications in autonomous driving, virtual reality and assistive living.What makes trajectory forecasting challenging is that the future is uncertain and multi-modal -vehicles can choose different routes and people can perform different future actions.In many safety-critical applications, it is important to consider a diverse set of possible future trajectories, even those that are less likely, so that necessary preemptive actions can be taken.For example, an autonomous vehicle should understand that a neighboring car can merge into its lane even though the car is most likely to keep driving straight.To address this requirement, we need to take a generative approach to trajectory forecasting that can fully characterize the multimodal distribution of future trajectories.To capture all modes of a data distribution, variational autoencoders (VAEs) are well-suited generative models.However, random samples from a learned VAE model with Gaussian latent codes are not guaranteed to be diverse for two reasons.First, the sampling procedure is stochastic and the VAE samples can fail to cover some minor modes even with a large number of samples.Second, since VAE sampling is based on the implicit likelihood function encoded in the training data, if most of the training data is centered around a specific mode while other modes have less data (Fig. 1 (a)), the VAE samples will reflect this bias and concentrate around the major mode (Fig. 1 (b)).To tackle this problem, we propose to learn a diversity sampling function (DSF) that can reliably generate a diverse set of trajectory samples (Fig. 1 (c)).The proposed DSF is a deterministic parameterized function that maps forecasting context features (e.g., past trajectories) to a set of latent codes.The latent codes are decoded by the VAE docoder into a set of future trajectory samples, denoted as the DSF samples.In order to optimize the DSF, we formulate a diversity loss based on a determinantal point process (DPP) (Macchi, 1975) to evaluate the diversity of the DSF samples.The DPP defines the probability of choosing a random subset from the set of trajectory samples.It models the negative correlations between samples: the inclusion of a sample reduces the probability of including a similar sample.This makes the DPP an ideal tool for modeling the diversity within a set.In particular, we use the expected cardinality of the DPP as the diversity measure, which is defined as the expected size of a random subset drawn from the set of trajectory samples according to the DPP.Intuitively, since the DPP inhibits selection of similar samples, if the set of trajectory samples is more diverse, the random subset is more likely to select more samples from the set.The expected cardinality of the DPP is easy to compute and differentiable, which allows us to use it as the objective to optimize the DSF to enable diverse trajectory sampling.Our contributions are as follows:(1) We propose a new forecasting approach that learns a diversity sampling function to produce a diverse set of future trajectories;(2) We propose a novel application of DPPs to optimize a set of items (trajectories) in continuous space with a DPP-based diversity measure;(3) Experiments on synthetic data and human motion validate that our method can reliably generate a more diverse set of future trajectories compared to state-of-the-art generative models."}
{"paper_id": 28, "abstract": "In this exploration, we delve into the intricate realm of training neural networks (NN), navigating the complexities that arise when we seek to impose specific structural constraints. To achieve this, we embrace the power of nonsmooth regularization\u2014think L1-norm\u2014and the precision of constraints, such as interval limitations. This endeavor leads us to formulate a constrained nonsmooth nonconvex optimization problem, a challenge worthy of our attention.   To tackle this, we introduce a novel algorithm: the convergent proximal-type stochastic gradient descent (Prox-SGD). Here, we reveal a fascinating insight: when learning rates are judiciously chosen, the momentum of our algorithm begins to mirror the elusive true gradient, a crucial element in our convergence analysis. We establish a significant result: with a probability of 1, every limit point in the sequence produced by our Prox-SGD algorithm converges to a stationary point, a beacon of stability in our optimization journey.  But we do not stop there. We tailor our Prox-SGD to specifically train sparse neural networks and binary neural networks, ensuring that our theoretical insights are not merely abstract concepts but are grounded in reality through extensive numerical experiments. In this paper, we weave together theory and practice, forging a path toward more effective neural network training methodologies.", "introduction": "In this paper, we consider the problem of training neural networks (NN) under constraints and regularization.It is formulated as an optimization problem minimizewhere x is the parameter vector to optimize, y i is the i-th training example which consists of the training input and desired output, and m is the number of training examples.The training loss f is assumed to be smooth (but nonconvex) with respect to x, the regularization r is assumed to be convex (but nonsmooth), proper and lower semicontinuous, and the constraint set X is convex and compact (closed and bounded).When r(x) = 0 and X = R n , stochastic gradient descent (SGD) has been used to solve the optimization problem (1).At each iteration, a minibatch of the m training examples are drawn randomly, and the obtained gradient is an unbiased estimate of the true gradient.Therefore SGD generally moves along the descent direction, see Bertsekas & Tsitsiklis (2000).SGD can be accelerated by replacing the instantaneous gradient estimates by a momentum aggregating all gradient in past iterations.Despite the success and popularity of SGD with momentum, its convergence had been an open problem.Assuming f is convex, analyzing the convergence was first attempted in Kingma & Ba (2015) and later concluded in Reddi et al. (2018).The proof for a nonconvex f was later given in Chen et al. (2019); Lei et al. (2019).In machine learning, the regularization function r is typically used to promote a certain structure in the optimal solution, for example sparsity as in, e.g., feature selection and compressed sensing, or a zero-mean-Gaussian prior on the parameters (Bach et al., 2011;Boyd et al., 2010).It can be interpreted as a penalty function since at the optimal point x of problem (1), the value r(x ) will be small.One nominant example is the Tikhonov regularization r(x) = \u00b5 x 2 2 for some predefined constant \u00b5, and it can be used to alleviate the ill-conditioning and ensure that the magnitude of the weights will not become exceedingly large.Another commonly used regularization, the 1 -norm where r(x) = \u00b5 x 1 = \u00b5 n j=1 |x j | (the convex surrogate of the 0 -norm), would encourage a sparse solution.In the context of NN, it is used to (i) promote a sparse neural network (SNN) to alleviate overfitting and to allow a better generalization, (ii) accelerate the training process, and (iii) prune the network to reduce its complexity, see Louizos et al. (2018) and Gale et al. (2019).Technically, it is difficult to analyze the regularizations as some commonly used convex regularizers are nonsmooth, for example, 1 -norm.In current implementations of TensorFlow, the gradient of |x| is simply set to 0 when x = 0.This amounts to the stochastic subgradient descent method and usually exhibits slow convergence.Other techniques to promote a SNN includes magnitude pruning and variational dropout, see Gale et al. (2019).Although regularization can be interpreted as a constraint from the duality theory, sometimes it may still be more desirable to use explicit constraints, for example,x 2 j \u2264 \u03b1, where the summation is over the weights on the same layer.This is useful when we already know how to choose \u03b1.Another example is the lower and upper bound on the weights, that is, l \u2264 w \u2264 u for some predefined l and u.Compared with regularization, constraints do not encourage the weights to stay in a small neighborhood of the initial weight, see Chapter 7.2 of Goodfellow et al. (2016) for more details.The set X models such explicit constraints, but it poses an additional challenge for stochastic gradient algorithms as the new weight obtained from the SGD method (with or without momentum) must be projected back to the set X to maintain its feasibility.However, projection is a nonlinear operator, so the unbiasedness of the random gradient would be lost.Therefore the convergence analysis for constrained problems is much more involved than unconstrained problems.In this paper, we propose a convergent proximal-type stochastic gradient algorithm (ProxSGD) to train neural networks under nonsmooth regularization and constraints.It turns out momentum plays a central role in the convergence analysis.We establish that with probability (w.p.) 1, every limit point of the sequence generated by ProxSGD is a stationary point of the nonsmooth nonconvex problem (1).This is in sharp contrast to unconstrained optimization, where the convergence of the vanilla SGD method has long been well understood while the convergence of the SGD method with momentum was only settled recently.Nevertheless, the convergence rate of ProxSGD is not derived in the current work and is worth further investigating.To test the proposed algorithm, we consider two applications.The first application is to train a SNN, and we leverage 1 -regularization, that is, minimize(2)The second application is to train a binary neural network (BNN) where the weights (and activations) are either 1 or -1 (see Courbariaux et al. (2015;2016); Hou et al. (2017); Yin et al. (2018); Bai et al. (2019) for more details).To achieve this, we augment the loss function with a term that penalizes the weights if they are not +1 or -1:where \u00b5 is a given penalty parameter.The binary variable a j can be interpreted as a switch for weight x j : when a j = 0, (1 -a j )(x j -1) 2 is activated, and there is a strong incentive for x j to be 1 (the analysis for a j = 1 is similar).Since integer variables are difficult to optimize, we relax a j to be a continuous variable between 0 and 1.To summarize, a BNN can be obtained by solving the following regularized optimization problem under constraints with respect to x and a minimizex,aIf \u00b5 is properly selected (or sufficiently large), the optimal a j will be exactly or close to 0 or 1.Consequently, regularization and constraints offer interpretability and flexibility, which allows us to use more accurate models to promote structures in the neural networks, and the proposed convergent ProxSGD algorithm ensures efficient training of such models."}
{"paper_id": 29, "abstract": "In this exploration, we delve into the intricacies of the learned iterative shrinkage thresholding algorithm (LISTA), a powerful tool for tackling the challenges of sparse coding. Building upon the foundations laid by previous research, we unveil a critical insight: the estimated code components often fall short of expectations, necessitating an infusion of gains. To remedy this, we introduce a sophisticated gated mechanism, crafted with a keen eye for theoretical rigor. The design of these gates draws inspiration from convergence analyses, ensuring that their efficacy is not merely conjectural but firmly established.  But we do not stop there. Recognizing the potential for insufficient step sizes within LISTA, we further enhance our approach with the introduction of overshoot gates\u2014an innovative solution designed to compensate for this shortcoming. Our extensive empirical results not only substantiate our theoretical assertions but also illuminate the transformative potential of our method. Through this work, we aim to forge a path toward more robust sparse coding solutions, merging theory with practical application in a manner that echoes the depth and complexity of a well-crafted tale.", "introduction": "Sparse coding serves as the foundation of many machine learning applications, e.g., the direction-ofarrival estimation (Xu et al., 2012), signal denoising (Elad & Aharon, 2006), and super resolution imaging (Yang et al., 2010).In general, it aims to recover an inherently sparse vector x s \u2208 R n from an observation y \u2208 R m corrupted by a noise vector \u03b5 \u2208 R m .That is,in which A \u2208 R m\u00d7n is an over-complete basis matrix.The problem of recovering x s , however, is a challenging task, in which the main difficulties are to incorporate the sparse constraint which is nonconvex and to further determine the indices of its non-zero elements, i.e., the support of the vector.A reasonable solution to the problem is to use convex functions as surrogates to relax the constraint of sparsity, among which the most classical one probably is the l 1 -norm penalty.Such a problem is carefully studied in Lasso (Tibshirani, 1996), and it can be solved via least angle regression (Efron et al., 2004), the iterative shrinkage and thresholding algorithm (ISTA) (Daubechies et al., 2004), etc.Despite the simplicity, these conventional solvers suffer from critical shortcomings.Taking ISTA as an example, we know that 1) it converges very slowly with only a sublinear rate (Beck & Teboulle, 2009), 2) the correlation between each of the two columns of A should be relatively low.In recent years, deep learning (LeCun et al., 2015) methods have achieved remarkable successes.Deep neural networks (DNNs) have been proven both effective and efficient in dealing with many tasks, including image classification (He et al., 2016), object detection (Girshick, 2015), speech recognition (Hinton et al., 2012), and also sparse coding (Gregor & LeCun, 2010;Wang et al., 2016;Borgerding et al., 2017;He et al., 2017;Zhang & Ghanem, 2018;Chen et al., 2018;Liu et al., 2019;Sulam et al., 2019).The core idea behind deep learning-based sparse coding is to train DNNs to approximate the optimal sparse code.For instance, an initial work of Gregor and LeCun's (2010) takes the inspiration from ISTA and develops an approximator named learned ISTA (LISTA), which is structurally similar to a recurrent neural network (RNN).It has been demonstrated both empirically and theoretically that LISTA is superior to ISTA (Wang et al., 2016;Moreau & Bruna, 2017;Giryes et al., 2018;Chen et al., 2018).Nevertheless, it is also uncontroversial that there exists much room for further enhancing it.In this paper, we delve deeply into the foundation of (L)ISTA and discover possible weaknesses of LISTA.First and foremost, we know from prior arts (Chen et al., 2018;Liu et al., 2019) that LISTA tends to learn large enough biases to achieve no \"false positive\" in the support of generated codes and further ensure linear convergence, and we prove that this tendency, however, also makes the magnitude of the code components being lower than that of the ground-truth.That said, there probably exists a requirement of gains in the code estimations.Second, regarding the optimization procedure of ISTA as to minimize an upper bound of its objective function at each step, we conjecture that the element-wise update of (L)ISTA normally \"lags behind\" the optimal solution, which suggests that it requires overshoots to reach the optimum, just like what has been suggested in fast ISTA (FISTA) (Beck & Teboulle, 2009) and learned FISTA (LFISTA) (Moreau & Bruna, 2017).In this paper, our main contributions are summarized as follows:\u2022 We discover weaknesses of LISTA by theoretically analyzing its optimization procedure, for mitigating which we introduce gain gates and overshoot gates, akin to update gate and reset gate mechanisms in the gated recurrent unit (GRU) Cho et al. (2014).\u2022 We provide convergence analyses for LISTA (with or without gates), which further give rise to conditions on which the performance of our method with gain gates can be guaranteed.A practical case is considered, where the assumption of no \"false positive\" is relaxed.\u2022 Insightful expressions for the gates are presented.In comparison with state-of-the-art sparse coding networks (not limited to previous extensions to LISTA), our method achieves superior performance.It also applies to variants of LISTA, e.g., LFSITA (Moreau & Bruna, 2017) and ALISTA (Liu et al., 2019).Notations: In this paper, unless otherwise clarified, vectors and matrices are denoted by lowercase and uppercase characters, respectively.For vectors/matrices originally introduced without any subscript, adding a subscript (e.g., i) indicates its element/column at the corresponding position.For instance, for x \u2208 R n , x i represents the i-th element of the vector, and W :,i and W i,: denote the i-th column and row of a matrix W respectively.While for vectors introduced with subscripts already, e.g., x s , we use (x s ) i to denote its i-th element.The operator is used to indicate element-wise multiplication of two vectors.The support of a vector is denoted as supp(x) := {i|x i = 0}.We use sup xs as the simplified form of sup xs\u2208X (B,s,0) , see Assumption 1 for the definition of X (B, s, 0)."}
{"paper_id": 30, "abstract": "In the realm of deep learning, Graph Neural Networks (GNNs) have emerged as a beacon of promise for deciphering the intricate tapestry of graph-structured data. Yet, a perplexing conundrum arises: as we stack layer upon layer, infusing our networks with non-linearity, their predictive prowess often stagnates or even diminishes. To unravel this mystery, we delve into the expressive potential of GNNs, exploring their asymptotic behaviors as the number of layers approaches infinity.  Our approach hinges on reimagining the forward propagation of a Graph Convolutional Network (GCN)\u2014a prominent variant in the GNN landscape\u2014as a specialized dynamical system. Through this lens, we reveal that when the weights of a GCN align with the spectral properties of the (augmented) normalized Laplacian, its output converges exponentially towards a realm of signals that encapsulate the essence of connected components and node degrees, serving merely to differentiate nodes.  This theoretical framework empowers us to forge a connection between the expressive capabilities of GCNs and the topological intricacies encoded within the graph spectra. To illustrate our findings, we scrutinize the asymptotic behavior of GCNs within the context of the Erd\u0151s\u2013R\u00e9nyi graph. Our analysis uncovers that in sufficiently dense and expansive Erd\u0151s\u2013R\u00e9nyi graphs, a wide array of GCNs grapples with \"information loss\" as the layers stretch towards infinity, a phenomenon occurring with high probability.  Armed with this understanding, we propose a principled guideline for weight normalization in graph NNs. Our experimental validation reveals that this weight scaling not only enhances the predictive performance of GCNs on real-world datasets but also paves the way for more robust applications of graph-based learning. For those eager to explore our findings further, the code is readily accessible at https://github.com/delta2323/gnn-asymptotics.", "introduction": "Motivated by the success of Deep Learning (DL), several attempts have been made to apply DL models to non-Euclidean data, particularly, graph-structured data such as chemical compounds, social networks, and polygons.Recently, Graph Neural Networks (graph NNs) (Duvenaud et al., 2015;Li et al., 2016;Gilmer et al., 2017;Hamilton et al., 2017;Kipf & Welling, 2017;Nguyen et al., 2017;Schlichtkrull et al., 2018;Battaglia et al., 2018;Xu et al., 2019;Wu et al., 2019a) have emerged as a promising approach.However, despite their practical popularity, theoretical research of graph NNs has not been explored extensively.The characterization of DL model expressive power, i.e., to identify what function classes DL models can (approximately) represent, is a fundamental question in theoretical research of DL.Many studies have been conducted for Fully Connected Neural Networks (FNNs) (Cybenko, 1989;Hornik, 1991;Hornik et al., 1989;Barron, 1993;Mhaskar, 1993;Sonoda & Murata, 2017;Yarotsky, 2017) and Convolutional Neural Networks (CNNs) (Petersen & Voigtlaender, 2018;Zhou, 2018;Oono & Suzuki, 2019).For such models, we have theoretical and empirical justification that deep and nonlinear architectures can enhance representation power (Telgarsky, 2016;Chen et al., 2018b;Zhou & Feng, 2018).However, for graph NNs, several papers have reported that node representations go indistinguishable (known as over-smoothing) and prediction performances severely degrade when we stack many layers (Kipf & Welling, 2017;Wu et al., 2019b;Li et al., 2018).Besides, Wu et al. (2019a) reported that graph NNs achieved comparable performance even if they removed intermediate non-linear functions.These studies posed a question about the current architecture and made us aware of the need for the theoretical analysis of the graph NN expressive power.In this paper, we investigate the expressive power of graph NNs by analyzing their asymptotic behaviors as the layer size goes to infinity.Our theory gives new theoretical conditions under which neither layer stacking nor non-linearity contributes to improving expressive power.We consider a specific dynamics that includes a transition defining a Markov process and the forward propagation of a Graph Convolutional Network (GCN) (Kipf & Welling, 2017), which is one of the most popular graph NN variants, as special cases.We prove that under certain conditions, the dynamics exponentially approaches a subspace that is invariant under the dynamics.In the case of GCN, the invariant space is a set of signals that correspond to the lowest frequency of graph spectra and that have \"no information\" other than connected components and node degrees for a node classification task whose goal is to predict the nodes' properties in a graph.The rate of the distance between the output and the invariant space is O((s\u03bb) L ) where s is the maximum singular values of weights, \u03bb is typically a quantity determined by the spectra of the (augmented) normalized Laplacian, and L is the layer size.See Sections 3.3 (general case) and 4 (GCN case) for precise statements.We can interpret our theorem as the generalization of the well-known property that if a finite and discrete Markov process is irreducible and aperiodic, it exponentially converges to a unique equilibrium and the eigenvalues of its transition matrix determine the convergence rate (see, e.g., Chung & Graham (1997)).Different from the Markov process case, which is linear, the existence of intermediate non-linear functions complicates the analysis.We overcame this problem by leveraging the combination of the ReLU activation function (Krizhevsky et al., 2012) and the positivity of eigenvectors of the Laplacian associated with the smallest positive eigenvalues.Our theory enables us to investigate asymptotic behaviors of graph NNs via the spectral distribution of the underlying graphs.To demonstrate this, we take GCNs defined on the Erd\u0151s -R\u00e9nyi graph G N,p , which has N nodes and each edge appears independently with probability p, for an example.We prove that if log N pN = o(1) as a function of N , any GCN whose weights have maximum singular values at most C N p log(N/\u03b5) approaches the \"information-less\" invariant space with probability at least 1 -\u03b5, where C is a universal constant.Intuitively, if the graph on which we define graph NNs is sufficiently dense, graph-convolution operations mix signals on nodes fast and hence the feature maps lose information for distinguishing nodes quickly.Our contributions are as follows:\u2022 We relate asymptotic behaviors of graph NNs with the topological information of underlying graphs via the spectral distribution of the (augmented) normalized Laplacian.\u2022 We prove that if the weights of a GCN satisfy conditions determined by the graph spectra, the output of the GCN carries no information other than the node degrees and connected components for discriminating nodes when the layer size goes to infinity (Theorems 1, 2).\u2022 We apply our theory to Erd\u0151s -R\u00e9nyi graphs as an example and show that when the graph is sufficiently dense and large, many GCNs suffer from the information loss (Theorem 3).\u2022 We propose a principled guideline for weight normalization of graph NNs and empirically confirm it using real data."}
{"paper_id": 31, "abstract": "In the ever-evolving realm of artificial intelligence, the quest for agents capable of interpreting and executing natural language instructions has emerged as a captivating frontier. Yet, as we delve deeper, we find that even the most sophisticated of humans can stumble upon the ambiguities woven into the fabric of spoken language. To navigate this treacherous terrain, we turn our gaze toward a more structured approach: the use of formal programming languages to articulate tasks with precision and clarity.  In response to this challenge, we introduce a modular framework designed to learn and execute tasks as delineated by these formal programs. Our framework possesses the remarkable ability to discern the unique circumstances surrounding each task, enabling it to adapt its multitask policy in real-time to tackle the subtasks that arise. This dynamic adaptability not only enhances performance but also reflects the nuanced understanding required to navigate complex instructions.  Through rigorous experimentation within the vibrant confines of a 2D Minecraft environment, our findings reveal that this innovative framework not only masters the execution of programmatic instructions with remarkable reliability but also demonstrates impressive zero-shot generalization capabilities when faced with more intricate directives. Furthermore, we validate the efficacy of our modulation mechanism, which plays a pivotal role in refining the multitask policy.  To deepen our understanding, we conduct a comprehensive analysis comparing various models that learn from both programs and natural language instructions in an end-to-end manner. This exploration not only sheds light on the strengths and weaknesses of each approach but also paves the way for future advancements in the field. In this dance of language and logic, we find the potential to forge agents that can truly comprehend and act upon the world around them.", "introduction": "Humans are capable of leveraging instructions to accomplish complex tasks.A comprehensive instruction usually comprises a set of descriptions detailing a variety of situations and the corresponding subtasks that are required to be fulfilled.To accomplish a task, we can leverage instructions to estimate the progress, recognize the current state, and perform corresponding actions.For example, to make a gourmet dish, we can follow recipes and procedurally create the desired dish by recognizing what ingredients and tools are missing, what alternatives are available, and what corresponding preparations are required.With sufficient practice, we can improve our ability to perceive (e.g.knowing when food is well-cooked) as well as master cooking skills (e.g.cutting food into same-sized pieces), and eventually accomplish difficult recipes.Can machines likewise learn to follow and exploit comprehensive instructions like humans?Utilizing expert demonstrations to instruct agents has been widely studied in (Finn et al., 2017;Yu et al., 2018b;Xu et al., 2018;Pathak et al., 2018;Stadie et al., 2017;Duan et al., 2017;Wang et al., 2017b).However, demonstrations could be expensive to obtain and are less flexible (e.g.altering subtask orders in demonstrations is nontrivial).On the other hand, natural language instructions are flexible and expressive (Malmaud et al., 2014;Jermsurawong & Habash, 2015;Kiddon et al., 2015;Misra et al., 2018;Fried et al., 2018;Kaplan et al., 2017;Bahdanau et al., 2019).Yet, language has the caveat of being ambiguous even to humans, due to its lacking of structure as well as unclear coreferences and entities.Andreas et al. (2017a); Oh et al. (2017) investigate a hierarchical approach, where the instructions consist of a set of symbolically represented subtasks.Nonetheless, those instructions are not a function of states (i.e.describe a variety of circumstances and the corresponding desired subtasks), which substantially limits their expressiveness.We propose to utilize programs, written in a formal language, as a structured, expressive, and unambiguous representation to specify tasks.Specifically, we consider programs, which are composed of control flows (e.g.if/else and loops), environmental conditions, as well as corresponding subtasks, as shown in Figure 1.Not only do programs have expressiveness by describing diverse situations (e.g. a river exists) and the corresponding subtasks which are required to be executed (e.g.mining wood), but they are also unambiguous due to their explicit scoping.To study the effectiveness of using programs as task specifications, we introduce a new problem, where we aim to develop a framework which learns to comprehend a task specified by a program as well as perceive and interact with the environment to accomplish the task.To address this problem, we propose a modular framework, program guided agent, which exploits the structural nature of programs to decompose and execute them as well as learn to ground program tokens with the environment.Specifically, our framework consists of three modules: (1) a program interpreter that leverages a grammar provided by the programming language to parse and execute a program, (2) a perception module that learns to respond to conditional queries (e.g.is_there [River]) produced by the interpreter, and (3) a policy that learns to fulfill a variety of subtasks (e.g.mine(Wood)) extracted from the program by the interpreter.To effectively instruct the policy with symbolically represented subtasks, we introduce a learned modulation mechanism that leverages a subtask to modulate the encoded state features instead of concatenating them.Our framework (shown in Figure 3) utilizes a rule-based program interpreter to deal with programs as well as learning perception module and policy when it is necessary to perceive or interact with the environment.With this modularity, our framework can generalize to more complex program-specified tasks without additional learning.To evaluate the proposed framework, we consider a Minecraft-inspired 2D gridworld environment, where an agent can navigate itself across different terrains and interact with objects, similar to Andreas et al. (2017a);Sohn et al. (2018).A corresponding domain-specific language (DSL) defines the rules of constructing programs for instructing an agent to accomplish certain tasks.Our proposed framework demonstrates superior generalization ability -learning from simpler tasks while generalizing to complex tasks.We also conduct extensive analysis on various end-to-end learning models which learns from not only program instructions but also natural language descriptions.Furthermore, our proposed learned policy modulation mechanism yields a better learning efficiency compared to other commonly used methods that simply concatenate a state and goal."}
{"paper_id": 32, "abstract": "In the ever-evolving realm of Natural Language Processing, unsupervised text encoding models have emerged as a beacon of innovation, propelling the field into uncharted territories. At the heart of this revolution lies a profound concept: harnessing the power of neural networks to transmute words into vector space representations\u2014embeddings that capture the essence of their positions and contexts within sentences. This transformation paves the way for seamless end-to-end training across a myriad of downstream tasks.  Yet, as we delve deeper, we uncover a parallel narrative unfolding within the domain of spatial analysis. Here, the challenge is to weave together the absolute positions and spatial contexts of geographic entities, such as Points of Interest (POIs), into cohesive models. The quest for a versatile representation model capable of encapsulating the intricacies of space holds immense promise for a diverse array of applications. Regrettably, the landscape remains barren of a comprehensive solution, often relegating practitioners to simplistic discretization or conventional feed-forward networks. Moreover, the intricate distributions that arise from Geographic Information Systems (GIS) data remain largely unaddressed.  Intriguingly, groundbreaking research in Neuroscience has illuminated the role of grid cells in mammals, revealing their ability to provide a multi-scale, periodic representation that serves as a vital metric for location encoding\u2014essential for navigating and recognizing our surroundings. Inspired by these insights, we introduce Space2Vec, a novel representation learning model designed to encode not only the absolute positions but also the spatial relationships of places.  To validate our approach, we embark on a series of experiments utilizing two real-world geographic datasets across distinct tasks: first, predicting the types of POIs based on their positions and contexts; second, employing geo-locations for image classification. The results are compelling. Space2Vec, with its multi-scale representations, decisively outshines established machine learning methodologies, including RBF kernels, multi-layer feed-forward networks, and tile embedding techniques, in both location modeling and image classification endeavors.  A thorough analysis reveals that while traditional baselines may excel at capturing distributions at a singular scale, they falter when confronted with the complexities of others. In stark contrast, Space2Vec\u2019s multi-scale framework adeptly navigates these diverse distributions, showcasing its capacity to adapt and thrive across varying scales. Thus, we stand at the precipice of a new understanding, where the fusion of neuroscience and spatial analysis opens doors to unprecedented possibilities in representation learning.", "introduction": "Unsupervised text encoding models such as Word2Vec (Mikolov et al., 2013), Glove (Pennington et al., 2014), ELMo (Peters et al., 2018), and BERT (Devlin et al., 2018) have been effectively utilized in many Natural Language Processing (NLP) tasks.At their core they train models which encode words into vector space representations based on their positions in the text and their context.A similar situation can be encountered in the field of Geographic Information Science (GIScience).For example, spatial interpolation aims at predicting an attribute value, e.g., elevation, at an unsampled location based on the known attribute values of nearby samples.Geographic information has become an important component to many tasks such as fine-grained image classification (Mac Aodha et al., 2019), point cloud classification and semantic segmentation (Qi et al., 2017), reasoning about Point of Interest (POI) type similarity (Yan et al., 2017), land cover classification (Kussul et al., 2017), and geographic question answering (Mai et al., 2019b).Developing a general model for vector space representation of any point in space would pave the way for many future applications.However, existing models often utilize specific methods to deal with geographic information and often disregards geographic coordinates.For example, Place2Vec (Yan et al., 2017) converts the coordinates of POIs into spatially collocated POI pairs within certain distance bins, and does not preserve information about the (cardinal) direction between points.Li et al. (2017) propose DCRNN for traffic forecasting in which the traffic sensor network is converted to a distance weighted graph which necessarily forfeits information about the spatial layout of sensors.There is, however, no general representation model beyond simply applying discretization (Berg et al., 2014;Tang et al., 2015) or feed-forward nets (Chu et al., 2019;Mac Aodha et al., 2019) to coordinates.A key challenge in developing a general-purpose representation model for space is how to deal with mixtures of distributions with very different characteristics (see an example in Figure 1), which often emerges in spatial datasets (McKenzie et al., 2015).For example, there are POI types with clustered distributions such as women's clothing, while there are other POI types with regular distributions such as education.These feature distributions co-exist in the same space, and yet we want a single representation to accommodate all of them in a task such as location-aware image classification (Mac Aodha et al., 2019).Ripley's K is a spatial analysis method used to describe point patterns over a given area of interest.Figure 1c shows the K plot of several POI types in Las Vegas.One can see that as the radius grows the numbers of POIs increase at different rates for different POI types.In order to see the relative change of density at different scales, we renormalize the curves by each POI type's density and show it in log scale in Figure 1d.One can see two distinct POI type groups with different distribution patterns with clustered and even distributions.If we want to model the distribution of these POIs by discretizing the study area into tiles, we have to use small grid sizes for women's clothing while using larger grid sizes for educations because smaller grid sizes lead to over-parameterization of the model and overfitting.In order to jointly describe these distributions and their patterns, we need an encoding method which supports multi-scale representations.Nobel Prize winning Neuroscience research (Abbott & Callaway, 2014) has demonstrated that grid cells in mammals provide a multi-scale periodic representation that functions as a metric for location encoding, which is critical for integrating self-motion.Moreover, Blair et al. (2007) show that the multi-scale periodic representation of grid cells can be simulated by summing three cosine grating functions oriented 60 \u02ddapart, which may be regarded as a simple Fourier model of the hexagonal lattice.This research inspired us to encode locations with multi-scale periodic representations.Our assumption is that decomposed geographic coordinates helps machine learning models, such as deep neural nets, and multi-scale representations deal with the inefficiency of intrinsically single-scale methods such as RFB kernels or discretization (tile embeddings).To validate this intuition, we propose an encoder-decoder framework to encode the distribution of point-features 2 in space and train such a model in an unsupervised manner.This idea of using sinusoid functions with different frequencies to encode positions is similar to the position encoding proposed in the Transformer model (Vaswani et al., 2017).However, the position encoding model of Transformer deals with a discrete 1D space -the positions of words in a sentence -while our model works on higher dimensional continuous spaces such as the surface of earth.In summary, the contributions of our work are as follows:1. We propose an encoder-decoder encoding framework called Space2Vec using sinusoid functions with different frequencies to model absolute positions and spatial contexts.We also propose a multi-head attention mechanism based on context points.To the best of our knowledge, this is the first attention model that explicitly considers the spatial relationships between the query point and context points.2. We conduct experiments on two real world geographic data for two different tasks: 1) predicting types of POIs given their positions and context, 2) image classification leveraging their geo-locations.Space2Vec outperforms well-established encoding methods such as RBF kernels, multi-layer feed-forward nets, and tile embedding approaches for location modeling and image classification.3. To understand the advantages of Space2Vec we visualize the firing patterns (response maps) of location models' encoding layer neurons and show how they handle spatial structures at different scales by integrating multi-scale representations.Furthermore the firing patterns for the spatial context models neurons give insight into how the grid-like cells capture the decreasing distance effect with multi-scale representations."}
{"paper_id": 33, "abstract": "In the intricate realm of graph representation, where the threads of unsupervised and semi-supervised learning intertwine, this paper embarks on a quest to unravel the mysteries of whole graph representations. These representations hold the key to unlocking a multitude of real-world applications, from predicting the enigmatic properties of molecules to deciphering the complex dynamics of social networks.   While traditional graph kernel methods have served as reliable guides in this journey, offering straightforward yet effective fixed-length representations, they are often shackled by their reliance on hand-crafted designs, leading to limitations in generalization. Recent advancements, such as those inspired by language models\u2014like graph2vec\u2014have attempted to chart new territories, yet they frequently focus on mere fragments of the whole, examining only specific substructures like subtrees.  Drawing inspiration from the latest breakthroughs in unsupervised representation learning, we introduce a groundbreaking approach known as InfoGraph. This method seeks to maximize the mutual information between the graph-level representation and the myriad representations of substructures across various scales\u2014nodes, edges, triangles, and beyond. In doing so, InfoGraph captures the essence of the data, weaving together the shared aspects of these diverse substructures into a cohesive whole.  But our journey does not end there. We further unveil InfoGraph*, an extension crafted for semi-supervised landscapes. InfoGraph* harnesses the power of mutual information between the unsupervised representations learned by InfoGraph and those gleaned from existing supervised methods. This interplay allows the supervised encoder to draw wisdom from unlabeled data, all while maintaining the latent semantic space that aligns with the current supervised task.  Through rigorous experimentation in graph classification and molecular property prediction, our findings reveal that InfoGraph outshines the state-of-the-art baselines, while InfoGraph* stands shoulder to shoulder with the best semi-supervised models. Join us as we explore this new frontier in graph representation learning, where every node and edge tells a story, and every structure holds the promise of discovery.", "introduction": "Graphs have proven to be an effective way to represent very diverse types of data including social networks Newman & Girvan (2004), biological reaction networksPavlopoulos et al. (2011), proteinprotein interactions Krogan et al. (2006), the quantum mechanical properties of individual molecules Xie & Grossman (2018); Jin et al. (2018), and many more.Graphs provide explicit information about the coupling between individual units in a larger part along with a well defined framework for assigning properties to the nodes and the edges connecting them.There has been a significant amount of previous work done studying many aspects of graphs including link prediction Gao et al. (2011); Wang et al. (2011) and node prediction Blei et al. (2003).Due to its flexibility, graph-like data structures can capture rich information which is critical in many applications.At the lowest level, much work has been done on learning node representations-low-dimensional vector embeddings of individual nodes Perozzi et al. (2014); Tang et al. (2015); Grover & Leskovec (2016).Another field that has attracted a large amount of attention recently is learning representations of entire graphs.Such a problem is critical in a variety of applications such as predicting the properties of molecular graphs in both drug discovery and material science Chen et al. (2019b;a).There has been some recent progress based on neural message passing algorithms Gilmer et al. (2017); Xie & Grossman (2018), which learn the representations of entire graphs in a supervised way.These methods have been shown achieving state-of-the-art results on a variety of different prediction tasks Kipf et al. (2018); Xie & Grossman (2018); Gilmer et al. (2017); Chen et al. (2019a).However, one of the most difficult obstacles for supervised learning on graphs is that it is often very costly or even impossible to collect annotated labels.For example, in the chemical domain labels are typically produced with a costly Density Functional Theory (DFT) calculation.One option is to use semi-supervised methods which combine a small handful of labels with a larger, unlabeled, dataset.In real-world applications, partially labeled datasets are common, making tools that are able to efficiently utilize the present labels particularly useful.Coming up with methods that are able to learn unsupervised representations of an entire graph, as opposed to nodes, is an important step in working with unlabeled or partially labeled graphs Narayanan et al. (2017); Hu et al. (2019); Nguyen et al. (2017).For example, there exists work that explores pre-training techniques for graphs to improve generalization Hu et al. (2019).Another common approach to unsupervised representation learning on graphs is through graph kernels Pr\u017eulj (2007); Kashima et al. (2003); Orsini et al. (2015).However, many of these methods do not provide explicit graph embeddings which many machine learning algorithms operate on.Furthermore, the handcrafted features of graph kernels lead to high dimensional, sparse or non-smooth representations and thus result in poor generalization performance, especially on large datasets Narayanan et al. (2017).Unsupervised learning of latent representations is also an important problem in other domains, such as image generation Kingma & Welling (2013); Kim & Mnih (2018) and natural language processing Mikolov et al. (2013a).A recent work introduced Deep Infomax, a method that maximizes the mutual information content between the input data and the learned representation Hjelm et al. (2018).This method outperforms other methods on many unsupervised learning tasks.Motivated by Deep InfoMax Hjelm et al. (2018), we aim to use mutual information maximization for unsupervised representation learning on the entire graph.Specifically, our objective is to maximize the mutual information between the representations of entire graphs and the representations of substructures of different granularity.We name our model InfoGraph.We also propose a semi-supervised learning model which we name InfoGraph*.We employ a student-teacher framework similar to Mean-Teacher method Tarvainen & Valpola (2017).We maximize the mutual information between intermediate representations of the two models so that the student model learns from the teacher model.The student model is trained on the labeled data using a supervised objective function while the teacher model is trained on unlabeled data with InfoGraph.Using InfoGraph*, we achieve performance competitive with state-of-the-art methods on molecular property prediction.We summarize our contributions as follows:\u2022 We propose InfoGraph, an unsupervised graph representation learning method based on Deep InfoMax (DIM) Hjelm et al. (2018).\u2022 We show that InfoGraph can be extended to semi-supervised prediction tasks on graphs.\u2022 We empirically show that InfoGraph surpasses state-of-the-art performance on graph classification tasks with unsupervised learning and obtains performance comparable with state-ofart methods on molecular property prediction tasks using semi-supervised learning."}
{"paper_id": 34, "abstract": "In the ever-evolving landscape of machine learning, neural ordinary differential equations (ODEs) have emerged as a beacon of innovation, drawing the gaze of researchers across diverse fields. While previous inquiries have delved into their optimization intricacies and approximation prowess, the question of their robustness looms large and unanswered. In this study, we embark on a quest to illuminate this crucial aspect, investigating the robustness of neural ODEs through both empirical and theoretical lenses.  Our journey begins with a comprehensive empirical analysis of neural ODE-based networks, affectionately dubbed ODENets. We subject these networks to a variety of perturbations, scrutinizing how their outputs respond to the chaos of input disturbances. In a surprising twist, we discover that ODENets exhibit a remarkable resilience, standing firm against the onslaught of random Gaussian noise and cunning adversarial attacks\u2014qualities that set them apart from their conventional convolutional neural network (CNN) counterparts.  But why, you may ask? Herein lies an intriguing revelation: the flow of a continuous-time ODE possesses a unique characteristic\u2014its integral curves refuse to intersect. This property provides a profound insight into the underlying robustness of ODENets, suggesting that their very design lends itself to stability in the face of perturbation.   Encouraged by these findings, we propose a novel enhancement to the standard neural ODE framework: the time-invariant steady neural ODE (TisODE). This innovative approach harnesses the time-invariant nature of the flow to regularize responses on perturbed data, imposing a steady-state constraint that fortifies the network\u2019s resilience. Our experiments reveal that TisODE not only surpasses its vanilla predecessors but also harmonizes beautifully with cutting-edge architectural techniques, paving the way for the construction of even more robust deep networks.  In essence, this work not only fills a critical void in the understanding of neural ODEs but also sets the stage for their evolution into foundational elements of resilient deep learning architectures.", "introduction": "Neural ordinary differential equations (Chen et al., 2018) form a family of models that approximate nonlinear mappings by using continuous-time ODEs.Due to their desirable properties, such as invertibility and parameter efficiency, neural ODEs have attracted increasing attention recently (Dupont et al., 2019;Liu et al., 2019).For example, Grathwohl et al. (2018) proposed a neural ODE-based generative model-the FFJORD-to solve inverse problems; Quaglino et al. (2019) used a higher-order approximation of the states in a neural ODE, and proposed the SNet to accelerate computation.Along with the wider deployment of neural ODEs, robustness issues come to the fore.However, the robustness of neural ODEs is still yet unclear.In particular, it is unclear how robust neural ODEs are in comparison to the widely-used CNNs.Robustness properties of CNNs have been studied extensively.In this work, we present the first systematic study on exploring the robustness properties of neural ODEs.To do so, we consider the task of image classification.We expect that results would be similar for other machine learning tasks such as regression.Neural ODEs are dimension-preserving mappings, but a classification model transforms a high-dimensional input-such as an image-into an output whose dimension is equal to the number of classes.Thus, we consider the neural ODE-based classification network (ODENet) whose architecture is shown in Figure 1.An ODENet consists of three components: the feature extractor (FE) consists of convolutional layers which maps an input datum to a multi-channel feature map, a neural ODE that serves as the nonlinear representation mapping (RM), and the fully-connected classifier (FCC) that generates a prediction vector based on the output of the RM.The robustness of a classification model can be evaluated through the lens of its performance on perturbed images.To comprehensively investigate the robustness of neural ODEs, we perturb original images with commonly-used perturbations, namely, random Gaussian noise (Szegedy et al., 2013) and harmful adversarial examples (Goodfellow et al., 2014;Madry et al., 2017).We conduct experiments in two common settings-training the model only on authentic non-perturbed images and training the model on authentic images as well as the Gaussian perturbed ones.We observe that ODENets are more robust compared to CNN models against all types of perturbations in both settings.We then provide an insightful understanding of such intriguing robustness of neural ODEs by exploiting a certain property of the flow (Dupont et al., 2019), namely that integral curves that start at distinct initial states are nonintersecting.The flow of a continuous-time ODE is defined as the family of solutions/paths traversed by the state, starting from different initial points, and an integral curve is a specific solution for a given initial point.The non-intersecting property indicates that an integral curve starting from some point is constrained by the integral curves starting from that point's neighborhood.Thus, in an ODENet, if a correctly classified datum is slightly perturbed, the integral curve associated to its perturbed version would not change too much from the original one.Consequently, the perturbed datum could still be correctly classified.Thus, there exists intrinsic robustness regularization in ODENets, which is absent from CNNs.Motivated by this property of the neural ODE flow, we attempt to explore a more robust neural ODE architecture by introducing stronger regularization on the flow.We thus propose a Time-Invariant Steady neural ODE (TisODE).The TisODE removes the time dependence of the dynamics in an ODE and imposes a steady-state constraint on the integral curves.Removing the time dependence of the derivative results in the time-invariant property of the ODE.To wit, given a solution z 1 (t), another solution !z 1 (t), with an initial state !z 1 (0) = z 1 (T \u2032 ) for some T \u2032 > 0, can be regarded as the -T \u2032 -shift version of z 1 (t).Such a time-invariant property would make bounding the difference between output states convenient.To elaborate, let the output of a neural ODE correspond to states at time T > 0. By the time-invariant property, the difference between outputs, \"! z 1 (T )z 1 (T )\", equals to \"z 1 (T + T \u2032 )z 1 (T )\".To control this distance, a steady-state regularization term is introduced to the overall objective to constrain the change of a state after time exceeds T .With the time-invariant property and the steady-state term, we show that TisODE even is more robust.We do so by evaluating the robustness of TisODE-based classifiers against various types of perturbations and observe that such models are more robust than vanilla ODE-based models.In addition, some other effective architectural solutions have also been recently proposed to improve the robustness of CNNs.For example, Xie et al. (2017) randomly resizes or pads zeros into test images to destroy the specific structure of adversarial perturbations.Besides, the model proposed by Xie et al. (2019) contains feature denoising filters to remove the feature-level patterns of adversarial examples.We conduct experiments to show that our proposed TisODE can work seamlessly and in conjunction with these methods to further boost the robustness of deep models.Thus, the proposed TisODE can be used as a generally applicable and effective component for improving the robustness of deep models.In summary, our contributions are as follows.Firstly, we are the first to provide a systematic empirical study on the robustness of neural ODEs and find that the neural ODE-based models are more robust compared to conventional CNN models.This finding inspires new applications of neural ODEs in improving robustness of deep models, a problem that concerns many deep learning theorists and practitioners alike.Secondly, we propose the TisODE method, which is simple yet effective in significantly boosting the robustness of neural ODEs.Moreover, the proposed TisODE can also be used in conjunction with other state-of-the-art robust architectures.Thus, TisODE can serve as a drop-in module to improve the robustness of deep models effectively.It has been shown that a residual block (He et al., 2016) can be interpreted as the discrete approximation of an ODE by setting the discretization step to be one.When the discretization step approaches zero, it yields a family of neural networks, which are called neural ODEs (Chen et al., 2018).Formally, in a neural ODE, the relation between input and output is characterized by the following set of equations:where f \u03b8 : R d \u00d7 [0, \u221e) \u2192 R d denotes the trainable layers that are parameterized by weights \u03b8 and z : [0, \u221e) \u2192 R d represents the d-dimensional state of the neural ODE.We assume that f \u03b8 is continuous in t and globally Lipschitz continuous in z.In this case, the input z in of the neural ODE corresponds to the state at t = 0, and the output z out is associated to the state at some T \u2208 (0, \u221e).Because f \u03b8 governs how the state changes with respect to time t, we also use f \u03b8 to denote the dynamics of the neural ODE.Given input z in , the output z out can be computed by solving the ODE in (1).If T is fixed, the output z out only depends on the input z in and the dynamics f \u03b8 , which also corresponds to the weighted layers in the neural ODE.Therefore, the neural ODE can be represented as the d-dimensional function \u03c6 T (\u2022, \u2022) of the input z in and the dynamics f \u03b8 , i.e.,The terminal time T of the output state z(T ) is set to be 1 in practice.Several methods have been proposed for training neural ODEs, such as the adjoint sensitivity method (Chen et al., 2018), SNet (Quaglino et al., 2019), and the auto-differentiation technique (Paszke et al., 2017).In this work, we use the most straightforward technique, i.e., updating the weights \u03b8 with the autodifferentiation technique in the PyTorch framework."}
{"paper_id": 35, "abstract": "In the realm of human learning, the ability to derive task-agnostic priors from interactive experiences stands as a testament to our adaptability. This paper introduces the Scoring-Aggregating-Planning (SAP) framework\u2014an innovative approach that harnesses these priors, enabling the mastery of novel tasks without the need for finetuning. At its core, SAP discovers a neural score function that evaluates local state-action pairs, aggregating these insights to form an approximation of the trajectory's quality. Additionally, it integrates a self-supervised dynamics model, empowering it to plan effectively in the face of unseen challenges. Unlike previous methods that demand extensive on-policy interactions or rely on expert data, our framework thrives on the richness of off-policy, albeit imperfect, data. The result is a versatile policy capable of adapting to new tasks with remarkable ease. Our experiments reveal that SAP not only meets but surpasses the performance of baseline methods across diverse applications, from gridworlds to complex robotics and immersive video games.", "introduction": "While deep Reinforcement Learning (RL) methods have shown impressive performance on video games (Mnih et al., 2015) and robotics tasks (Schulman et al., 2015;Lillicrap et al., 2015), they solve each problem tabula rasa.Hence, it will be hard for them to generalize to new tasks without re-training even due to small changes.However, humans can quickly adapt their skills to a new task that requires similar priors e.g.physics, semantics, affordances to past experience.The priors can be learned from a spectrum of examples ranging from perfect demonstrative ones that accomplish certain tasks to aimless exploration.A parameterized intelligent agent \"Mario\" which learns to move to the right in the upper level in Figure 1 would fail to transfer the priors from to the lower level in Figure 1 and further play the game in the new level because change of configurations and background, e.g.different shapes of ladder, new fence.When an inexperienced human player is controlling the Mario to move it to the right in the upper level, it might take many trials for him/her to realize the falling to a pit and approaching the \"koopa\"(turtle) from the left are harmful while standing on the top of the \"koopa\"(turtle) is not.However, once learned, s/he can infer similar mechanisms in the lower level in Figure 1 without additional trials because human have a variety of priors including the concept of object, similarity, semantics, affordance, etc (Gibson, 2014;Dubey et al., 2018).In this paper, we intend to teach machine agents to realize and utilize useful priors to generalize to new tasks without finetuning.Toward addressing the generalization problem with learned priors, we follow the intuition that: (1) each trajectory in a video game or a robotics task is composed of state-action pairs with object interactions (2) terminal rewards can be approximated by the aggregation of the scores for each state-action pairs.With those intuitions in mind, we summarize our proposed method in broad strokes.Given a trajectory with a terminal sparse reward, we first parameterize the score of an action-local region pair with a convolutional neural network F and then aggregate the scores to approximate the final sparse reward.To further enable actionable agents to utilize the scores, a neural dynamics model Figure 1: Illustrative figure: An agent is learning priors from exploration data from World 1 Stage 1 in Nintendo Super Mario Bros game.In this paper, the agent focuses on learning two types of priors: learning an action-state preference score for local regions and a dynamics model.The action-state scores on the middle left learns that approaching the \"Koopa\" from the left is undesirable while from the top is desirable.On the middle right, a dynamics model can be learned to predict a future state based on the current state and action.The agent can apply the priors to a new task World 2 Stage 1 to achieve reasonable policy with zero shot.can be learned from the interaction data using self-supervision.We show that how an agent can take advantage of the scoring function and the learned dynamics model with planning algorithms (Mayne et al., 2000).We adopt the sparse terminal reward setting because in most of the tasks, step-by-step rewards are hard to obtain while final evaluations for trajectories are relatively easy.Readers may argue that learning a dense score for every interaction step is reminiscent of Inverse Reinforcement Learning (Ng et al., 2000;Abbeel & Ng, 2004).The distinctions between the proposed method and IRL are threefold: First, instead of learning a reward function of state s, we learn a scoring function of a local state s l and an action a, which is sufficiently rich in a physical environment and experimentally can generalize well.Second, with the scoring function in hand, we use a dynamics model learned from passive data to obtain the actual policy in a model-based manner while IRL needs to re-train an agent that can be as data inefficient as model-free RL.However, IRL can have difficulty learning a useful model because the expert demonstrations usually only cover a small portion of the true dynamics.Third, we eliminate the assumption of expensive expert demonstrations with the cost of adding a relatively economical sparse reward in the end.This elimination not only reduces the cost for data collection, but also includes more diverse data to train a robust model.The proposed scoring function, beyond being a cost function for planning, can also be treated as an indicator of the existence of objects, which affect the evaluation of a trajectory.We empirically evaluate the scores for objects extracted in the context of human priors and hence find the potential of using our method as an unsupervised method for object discovery.In this paper, we have three major contributions.First, we propose a framework that can learn task-agnostic priors that can generalize to novel tasks.Second, we incorporate a self-supervised learned dynamics model with scoring function to learn a useful policy.Third, we demonstrate the effectiveness of the proposed method on a didactic grid-world example, a well-known video game \"Super Mario Bros\" and a robotics Blocked-Reach environment and show our method outperforms various baselines.Last but not least, we find that objects emerge from our method in an unsupervised manner, which could be useful to other visual tasks."}
{"paper_id": 36, "abstract": "In the realm of deep learning, convolutional neural networks have long been celebrated for their remarkable ability to extract compact and resilient knowledge from vast troves of data. Yet, what if we could harness their potential even in the absence of a training dataset? In this paper, we unveil a groundbreaking concept: the Deep Audio Prior (DAP). This innovative approach elegantly intertwines the inherent structure of a neural network with the temporal nuances of a single audio file.  Imagine a randomly-initialized neural network, poised at the brink of discovery, equipped with a meticulously crafted audio prior. With this, we embark on a quest to conquer formidable audio challenges\u2014think universal blind source separation, interactive audio editing, audio texture synthesis, and the intricate dance of audio co-separation.  To rigorously assess the resilience of our Deep Audio Prior, we introduce the Universal-150 benchmark dataset, a carefully curated collection designed for universal sound source separation, encompassing a rich tapestry of diverse audio sources. Our findings reveal that DAP not only surpasses previous efforts but does so with striking clarity in both qualitative and quantitative evaluations. Furthermore, we delve into an exhaustive ablation study, meticulously validating our design choices along the way. Join us as we explore the uncharted territories of audio processing, driven by the power of deep learning and the magic of sound.", "introduction": "Typically, a deep neural network distills robust priors from a large amount of labeled or unlabeled data (Deng et al., 2009;Jansen et al., 2018).In audio research, neural networks such as VGGish (Hershey et al., 2017) and Wave-U-Net (Stoller et al., 2018) have shown great success in audio classification, audio source separation, and many other challenging tasks.While large audio datasets have greatly improved supervised training, the collection and especially the cleaning of a large amount of audio data still remain an open challenge (Fonseca et al., 2017).For example, in AudioSet (Gemmeke et al., 2017), one of the popular audio datasets, we often find audios/videos that contain content beyond what the label specifiesfoot_0 .In this paper, we combine the power of deep neural networks and temporal prior in audio without any external training data.Similar ideas have been explored in deep image prior (DIP) (Ulyanov et al., 2018).Double-DIP from Gandelsman et al. (2019) further showed that it is possible to achieve robust unsupervised image decomposition from a single-image input, without pre-training on any data.Deep Audio Prior (DAP)'s capability to train on a single audio file has several advantages.First, with proper selection of the audio priors, we show that DAP generalizes well to a wide variety of unseen types of data.Second, our training process is fully unsupervised and therefore make it possible to pre-process large volumes of data in the wild.Last but not least, we show several novel applications that are only possible because of the unique features of DAP, including universal source separation, interactive editing, audio texture synthesis, and audio co-separation.Coherence and discontinuity in audio.Domain gap between audio and visual images precludes direct adoption of the image priors.Many assumptions or priors that are true for images no longer hold for audio.By nature, audio signals exhibit strong temporal coherence, e.g.one's voice changes smoothly (See A \u2192 B in inset).Since images tend to have more spatial patterns, most existing deep image priors have focused on how to encapsulate the spatial redundancy.Another challenge specific to audio is the activation discontinuity.Unlike in videos where an object moves continuously in the scene, a sound source can make sound or turn complete silence at any given time (see A \u2192 C in inset).Our proposed deep audio prior framework has the following main contributions:\u2022 A temporally coherent source generator that can reproduce a wide spectrum of natural sound sources including human speech, musical instruments, animal sounds, etc.With random noises as inputs, we use two sound prediction networks: S 1 and S 2 and two mask modulation networks: M 1 and M 2 to perform source separation.The four networks share a same U-Net structure (Ulyanov et al., 2018).\u2022 A novel mask activation scheme to enable and disable sources with frequency binning and without temporal dependence.\u2022 We demonstrated the effectiveness of DAP in several challenging applications, including universal audio source separation, interactive mask-based editing, audio texture synthesis, and audio co-separation.\u2022 We also introduce Universal-150, a benchmark dataset on blind source separation for universal sound sources.We showed that DAP outperforms other blind source separation (BSS) methods in multiple numerical metrics."}
{"paper_id": 37, "abstract": "In the realm of artificial intelligence, where the quest for optimal performance often feels like a hero's journey through a labyrinth of complexity, reinforcement learning (RL) stands as a beacon of hope. It offers a structured approach to mastering the intricate dance of discrete-time nonlinear stochastic systems, elegantly framed as Markov decision processes. The fusion of RL with deep learning techniques has birthed the vibrant field of deep RL, showcasing remarkable feats in tackling challenging continuous control tasks.  Yet, as any seasoned scholar of control theory will tell you, the cornerstone of a robust system is stability. This vital attribute, however, remains an elusive specter within the domain of RL, posing a formidable challenge that has yet to be fully unraveled. In this paper, we embark on a groundbreaking expedition to forge a stability-guaranteed RL framework. Our innovative approach intertwines the learning of a Lyapunov function with the development of a controller or policy, both intricately woven together through the power of deep neural networks, drawing inspiration from the time-honored principles of control theory.  What sets our framework apart is not only its ability to rival or even surpass the performance of leading RL algorithms but also its capacity to construct a Lyapunov function that serves as a testament to closed-loop stability. Through a series of simulated experiments, we put our approach to the test across a variety of renowned scenarios, including the classic CartPole balancing act, the intricate maneuvers of a 3-dimensional robot, and the complex regulation of synthetic biology gene networks. When pitted against RL algorithms lacking stability guarantees, our method shines, enabling systems to resiliently return to their operating points in the face of uncertainties\u2014be it unforeseen disturbances or variations in system parameters.  In this endeavor, we not only advance the field of reinforcement learning but also lay down a foundation upon which future explorations can build, ensuring that the path to mastery is both stable and illuminated.", "introduction": "Control of discrete-time nonlinear stochastic systems is an important topic in both control theory and reinforcement learning.In the past decades, the advancement of nonlinear control theory in the control community has been successfully applied in aircraft, automobiles, advanced robots and space systems (Slotine et al., 1991;Isidori, 1995).Concurrently, reinforcement learning was developed in the machine learning community to address similar nonlinear control problems (Sutton et al., 1992;Tesauro, 1995;Bertsekas & Tsitsiklis, 1996).Until recently, significant progress has been made by combining advances in deep learning (LeCun et al., 2015) with reinforcement learning.Impressive results are obtained in a series of high-dimensional continuous nonlinear control problems (Duan et al., 2016;Zhang et al., 2016;Zhu et al., 2017;Gu et al., 2017) in which control-theoretic approach is typically difficult to apply.Given a control system, regardless of which controller design method is used, control theory or reinforcement learning, the first and most important property of a system needs to be guaranteed is stability, because an unstable control system is typically useless and potentially dangerous (Slotine et al., 1991).Qualitatively, a system is described as stable if starting the system in the neighborhood of its desired operating point implies that it will stay around the point ever after.For aircraft control systems, a typical stability problem is intuitively related to the following question: will a trajectory perturbation caused by a gust result in a significant deviation in the later flight trajectory?Here, the desired operating point of the system is the flight trajectory in the absence of disturbance.Every control system, whether linear or nonlinear, involves a stability problem which should be carefully studied.The most useful and general approach for studying the stability of control systems is Lyapunov method (Lyapunov, 1892), which is dominant in control engineering ( \u00c5str\u00f6m & Wittenmark, 1989;Mayne et al., 2000).In Lyapunov method, a scalar \"energy-like\" function called Lyapunov function is constructed to analyze the stability of the system.For a linear dynamical system, a quadratic function is typically chosen as Lyapunov function in some classic controller design method, such as linear quadratic regulator (LQR) and model predictive control (MPC).Unfortunately, there is no universal method for constructing Lyapunov functions.In this paper, we propose a stability guaranteed reinforcement learning framework to jointly learn the controller or policyfoot_1 and a Lyapunov function both of which are parameterized by deep neural networks, with a focus on stabilization and tracking problems in discrete-time nonlinear stochastic systems modeled by Markov decision process.The contribution of our paper can be summarized as follows: 1) a novel data-based approach for analyzing the stability of the closed-loop system is proposed by constructing a Lyapunov function parameterized by deep neural network; 2) a practical learning algorithm is designed to search the stability guaranteed controller; 3) the learned controller is able to stabilize the system when interfered by uncertainties such as unseen disturbance and system parameters variations of certain extent.In our experiment, we can show that the stability guaranteed controller is more capable of handling uncertainties compared to those without such guarantees in nonlinear control problems including classic CartPole stabilization tasks, control of 3D legged robots and manipulator and reference tracking tasks for synthetic biology gene regulatory networks."}
{"paper_id": 38, "abstract": "In our exploration of Deep Neural Networks (DNNs), we approach the training process through the lens of Fourier analysis, unveiling a concept we term the Frequency Principle, or F-Principle. This principle reveals a striking tendency: DNNs often learn to approximate target functions by traversing from low frequencies to high, a behavior we observe across high-dimensional benchmark datasets like MNIST and CIFAR10, and within the architecture of deep networks such as VGG16.  This F-Principle stands in stark contrast to the learning dynamics of traditional iterative numerical methods, such as the Jacobi method, which typically converge more swiftly on higher frequencies in a variety of scientific computing contexts. Through a straightforward theoretical framework, we demonstrate that the origins of this F-Principle lie in the inherent regularity of the activation functions commonly employed in DNNs.   The implications of this principle are profound: it suggests an implicit bias in DNNs toward fitting training data using low-frequency functions. This insight not only elucidates why DNNs generally exhibit strong generalization capabilities on most real-world datasets but also sheds light on their struggles with specific cases, such as parity functions or randomized datasets. In essence, our findings weave together the intricate tapestry of DNN training, revealing deeper truths about their behavior and the underlying mechanics that govern their learning processes.", "introduction": "Understanding the training process of Deep Neural Networks (DNNs) is a fundamental problem in the area of deep learning.We find a common behavior of the gradient-based training process of DNNs, that is, a Frequency Principle (F-Principle):DNNs often fit target functions from low to high frequencies during the training process.In another word, at the early stage of training, the low-frequencies are fitted and as iteration steps of training increase, the high-frequencies are fitted.For example, when a DNN is trained to fit y = sin(x) + sin(2x), its output would be close to sin(x) at early stage and as training goes on, its output would be close to sin(x) + sin(2x).F-Principle was observed empirically in synthetic low-dimensional data with MSE loss during DNN training (Xu et al., 2018;Rahaman et al., 2018).However, in deep learning, empirical phenomena could vary from one network structure to another, from one dataset to another and could exhibit significant difference between synthetic data and highdimensional real data.Therefore, the universality of the F-Principle remains an important problem for further study.Especially for high-dimensional real problems, because the computational cost of high-dimensional Fourier transform is prohibitive in practice, it is of great challenge to demonstrate the F-Principle.On the other hand, the mechanism underlying the F-Principle and its implication to the application of DNNs, e.g., design of DNN-based PDE solver, as well as their generalization ability are also important open problems to be addressed.In this work, we design two methods, i.e., projection and filtering methods, to show that the F-Principle exists in the training process of DNNs for high-dimensional benchmarks, i.e., MNIST (LeCun, 1998), CIFAR10 (Krizhevsky et al., 2010).The settings we have considered are i) different DNN architectures, e.g., fully-connected network, convolutional neural network (CNN), and VGG16 (Simonyan & Zisserman, 2014); ii) different activation functions, e.g., tanh and rectified linear unit (ReLU); iii) different loss functions, e.g., cross entropy, mean squared error (MSE), and loss energy functional in variational problems.These results demonstrate the universality of the F-Principle.To facilitate the designs and applications of DNN-based schemes, we characterize a stark difference between DNNs and conventional numerical schemes on various scientific computing problems, where most of the conventional methods (e.g., Jacobi method) exhibit the opposite convergence behavior -faster convergence for higher frequencies.This difference implies that DNN can be adopted to accelerate the convergence of low frequencies for computational problems.We also intuitively explain with theories under an idealized setting how the smoothness/regularity of commonly used activation functions contributes to the F-Principle.Note that this mechanism is rigorously demonstrated for DNNs of general settings in a subsequent work (Luo et al., 2019).Finally, we discuss that the F-Principle provides an understanding of good generalization of DNNs in many real datasets (Zhang et al., 2016) and poor generalization in learning the parity function (Shalev-Shwartz et al., 2017;Nye & Saxe, 2018), that is, the F-Principle which implies that DNNs prefer low frequencies, is consistent with the property of low frequencies dominance in many real datasets, e.g., MNIST/CIFAR10, but is different from the parity function whose spectrum concentrates on high frequencies.Compared with previous studies, our main contributions are as follows:1.By designing both the projection and filtering methods, we consistently demonstrate the F-Principle for MNIST/CIFAR10 over various architectures such as VGG16 and various loss functions.2. For the application of solving differential equations, we show that (i) conventional numerical schemes learn higher frequencies faster whereas DNNs learn lower frequencies faster by the F-Principle, (ii) convergence of low frequencies can be greatly accelerated with DNN-based schemes.3. We present theories under an idealized setting to illustrate how smoothness/regularity of activation function contributes to the F-Principle.4. We discuss in detail the implication of the F-Principle to the generalization of DNNs that DNNs are implicitly biased towards a low frequency function and provide an explanation of good and poor generalization of DNNs for low and high frequency dominant target functions, respectively."}
{"paper_id": 39, "abstract": "In the ever-evolving realm of machine learning, the arcane art of graph convolutional networks (GCNs) has seen remarkable advancements. In this paper, we embark on a groundbreaking journey, exploring the intricate connections between GCNs and the venerable technique of matrix factorization (MF). For the first time, we weave these threads together, presenting GCN as a manifestation of matrix factorization enriched by the dual forces of co-training and unitization.  Guided by this newfound theoretical framework, we unveil an innovative alternative to GCN, which we have dubbed Co-training and Unitized Matrix Factorization (CUMF). Our rigorous analysis finds validation through comprehensive experiments, revealing that CUMF not only matches but often surpasses the performance of its predecessor, GCN.   Moreover, CUMF embraces the inherent advantages of MF-based methodologies, effortlessly accommodating the construction of mini-batches and proving to be more amenable to the rigors of distributed computing. When deployed for semi-supervised node classification, our distributed CUMF decisively eclipses traditional distributed GCN approaches. In doing so, CUMF emerges as a powerful ally for tackling the challenges posed by large-scale, complex real-world applications, heralding a new chapter in the saga of graph-based learning.", "introduction": "In recent years, works on graph convolutional networks (GCN) (Kipf & Welling, 2017) have achieved great success in many graph-based tasks, e.g., semi-supervised node classification (Kipf & Welling, 2017), link prediction (Zhang & Chen, 2018) and recommendation systems (Ying et al., 2018).GCN defines a graph convolution operation, which generates the embedding of each node by aggregating the representations of its neighbors.Given a graph, GCN performs the graph convolution operation layer by layer to obtain the final node representations, which will be passed to neural networks to support various tasks.To perform GCN on large scale graphs in constrained memory or distributed computing environments, different sampling methods have been proposed, such as neighbor sampling (Hamilton et al., 2017) and importance sampling (Chen et al., 2018b).Instead of sampling, Cluster-GCN (Chiang et al., 2019) proposes an approach to convert computation on a huge matrix to computing on a set of small matrices.However, these methods still suffer from performance loss when conducting distributed computing.To take use of various contextual information on edges in a graph, Relational GCN (RGCN) (Schlichtkrull et al., 2018) extends neighbor aggregation by using edge types in link prediction.Besides the edge types, Edge-enhanced Graph Neural Networks (EGNNs) (Gong & Cheng, 2019) takes more contextual features into consideration.However, in general, GCN still has the efficiency problem when facing complex forms of contextual information.Besides GCN, graph embedding methods (Perozzi et al., 2014;Tang et al., 2015b;a;Grover & Leskovec, 2016) are also widely applied.In general, these methods rely on first-order and secondorder proximity to embed very large information networks into low-dimensional vector spaces.The first-order proximity in a graph is the local pairwise proximity between two vertices, and the secondorder proximity between a pair of vertices in a graph is the similarity between their neighborhood structures.As for GCN, previous work shows that the graph convolution operation is actually a special form of Laplacian smoothing (Li et al., 2018).Thus, as the converging of the model, the smoothing process can keep the final representation of a node more and more similar to those of its neighbors.Therefore, GCN is consistent with graph embedding methods in capturing the structural information.According to previous work (Qiu et al., 2018), graph embedding methods have been successfully unified as matrix factorization (MF).Thus, we believe that there might be some connections between GCN and MF.Meanwhile, comparing with GCN, MF-based methods are extremely flexible and suitable for distributed computing (Gemulla et al., 2011;Zhuang et al., 2013;Yu et al., 2014).MF-based methods are also easy and efficient to be extended to tasks with complex forms of contextual information on graph edges (Rendle et al., 2011;Rendle, 2012;Jamali & Lakshmanan, 2013;Shi et al., 2014;Liu et al., 2015).Thus, if we can unify the GCN model as a special form of MF, large scale and complex real-world applications will benefit from this.In this paper, we theoretically reveal the connections between GCN and MF, and unify GCN as matrix factorization with co-training and unitization in section 2. Here, the co-training process means co-training with the classification task of labeled nodes as in (Weston et al., 2012;Yang et al., 2016), and the unitization indicates conducting vector unitization on node representations.Then, under the guidance of our theoretical analysis, we formally propose an alternative model to GCN named Co-training and Unitized Matrix Factorization (CUMF) in section 3. Extensive experiments are conducted on several real-world graphs, and show co-training and unitization are two essential components of CUMF.Under centralized computing settings, CUMF achieves similar or superior performances comparing with GCN.These observations strongly verify the correctness of our theoretical analysis.Moreover, GCN performs poor on dense graphs, while CUMF has great performances.This is may caused by the over-smoothing of graph convolution on dense graphs, while CUMF can balance the smoothing of neighbours and the classification of labeled nodes through the co-training process.Experiments under distributed computing settings are also conducted, and distributed CUMF significantly outperforms the state-of-the-art distributed GCN method, i.e., cluster-GCN (Chiang et al., 2019).Thus, CUMF is extremely friendly to large scale real-world graphs.Meanwhile, lots of works have been done to model contextual information in MF-based methods (Rendle et al., 2011;Rendle, 2012;Jamali & Lakshmanan, 2013;Shi et al., 2014;Liu et al., 2015), which have shown great effectiveness, efficiency and flexibility."}
{"paper_id": 40, "abstract": "In this paper, we embark on a journey to unravel the complexities of the class mismatch dilemma that plagues unsupervised domain adaptation (UDA) in multi-class distributions. Traditional adversarial learning approaches have relied heavily on pseudo labels for domain alignment, yet these labels often suffer from noise and inaccuracies, undermining the delicate multi-class distribution woven into probabilistic predictions. As a result, the latent mismatch problem remains stubbornly unresolved.  Enter our innovative solution: the Prototype-Assisted Adversarial Learning (PAAL) scheme. Unlike pseudo labels, class prototypes emerge as steadfast allies in our quest for accuracy and reliability. These prototypes encapsulate the essence of all instances, capturing the intrinsic semantic distribution shared across domains. By weaving together instance probabilistic predictions and class prototypes, PAAL provides robust indicators for adversarial domain alignment.  Through this novel approach, we align not only the instance feature representations but also the class prototype representations, effectively bridging the gaps between semantically disparate classes. Moreover, we harness the power of class prototypes as proxies to minimize within-class variance in the target domain, thereby alleviating the mismatch among semantically similar classes.  With these groundbreaking innovations, we establish the Prototype-Assisted Conditional Domain Adaptation (PACDA) framework, meticulously designed to confront the class mismatch challenge head-on. Our experiments reveal the impressive performance and generalization capabilities of the PAAL scheme and the PACDA framework across two pivotal UDA tasks: object recognition (Office-Home, ImageCLEF-DA, and Office) and synthetic-to-real semantic segmentation (GTA5\u2192Cityscapes and Synthia\u2192Cityscapes). Join us as we explore this uncharted territory and illuminate the path forward in the realm of domain adaptation.", "introduction": "Unsupervised domain adaptation (UDA) aims to leverage the knowledge of a labeled data set (source domain) to help train a predictive model for a unlabeled data set (target domain).Deep UDA methods bring noticeable performance gain to many tasks (Long et al., 2015;Saito et al., 2017;Richter et al., 2016;Tsai et al., 2018;Lee et al., 2019;Vu et al., 2019a) by exploiting supervision from heterogeneous sources.Some methods exploit maximum mean discrepancy (MMD) (Gretton et al., 2008;Long et al., 2015) or other distribution statistics like central moments (Sun & Saenko, 2016;Zellinger et al., 2017;Koniusz et al., 2017) for domain adaptation.Recently, generative adversarial learning (Goodfellow et al., 2014) provides a promising alternative solution to UDA problem.Since the labels of the target instances are not given in UDA, adversarial learning scheme for adaptation (Ganin & Lempitsky, 2015) suffers from the cross-domain misalignment, where the target instances from a class A are potentially misaligned with source instances from another class B. Inspired by the pseudo-labeling strategy from semi-supervised learning, previous methods either used the pseudo labels in the target domain to perform joint distribution discrepancy minimization (Long et al., 2013;2015) or developed conditional adversarial learning methods that involve one high-dimensional domain discriminator (Long et al., 2018) or multiple discriminators (Chen et al., 2017b;Pei et al., 2018).Though effective, these conditional domain adversarial learning methods align different instances from different domains relying only on their own predictions.Simple probabilistic predictions or pseudo labels may not accurately represent the semantic information of input instances, misleading the alignment.A toy example is given in Fig. 1(a).The pseudo label of the chosen instance x is inclined to be class 'square' while the ground truth label is class 'circle'.Only guided by the instance prediction, the 'circle' class in the target domain and the 'square' class in the source domain are easily confused, causing the misalignment in the adversarial domain adaptation.To remedy the misalignment, we propose to exploit the class prototypes for adversarial domain alignment, instead of using only the possibly inaccurate predictions.Prototypes are global feature representations of different classes and are relevant to the inherent semantic structures shared across Specifically, we summarize the class prototypes from all instances according to their predictions.In this way, on one hand, we lower the dependence of class prototypes on instance predictions which may be inaccurate, and on the other hand, we encourage the instances with greater certainty to contribute more to their corresponding class prototypes.The prototypes are updated dynamically through a moving average strategy to make them more accurate and reliable.Then by broadcasting class prototypes to each instance according to its probability prediction, the inaccurate semantic distribution depicted by instance predictions can be alleviated.Based on reliable prototype-based conditional information, we align both the instance feature representations and the class prototypes through the proposed PAAL scheme to relieve the alignment among semantically dissimilar instances.However, such a conditional domain alignment may promote the confusion among semantically similar instances across domains to some degree.To further alleviate it, we introduce an intra-class objective in the target domain to pursue the class compactness.Built on the proposed PAAL scheme and this intra-class compactness objective, we develop a Prototype-Assisted Conditional Domain Adaptation (PACDA) framework for solving UDA problems.Extensive experimental evaluations on both object recognition and semantic segmentation tasks clearly demonstrate the advantages of our approaches over previous state-of-the-arts (Long et al., 2018;Xu et al., 2019;Luo et al., 2019;Tsai et al., 2019).The contributions of this work can be summarized into three folds: 1) To the best of our knowledge, we are the first to leverage the class prototypes in conditional adversarial learning to prevent the misalignment in UDA; 2) We propose a simple yet effective domain adversarial learning framework PACDA to remedy the misalignment among semantically similar instances as well as semantically dissimilar instances; 3) The proposed PAAL scheme and PACDA framework are generic, and our framework achieves the state-of-the-art results on several unsupervised domain adaptation tasks including object recognition and semantic segmentation."}
{"paper_id": 41, "abstract": "In the realm of weakly supervised localization (WSL), the challenge lies in training models to pinpoint the locations of objects armed only with abstract labels\u2014often merely the class of the primary object within an image. In this paper, we embark on an ambitious journey to broaden the horizons of WSL by introducing a novel approach that leverages counting machines, harnessing the power of convolutional neural networks (CNNs) alongside density maps for precise counting. We reveal that, with nothing more than ground-truth count numbers, we can train a hidden layer in the form of a density map to not only localize objects but also unearth intricate features.   At the heart of CNNs lie two fundamental processes: convolution and pooling, both of which we explore in depth, analyzing their profound effects on the architecture of an end-to-end WSL network. The features learned within our density maps manifest as clusters of dots, a representation that begs for human interpretation. To bridge this gap, we introduce a Gini impurity penalty designed to regularize the density map, enhancing its interpretability. Intriguingly, we demonstrate that this regularization bears a striking resemblance to the variational term found in the $\\beta$-variational autoencoder, weaving a connection between these seemingly disparate concepts.  We illustrate the efficacy of our proposed algorithm through the lens of a straightforward bubble counting task, laying the groundwork for our broader application. Ultimately, we extend our methods to the widely recognized Mall crowd counting dataset, where we strive to extract discriminative features of human figures, pushing the boundaries of what WSL can achieve in the intricate tapestry of object localization.", "introduction": "Deep convolutional neural networks (CNN) have significantly pushed the frontier of image processing.Trained with enormous amount of data, CNNs have surpassed human performance in many object detecting tasks.However, humans are still ahead in many aspects.One of them is the segmentation based on abstract knowledge: once a human can tell what is in an image, he/she can easily crop the objects out.Researches have been exploring similar potentials of CNNs.Related techniques are referred to as Weakly Supervised Localization (WSL), in which models are trained with only image-level labels but expected to give the positions of objects at once.Most of existing WSL methods are guided by class labels and realized in two different ways.The first is the Multiple Instance Mining (MIM).MIM is based on multiple instance learning (MIL), which trains a model to identify multiple instances from incomplete labels (Babenko, 2008;Siva et al., 2012).For object detection, the regions around the target have the highest influence on the label decision.By evaluating the contributions of different regions, the location of the target can be determined.A straightforward approach to find the relevant regions is to compute the derivative of the decision with respect to the input image (Simonyan et al., 2013).But the found regions can also be the ones that introduce the most uncertainty to the result.A more effective approach is to use an attention mechanism, in which one detector proposes potential regions according to a classifier's evaluation of contribution (Gokberk Cinbis et al., 2014;Song et al., 2014a;Bilen et al., 2014;Ren et al., 2016;Cinbis et al., 2017).The second way to WSL is to train the locations as latent variables.Zhou et al. reported that a deep CNN can maintain spatial information through layers (Zhou et al., 2016).Based on this observation they proposed the class activation map (CAM) which averages every channel of a CNN's final layer and feeds the averages to a soft-max classifier.This structure is analogous to the density map used for object counting.The density map method annotates each object by one pixel with a value of one and computes the count by summing up the map (Lempitsky & Zisserman, 2010).Inspired by the similarities between the CAM and the density map, we generalize WSL to counting models based on density map.Our method trains the density map as latent variables of an end-"}
{"paper_id": 42, "abstract": "In the realm of machine learning, we often find ourselves navigating the treacherous waters of incomplete datasets. The literature has largely focused on the straightforward task of filling in these missing values, but this approach overlooks a deeper truth: the absence of data is not merely a gap to be filled, but a harbinger of uncertainty. This uncertainty extends not only to the distribution of the missing values but also to the very assignments of target classes, which demand our careful attention.  In this paper, we unveil a novel approach\u2014a method both simple and effective\u2014for imputing missing features while simultaneously estimating the distribution of target assignments in the face of incomplete data. Our strategy hinges on the creation of a generator network, adept at producing plausible imputations, which a discriminator network then challenges to discern. This interplay between generator and discriminator lays the groundwork for a predictor network, trained on the imputed samples to capture the nuances of classification uncertainty and yield informed predictions.  We put our method to the test on the CIFAR-10 image dataset, alongside three real-world tabular classification datasets, exploring various rates and structures of missingness. The results of our experiments reveal the power of our approach\u2014not only in generating compelling imputations but also in illuminating the uncertainties inherent in class assignments when confronted with missing values. Through this work, we aim to advance the understanding of how to wield the tools of machine learning in the presence of uncertainty, turning what might seem a weakness into a source of strength.", "introduction": "While a large body of the machine learning literature is built upon the assumption of having access to complete datasets, in many real-world problems only incomplete datasets are available.The existence of missing values can be due to many different causes such as human subjects not adhering to certain questions or features not being collected frequently due to financial or experimental limitations, sensors failures, and so forth.Data imputation techniques have been suggested as a solution to bridge this gap in the literature by replacing missing values with observed values.Missing data imputation approaches can be categorized into single and multiple imputation methods.Single imputation methods try to replace each missing value with a plausible value that is the best fit given the value of other correlated features and knowledge extracted from the dataset (Hastie et al., 1999;Anderson, 1957).While these methods are easy to implement and use in practice, imputed values may induce bias by eliminating less likely but important values.Also, these methods do not suggest a way to measure to what extent the imputed values are representative of the missing values (Little & Rubin, 2019).Multiple imputation (MI) techniques, as suggested by the name, try to use multiple imputed values to impute each missing value.The result would be having a set of imputed datasets that enables measuring how consistent and statistically significant are the results of the experiments (Rubin, 1976).While MI offers interesting statistical insights about the reliability of analysis on incomplete data, the insight is imprecise as it is mainly concerned about the population of data samples rather than individual instances.Specifically, MI methods reason about the statistical properties on a limited number of imputed datasets (less than 10 in most practical implementations) on the population of samples within the dataset (Schafer & Graham, 2002;Murray et al., 2018).The existence of missing values is synonymous with having uncertainty over these values that requires careful consideration.In many real-world applications, we are dealing with supervised problems that demand modeling and prediction based on incomplete data.Take for instance, prediction of class assignments given an image in which a large portion of the frame is missing.In such a scenario, based on observed frame parts, there might be multiple probable class assignments each having a different likelihood.Here, we are not only interested in imputing missing values or measuring how robust our imputations are, but also it is highly desirable to measure the impact of missing values on the prediction outcome for each instance.In this paper, we propose the idea of Generative Imputation and Stochastic Prediction (GI) as a novel approach to impute missing values and to measure class uncertainties arising from the distribution of missing values.The suggested approach is based on neural networks trained using an adversarial objective function.Additionally, a predictor is trained on the generated samples from the imputer network which is able to reflect the impact of uncertainties over missing values.This enables measuring different prediction outcomes and certainties for each specific instance.We evaluate the effectiveness of the proposed method on different incomplete image and tabular datasets under various missingness structures.foot_0"}
{"paper_id": 43, "abstract": "In the realm of intelligent behavior, one of the most remarkable traits is the capacity to grasp abstract strategies that can be wielded against the unknown. To this end, we unveil a groundbreaking architecture, a synthesis of memory-augmented networks, drawing inspiration from the storied von Neumann and Harvard architectures that underpin modern computing. This innovative framework empowers the learning of abstract algorithmic solutions through the lens of Evolution Strategies within a reinforcement learning landscape. When applied to challenges such as Sokoban, sliding block puzzles, and robotic manipulation tasks, our architecture reveals its prowess, mastering algorithmic solutions with an impressive blend of generalization and abstraction. It scales effortlessly to a myriad of task configurations and complexities, remaining resiliently independent of both data representation and task domain. In essence, we have crafted a tool that not only learns but adapts, transcending the boundaries of conventional problem-solving.", "introduction": "Transferring solution strategies from one problem to another is a crucial ability for intelligent behavior (Silver et al., 2013).Current learning systems can learn a multitude of specialized tasks, but extracting the underlying structure of the solution for effective transfer is an open research problem (Taylor & Stone, 2009).Abstraction is key to enable these transfers (Tenenbaum et al., 2011) and the concept of algorithms in computer science is an ideal example for such transferable abstract strategies.An algorithm is a sequence of instructions, which solves a given problem when executed, independent of the specific instantiation of the problem.For example, consider the task of sorting a set of objects.The algorithmic solution, specified as the sequence of instructions, is able to sort any number of arbitrary classes of objects in any order, e.g., toys by color, waste by type, or numbers by value, by using the same sequence of instructions, as long as the features and compare operations defining the order are specified.Learning such structured, abstract strategies enables the transfer to new domains and representations (Tenenbaum et al., 2011).Moreover, abstract strategies as algorithms have built-in generalization capabilities to new task configurations and complexities.Here, we present a novel architecture for learning abstract strategies in the form of algorithmic solutions.Based on the Differential Neural Computer (Graves et al., 2016) and inspired by the von Neumann and Harvard architectures of modern computers, the architectures modular structure allows for straightforward transfer by reusing learned modules instead of relearning, prior knowledge can be included, and the behavior of the modules can be examined and interpreted.Moreover, the individual modules of the architecture can be learned with different learning settings and strategies -or be hardcoded if applicable -allowing to split the overall task into easier subproblems, contrary to the end-to-end learning philosophy of most deep learning architectures.Building on memory-augmented neural networks (Graves et al., 2016;Neelakantan et al., 2016;Weston et al., 2015;Joulin & Mikolov, 2015), we propose a flexible architecture for learning abstract strategies as algorithmic solutions and show the learning and transferring of such in symbolic planning tasks."}
{"paper_id": 44, "abstract": "In the ever-evolving realm of reinforcement learning, the quest to unlock valuable skills without the crutch of a manually crafted reward function remains a formidable challenge. In this paper, we unveil a groundbreaking approach: Mutual Information-based State-Control (MISC). This innovative self-supervised reinforcement learning technique empowers agents to learn to navigate and manipulate states of interest, all without relying on external rewards. We redefine the intrinsic objective, framing it as a quest to maximize the mutual information between contextual states\u2014think of them as the robot's own parameters\u2014and the states that truly matter, such as the position of an object in its grasp.   Our experiments span a variety of simulated robotic manipulation tasks sourced from OpenAI Gym, where we demonstrate that our method excels in teaching robots to perform actions like pushing and picking up objects, driven solely by the intrinsic rewards derived from mutual information. Remarkably, the pre-trained policy and the mutual information discriminator not only enhance our agent's capabilities but also accelerate the learning process, paving the way for achieving high task rewards more efficiently. Our findings underscore the power of mutual information as a pivotal element in overcoming the hurdles posed by sparse rewards in robotic manipulation. For a visual showcase of our experimental results, please visit https://youtu.be/cLRrkd3Y7vU.", "introduction": "Reinforcement Learning (RL) (Sutton & Barto, 1998) combined with Deep Learning (DL) (Goodfellow et al., 2016) has led to great successes in various reward-driven tasks, such as playing video games (Mnih et al., 2015), learning continuous control (Ng et al., 2006;Peters & Schaal, 2008;Levine et al., 2016;Chebotar et al., 2017), navigating in complex environments (Mirowski et al., 2017;Zhu et al., 2017), and manipulating objects (Andrychowicz et al., 2017;2018).Despite these successes, RL agents that learn only from reward signals differ in the manner that humans learn.In the case of learning to manipulate objects, a human agent not only attempts to accomplish the task but also learns to master the controllable aspects of the environment (Lake et al., 2017).Notably, a human agent can quickly learn the correlation between its own action and the state change of an object, even without supervision, to later use the acquired skill to manipulate the object into the desired state.The ability to fully autonomously learn to control the states of interest has many benefits.First, it would make learning possible in the absence of hand-engineered reward functions or manuallyspecified agent goals.It is known that designing a reward function that ensures the agent to learn the desired behaviors is challenging (Hadfield-Menell et al., 2017).Secondly, to learn to \"master\" the environment potentially helps the agent to learn to achieve goals in sparse reward settings.Thirdly, the policy of controlling states of interest can be quickly adapted to unknown tasks.It is currently an open challenge to design RL agents that automatically learn useful skills to control the states of the environment, without rewards (Warde-Farley et al., 2019).In this paper, a skill is a policy that changes the state of the environment in a consistent way.The policy can be a single unconditioned policy (Peters & Schaal, 2008) or a latent-condionted policy (Eysenbach et al., 2019).One way to learn skills is to simultaneously train a discriminator to discern the skill-options of the agent (Szepesvari et al., 2014) based on the states of the trajectory (Eysenbach et al., 2019).In this way, the agent should be able to learn a diverse set of skills.Another way is to learn an environment model to encourage the agent to explore the states, which are relatively unpredictable (Houthooft et al., 2016;Pathak et al., 2017;Ha & Schmidhuber, 2018).These methods are more focused on efficiently exploring novel states instead of controlling states of interest.We propose a new self-supervised reinforcement learning method, Mutual Information-based State-Control (MISC), which is an approach that learns skills to control states of interest without any reward signal.States of interest are the states of the environment that we are interested in.Context states are the states of the environment excluding the states of interest.The states of interest and the context states are given by the user before training the model.The idea of our method is to encourage the agent to find skills that maximize the mutual information between the context states and the states of interest.We first divide the observation states into two sets, the context states and the states of interest.In the case of robotic manipulation tasks, see Figure 1, the context states are the robot states; the states of interest are the states of an object.During the learning process of the agent, a discriminator learns to evaluate the mutual information between the context states and the states of interest.The agent receives high intrinsic rewards from the discriminator when there is high mutual information.Our hypothesis is that if the agent learns to control the object, then the mutual information of the agent states and the object states should be relatively high.Interestingly, this automated RL scheme bears similarities to the self-supervised learning of feature representations in computer vision tasks, where a neural network is trained to predict a part of the image given another part of the same image (Doersch et al., 2015).During the process, the neural network captures the mutual information among different parts of the image.Similarly, in our work, we want to capture the mutual information among different sets of states.This paper contains the following five contributions.First, we introduce a new self-supervised RL method, Mutual Information-based State-Control, for learning to control states of interest.Secondly, we evaluate the developed framework in the robotic simulations of OpenAI Gym and a navigation task in Gazebo and demonstrate that MISC enables the agent to learn skills, such as reaching, pushing, picking up, and sliding the object without rewards.Thirdly, we show that the pre-trained policy can be quickly adapted to the specific tasks with external sparse reward signal.Fourthly, the pretrained mutual information discriminator also improves the learning process of the agent, either as intrinsic rewards or as priorities for experience replay, in addition to the sparse rewards.Finally, we show that the learned mutual information discriminator can be transferred among different tasks and still improves the performance."}
{"paper_id": 45, "abstract": "In the ever-evolving realm of recommendation systems, we unveil an innovative learning framework that breathes new life into collaborative filtering through the power of a synthetic feedback loop. Picture this: at the heart of our framework lies a dynamic duo\u2014the \"recommender\" and the \"virtual user.\" The recommender, a master of collaborative filtering, sifts through the tapestry of observed user behavior to suggest items tailored to individual tastes.  But the magic truly happens when the virtual user steps into the spotlight. This entity, a clever construct, assesses the rewards derived from the recommended items and intricately weaves the impact of these rewards back into the fabric of user behavior. Together, the recommender and the virtual user forge a seamless closed loop, where recommendations flow like a river, and the unobserved feedback of users dances in response to the items they encounter.  This synthetic feedback serves as a catalyst, enriching the landscape of observed user behavior and elevating the quality of recommendations to new heights. Our model, akin to the principles of inverse reinforcement learning, can be adeptly honed through simulation, or as we like to call it, rollout.  In rigorous experiments across diverse datasets, our framework has proven its mettle, significantly enhancing the performance of existing collaborative filtering methods. Join us as we embark on this journey to reshape the future of recommendation systems, where every suggestion is a step toward a more personalized experience.", "introduction": "Recommendation systems are important modules for abundant online applications, helping users explore items of potential interest.As one of the most effective approaches, collaborative filtering Sarwar et al. (2001); Koren & Bell (2015); He et al. (2017) and its deep neural networks based variants He et al. (2017); Wu et al. (2016); Liang et al. (2018); Li & She (2017); Yang et al. (2017); Wang et al. (2018) have been widely studied.These methods leverage patterns across similar users and items, predicting user preferences and demonstrating encouraging results in recommendation tasks Bennett & Lanning (2007); Hu et al. (2008); Schedl (2016).Among these work, beside \"useritem\" pairs, side information, e.g., user reviews and scores on items, are involved and have achieved remarkable success Menon et al. (2011); Fang & Si (2011).Such side information is a kind of user feedback to the recommended items, which is promising to improve the recommendation systems.Unfortunately, both the user-item pairs and user feedback are extremely sparse compared with the search space of items.What is worse, when the recommendation systems are trained on static observations, the feedback is unavailable until it is deployed in real-world applications -in both training and validation phases, the target systems have no access to any feedback because no one has observed the recommended items.Therefore, the recommendation systems may suffer overfitting, and their performance may degrade accordingly, especially in the initial phase of deployment.Although real-world recommendation systems are usually updated in an online manner with the assist of increasing observed user behavior Rendle & Schmidt-Thieme (2008); Agarwal et al. (2010); He et al. (2016), introducing a feedback mechanism during their training phases can potentially improve the efficiency of the initial systems.However, this is neglected by existing learning frameworks.Motivated by the above observations, we propose a novel framework that achieves collaborative filtering with a synthetic feedback loop (CF-SFL).As shown in Figure 1, the proposed framework consists of a \"recommender\" and a \"virtual user.\"The recommender is a collaborative filtering (CF) model, that predicts items from observed user behavior.The observed user behavior reflects intrinsic preferences of users, while the recommended items represent the potential user preferences estimated by the model.Regarding the fusion of the observed user behavior and the recommended items as inputs, the virtual user, which is the key of our model, imitates real-world scenarios and synthesizes user feedback.In particular, the virtual user contains a reward estimator and a feedback generator: the reward estimator estimates rewards based on the fused inputs (the compatible representation of the user observation and its recommended items), learned with a generative adversarial regularizer.The feedback generator provides feedback embeddings to augment the original user embeddings, conditioned on the estimated rewards as well as the fused inputs.Such a framework constructs a closed loop between the target CF model and the virtual user, synthesizing user feedback as side information to improve recommendation results.Observed User Behavior Recommender Reward Estimator Feedback Generator Recommended Items Fused Input Feedback Embedding Reward Virtual User Figure 1: Illustration of our proposed CF-SFL framework for collaborative filtering.The proposed CF-SFL framework can be interpreted as inverse reinforcement learning (IRL) approach, in which the recommender learns to recommend user items (policy) with the estimated guidance (feedback) from the proposed virtual user.The proposed feedback loops can be understood as an effective rollout procedure for recommendation, jointly updating the recommender (policy) and the virtual user (the reward estimator and the feedback generator).Eventually, even if side information (i.e., real-world user feedback) is unobservable, our algorithm is still applicable to synthesize feedback in both the training and inference phases.The proposed framework is general and compatible with most CF methods.Experimental results show that the performance of existing approaches can be remarkably improved within the proposed framework."}
{"paper_id": 46, "abstract": "In the ever-evolving landscape of neural networks, a burgeoning curiosity has emerged\u2014a quest to delve into the intricate design space of architectures, seeking those that elevate performance to new heights, whether through enhanced accuracy, swifter training, or optimized resource utilization. Yet, amidst this fervor, our grasp of how the architecture itself influences these pivotal metrics remains frustratingly elusive.   In this exploration, we embark on a journey through the realm of model architecture and its profound impact on the speed of training, particularly under the lens of gradient descent optimization. By modeling gradient descent as a first-order ordinary differential equation (ODE), we wield the coefficient matrix H as our guiding compass to navigate the convergence landscape.   Our approach introduces a novel analytical technique, one that meticulously enumerates H in relation to the myriad of potential \u201cpaths\u201d woven throughout the network. Through this lens, we unveil a striking revelation: alterations in architectural parameters resonate as shifts in both the quantity and characteristics of these paths, which together orchestrate the rhythm of convergence.   We posit that our analytical framework holds promise, not merely as a tool for understanding the nuances of simpler architectures but as a beacon for grappling with the complexities inherent in more intricate modifications. In this way, we aim to illuminate the path forward in the realm of neural network design, one step at a time.", "introduction": "Gradient descent and its variants are the cornerstones of deep learning.The theoretical properties of gradient descent have been widely studied in the literature; study of the convergence bounds and guarantees [6,12,15,13], the characterization of the local geometry of stationary points [21,11,10,19,25,30,24,23,13], exploration of better algorithms to optimize the descent process [28,16,26,33,7] are just a few example of active research areas in this domain.Another major research area is the exploration of network architecture and its impact on performance.In recent years, network architecture search and design have shown major performance boosts in various aspects of deep learning [36,35,18,8,34].Recent theoretical results are trying to relate model architecture and gradient descent properties.It has been shown that over-parametrization in width guarantees convergence in deep neural network [15].It has also been shown that increasing depth has an impact similar to adding momentum optimization and adaptive learning rate to the objective function [4].The gradient descent process can be described by a system of first-order differential equations in the continuous-time limit, in the form of \u1e8b(t) = H(t)x(t).In this paper, we study the properties of the coefficient matrix H(t), which governs the dynamics of the gradient descent.Specifically, we formulate H(t) in terms of all possible paths in the network.This representation is powerful as it enables us to analyze the gradient behavior symbolically and abstracts away the unnecessary bookkeeping of derivatives and weights.Specifically, we use this representation to explore the followings.(1) We study the impact of width on convergence.Using a simple path counting argument, we show that H(t) is a sum of m i.i.d.terms where m is dictated by the width of the network.This has been implied in the work of Simon Du, et al. [12] for the special case of a 2-layer RELU-activated network.We contrast the ease of arriving at such conclusions using our representation against methods used in previous work [12,15,13,6].(2) We study the impact of depth on convergence.While prior work [3] established the relationship between momentum and depth, we show a direct relationship between depth-induced momentum and the scale of the output value.We also discuss how depth-induced momentum is different than explicit momentum.(3) We argue why the number of paths is more important than the number of nodes in a network."}
{"paper_id": 47, "abstract": "In the ever-evolving realm of deep learning, where the prowess of deep neural networks (DNNs) shines brightly across a multitude of tasks, an unsettling truth lurks in the shadows: these powerful constructs can behave in surprisingly erratic ways. One particularly vexing quirk is their uncanny sensitivity to various noise attacks, a flaw that has ignited a fervent quest among researchers to forge networks that can withstand such disruptions. In this endeavor, we unveil a novel training regularizer designed to minimize the probabilistic expected training loss of a DNN when faced with a generic Gaussian input.  Our approach is both efficient and elegantly simple, allowing us to approximate this regularizer for networks of any depth without succumbing to the burdensome demands of memory and computation typically associated with data augmentation. By harnessing the analytic expression of the output mean from a shallow neural network, we sidestep the complexities that often plague deeper architectures.  To validate our method, we embarked on a rigorous series of experiments utilizing LeNet and AlexNet across diverse datasets, including MNIST, CIFAR10, and CIFAR100. The results were compelling: networks trained with our proposed regularizer displayed a remarkable enhancement in resilience against Gaussian noise, equivalent to the effect of performing 3 to 21 times the amount of noisy data augmentation. Furthermore, our findings reveal a broader truth: by bolstering robustness against Gaussian noise through our innovative regularizer, we can significantly enhance overall resilience against six other types of attacks\u2014by two orders of magnitude, no less.  Thus, we stand at the precipice of a new frontier in the quest for robust neural networks, equipped with tools that promise to reshape our understanding of resilience in the face of adversity.", "introduction": "Deep neural networks (DNNs) have emerged as generic models that can be trained to perform impressively well in a variety of learning tasks ranging from object recognition (He et al., 2016) and semantic segmentation (Long et al., 2015) to speech recognition (Hinton et al., 2012) and bioinformatics (Angermueller et al., 2016).Despite their increasing popularity, flexibility, generality, and performance, DNNs have been recently shown to be quite susceptible to small imperceptible input noise (Szegedy et al., 2014;Moosavi-Dezfooli et al., 2016;Goodfellow et al., 2015).Such analysis gives a clear indication that even state-of-the-art DNNs may lack robustness.Consequently, there has been an ever-growing interest in the machine learning community to study this uncanny behaviour.In particular, the work of (Goodfellow et al., 2015;Moosavi-Dezfooli et al., 2016) demonstrates that there are systematic approaches to constructing adversarial attacks that result in misclassification errors with high probability.Even more peculiarly, some noise perturbations seem to be doubly agnostic (Moosavi-Dezfooli et al., 2017), i.e. there exist deterministic perturbations that can result in misclassification errors with high probability when applied to different networks, irrespective of the input (denoted network and input agnostic).Understanding this degradation in performance under adversarial attacks is of tremendous importance, especially for real-world DNN deployment, e.g.self-driving cars/drones and equipment for the visually impaired.A standard and popular means to alleviate this nuisance is noisy data augmentation in training, i.e. a DNN is exposed to noisy input images during training so as to bolster its robustness during inference.Several works have demonstrated that DNNs can in fact benefit from such augmentation (Moosavi-Dezfooli et al., 2016;Goodfellow et al., 2015).However, data augmentation in general might not be sufficient for two reasons.(1) Particularly with high-dimensional input noise, the amount of data augmentation necessary to sufficiently capture the noise space will be very large, which will increase training time.(2) Data augmentation with high energy noise can negatively impact the performance on noise-free test examples.This can be explained by the fundamental trade-off between accuracy and robustness (Tsipras et al., 2018;Boopathy et al., 2019).It can also arise from the fact that augmentation forces the DNN to have the same prediction for two vastly different versions of the same input, noise-free and a substantially corrupted version.Therefore, in this paper, we propose a new regularizer for noise-robust networks to circumvent the aforementioned setbacks of data augmentation.A natural objective for training against attacks sampled from a distribution D, that bypasses the need for data augmentation, is the expected loss under this distribution.Since a closed-form expression is generally difficult to obtain or an approximate surrogate is expensive to evaluate (Monte Carlo estimates), we propose instead a closely related objective that is the loss of the expected predictions of the network under D-distributed adversarial noise.Since it has been shown that Gaussian noise can be adversarial (Bibi et al., 2018) and that such noise is widely studied in applications such as image processing, we restrict the focus in this paper to the case where D is Gaussian.While this may seem to be too restrictive, we later show that improving the robustness of networks against Gaussian attacks also improves the robustness against a family of other types of attacks.However, even under such an assumption, only a memory and computationally expensive (expensive due to two-stage network linearization), closed-form approximate surrogate for network expected predictions exists (Bibi et al., 2018).We formalize a new regularizer that is a function of the probabilistic first moment of the output of a DNN to train robust DNNs against noise sampled from distribution D. (ii) Under the special choice of Gaussian attacks, i.e.D is Gaussian, we show how the first moment expression can be evaluated very efficiently during training for an arbitrary deep DNN by bypassing the need to perform memory and computationally expensive two-stage linearization.(iii) Extensive experiments using LeNet (LeCun et al., 1999) and AlexNet (Krizhevsky et al., 2012) architectures on MNIST (LeCun, 1998), CIFAR10, and CIFAR100 (Krizhevsky & Hinton, 2009) datasets demonstrate that a substantial enhancement in robustness can be achieved when using our regularizer in training.In fact, in the majority of the experiments, the improvement is better than training on the same dataset, augmented with 3 to 21 times Gaussian noisy data.Interestingly, the results suggest an excellent trade-off between accuracy and robustness.Moreover, we show that networks that are trained to be robust against Gaussian attacks using our proposed regularizer enjoy orders of magnitude boost in robustness against a family of other types of attacks."}
{"paper_id": 48, "abstract": "In the realm of matrix decomposition, few techniques shine as brightly as non-negative matrix factorization (NMF). This celebrated algorithm promises strictly non-negative factors, yet the optimization problem it faces is a labyrinthine challenge, often deemed computationally intractable. However, much like a hero in a fantasy tale who defies the odds, gradient descent-based solvers frequently unearth surprisingly effective solutions amidst this complexity. This intriguing disparity between computational difficulty and practical triumph echoes recent revelations in the world of deep learning, where the intricacies of optimization have sparked fervent debate and exploration.  In this paper, we embark on a quest to unravel the NMF optimization conundrum, delving into its loss landscape beyond the confines of worst-case scenarios. Recent studies have illuminated a fascinating phenomenon: gradients in deep networks tend to align with the ultimate minimizer throughout the optimization journey. We unveil that a similar enchantment holds true for NMF, demonstrating, with high probability, that gradients point towards the final solution in a non-worst-case framework featuring a planted solution. Our empirical investigations across a diverse array of real-world NMF challenges further bolster this claim.  As we navigate this landscape, our analysis suggests that the likelihood of this gradient behavior amplifies with an increasing number of parameters. Curiously, our experiments hint that this trend may also resonate within the realm of deep neural networks, transforming the expansion of datasets and model complexity into a veritable boon for optimization. In this intricate tapestry of mathematics and discovery, we find that the path to understanding NMF not only illuminates its own mysteries but may also cast light on the broader intricacies of deep learning itself.", "introduction": "Non-negative matrix factorization (NMF) is a ubiquitous technique for data analysis where one attempts to factorize a measurement matrix X into the product of non-negative matrices U, V (Lee and Seung, 1999).This simple problem has applications in recommender systems (Luo et al., 2014), scientific analysis (Berne et al., 2007;Trindade et al., 2017), computer vision (Gillis, 2012), internet distance prediction (Mao et al., 2006), audio processing (Schmidt et al., 2007) and many more domains.Often, the non-negativity is crucial for interpretability, for example, in the context of crystallography, the light sources, which are represented as matrix factors, have non-negative intensity (Suram et al., 2016).Like many other non-convex optimization problems, finding the exact solution to NMF is NP-hard (Pardalos and Vavasis, 1991;Vavasis, 2009).NMF's tremendous practical success is however at odds with such worst-case analysis, and simple algorithms based upon gradient descent are known to find good solutions in real-world settings (Lee and Seung, 2001).At the time when NMF was proposed, most analyses of optimization problems within machine learning focused on convex formulations such as SVMs (Cortes and Vapnik, 1995), but owing to the success of neural networks, non-convex optimization has experienced a resurgence in interest.Here, we revisit NMF from a fresh perspective, utilizing recent tools developed in the context of optimization in deep learning.Specifically, our main inspiration is the recent work of Kleinberg et al. (2018) and Zhou et al. (2019) that empirically demonstrate that gradients typically point towards the final minimizer for neural networks trained on real-world datasets and analyze the implications of such convexity properties for efficient optimization.In this paper, we show theoretically and empirically that a similar property called star-convexity holds in NMF.From a theoretical perspective, we consider an NMF instance with planted solution, inspired by the stochastic block model for social networks (Holland et al., 1983;Decelle et al., 2011) and the planted clique problem studied in sum-of-squares literature (Barak et al., 2016).We prove that between two points the loss is convex with high probability, and conclude that the loss surface is star-convex in the typical case -even if the loss is computed over unobserved data.From an empirical perspective, we verify that our theoretical results hold for an extensive collection Figure 1: A non-convex loss surface is illustrated in a).In general, the loss will be non-convex on straight paths connecting random points x a , x b and the global minimizer x \u02da.We consider a model of NMF with a planted solution, and as shown in b) the loss is typically convex on straight paths between points x a and a planted solution x \u02da.Additionally, as illustrated in c), the loss is typically convex on straight paths between points x a and x b . of real-world datasets spanning collaborative filtering (Zhou et al., 2008;Kula, 2017;Harper and Konstan, 2016), signal decomposition (Zhu, 2016;Li and Ngom, 2013;Li et al., 2001;Erichson et al., 2018) and audio processing (Flenner and Hunter, 2017;canto Foundation), and demonstrate that the star-convex behavior results in efficient optimization.Finally, we show that star-convex behavior becomes more likely with growing number of parameters, suggesting that a similar result may hold as neural networks become wider.We provide supporting empirical evidence for this hypothesis on modern network architectures."}
{"paper_id": 49, "abstract": "In the ever-evolving landscape of deep learning, training Deep Neural Networks (DNNs) to withstand norm-bounded adversarial attacks remains a formidable challenge, one that has eluded many. Traditional verification methods, while theoretically sound, often prove too cumbersome for the robust training of expansive networks. However, a breakthrough by Gowal et al. revealed a promising avenue: the efficient propagation of bounded input intervals through the intricate layers of deep networks, a technique known as interval bound propagation (IBP). This method unlocked a new level of robustness and was notably the first to be successfully applied to large-scale networks.  Yet, the IBP bounds, while useful, are notoriously loose\u2014particularly in the context of deeper architectures\u2014rendering the training process complex and unwieldy. In this paper, we delve into the bounds of a specific configuration of layers: an affine layer, followed by a ReLU activation, and concluding with another affine layer. Our exploration leads us to introduce what we term \\emph{expected} bounds\u2014true bounds in expectation\u2014which are provably tighter than their IBP counterparts.   We extend this revelation to deeper networks through a method of blockwise propagation, showcasing that we can achieve bounds that are orders of magnitude tighter than those provided by IBP. Armed with these refined bounds, we demonstrate that a straightforward standard training procedure can yield remarkable improvements in the robustness-accuracy trade-off across various architectures, tested on the venerable MNIST and CIFAR10 datasets. In this way, we not only push the boundaries of what is possible in adversarial training but also illuminate a path forward for future research in the realm of robust deep learning.", "introduction": "Deep neural networks (DNNs) have demonstrated impressive performance in many fields of research with applications ranging from image classification (Krizhevsky et al., 2012;He et al., 2016) and semantic segmentation (Long et al., 2015) to speech recognition (Hinton et al., 2012), just to name a few.Despite this success, DNNs are still susceptible to small imperceptible perturbations, which can lead to drastic performance degradation, especially in visual classification tasks.Such perturbations are best known and commonly referred to as adversarial attacks.Early work showed that simple algorithms (e.g.maximizing the classification loss with respect to the input using a single optimization iteration (Goodfellow et al., 2014)) can easily construct such adversaries.Since then, a research surge has emerged to develop simple routines to construct adversarial examples consistently.For instance, Moosavi-Dezfooli et al. (2016) proposed a simple algorithm, called DeepFool, which finds the smallest perturbation that fools a linearized version of the network.Interestingly, the work of Moosavi-Dezfooli et al. (2017) demonstrated that such adversaries can be both network and input agnostic, i.e. universal deterministic samples that fool a wide range of DNNs across a large number of input samples.More recently, it was shown that such adversaries can also be as simple as Gaussian noise (Bibi et al., 2018).Knowing that DNNs are easily susceptible to simple attacks can hinder the public confidence in them, especially for real-world deployment, e.g. in self-driving cars and devices for the visually impaired.Such a performance nuisance has prompted several active research directions, in particular, work towards network defense and verification.Network defense aims to train networks that are robust against adversarial attacks through means of robust training or procedures at inference time that dampen the effectiveness of the attack (Madry et al., 2018;Wong & Kolter, 2018;Raghunathan et al., 2018;Alfadly et al., 2019).On the other hand, verification aims to certify/verify for a given DNN that there exists no small perturbation of a given input that can change its output prediction (Katz et al., 2017;Sankaranarayanan et al., 2016;Weng et al., 2018a).However, there are also works at the intersection of both often referred to as robustness verification methods, which use verification methods to train robust networks.Such algorithms often try to minimize the exact (or upper bound) of the worst adversarial loss over all possible bounded energy (often measured in \u221e norm) perturbation around a given input.Although verification methods prove to be effective in training robust networks (Wong & Kolter, 2018), they are computationally expensive, thus limiting their applicability to only small, at best medium, sized networks.However, Gowal et al. (2019) recently demonstrated that robustly training large networks is possible by leveraging the cheap-to-compute but very loose interval-based verifier, known as interval domain from Mirman et al. (2018).In particular, they propagate the -\u221e norm bounded input centered at x \u2208 R n , i.e. [x -1 n , x + 1 n ], through every layer in the network at a time.This interval bound propagation (IBP) is inexpensive and simple; however, it results in very loose output interval bounds, which in turn necessitates a complex and involved training procedure.Closer to our work, there has been several prior arts that propose to perform verification differently.In particular, Webb et al. (2018) presents a statistical approach to assessing robustness of neural networks.As opposed to verification methods, which in many cases can be hard to scale, that provide a binary measure of robustness per sample, they propose to frame verification as a probability of violation instead.That is to say, they investigate the probability of failure over a violation rather than confirming that this probability is exactly zero.Moreover, (Weng et al., 2018b) propose CLEVER which estimates a lower bound to the minimum perturbation rather that finding the lower bounds exactly.In both works, the verification is tackled by, and closely related to our direction of work, probabilistically estimating the bounds.In this paper, we are interested in improving the tightness of output interval bounds (referred to as bounds from now on).We do so by closely examining the bounds for a block of layers composed of an affine layer, followed by a ReLU nonlinearity, followed by another affine layer under -\u221e bounded input.In fact, we propose new expected bounds for this block of layers, which we prove to be not only supersets to the true bounds of this block in expectation but also very tight to the true bounds.Lastly, we show how to extend such a result to deeper networks through blockwise bound propagation leading to several orders of magnitude tighter bounds as compared to IBP.Contributions.Our contributions are three-fold.(i) We propose new bounds for the block of layers composed of an affine layer, followed by a ReLU, followed by another affine layer.We prove that these bounds are in expectation, under a distribution of network parameters, supersets to the true bounds of this block.Moreover, we prove that these bounds are much tighter, in expectation (will be formalized later) than the IBP bounds (Gowal et al., 2019) generated by propagating the input bounds through every layer in the block.Our bounds get even tighter as the number of hidden nodes in the first affine layer increases.(ii) We show a practical and efficient approach to propagate our bounds (for the block of layers) through blocks (not through individual layers) of a deep network, thus resulting in magnitudes tighter output bounds compared to IBP. (iii) We conduct experiments on synthetic networks and on real networks, to verify the theory, as well as the factors of improvement over IBP.Due to the tightness of our proposed expected bounds, we show that with a simple standard training procedure, large/deep networks can be robustly trained on both MNIST (LeCun, 1998) and CIFAR10 (Krizhevsky & Hinton, 2009) achieving state-of-art robustness-accuracy trade-off compared to IBP.In other words, we can consistently improve robustness by significant margins with minimal effect on test accuracy as compared to IBP."}
{"paper_id": 50, "abstract": "In the realm of understanding the laws that govern our physical world, we embark on a quest to unravel the mysteries of intuitive physics, drawing our insights from the chaotic tapestry of unlabelled visual input. Unlike the scholars of yore, who sought only to grasp the broad strokes of universal principles, we delve deeper, seeking to adapt our understanding to the unique contours of new environments, drawing upon just a handful of experiences like a seasoned traveler navigating uncharted lands.  Our approach is akin to wielding a powerful magic: we employ a meta-learning framework that operates in the shadows of unsupervised learning, aiming to predict vivid sequences of motion\u2014videos that capture the dance of objects as they collide and interact against intricate backdrops. To distill the essence of past encounters, we conjure dynamic images that serve as compact vessels of knowledge, allowing us to tackle the challenges of physical prediction with both grace and efficiency.  Through rigorous experimentation and meticulous ablation studies, we unveil a model that not only predicts the behavior of physical entities with remarkable accuracy but also demonstrates an uncanny ability to generalize across time and space, adapting seamlessly to a shifting landscape of interacting objects. Thus, we lay the groundwork for a new era of understanding, where the nuances of our environment can be learned and anticipated with the deftness of a masterful storyteller weaving together the threads of experience into a coherent narrative.", "introduction": "Many animals possess an intuitive understanding of the physical world.They use this understanding to accurately and rapidly predict events from sparse sensory inputs.In addition to general physical principles, many animals also learn specific models of new environments as they experience them over time.For example, they can explore an environment to determine which parts of it can be navigated safely and remember this knowledge for later reuse.Authors have looked at equipping artificial intelligences (AIs) with analogous capabilities, but focusing mostly on performing predictions from instantaneous observations of an environment, such as a few frames in a video.However, such predictions can be successful only if observations are combined with sufficient prior knowledge about the environment.For example, consider predicting the motion of a bouncing ball.Unless key parameters such as the ball's elasticity are known a priori, it is impossible to predict the ball's trajectory accurately.However, after observing at least one bounce, it is possible to infer some of the parameters and eventually perform much better predictions.In this paper, we are interested in learning intuitive physics in an entirely unsupervised manner, by passively watching videos.We consider situations in which objects interact with scenarios that can only be partially inferred from their appearance, but that also contain objects whose parameters cannot be confidently predicted from appearance alone (fig.1).Then, we consider learning a system that can observe a few physical experiments to infer such parameters, and use this knowledge to perform better predictions in the future.Our model has three goals.First, it must learn without the use of any external or ad-hoc supervision.We achieve this by training our model from raw videos, using a video prediction error as a loss.Second, our model must be able to extract information about a new scenario by observing a few experiments, which we formulate as meta-learning.We also propose a simple representation of the experiments based on the concept of \"dynamic image\" that allows to process long experiments more efficiently than using a conventional recurrent network.Third, our model must learn a good representation of physics without access to any explicit or external supervision.Instead, we propose three tests to support this hypothesis.(i) We show that the model can predict far in the future, which is a proxy to temporal invariance.(ii) We further show that the model can extend to scenarios that are geometrically much larger than the ones used for training, which is a proxy to spatial invariance.(iii) Finally, we show that the model can generalize to several In order to support these claims, we conduct extensive experiments in simulated scenarios, including testing the ability of the model to cope with non-trivial visual variations of the inputs.While the data is simpler than a real-world application, we nevertheless make substantial progress compared to previous work, as discussed in section 2. We do so by learning from passive, raw video data a good model of dynamics and collisions that generalizes well spatially, temporally, and to a variable number of objects.The scalability of our approach, via the use of the dynamic image, is also unique.Finally, we investigate the problem of learning the parameters of new scenarios on the fly via experiences and we propose an effective solution to do so."}
{"paper_id": 51, "abstract": "In the realm of robotic learning, the path to mastering control policies often feels like an arduous journey, fraught with the constraints of slow learning rates, limited update bounds, and the ever-elusive nature of unknown constraints. Yet, in stark contrast, humans possess an uncanny ability to glean protective and safe solutions from mere moments of failure or unexpected observations.   To bridge this chasm between human intuition and robotic learning, we have crafted a hierarchical Bayesian optimization algorithm that mirrors the cognitive processes of inference and memory, enabling the avoidance of failures in motor control tasks. At its core, a Gaussian Process serves as the backbone for modeling and sampling the acquisition function, allowing for swift learning through larger learning rates. Meanwhile, a mental replay phase acts as a safeguard, inhibiting policy regions associated with past failures during the sampling process.  We put our hierarchical Bayesian optimization method to the test within a simulated and physiological humanoid postural balancing task. By quantitatively comparing our learning approach to human performance, we assess the deviations of the center of mass throughout training. The results are compelling: our method not only replicates the efficient learning exhibited by human subjects in postural control tasks but also offers a robust, testable model for future physiological motor control challenges. In this arena, our approach outshines traditional Bayesian Optimization, demonstrating superior efficiency in the number of interactions required to solve the task, reduced computational demands, and a notable decrease in the frequency of observed failures.", "introduction": "Autonomous systems such as anthropomorphic robots or self-driving cars must not harm cooperating humans in co-worker scenarios, pedestrians on the road or them selves.To ensure safe interactions with the environment state-of-the-art robot learning approaches are first applied to simulations and afterwards an expert selects final candidate policies to be run on the real system.However, for most autonomous systems a fine-tuning phase on the real system is unavoidable to compensate for unmodelled dynamics, motor noise or uncertainties in the hardware fabrication.Several strategies were proposed to ensure safe policy exploration.In special tasks like in robot arm manipulation the operational space can be constrained, for example, in classical null-space control approaches Baerlocher & Boulic (1998); Slotine (1991); Choi & Kim (2000); Gienger et al. (2005); Saab et al. (2013); Modugno et al. (2016) or constraint black-box optimizer Hansen et al. (2003); Wierstra et al. (2008); Kramer et al. (2009); Sehnke et al. (2010); Arnold & Hansen (2012).While this null-space strategy works in controlled environments like research labs where the environmental conditions do not change, it fails in everyday life tasks as in humanoid balancing where the priorities or constraints that lead to hardware damages when falling are unknown.Alternatively, limiting the policy updates by applying probabilistic bounds in the robot configuration or motor command space Bagnell & Schneider (2003); Peters et al. (2010); Rueckert et al. (2014); Abdolmaleki et al. (2015); Rueckert et al. (2013) were proposed.These techniques do not assume knowledge about constraints.Closely related are also Bayesian optimization techniques with modulated acquisition functions Gramacy & Lee (2010); Berkenkamp et al. (2016); Englert & Toussaint (2016); Shahriari et al. (2016) to avoid exploring policies that might lead to failures.However, all these approaches do not avoid failures but rather an expert interrupts the learning process when it anticipates a potential dangerous situation.All the aforementioned strategies cannot avoid harming the system itself or the environment without thorough experts knowledge, controlled environmental conditions or human interventions.As humans require just few trials to perform reasonably well, it is desired to enable robots to reach similar performance even for high-dimensional problems.Thereby, most approaches are based on the assumption of a \"low effective dimensionality\", thus most dimensions of a high-dimensional problem do not change the objective function significantly.In Chen et al. (2012) a method for relevant variable selection based on Hierarchical Diagonal Sampling for both, variable selection and function optimization, has been proposed.Randomization combined with Bayesian Optimization is proposed in Wang et al. (2013) to exploit effectively the aforementioned \"low effective dimensionality\".In Li et al. (2018) a dropout algorithm has been introduced to overcome the high-dimensionality problem by only train onto a subset of variables in each iteration, evaluating a \"regret gap\" and providing strategies to reduce this gap efficiently.In Rana et al. (2017) an algorithm has been proposed which optimizes an acquisition function by building new Gaussian Processes with sufficiently large kernellengths scales.This ensures significant gradient updates in the acquisition function to be able to use gradient-dependent methods for optimization.The contribution of this paper is a computational model for psychological motor control experiments based on hierarchical acquisition functions in Bayesian Optimization (HiBO).Our motor skill learning method uses features for optimization to significantly reduce the number of required roll-outs.In the feature space, we search for the optimum of the acquisition function by sampling and later use the best feature configuration to optimize the policy parameters which are conditioned on the given features, see also Figure 1.In postural control experiments, we show that our approach reduces the number of required roll-outs significantly compared to standard Bayesian Optimization.The focus of this study is to develop a testable model for psychological motor control experiments where well known postural control features could be used.These features are listed in Table 3.In future work we will extend our model to autonomous feature learning and will validate the approach in more challenging robotic tasks where 'good' features are hard to hand-craft."}
{"paper_id": 52, "abstract": "In the intricate realm of neural networks, where the threads of computation weave the fabric of understanding, various methods have emerged to measure unit selectivity\u2014a quest to unravel the mysteries of how these artificial minds perceive the world. Yet, much like the differing perspectives of a diverse fellowship on a single quest, these measures yield divergent estimates of selectivity, leading to conflicting conclusions about the conditions under which selective representations of objects are forged and the significance of these representations in the grand tapestry of cognition.  In our pursuit to illuminate the nature of object selectivity, we embark on a comparative exploration of several selectivity measures across a vast array of units within the legendary AlexNet. Our investigation encompasses localist selectivity, precision, class-conditional mean activity selectivity (CCMAS), network dissection, the human interpretation of activation maximization (AM) images, and standard signal-detection measures. What we uncover is a landscape marked by inconsistency: different measures paint varying portraits of object selectivity, with precision and CCMAS often inflating estimates to misleading heights. Strikingly, the units deemed most selective frequently exhibit dismal hit rates or alarmingly high false-alarm rates\u2014rendering them ineffective as reliable object detectors.  Moreover, our search yields no units that approach the mythic 'grandmother cell' status celebrated in the annals of recurrent neural networks. To extend our findings beyond the confines of AlexNet, we turn our gaze to selectivity measures on select units in VGG-16 and GoogLeNet, both trained on the esteemed ImageNet and Places-365 datasets, heralded as 'object detectors.' Yet again, we confront the same disheartening reality: poor hit rates and pervasive false alarms mar the quest for true object classification.  In this tale of exploration and discovery, we find ourselves grappling with the complexities of neural selectivity, challenging the very foundations of our understanding and beckoning further inquiry into the enigmatic workings of these artificial intelligences.", "introduction": "There have been recent attempts to understand how neural networks (NNs) work by analyzing hidden units one-at-a-time using various measures such as localist selectivity (Bowers et al., 2014), class-conditional mean activity selectivity (CCMAS) (Morcos et al., 2018), precision (Zhou et al., 2015), network dissection (Zhou et al., 2018a), and activation maximization (AM) (Erhan et al., 2009).These measures are all taken to provide evidence that some units respond highly selectively to categories of objects under some conditions.Not only are these findings surprising given the widespread assumption that NNs only learn highly distributed and entangled representations, they raise a host of questions, including the functional importance of these selective representations (Zhou et al., 2018b), the conditions in which they are learned (e.g., Morcos et al., 2018), and the relation between these representations and the selective neurons observed in cortex (Bowers, 2009).To answer these question, it is necessary to have a better understanding of what these metrics actually measure, and how they relate to one another.Accordingly, we directly compare these measures of selectivity on the same set of units as well as adopt standard signal-detection measures in an attempt to provide better measures of single-unit selectivity to object category.In addition, to provide a more intuitive assessment of selectivity, we report jitterplots for a few of the most selective units that visually display how the unit responds to the different image categories.We focus on AlexNet (Krizhevsky et al., 2012) trained on ImageNet (Deng et al., 2009) because many authors have studied the selectivity of single hidden units in this model using a range of quantitative (Zhou et al., 2018a;2015) and qualitative (Nguyen et al., 2017;Yosinski et al., 2015;Simonyan et al., 2013) methods.But we also compare different selectivity measures on specific units in VGG-16 (Simonyan and Zisserman, 2014) and GoogLeNet (Szegedy et al., 2015) trained on the the ImageNet and Places-365 datasets that were characterized by Zhou et al. (2018a) as \"object detectors\" based on their Network Dissection method (Zhou et al., 2018a).Our main findings are:1.The precision and CCMAS measures are misleading with near-maximum selectivity scores associated with units that strongly respond to many different image categories.By contrast, the signal-detection measures more closely capture the level of selectivity displayed in the jitterplots (Sec.3.1).2. Units with interpretable AM images do not correspond to highly selective representations (Sec.3.2).3. The Network Dissection method also provides a misleading measure for \"object detectors\" (Sec.3.3).In one line of research, Bowers et al. (2014;2016) assessed the selectivity of single hidden units in recurrent neural networks (RNNs) designed to model human short-term memory.They reported many 'localist' or 'grandmother cell' units that were 100% selective for specific letters or words, where all members of the selective category were more active than and disjoint from all non-members, as can be shown in jitterplots (Berkeley et al., 1995) (see Fig. 1 for a unit selective to the letter 'j').The authors argued that the network learned these representations in order to co-activate multiple letters or words at the same time in short-term memory without producing ambiguous blends of overlapping distributed patterns (the so-called 'superposition catastrophe').Consistent with this hypothesis, localist units did not emerge when the model was trained on letters or words one-at-a-time (Bowers et al., 2014) (see Fig. 1 for an example of a non-selective unit).In parallel, researchers have reported selective units in the hidden layers of various CNNs trained to classify images into one of multiple categories (Zhou et al., 2015;Morcos et al., 2018;Zeiler and Fergus, 2014;Erhan et al., 2009), for a review see Bowers (2017).For example, Zhou et al. (2015) assessed the selectivity of units in the pool5 layer of two CNNs trained to classify images into 1000 objects and 205 scene categories, respectively.They reported many highly selective units that they characterized as 'object detectors' in both networks.Similarly, Morcos et al. (2018) reported that CNNs trained on CIFAR-10 and ImageNet learned many highly selective hidden units, with CCMAS scores approaching the maximum of 1.0.These later findings appear to be inconsistent with Bowers et al. (2016) who failed to observe selective representations in fully connected NNs trained on stimuli one-at-a-time (see Fig. 1), but the measures of selectivity that have been applied across studies are different, and accordingly, it is difficult to directly compare results.A better understanding of the relation between selectivity measures is vital given that different measures are frequently used to address similar issues.For example, both the human interpretability of generated images (Le, 2013) and localist selectivity (Bowers et al., 2014) have been used to make claims about 'grandmother cells', but it is not clear whether they provide similar insights into unit selectivity.Similarly, based on their precision metric, Zhou et al. (2015) claim that the object detectors learned in CNNs play an important role in identifying specific objects, whereas Morcos et al. (2018) challenge this conclusion based on their finding that units with high CCMAS measures were not especially important in the performance of their CNNs and concluded: \"...it implies that methods for understanding neural networks based on analyzing highly selective single units, or finding optimal inputs for single units, such as activation maximization (Erhan et al., 2009) may be misleading\".This makes a direct comparison between selectivity measures all the more important.In order to directly compare and have a better understanding of the different selectivity measures we assessed (1) localist, (2) precision, and (3) CCMAS selectivity of the conv5, fc6, and fc7 of AlexNet trained on ImageNet, and in addition, we employed a range of signal detection methods on these units, namely, (4) recall with 100% and 95% precision, (5) maximum informedness, (6) specificity at maximum informedness , and (7) recall (also called sensitivity) at maximum informedness, and false alarm rates at maximum informedness (described in Sec. 2).We also assessed the selectivity of a few units in VGG-16 and GoogLeNet models trained on the ImageNet and Places-365 dataset that were highly selective according to the Network Dissection method (Zhou et al., 2018a).We show that the precision and CCMAS measures often provide misleadingly high estimates of object selectivity compared to other measures, and we do not find any units that can be reasonably described as 'object detectors' given that the most selective units show a low hit-rate or a high false-alarm rate (or both) when classifying images.At best, the most selective units in CNNs are sensitive to some unknown feature that is weakly associated with the class in question.Figure 1: Examples of selectivity measures used.Top left: jitterplot of unit 113 in an RNN (under the superposition constraint) selective to the letter 'j' (Bowers et al., 2016).Top middle: jitterplot of a non-selective unit 160 found in an RNN trained on words one-at-a-time from (Bowers et al., 2016).Top right: Activation maximization image of unit conv5 9 AlexNet that resembles a lighthouse (Nguyen et al., 2016).Bottom: highest-activation images for a 'lamp' detector with 84% precision in the layer conv5 of AlexNet; from (Zhou et al., 2015).In addition to these quantitative measures and jitterplots we assessed selectivity with a common qualitative measure, namely, human interpretation of images generated by a state-of-the-art activation maximization (AM) method (Nguyen et al., 2017).AM images are generated to strongly activate individual units, and some of them are interpretable by humans (e.g., a generated image that looks like a lighthouse, see Fig. 1).For the first time, we systematically evaluated the interpretability of the AM images and compare these ratings with the selectivity measures for corresponding units.We show that the few hidden units with interpretable AM images are not highly selective."}
{"paper_id": 53, "abstract": "In the ever-evolving realm of graph neural networks, a captivating surge of interest has emerged from researchers, drawn to the dual challenges of node classification within a graph and the broader task of graph classification across diverse sets. The integration of attention mechanisms has proven to be a powerful ally, enhancing the performance of these networks by honing in on the pivotal neighbors that shape a node's identity. Yet, the complexities of the real world reveal a deeper truth: sometimes, it is not the individual nodes that hold the key, but rather a specific subset of them, united in purpose, that collectively influence a node's label.  To unravel this intricate tapestry, we introduce the innovative concept of subgraph attention\u2014a transformative approach that recognizes the significance of these interconnected groups. By weaving subgraph attention into the fabric of graph convolution, we elevate node classification to new heights. But our journey does not end there. We extend this concept further, unveiling a groundbreaking hierarchical neural graph pooling architecture designed for comprehensive graph classification. This architecture not only employs subgraph attention to spotlight crucial substructures but also harnesses attention mechanisms to identify the most significant nodes within each level of the graph and to discern the vital levels throughout the entire hierarchy.  Our results speak volumes, showcasing competitive performance that surpasses the state-of-the-art in both node and graph classification tasks. The algorithms we present are not merely enhancements; they are a testament to the potential of subgraph attention, illuminating a path forward in the quest for deeper understanding within the complex world of graphs.", "introduction": "Graphs are the most suitable way to represent different types of relational data such as social networks, protein interactions and molecular structures.Typically, A graph is represented by G = (V, E), where V is the set of nodes and E is the set of edges.Further, each node v i \u2208 V is also associated with an attribute (or feature) vector x i \u2208 R D .Recent advent of deep representation learning has heavily influenced the field of graphs.Graph neural networks are developed to use the underlying graph as a computational graph and aggregate node attributes from the neighbors of a node to generate the node embeddings (Kipf & Welling, 2017;Niepert et al., 2016).Different types of attribute aggregation approaches are proposed in the literature (Hamilton et al., 2017).Attention mechanisms on graphs show promising results for both node classification (Veli\u010dkovi\u0107 et al., 2018) and graph classification (Lee et al., 2019;2018) tasks.There are different ways to compute attention mechanisms on graph.Veli\u010dkovi\u0107 et al. (2018) compute attention between a pair of nodes in the immediate neighborhood to capture the importance of a node on the embedding of the other node by learning an attention vector.Lee et al. (2018) compute attention between a pair of nodes in the neighborhood to guide the direction of a random walk in the graph for graph classification.Lee et al. (2019) propose self attention pooling of the nodes which is then used to capture the importance of the node to generate the label of the entire graph.Most of the attention mechanisms developed in graph literature use attention to derive the importance of a node or a pair of node for different tasks.But in real world situation, calculating importance up to a pair of nodes is not adequate.Often due to the presence of a substructure in the vicinity of a node v determines its role (or label) in the graph, or determines the label of the entire graph.But the influence of each node individually from that substructure to the node v may not be significant.In Figure 1, each node (indexed from a to g) in the small synthetic graph can be considered as an agent whose attributes determine its opinion (1:positive, 0: neutral, -1: negative) about 4 products.Suppose the graph can be labelled +1 only if there is a subset of connected (by edges) agents who jointly have positive opinion about all the product.In this case, the blue shaded connected subgraph (a, b, c) is important to determine the label of the graph.Please note, attention over the pairs (Veli\u010dkovi\u0107 et al., 2018) is not enough as (a, b) cannot make the label of the graph +1 by itself.Also multiple layers of such attention may not work as the aggregated features of a node get corrupted after the feature aggregation by the first attention layer.With this motivation, we develop a novel attention mechanism in the graph which operates in the subgraph level in the vicinity of a node.We call it subgraph attention mechanism and use it for both node classification and graph classification tasks, which we define formally next.Node classification: Given a graph G = (V, E) with each node v i associated with an attribute vector x i \u2208 R D , and a subset of nodes V s \u2286 V with each node v i \u2208 V s labelled with y i \u2208 L n (set of discrete labels for the nodes of the graph), the task is to predict the label of a node v j \u2208 V u = V \\ V s using the structure and the node attributes of the entire graph and the node labels from V s .Essentially, this leads to learning a function f n : V \u2192 L n for the given graph G.and a subset of graphs G s \u2286 G with each graph G i \u2208 G s are labelled with Y i \u2208 L g (the subscript g stands for 'graphs'), the task is to predict the label of a graph G j \u2208 G u = G \\ G s using the structure of the graphs and the node attributes, and the graph labels from G s .Again, this leads to learning a function f g : G \u2192 L g .Here, L g is the set of discrete labels for the graphs."}
{"paper_id": 54, "abstract": "In the realm of neuroscience, a remarkable innovation has emerged: Spiking Neural Networks (SNNs). These brain-inspired constructs offer a dynamic approach to information processing, mimicking the event-driven nature of biological neurons. Yet, despite their promise, SNNs have struggled to match the performance of their more established counterparts, the Artificial Neural Networks (ANNs), particularly when faced with complex tasks. While ANNs often bear the brunt of criticism for their costly computations and lack of genuine biological fidelity, the temporal dynamics intrinsic to Recurrent Neural Networks (RNNs) present a unique opportunity. By integrating SNNs into an RNN framework, we can replicate the intricate dance of membrane potential over time. To this end, we propose the Recurrent Leaky Integrate-and-Fire (RLIF) model, designed to tackle a host of challenges, including the limitations of discrete binary outputs and dynamic behaviors.  Our experimental results reveal that this recurrent architecture exhibits exceptional resilience to interference, adhering closely to the principles of SNNs with its inherently discrete spike outputs. Moreover, it demonstrates impressive performance on neuromorphic datasets and holds promise for broader applications, such as text summarization and video comprehension. In this way, we stand on the precipice of a new frontier in neural network design, one that harmonizes the elegance of biological inspiration with the rigor of computational efficiency.", "introduction": "The terms of deep learning and the corresponding artificial neural networks (ANNs) derivatives have been dominating in subject of computer science and keep the current state-of-the-art performance in a widespread of machine learning's application scenario such as computer vision (Simonyan & Zisserman, 2014), natural language processing (Collobert & Weston, 2008), speech/audio recognition (Hinton et al., 2012), video understanding (Ye et al., 2015) since the first arising of the AlexNet (Krizhevsky et al., 2012), even some of them has beat the humans' cognitive level in certain tasks.However, ANNs fail to uptake the advantages of the Neuronal Dynamics, which instantiates as high-power consumption, relatively low responses and etc.Spiking Neuron Networks(SNNs) (Maass, 1997), with inspiration for the propagation of the cortex neurons (Perrett et al., 1982;Tuckwell, 1988), have been presented continuous attention as a new, power-efficient and hardware friendly technology.In contrast to the mere implementation of spatial information and complicated float point computation of ANNs, SNNs utilize spatial-temporal dynamics to mimic the bio-behavior of neurons, as well as its dyadic-valued computation whose feeding electrical sequential impulses (i.e., spikes), belong to the binary-like set of {0,1}.Benefit from the capabilities of processing binary-spiking signal and consequential effectiveness, there is an alternative for SNNs that has a feasibility of further development of machine learning and neuromorphic application, which has been long-term significantly deployed in many neuromorphic hardware including SpiNNaker (Furber et al., 2014), TrueNorth (Akopyan et al., 2015) and Loihi (Davies et al., 2018).In contrast to the ANNs' well advanced, salient, proficient training methodology that indicate the conception of BackPropagation(BP) (LeCun et al., 1998) along with its derivatives that consequently give rise to the convergence of ANNs and diverse categories of frameworks(ie.TensorFlow, Py-Torch, et al.) that make it succinct and available to train more deeper networks.However, for one thing, there are not so much theoretically supported or potent procedure for tackling the issue of training SNNs, which limits SNNs from going deeper, therefore SNNs hardly fulfill the ability in real-world complex missions, such as video-based recognition/detection, natural language pro-cessing et al..For another thing, there no exit practical auxiliary frameworks that are capable to promote the mature structure of SNNs, which leads to the consequence of few application and rare forward-step development of SNNs.There are still various efforts to make progress in training, deepening the depth and applications of SNNs, whereas many obstacles block the development of SNNs at the same time.As for training, there are many circumvention ways to strengthen the accuracy of SNNs, except for neuromorphic methodology such as spike-timing-dependent plasticity (STDP) (Serrano-Gotarredona et al., 2013), winner-taken all (WTA) (Makhzani & Frey, 2015).In the first alternative scheme, an ANN is trained firstly, then it is transformed into the SNN version whose network structure is the same as the abovementioned ANN, and neurons analog the behavior of ANN neurons (Diehl et al., 2015).The other is the direct supervised learning, also called Gradient descend, which is a superior, prevalent optimization method for this learning procedure.In order to solve the issue of the non-differential problems of spikes, (Lee et al., 2016) proposed an alternate that treats membrane potential as differential signals and directly uses BP algorithm to train deep SNNs.To act as more bio-behavior, (Ponulak & Kasi\u0144ski, 2010) introduced the remote supervised STDP-like rule to be capable of the learning of sequential output spike.Besides, (Urbanczik & Senn, 2009) proposed a novel leaning rule whose information will be embedded into the spatio-temporal information during learning of the spike signals.Nevertheless, most of the learning methods presented above are merely engaged in a single aspect of either spatial or temporal information.The applications started to spring up due to the incoming of the event-based cameras composed of Dynamic Visual Sensors(DVS) (Shi et al., 2018).The mechanism of DVS can be outlined as a simulation of the visual path structures and functionalities of the biological visual systems whose neurons asynchronously communicate and encode the visual information from environment as spatiotemporally sparse light intensity change in the form of spikes.On the strength of the event-based cameras, diverse event-based datasets were acquired such as Poker-DVS, MNIST-DVS (Serrano-Gotarredona & Linares-Barranco, 2015) and CIFAR10-DVS (Wu et al., 2019).Embracing the event-based cameras and their derived datasets, a variety of monographs demonstrate the different methodologies whose intentions are to make a plausibility of the application of accordingly components.(Peng et al., 2016) proposed an event-based classification based on static learning method, named Bag of Events (BOE in short).This method denotes the events of corresponding to the activated pixel of the DVS as joint probability distribution.Moreover, this method tests on multiple datasets such as NMNIST, MNIST-DVS, Poker-DVS, and it reveals that BOE can significantly achieve competitive results in real-time for feature extraction and implementation time as well as the classification accuracy.(Neil & Liu, 2016) proposed a deep CNN to pre-process spiking data from DVS, which is used in various deep network architecture and is also used to achieve an accuracy of 97.40% on N-MNIST datasets, in spite of its complicated pre-processing approach.In terms of SNNs, (Indiveri et al., 2015) proposed a SNN architecture, named Feedforward SNN, which is based on spike-based learning and temporary learning, and it achieves 87.41% accuracy on MNIST-DVS datasets.(Stromatias et al., 2015) proposed a composite system, including convolutional SNNs, non-spiking fully connected classifier, and spiking output layer with its performance of 97.95% of accuracy.Together with improving the performance and enhancing the convergence rate of SNNs, the goal that whether a method that can absorb both advantages of ANNs and SNNs can be achieved.To this end, we propose RLIF with both low computational complexity and biological plausibility, to explore its usage in real-world tasks.In summary, the major contributions of this paper can be listed as follows:\u2022 We propose RLIF, which absorbs the biological traits from SNNs, follows the unroll structure of RNNs, and enables a seamless way to insert into any sequential model in common deep learning frameworks.\u2022 A mass throughput can be implemented through the transition of binary information between an interlayer of RLIF and other sequential layers, which meets the basic principle that the emission of neuron trains are binary values.Furthermore, RLIF can be easily extended into neuromorphic chips since its peculiarity of hardware-friendly.and Chinese text summarization (LCSTS-2.0)show that our RLIF is capable of capturing key information through time and has lower parameters compared to its counterparts.As mentioned before, the core idea in our architecture is about how to absorb the biological traits of SNN into RNN.To this end, learning algorithm in SNN will be introduced first and then we do a simple analysis on basic LIF neuron model, which aims to highlight the most relevant parts to our RLIF."}
{"paper_id": 55, "abstract": "In the realm of statistical modeling, the quest to capture relationships that extend beyond mere averages is a journey fraught with challenges. Conditional density estimation (CDE) emerges as a powerful ally, seeking to unveil the intricate tapestry of full conditional probability densities woven from data. Yet, as with all potent tools, CDE models built on neural networks often find themselves ensnared by the specter of overfitting, particularly when wielded with the maximum likelihood objective. The very architecture that grants these models their expressive power also renders traditional regularization techniques ineffective, leaving them vulnerable in the face of limited training data.  To combat this formidable dilemma, we introduce a novel, model-agnostic noise regularization strategy for CDE. This innovative approach infuses the training process with random perturbations, akin to a gentle breeze that smooths the rough edges of our data. We reveal that this method not only serves as a form of smoothness regularization but also demonstrate its asymptotic consistency, providing a robust theoretical foundation for our claims.  Through rigorous experimentation, we showcase the remarkable prowess of noise regularization, which consistently eclipses other regularization strategies across a diverse array of seven datasets and three distinct CDE models. The compelling results affirm that with this new technique, neural network-based CDE stands as a formidable contender\u2014often surpassing traditional non-parametric and semi-parametric methods\u2014even in the face of scarce training data. Thus, we embark on a new chapter in the realm of statistical modeling, where the power of neural networks shines brighter than ever before.", "introduction": "While regression analysis aims to describe the conditional mean E[y|x] of a response y given inputs x, many problems such as risk management and planning under uncertainty require gaining insight about deviations from the mean and their associated likelihood.The stochastic dependency of y on x can be captured by modeling the conditional probability density p(y|x).Inferring such a density function from a set of empirical observations {(x n , y n )} N n=1 is typically referred to as conditional density estimation (CDE) and is the focus of this paper.In the recent machine learning literature, there has been a resurgence of interest in high-capacity density models based on neural networks (Dinh et al., 2017;Ambrogioni et al., 2017;Kingma & Dhariwal, 2018).Since this line of work mainly focuses on the modelling of images based on large scale data sets, over-fitting and noisy observations are of minor concern in this context.In contrast, we are interested in CDE in settings where data may be scarce and noisy.When combined with maximum likelihood estimation, the flexibility of such high-capacity models results in over-fitting and poor generalization.While regression typically assumes Gaussian conditional noise, CDE uses expressive distribution families to model deviations from the conditional mean.Hence, the overfitting problem tends to be even more severe in CDE than in regression.Classical regularization of the neural network weights such as weight decay (Pratt & Hanson, 1989) has been shown to be effective for regression and classification.However, in the context of CDE, the output of the neural network merely controls the parameters of a density model such as a Gaussian Mixture or Normalizing Flow.This makes the standard regularization methods in the parameter space less effective and harder to analyze.Aiming to address this issue, we propose and analyze noise regularization, a method well-studied in the context of regression and classification, for the purpose of conditional density estimation.In that, the paper attempts to close a gap in previous research.By adding small random perturbations to the data during training, the conditional density estimate is smoothed and tends to generalize better.In fact, we show that adding noise during maximum likelihood estimation is equivalent to penalizing the second derivatives of the conditional log-probability.Visually, the respective regularization term punishes very curved or even spiky density estimators in favor of smoother variants, which proves to be a favorable inductive bias in many applications.Moreover, under some regularity conditions, we show that the proposed regularization scheme is asymptotically consistent, converging to the unbiased maximum likelihood estimator.This does not only support the soundness of the proposed method but also endows us with useful insight in how to set the regularization intensity relative to the data dimensionality and training set size.Overall, the proposed noise regularization scheme is easy to implement and agnostic to the parameterization of the CDE model.We empirically demonstrate its effectiveness on three different neural network based models.The experimental results show that noise regularization outperforms other regularization methods significantly and consistently across various data sets.Finally, we demonstrate that, when properly regularized, neural network based CDE is able to improve upon state-of-the art non-parametric estimators, even when only 400 training observations are available."}
{"paper_id": 56, "abstract": "In the vast expanse of user-generated content, opinions flourish not only in the dominant tongues of the world, such as English, but also in the rich tapestry of less prominent languages, like Amharic. Yet, amidst this linguistic diversity, the art of negation handling\u2014a crucial element in the realm of sentiment detection\u2014has remained largely underdeveloped for these less dominant languages. This presents a formidable challenge for the task of sentiment classification.   In response, this study forges a new path, crafting innovative negation handling schemes that significantly enhance sentiment classification for the Amharic language. Our proposed framework unites the strengths of a lexicon-based approach with the precision of a character n-gram machine learning model, creating a powerful tool for sentiment analysis.   We rigorously evaluate the performance of our framework using a dataset of annotated Amharic News Comments, where it emerges victorious, surpassing all existing models and baselines with an impressive accuracy of 98.0%. The results stand in stark contrast to those obtained without negation handling and from traditional word-level n-gram models, illuminating the transformative potential of our approach in the realm of sentiment analysis for less dominant languages.", "introduction": "Due to the emergence of social media including Facebook, Twitter, YouTube, Insta-grams, LinkedIn and so on, the number of users who participates and consumes in-formation is increasing very fast.Users usually express their feelings, emotions and opinions as comments in response to the posted news, photo, audio and video.The texts in social media are informal as it contains spelling errors, slangs, abbreviations or users might use different language.Preprocessing these unstructured and informal texts is very challenging prior to sentiment analysis.Preprocessing includes tokeniza-tion, punctuation mark removal, stop word removal, abbreviation expansion, language detection, spelling error handling, normalization and stemming.The preprocessing task could be harder for under resource languages (e.g.Amharic).Sentiment Analysis is the process of framing the unstructured texts to detect, extract and classify opinion words.Currently, opinionated sources are increasing in languages other than English.Amharic is one of these resource-limited languages.However, Amharic sentiment analysis researches are very few as it has no sufficient linguistic resources for linguistic preprocessing and sentiment analysis.There are several challenges in lexicon based sentiment analysis.One of these is that handling negation in the text.The most common approach for negation handling is carried out relying on negation keywords.However, it is challenging to identify the scope of negation where the process of correctly identifying the part of the text affected by the presence of negation word.In most negation handling researches, simple approach of detection of negation scope is used.Amharic is one of the most spoken Semitic languages in the world next to Arabic.Amharic is morphologically rich and this adds another challenge to detect and extract opinion words in sentiment analysis.Inspired by the aforementioned preprocessing problems, negation handling is never studied in Amharic language to the best of our knowledge.Thus, the main aim of this research is develop an automatic method to handle negation and combined with char ngram features for Amharic Sentiment Analysis relying on these linguistic features.The research questions to be addressed in this work are as follows: (a) how can we automatically detect negation words in Amharic texts?, (b) how can we design a framework for handling negation in Amharic sentiment analysis?, (c) how to capture char level ngram features for improving Amharic Sentiment Analysis in Social media(e..g.Facebook) and (d) how can we evaluate the performance of the framework?The rest of this paper is organized as follows: in the section 2, the related works are presented.The proposed approaches including negation handling and capturing char level ngram features for Amharic Sentiment Analysis is described in section 3.In section 4, results and discussions are presented.Conclusions and recommendations are drawn in section 5.In this section, we briefly present the key related works.Asmi & Ishaya (2012) proposed an approach that can detect negation and considers scope of negation relying on syntactic dependency for sentiment analysis.In (Amalia et al., 2018), proposed rule based negation handling and its scope based on syntactic parsing of Indonesian language for machine learning based sentiment analysis in twitter and the result of support vector machine performs well as compared with other experiments.The F-Score on two Twitter data sets is improved by 1.79% and 2.69% from the existing baseline without negation handling.In (Farooq et al., 2017), develop negation scope handling strategies relying on the linguistic features of the language.The accuracy of negation scope identification is 83.3% which outperforms very well above the baseline.In (Diamantini et al., 2016), develops negation scope detection relying on dependency parsing tree and semantic disambiguation technique and evaluated on integrated social networks in real time.The performance of negation handling is outperforming with accuracy of 6% more than the baseline.The work in (Heerschop et al., 2011) develops negation handling relying on wordbank creation and document sentiment scoring for sentiment analysis and outperforms the human rating by an increase precision with 1.17%.In (Enger et al., 2017), developed negation cue and scope handling open source tool relying on dependency parser, negation cue(negation lists, prefix, suffix) as input to machine learning(e.g.SVM) and its performance is slightly unchanged from the baseline."}
{"paper_id": 57, "abstract": "In the realm of graph theory, the quest for the Maximum Common Subgraph (MCS)\u2014that elusive fragment shared between two graphs\u2014has long been a formidable challenge. This task, while deceptively simple in its definition, is entangled in the complexities of NP-hardness. Existing exact solvers, reliant on heuristic searches, often languish in inefficiency, their performance lacking the guarantees of time complexity that one might hope for.   Recognizing the critical significance of MCS detection, we present a groundbreaking solution: NeuralMCS. This innovative algorithm is underpinned by a cutting-edge neural network model, meticulously designed to learn the intricate node-node correspondences derived from the ground-truth MCS. Coupled with a sophisticated subgraph extraction procedure, NeuralMCS harnesses the neural network's insights to guide its final predictions, ensuring a streamlined and efficient approach.  What sets NeuralMCS apart is its promise of polynomial time complexity, scaling gracefully with the number of nodes in the larger of the two input graphs. Our extensive experiments across four real-world graph datasets reveal a staggering breakthrough: NeuralMCS operates a staggering 48.1 times faster than traditional exact solvers, while simultaneously surpassing the accuracy of all current competitive approximate methods in MCS detection. In this endeavor, we have not merely created a tool; we have forged a new path through the dense forest of graph analysis, illuminating the way forward for researchers and practitioners alike.", "introduction": "Graph data are ubiquitous.Due to its flexible and expressive nature, graphs have been used to store data of various domains.In computational biology, atomic networks can represent molecular compounds.In software analysis, program dependence graphs can describe the data and control dependencies.In social science, social networks can represent community structures.Because of graphs' unique ability to capture these data, algorithms tackling novel tasks on graphs across different domains have been gaining increased interest in the representation learning community.Graph matching, in particular, is a recently popular task with new approaches such as Zanfir & Sminchisescu (2018), Bai et al. (2019a), and Li et al. (2019).These methods either produce a score indicating how much and/or how well two graphs match or a graph alignment indicating how and where two graphs match.The latter case is a far more difficult task than the former one.Current methods perform a special case of graph alignment, namely image matching, where the two input graphs are of spatial structures (ex.pixel grids, object orientation, etc.) To account for more general graphs, we extract the Maximum Common Subgraph (MCS) (Bunke & Shearer, 1998), a widely used metric for graph alignment, and perform matching on the extracted subgraphs.MCS is a very useful metric to match two graphs in real-world applications.For example, in drug discovery, identifying compounds sharing similar substructures which tend to share similarity properties can dramatically reduce the amount of molecules that need to be manually tested (Ehrlich & Rarey, 2011).In addition to molecular science, MCS also has application values in malware detection (Park et al., 2013), pattern recognition (Solnon et al., 2015), computer-aided circuit design (Djoko et al., 1997;Li et al., 2012), etc.Unfortunately, MCS is NP-hard and, to the best of our knowledge, no existing algorithms tackle this problem from a purely machine learning approach.We are among the first to tackle graph matching defined by MCS.This is more challenging than image matching or graph similarity computation, because MCS requires the extraction of the largest connected subgraph that is commonly present in both input graphs.This implies that the two extracted subgraphs must not only be contained in both graphs but also be isomorphic to each other.To capture the MCS definition, our proposed model, NEURALMCS, fundamentally changes the way that the representations are learned.Instead of computing similarity all in one step, we introduce an iterative procedure to match nodes one at a time.By selecting nodes successively, we ensure the extracted subgraphs are connected.We utilize subgraph embeddings to perform a stopping condition check, dealing with subgraph isomorphism and ending the procedure when the connected subgraphs Figure 1: For a graph pair (G 1 , G 2 ), previous works (Bai et al., 2019a;2018;Li et al., 2019) focus on predicting their graph-graph similarity score.In this work, we aim to find the Maximum Common Subgraph (MCS) (circled in red), which requires fine-grained node-node correspondence prediction.It is more useful both due to the application value as described in Section 1 and because of the interpretable similarity result indicated by the node-node mapping.Node text indicates node labels.are the largest.By performing this iterative procedure, we both better ensure the isomorphism of the extracted subgraph as well as capture the recurrent relationship between matching pairs of nodes.We experimentally verify NEURALMCS on four real graph datasets.and show that NEURALMCS can achieve 48.1\u00d7 runtime gain over state-of-the-art exact MCS computation algorithm MC-SPLIT (McCreesh et al., 2017), and is much more accurate than all the baseline approximate approaches to graph matching."}
{"paper_id": 58, "abstract": "In the vast realm of artificial intelligence, the quest for understanding the intricacies of language often hinges on a crucial task: coreference resolution. This endeavor allows machines to weave together the threads of meaning within texts, creating a tapestry of comprehension. At the forefront of this challenge is a cutting-edge end-to-end neural coreference model, which bravely considers every possible span within a document as a potential mention, striving to connect each antecedent with its corresponding mention. Yet, within this ambitious framework lies a hidden flaw\u2014when faced with identical mentions, the model tends to produce eerily similar representations, leading to missteps in its predictions.  In this paper, we embark on a journey to enhance this end-to-end system. Our approach involves the crafting of an innovative attention model, one that skillfully reweighs features based on the diverse contexts surrounding each mention. Through this method, we unlock new potential, allowing our model to rise above its predecessors. The results speak for themselves: our proposed model achieves a remarkable 73.45% F1 score on the development set and an impressive 72.84% on the test set of the CoNLL 2012 Shared Task, decisively outpacing the state-of-the-art. Thus, we stand on the precipice of a new understanding, where context reigns supreme and the intricacies of language are unraveled with newfound clarity.", "introduction": "Coreference is one of the most frequent phenomena in English and the other languages.Coreference resolution is a crucial task before artificial intelligent systems capable of fully understanding the human language.Supervised methods, especially the models using neural-network-generated word representations, achieve outstanding performances Clark & Manning (2016); Lee et al. (2017;2018).However, for similar or identical text units, the problem of wrongfully getting the same coreferences is still puzzling.For example, the following conversation has the sentences, A and B.A: Yeah,it's not far.Through the S-bahn here.I mean it's like twenty minutes.B: Or something.And so,if I do it,I'd love to have you join me.It's a fancy wedding too.The pronoun \"it's\" in the sentence A and the \"it's\" in the sentence B are obviously referring to the different things.As they are likely to get similar or even the same expression, a false link between them is often predicted.A similar case in sentence B. Due to different lemma and lexeme, the model would not predicate that \"I\" in A and \"you\" in B are coreferential.On the opposite, a false coreference between \"I\" in A and \"I\" in B would be predicted.In coreference resolution tasks, words referring each other are called mentions, while a mention could be a common noun, a proper noun or a pronoun.Taking the above example, a coreference system partitions the mentions in a sentence into one coreference chain-(\"it's\", \"it's\"), and singleton: \"the S-bahn\" for speaker1.One coreference chain-(\"I\", \"I'd\",\"me\"), and singleton: \"It's\" for speaker2 and one coreference chain-(\"I\", \"you\") between two speakers.In recent years, several supervised approaches have been proposed to coreference resolution.The work can be categorized into three classes.1) mention-pair models: A mention pair model is a binary classifier that determines whether a pair of mentions is co-referring or not McCarthy & Lehnert (1995).One of the common limitation of the mention-pair model is that it cannot capture information beyond the mention pair.The information that can be obtained from the two markables to determine their coreferential status is very limited.; 2) entity-mention models: The entity-mention model aims to classify whether an NP (Noun phrase) is coreferent with a preceding cluster Yangy et al. (2004); Culotta et al. (2007); Daum\u00e9 III & Marcu (2005).This strategy considers the candidates independently.It cannot measure how likely a candidate is the antecedent for a given anaphor, relative to the other candidates; 3) ranking models: Ranking models allows candidate antecedents of a mention to be ranked simultaneously Iida et al. (2003); Denis & Baldridge (2008); Durrett & Klein (2013).In the above three classes, the ranking models recently obtained the state-of-the-art performance Wiseman et al. (2015;2016); Clark & Manning (2016).More recently, Lee et al. (2018) proposed the first state-of-the-art end-to-end neural coreference resolution system.They consider all spans as potential mentions and learn distributions over possible antecedents for each.In addition, they use a fully differentiable approximation to higher-order inference to iteratively refine span representations, and the model only uses a minimal set of hand-engineered features (speaker ID, document genre, span distance, span width).This leads to the problem that identical mentions tend to get similar or even identical representations, and further misled coreference resolutions to make mistakes.In this paper, we demonstrate a novel method utilizing attention mechanism to adaptively exploit features to represent identical mentions with different contexts.Inspired by the recent success of attention mechanism, we focus on this issue and develop a general attention mechanism that learns the importance/weight of each feature based on mention's contexts and then add this information to the end-to-end neural model.The entire model could be trained end-to-end with gradient descent.The proposed model is evaluated on the CoNLL 2012 Shared Task Pradhan et al. (2012).The results show that the method outperforms the baselines.Meanwhile, we made a statistic of the different features' weights in the attention mechanism.The statistic shows that the feature attention algorithm does help to distinguish the features for identical mentions based on different contexts."}
{"paper_id": 59, "abstract": "In the ever-evolving landscape of machine learning, we unveil a powerful technique designed to bolster the generalization of deep representations derived from small labeled datasets. Our approach harnesses the potential of self-supervised tasks, weaving them seamlessly into the fabric of auxiliary loss functions. While recent strides in self-supervised learning (SSL) have illuminated its advantages on vast, unlabeled datasets, its true prowess on smaller datasets has remained shrouded in mystery.   Through our exploration, we have discovered that SSL can significantly diminish the relative error rate of few-shot meta-learners by an impressive 4% to 27%, even when constrained to the images within the datasets themselves. The enhancements are particularly pronounced when the training set shrinks or when the tasks at hand grow more formidable. Although the advantages of SSL tend to amplify with larger training sets, we caution that its application may falter in the presence of domain shifts\u2014instances where the distribution of images employed for meta-learning diverges from those utilized in SSL.  In light of these findings, we introduce a novel technique that intelligently selects images for SSL from a vast, generic pool of unlabeled images, employing a domain classifier to ensure compatibility with the target dataset. This innovation yields further improvements, fortifying the efficacy of our approach. Our results, drawn from a diverse array of meta-learners and self-supervised tasks across datasets exhibiting varying degrees of domain shifts and label sizes, serve to illuminate the transformative potential of SSL in the realm of few-shot learning.", "introduction": "Current machine learning algorithms require enormous amounts of training data to learn new tasks.This is a problem for many practical problems across domains such as biology and medicine where labeled data is hard to come by.In contrast, we humans can quickly learn new concepts from limited training data.We are able to do this by relying on our past \"visual experience\".Recent work attempts to emulate this by training a feature representation to classify a training dataset of \"base\" classes with the hope that the resulting representation generalizes not just to unseen examples of the same classes but also to novel classes, which may have very few training examples (called fewshot learning).However, training for base class classification can force the network to only encode features that are useful for distinguishing between base classes.In the process, it might discard semantic information that is irrelevant for base classes but critical for novel classes.This might be especially true when the base dataset is small or when the class distinctions are challenging.One way to recover this useful semantic information is to leverage representation learning techniques that do not use class labels, namely, unsupervised or self-supervised learning.The key idea is to learn about statistical regularities (e.g., spatial relationship between patches, orientation of an images) that might be a cue to semantics.Despite recent advances, these techniques have only been applied to a few domains (e.g., entry-level classes on internet imagery), and under the assumption that large amounts of unlabeled images are available.Their applicability to the general few-shot scenario described above is unclear.In particular, can these techniques help prevent overfitting to base classes and improve performance on novel classes in the few-shot setting?If so, does the benefit generalize across domains and to more challenging tasks?Moreover, can we use self-supervised training to boost performance in domains where even unlabeled images are hard to get?This paper seeks to answer these questions.We show that with no additional training data, adding a self-supervised task as an auxiliary task (Figure 1) improves the performance of existing few-shot techniques on benchmarks across a multitude of domains (Figure 2).Intriguingly, we find that the benefits of self-supervision increase with the difficulty of the task, for example when training from a smaller base dataset, or with degraded inputs such as low resolution or greyscale images (Figure 3).One might surmise that as with traditional SSL, additional unlabeled images might improve performance further.But what unlabeled images should we use for novel problem domains where unlabeled data is not freely available?To answer this, we conduct a series of experiments with additional unlabeled data from different domains.We find that adding more unlabeled images improves performance only when the images used for self-supervision are within the same domain as the base classes (Figure 4a); otherwise they can even negatively impact the performance of the few-shot learner (Figure 4b,4c,4d).Based on this analysis we present a simple approach that uses a domain classifier to pick similar-domain unlabeled images for self-supervision from a large, generic pool of images.The resulting method improves over the performance of a model trained with self-supervised learning from images within the domain (Figure 4c).Taken together, this results in a powerful, general and practical approach for improving few-shot learning from small datasets in novel domains.Finally, these benefits are also observed on standard classification tasks (Appendix A.3)."}
{"paper_id": 60, "abstract": "In the realm of deep learning, the construction of mini-batches plays a pivotal role in determining the efficacy of neural networks. In this paper, we unveil an innovative adaptive batch selection algorithm, aptly named Recency Bias. This approach harnesses the power of uncertain samples\u2014those that have been predicted inconsistently during recent iterations. By analyzing the historical label predictions of each sample within a dynamic sliding window, Recency Bias adeptly assesses predictive uncertainty. This strategic design not only accelerates the training process but also enhances the accuracy of the network itself. Our extensive evaluations across two independent tasks reveal the remarkable prowess of Recency Bias. When compared to existing batch selection methodologies, our results indicate a reduction in test error by as much as 20.5% within a fixed wall-clock training duration. Furthermore, Recency Bias demonstrates a remarkable improvement in training efficiency, achieving the same test error in up to 59.3% less time.", "introduction": "Stochastic gradient descent (SGD) for randomly selected mini-batch samples is commonly used to train deep neural networks (DNNs).However, many recent studies have pointed out that the performance of DNNs is heavily dependent on how well the mini-batch samples are selected (Shrivastava et al., 2016;Chang et al., 2017;Katharopoulos & Fleuret, 2018).In earlier approaches, a sample's difficulty is employed to identify proper mini-batch samples, and these approaches achieve a more accurate and robust network (Han et al., 2018) or expedite the training convergence of SGD (Loshchilov & Hutter, 2016).However, the two opposing difficulty-based strategies, i.e., preferring easy samples (Kumar et al., 2010;Han et al., 2018) versus hard samples (Loshchilov & Hutter, 2016;Shrivastava et al., 2016), work well in different situations.Thus, for practical reasons to cover more diverse situations, recent approaches begin to exploit a sample's uncertainty that indicates the consistency of previous predictions (Chang et al., 2017;Song et al., 2019).An important question here is how to evaluate the sample's uncertainty based on its historical predictions during the training process.Intuitively, because a series of historical predictions can be seen as a series of data indexed in chronological order, the uncertainty can be measured based on two forms of handling time-series observations: (i) a growing window (Figure 1(a)) that consistently increases the size of a window to use all available observations and (ii) a sliding window (Figure 1(b)) that maintains a window of a fixed size on the most recent observations by deleting outdated ones.While the state-of-the-art algorithm, Active Bias (Chang et al., 2017), adopts the growing window, we propose to use the sliding window in this paper.In more detail, Active Bias recognizes uncertain samples based on the inconsistency of the predictions in the entire history of past SGD iterations.Then, it emphasizes such uncertain samples by choosing them with high probability for the next mini-batch.However, according to our experiments presented in Section 5.2, such uncertain samples slowed down the convergence speed of training, though they ultimately reduced the generalization error.This weakness is attributed to the inherent limitation of the growing window, where older observations could be too outdated (Torgo, 2011).In other words, the outdated predictions no longer represent a network's current behavior.As illustrated in Figure 2, when the label predictions of two samples were inconsistent for a long time, Active Bias invariably regards them as highly uncertain, although their recent label predictions become consistent along with the network's training progress.This characteristic evidently entails the risk of emphasizing uninformative samples that are too easy or too hard at the current moment, thereby slowing down the convergence speed of training.Therefore, we propose a simple but effective batch selection method, called Recency Bias, that takes advantage of the sliding window to evaluate the uncertainty in fresher observations.As opposed to Active Bias, Recency Bias excludes the outdated predictions by managing a sliding window of a fixed size and picks up the samples predicted inconsistently within the sliding window.Thus, as shown in Figure 2, the two samples uninformative at the moment are no longer selected by Recency Bias simply because their recent predictions are consistent.Consequently, since informative samples are effectively selected throughout the training process, this strategy not only accelerates the training speed but also leads to a more accurate network.To validate the superiority of Recency Bias, two popular convolutional neural networks (CNNs) were trained for two independent tasks: image classification and fine tuning.We compared Recency Bias with not only random batch selection (baseline) but also two state-of-the-art batch selection strategies.Compared with three batch selection strategies, Recency Bias provided a relative reduction of test error by 1.81%-20.5% in a fixed wall-clock training time.At the same time, it significantly reduced the execution time by 24.6%-59.3% to reach the same test error."}
{"paper_id": 61, "abstract": "In the ever-evolving realm of artificial intelligence, we unveil a groundbreaking innovation: Deep Reasoning Networks, or DRNets. This end-to-end framework masterfully intertwines the intricacies of deep learning with the precision of logical reasoning, specifically designed to tackle the formidable challenge of pattern de-mixing, often in the murky waters of unsupervised or weakly-supervised environments.  At the heart of DRNets lies a clever synergy, harnessing the underlying structure of problems and leveraging prior knowledge. By fusing the rigor of logic and constraint reasoning with the dynamic adaptability of stochastic-gradient-based neural network optimization, DRNets emerge as a formidable force in the landscape of computational problem-solving.  We put DRNets to the test in two compelling arenas: first, the de-mixing of overlapping hand-written Sudokus, known as Multi-MNIST-Sudoku, where DRNets achieved a flawless recovery of digits with an astounding 100% accuracy. But the true test of their prowess came in the realm of scientific discovery, where we ventured into the complex task of inferring crystal structures from X-ray diffraction data\u2014a challenge we term Crystal-Structure-Phase-Mapping. Here, DRNets not only eclipsed the existing state-of-the-art but also surpassed the capabilities of seasoned experts, unveiling crystal structures with unparalleled precision and physical relevance.  In a world where the boundaries of knowledge are ever-expanding, DRNets stand as a beacon of innovation, illuminating the path forward in both artificial intelligence and scientific inquiry.", "introduction": "Deep learning has achieved tremendous success in areas such as vision, speech recognition, language translation, and autonomous driving.Nevertheless, certain limitations of deep learning are generally recognized, in particular, limitations due to the fact that deep learning approaches heavily depend on the availability of large amounts of labeled data.In certain domains, such as scientific discovery, it is often the case that scientists don't have large amounts of labeled data and instead have to rely on prior knowledge to make sense of the data.One grand challenge in scientific discovery is to perform high-throughput unsupervised interpretation of scientific data, given its exponential growth in generation rates, dramatically outpacing humans' ability to analyze them.Herein we consider pattern de-mixing problems, which involve decomposing a mixed signal into the collection of source patterns, such as separating mixtures of X-ray diffraction (XRD) signals into the source XRD signals of the corresponding crystal structures, a key challenge in materials discovery.More generally, pattern de-mixing problems are pervasive in scientific areas as diverse as biology, astronomy, and materials science, as well as in commercial applications for e.g., healthcare and music.We propose Deep Reasoning Networks (DRNets), an end-to-end framework that combines deep learning with logical and constraint reasoning for solving unsupervised or very-weakly-supervised pattern de-mixing tasks.We illustrate the power of DRNets for disentangling two overlapping handwritten Sudokus (Multi-MNIST-Sudoku) (see Fig. 1) and for solving a substantially more complex de-mixing task in scientific discovery that concerns inferring crystal structures of materials from X-ray diffraction data, which we refer to as Crystal-Structure-Phase-Mapping.Both de-mixing tasks require probabilistic reasoning to interpret noisy and uncertain data, while satisfying a set of rules: Sudoku rules and thermodynamic rules, respectively.For example, de-mixing hand written digits is challenging, but it becomes more feasible when we reason about the prior knowledge concerning the two overlapping Sudokus.Crystal structure phase mapping is yet substantially more complex.In fact, crystal structure phase mapping easily becomes too complex for experts to solve and is a major bottleneck in high-throughput materials discovery.DRNets are inspired and motivated by problems from scientific discovery, such as crystal structure phase mapping.Our contributions: (1) We introduce Deep Reasoning Networks (DRNets), an end-to-end framework that combines deep learning with logical and constraint reasoning for unsupervised or veryweakly-supervised de-mixing tasks.Specifically, DRNets perform end-to-end deep reasoning by  encoding a latent space of the input data that captures the structure and prior knowledge constraints within and among data points (Fig. 2).The latent space is used by a generative decoder to generate the targeted output, which should be consistent with the input data and prior knowledge.Subsequently, DRNets optimize an objective function capturing the overall problem objective as well as prior knowledge in the form of weighted constraints.(2) To instantiate the logical constraints in DRNets, we introduce a group of entropy-based continuous relaxations that use probabilistic modeling to encode general discrete constraints including sparsity, cardinality and so-called All-Different constraints.To optimize those constraints, we introduce a variant of standard SGD method (Robbins & Monro, 1985) called constraint-aware stochastic gradient descent, which batches data points involved in the same constraint component together and dynamically adjust the constraints' weights as a function of their satisfiability.In the following sections, we show how to encode Multi-MNIST-Sudoku and Crystal-Structure-Phase-Mapping as DRNets, by properly defining the structure of the latent space, additional reasoning modules to model the problem constraints (prior knowledge), and the components of the objective function.De facto, these examples illustrate how to develop \"gadgets\" to encode a variety of constraints and prior knowledge in DRNets.(3) We demonstrate the potential of DRNets on two de-mixing tasks with detailed experimental results.We show how (3.1) DRNets significantly outperformed the state of the art and human experts on Crystal-Structure-Phase-Mapping instances, recovering more precise, interpretable, and physically meaningful crystal structure pattern decompositions.In this task, DRNets solve a previously unsolved chemical system, which subsequently led to the discovery of a new material that is important for solar fuels technology.(3.2) On Multi-MNIST-Sudoku instances, without direct supervision, DRNets perfectly recovered the digits in the mixed Sudokus with 100% digit accuracy, outperforming the supervised state-of-the-art MNIST de-mixing models, including CapsuleNet (Sabour et al., 2017) and ResNet (He et al., 2016)."}
{"paper_id": 62, "abstract": "In the intricate realm of neural network optimization, the gradient serves as a beacon, illuminating the path for weight adjustments in the quest to minimize loss. A delicate balance is struck when gradients dwindle, suggesting that certain weights have found their optimal state and can be left undisturbed during the training odyssey. This paper embarks on an experimental journey to unveil the significance of neural network weights and the extent to which they truly require recalibration. Our exploration reveals that, beginning from the third epoch, the strategic freezing of weights\u2014those whose gradients whisper of insignificance\u2014yields only a whisper of accuracy decline, and in some cases, even an unexpected improvement.   We delve into this phenomenon across the MNIST, CIFAR10, and Flickr8k datasets, employing a diverse array of architectures, including VGG19, ResNet-110, and DenseNet-121. In our findings, we illustrate that by freezing 80% of VGG19's parameters post-epoch three, we encounter a mere 0.24% dip in accuracy. Similarly, the ResNet-110 model experiences a 0.9% decline with 50% of its parameters frozen, while the DenseNet-121 sees a 0.57% drop with 70% of its weights held steady.   To bridge our theoretical insights with real-world applications, we also train an image captioning model utilizing an attention mechanism on the Flickr8k dataset, employing LSTM networks. Here, freezing 60% of the parameters from the third epoch onward surprisingly yields a superior BLEU-4 score compared to the fully trained counterpart. For those eager to replicate our findings, our source code awaits in the appendix, ready to illuminate your own path through the intricate landscape of neural network training.", "introduction": "The immense success of deep neural networks we are witnessing since the deep learning revolution occurred is surprising.A large variety of vision and language applications ranging from image classification, object detection, image synthesis, image super-resolution, image captioning, language modeling....etc.has proved that neural networks possess a powerful capability of learning very complex data.However, training these networks to perform as expected is very time-consuming and requires powerful graphical processing units (GPUs).A recently published open-source project by NVIDIA 1 claimed that training a generative adversarial network (GAN) took more than 6 days on 8 Tesla V100 GPUs.However, we argue that a lot of parameters involved during training are important for update only for the first few epochs (in our experiments, the first two epochs only), and can be frozen for the rest of the training epochs.The backpropagation algorithm is the base algorithm used to optimize deep neural networks.For each weight, a gradient is computed with respect to the loss which indicates the amount a weight should change.Large gradients correspond to a large change that will occur in the weight, while small ones (near to zero) indicate that the weight is nearly optimized and does not need much change.In particular, if a gradient for a particular weight is zero or close to zero, this means that it has either reached its optimal solution, or it is stuck at a saddle point.The former means that the weight has a good value and is less likely to change throughout the training and can be kept frozen.In this paper, we wish to show the redundancy of weights in a neural network that have no influence and can be kept frozen during training.In particular, we demonstrate that fully training a model with all its weights is required for the first two epochs only.To justify this, we propose an experimental technique named Partial Backpropagation, which freezes weights that have gradients very near to zero and are less likely to change, with the rest of the weights trained normally.This induces a very slight drop in accuracy (and no harm in accuracy for lesser freezing).An overview of  We can further visualize the histogram of gradients across the network layers to have a better understanding of their distributions.In Figure 2, we visualize the distribution of gradients from several layers in a VGG19 convolutional network (Simonyan & Zisserman, 2015).In particular, we visualize the gradients of layers 3, 7, 10 and 13 after training for 2 epochs.We can see a large number of gradients with values very near to zero, suggesting that a lot of weights in these layers have already been optimized and are less likely to change throughout the training."}
{"paper_id": 63, "abstract": "In the realm of optimization, we venture into the intricate landscape of composition problems, specifically those of the form \\(\\frac{1}{n}\\sum\\nolimits_{i= 1}^n F_i\\left(\\frac{1}{n}\\sum\\nolimits_{j = 1}^n G_j(x)\\right)\\). These challenges are not mere academic exercises; they pulse at the heart of critical machine learning applications, from reinforcement learning to nonlinear embedding, and beyond.   While the paths of gradient descent and stochastic gradient descent offer straightforward routes to solutions, they come with a significant burden: each iteration demands the computation of \\(\\frac{1}{n}\\sum\\nolimits_{j = 1}^n G_j(x)\\), a task that becomes increasingly cumbersome as \\(n\\) swells. Recognizing this inefficiency, we set forth on a quest to forge a new path\u2014one that dramatically reduces the query complexity of these composition problems.  Thus, we unveil our stochastically controlled compositional gradient algorithm, a robust creation that harnesses the power of dual variance reduction techniques, adeptly navigating both strongly convex and non-convex terrains. To further enhance its prowess, we introduce a mini-batch variant of our method, which optimizes query complexity in relation to mini-batch size.  Through rigorous experimentation, we lay bare the superiority of our approach, illuminating its advantages over existing methodologies. In this journey through the complexities of composition optimization, we have crafted a tool that holds promise for the future of machine learning, paving the way for more efficient solutions in a demanding landscape.", "introduction": "In this paper, we study the following composition minimization problem, where f : R N \u2192 R is differentiable and possibly non-convex, each F i : R M \u2192 R is a smooth function, each G i : R N \u2192 R M is a mapping function, both the numbers of F i 's and G j 's are assumed to be n for simplicity We call G(x):= 1 n n j=1 G j (x) the inner function, and F (w):= 1 n n i=1 F i (w) the outer function.Many machine learning problems can be cast as composition problems that include two finite-sum structures: reinforcement learning (Sutton et al., 1998;Wang et al., 2017;Liu et al., 2016), variance-averse learning (Lian et al., 2017), and nonlinear embedding (Hinton & Roweis, 2003;Dikmen et al., 2015).In particular,\u2022 (reinforcement learning) The S \u00d7 S system of Bellman equations Wang et al. (2017) can be written as, where E[B] = I -\u03b3P \u03c0 , \u03b3 \u2208 (0, 1) is a discount factor, P \u03c0 is the transition probability under policy \u03c0, and E[b] is the expected state transition reward.This is one of key problems in reinforcement learning for evaluating the value of a policy \u03c0. \u2022 (risk-averse learning) The risk-averse learning Lian et al. (2017) aims to maximize the expected return while control the variance (or risk) in the meantime: min x -E a [h(x; a)] + \u03bbVar a [h(x; a)], where h(x; a) is the loss function including a random variable a, \u03bb > 0 is a regularization parameter.\u2022 (nonlinear embedding) Stochastic nonlinear embedding Hinton & Roweis (2003) aims to map a group of points from a high dimensional space to a low dimensional space by minimizing the KL divergence.It is a non-convex composition optimizationwhere p i|t and q i|t are the conditional probabilities w.r.p. {z i } n i=1 and {x i } n i=1 , p i|t = d(zt,zi) j =t d(zt,zj ) , q i|t = d(xt,xi) j =t d(xt,xj ) , where d(\u2022, \u2022) is the dissimilar distance function between two samples.To solve the composition optimization including the finite-sum structure in (1.1), two most straightforward approaches are the gradient descent (GD) and the stochastic gradient descent (SGD).However, it is extremely expensive to scan all the inner functions (for both SGD and GD) as well as all the outer functions (for GD) in each iteration.However, note that, unlike solving common stochastic optimization problems, randomly sampling one inner function and one outer function does not give an unbiased estimate for the true gradient; that is, E i\u223c[n],j\u223c [n] [(\u2202G j (x)) T \u2202F i ( G(x))] = \u2207f (x), where G(x)is the estimation of G(x).The key to solving this composition objective is how to estimate the value of G(x k ) and its Jacobian with high accuracy using only a few samples in each iteration.Recently, many stochastic optimization methods solving the composition problem have been developed, such as the stochastic gradient based method and the variance-reduction based method.For example, stochastic compositional gradient descent (SCGD) (Wang et al., 2017;Liu et al., 2016) estimates the inner function G(x) by using an iterative weighted average of the past values of G(x), and then performs the stochastic quasi-gradient iteration.The advantage of this method is that convergence rate does not depend on n; however, it queries more samples to the desired point.Another set of approaches is based on variance reduction -for instance, compositional stochastic variance reduction gradient (Compositional-SVRG) (Lian et al., 2017) estimates the inner function G(x) and the gradient of function f (x) by using the variance reduction technique; however, the derived linear convergence rate is related to n. Motivated by a few recent works (Lei & Jordan, 2017;Lei et al., 2017;Allen-Zhu, 2017) that focus on the stochastically controlled gradient, we were inspired to look for a way to improve the query complexity and reduce the dependence on n to solve the composition optimization in (1.1).Hence, this paper presents a novel and more efficient method named stochastically controlled compositional gradient (SCCG) for solving composition problems involving a two-finite-sum structure.The result is improved query complexity over existing approaches.Further, all results in this paper can be easily extended to cases where the number of F i and the number of G j are different.The main contributions of this article are summarized below.\u2022 We provide a stochastically controlled function to estimate the inner function G(x).Inspired by stochastically controlled stochastic gradient (SCSG) (Lei & Jordan, 2017) that estimates the gradient, G(x) can also be estimated by using a snapshot xs , in which G(x s ) is not computed directly, but is estimated through a random subset from [n].This is the first time that a stochastically controlled function has been incorporated into the process of estimating the inner function.We have also analyzed how the size of the subset might influence the query complexity for both strongly convex and non-convex functions.\u2022 We provide a stochastically controlled compositional gradient to estimate the \u2207f (x).However, there are two potential situations that could be encountered in the estimation process that can impede convergence.First, the expectation of the gradient is no longer an unbiased estimation; and, second, the gradient of f (x s ) at the snapshot is formed by two random subsets, which are used for the functions F i and G j respectively.Moreover, the biased gradient bring more difficulty in proving the convergence, which are greatly different from those encountered in (Lei & Jordan, 2017;Lian et al., 2017;Lei et al., 2017).To address these scenarios, we have identified a bound on the size of the subsets that are used to estimate the gradient.The details of the analysis can be referred to Section 3.1 and 3.2.\u2022 A mini-batch version of the proposed algorithm is also provided for both strongly convex and non-convex functions.The corresponding query complexities are improved according to the size of the mini-batch.More information can be referred to Section 3.3."}
{"paper_id": 64, "abstract": "In this exploration, we delve into the intricate workings of Deep Neural Networks (DNNs), revealing a fundamental truth: the parameters governing these networks defy the commonly held assumption of independent and identically distributed (i.i.d.) prior distributions. Furthermore, we uncover that the notion of activations being i.i.d. does not hold true across all hidden layers of DNNs. This realization leads us to a pivotal conclusion: the Gaussian Process falls short in adequately capturing the complexities of these hidden layers.  In response to this challenge, we unveil a groundbreaking probabilistic framework for understanding the hidden layers of DNNs through two key innovations. First, we propose that each hidden layer can be conceptualized as a Gibbs distribution, where the neurons themselves dictate the energy function. Second, we introduce a model that captures the interplay between adjacent layers as a product of experts, enriching our understanding of their interconnections.  By leveraging this probabilistic representation, we effectively reframe the entire architecture of DNNs as a Bayesian hierarchical model. This perspective not only enhances our comprehension of the networks' structures but also reveals that the hidden layers inherently provide explicit regularizations, acting as prior distributions that guide the learning process.  Building upon this Bayesian interpretation of DNN regularization, we introduce a novel approach designed to bolster the generalization capabilities of these networks. The results from our simulations affirm the validity of our theoretical framework, showcasing the potential of this new understanding to advance the field of deep learning.", "introduction": "Recently, interpreting the hidden layers of Deep Neural Networks (DNNs) as the Gaussian Process (GP) has attracted a great deal of attention because it provides a novel probabilistic perspective to clarify the working mechanism of deep learning.Neal (1994) initially demonstrates the equivalence between GP and neural networks with a single fully connected layer in the limit of infinite neurons.Following this seminal work, Lee et al. (2018); Matthews et al. (2018) further extend the equivalence to neural networks with multiple hidden layers and Garriga-Alonso et al. (2018); Novak et al. (2018) establish the correspondence between the convolutional layers and GP.It is important to note that the GP explanations rely on a fundamental probabilistic premise that the activations of a hidden layer are independent and identically distributed (i.i.d.).More specifically, based on the classical Central Limit Theorem (CLT), the prerequisite of an activation of a hidden layer approaching a Gaussian distribution is that all the activations of the previous layer are i.i.d.. Though the Lyapunov CLT could relax the probabilistic premise to be independence only, all the previous works do not thoroughly discuss the probabilistic premise except assuming an i.i.d.prior for all the parameters of DNNs.Due to its extreme importance but unclear status, it is necessary to thoroughly investigate the probabilistic premise for improving the interpretability of DNNs.In this work, we demonstrate that the parameters of DNNs are correlated and depend on the training dataset, i.e., they do not satisfy the i.i.d.prior.Moreover, we demonstrate that activations being i.i.d.cannot hold for all the hidden layers of DNNs.In the context of Bayesian probability, we theoretically derive the necessary conditions for the activations of a hidden layer being i.i.d.given the assumption that the activations of the previous layer are i.i.d.. Subsequently, we experimentally show that typical DNNs, such as the Multilayer Perceptron (MLP) and the Convolutional Neural Networks (CNNs), cannot satisfy the necessary conditions.As a result, activations being i.i.d. is not valid for all hidden layers.In other words, GP with the i.i.d.assumption for the parameters of DNNs cannot correctly explain all the hidden layers of DNNs.In addition, some previous works show that GP is sensitive to the curse of dimensionality (Bengio et al., 2005;Hinton et al., 2012) and cannot clarify the hierarchical representation, an essential of deep learning (Matthews et al., 2018).Alternatively, we propose a novel probabilistic representation for the hidden layers based on the Gibbs distribution (LeCun et al., 2006) and the Product of Experts (PoE) model (Hinton, 2002).The probabilistic representation explains a hidden layer in two aspects: (i) the distribution of a hidden layer can be formulated as a Gibbs distribution, of which the energy function is determined by the activations, and (ii) the connection between two adjacent layers can be modeled by a PoE model.Specifically, the distribution of a single neuron or hidden unit (e.g., convolutional channel) can be expressed as a PoE, in which all the experts are defined by the neurons or hidden units in the previous hidden layer.Compared to the GP explanations, the proposed probabilistic representation provides a more specific explanation for the hidden layers of DNNs without any assumption.Since the output of a hidden layer is commonly the input of the next layer, DNNs form a Markov chain, thereby corresponding to a joint distribution.Moreover, we confirm the equivalence between the Stochastic Gradient Descent (SGD) and the variational inference (Mandt et al., 2017;Chaudhari & Soatto, 2018) and prove the former is also equivalent to the energy minimization optimization based on the proposed probabilistic representation.As a result, the entire architecture of DNNs can be explained as a Bayesian network (Nielsen & Jensen, 2009;Koski & Noble, 2011).In particular, we demonstrate that the hidden layers close to the input formulate prior distributions of the training dataset and propose a novel regularization approach to improve the generalization performance of DNNs through pre-training the hidden layers corresponding to prior distributions."}
{"paper_id": 65, "abstract": "In this exploration, we embark on a quest to unravel the intricate tapestry of decision boundaries woven by neural networks that employ piece-wise linear activations, specifically the ReLU function. Our journey takes us into the realm of tropical geometry, a fascinating frontier within algebraic geometry, where we seek to illuminate the hidden structures behind the decision-making processes of a straightforward neural architecture\u2014namely, one that follows the pattern of (Affine, ReLU, Affine).  What we uncover is both profound and elegant: the decision boundaries manifest as a subset of a tropical hypersurface, intricately linked to a polytope shaped by the convex hull of two zonotopes. The generators of these zonotopes are not mere abstractions; they are direct manifestations of the neural network's parameters, revealing a deep connection between geometry and function.  Armed with this geometric insight, we turn our attention to three pivotal tasks, each shedding new light on our understanding of neural networks. First, we offer a fresh tropical perspective on the lottery ticket hypothesis, illuminating how various initializations influence the tropical geometric representation of decision boundaries. Next, we introduce a novel class of tropical regularizers, designed to engage directly with the decision boundaries themselves, enhancing our control over the network's behavior.  Finally, we venture into the practical applications of our findings: employing these regularizers in the art of neural network pruning\u2014strategically excising parameters that fail to enhance the tropical geometric representation\u2014and crafting adversarial input attacks that intentionally distort the geometry of decision boundaries, thereby altering the network's predictions. Through this multifaceted approach, we not only deepen our understanding of neural networks but also forge new pathways for their optimization and robustness.", "introduction": "Deep Neural Networks (DNNs) have demonstrated outstanding performance across several research domains, including computer vision (Krizhevsky et al., 2012), speech recognition (Hinton et al., 2012), natural language processing (Bahdanau et al., 2015;Devlin et al., 2018), quantum chemistry (Sch\u00fctt et al., 2017), and healthcare (Ardila et al., 2019;Zhou et al., 2019) to name a few (Le-Cun et al., 2015).Nevertheless, a rigorous interpretation of their success remains evasive (Shalev-Shwartz & Ben-David, 2014).For instance, and in an attempt to uncover the expressive power of DNNs, Montufar et al. (2014) studied the complexity of functions computable by DNNs that have piecewise linear activations.They derived a lower bound on the maximum number of linear regions.Several other works have followed to improve such estimates under certain assumptions (Arora et al., 2018).In addition, and in attempt to understand some of the subtle behaviours DNNs exhibit, e.g. the sensitive reaction of DNNs to small input perturbations, several works directly investigated the decision boundaries induced by a DNN used for classification.The work of Seyed-Mohsen Moosavi-Dezfooli (2019) showed that the smoothness of these decision boundaries and their curvature can play a vital role in network robustness.Moreover, He et al. (2018a) studied the expressiveness of these decision boundaries at perturbed inputs and showed that these boundaries do not resemble the boundaries around benign inputs.Li et al. (2018) showed that under certain assumptions, the decision boundaries of the last fully connected layer of DNNs will converge to a linear SVM.Also, Beise et al. (2018) showed that the decision regions of DNNs with width smaller than the input dimension are unbounded.More recently, and due to the popularity of the piecewise linear ReLU as an activation function, there has been a surge in the number of works that study this class of DNNs in particular.As a result, this has incited significant interest in new mathematical tools that help analyze piecewise linear functions, such as tropical geometry.While tropical geometry has shown its potential in many applications such as dynamic programming (Joswig & Schr\u00f6ter, 2019), linear programming (Allamigeon et al., 2015), multi-objective discrete optimization (Joswig & Loho, 2019), enumerative geometry (Mikhalkin, 2004), economics (Akian et al., 2009;Mai Tran & Yu, 2015), it has only been recently used to analyze DNNs.For instance, Zhang et al. (2018) showed an equivalency between the family of DNNs with piecewise linear activations and integer weight matrices and the family of tropical rational maps, i.e. ratio between two multi-variate polynomials in tropical algebra.The work of Zhang et al. (2018) was mostly concerned about characterizing the complexity of a DNN and specifically counting the number of linear regions, into which the function represented by the DNN can divide the input space, by counting the number of vertices of some polytope representation.This novel approach recovered the results of Montufar et al. (2014) with a much simpler analysis.Contributions.In this paper, we take the results of Zhang et al. (2018) some steps further and present a novel perspective on the decision boundaries of DNNs using tropical geometry.To that end, our contributions are three-fold.(i) We derive a geometric representation (convex hull between two zonotopes) for a super set to the decision boundaries of a DNN in the form (Affine, ReLU, Affine).(ii) We demonstrate support for the lottery ticket hypothesis (Frankle & Carbin, 2019) using a geometric perspective.(iii) We leverage the geometrical representation of the decision boundaries (the decision boundaries polytope) in two interesting applications: network purning and adversarial attacks.In regards to tropical pruning, we provide a new geometric perspective in which one can directly compress the decision boundaries polytope efficiently resulting in only minor perturbations to the decision boundaries.We conduct extensive experiments on AlexNet (Krizhevsky et al., 2012) and VGG16 (Simonyan & Zisserman, 2014) on SVHN (Netzer et al., 2011), CIFAR10, and CI-FAR 100 (Krizhevsky & Hinton, 2009) datasets, in which 90% pruning rate can be achieved with a marginal drop in testing accuracy.As for tropical adversarial attack, we show that one can construct input adversaries that can change network predictions by perturbing the decision boundaries polytope.We conduct extensive experiments on MNIST (LeCun, 1998)."}
{"paper_id": 66, "abstract": "In the realm of machine learning, the specter of noisy labels looms large, often leading to models that falter when faced with unseen data, ensnared by the very imperfections they were trained on. In this paper, we propose a compelling solution to this dilemma: a technique we call \"Prestopping.\" Our approach hinges on the concept of early stopping\u2014halting the training of a deep neural network just before it succumbs to the temptation of memorizing these misleading labels.   But we do not merely stop there. Once we\u2019ve paused the training, we embark on a second phase, reinvigorating the network with a carefully curated \"maximal safe set.\" This collection comprises samples that we can almost certainly trust, ensuring that each epoch post-early stop is fortified with reliable data. Together, these strategies forge a powerful two-phase training method that allows us to navigate the treacherous waters of label noise, achieving noise-free training that is not just theoretical but practical.  Our extensive experiments, conducted across four benchmark image datasets, reveal the potency of Prestopping. We demonstrate that our method significantly outstrips four leading state-of-the-art techniques, achieving a remarkable reduction in test error by 0.4 to 8.2 percentage points, even in the presence of real-world noise. This work not only advances the field but also offers a beacon of hope for practitioners grappling with the chaos of imperfect data.", "introduction": "By virtue of massive labeled data, deep neural networks (DNNs) have achieved a remarkable success in numerous machine learning tasks, such as image classification (Krizhevsky et al., 2012) and object detection (Redmon et al., 2016).However, owing to their high capacity to memorize any label noise, the generalization performance of DNNs drastically falls down when noisy labels are contained in the training data (Jiang et al., 2018;Han et al., 2018;Song et al., 2019).In particular, Zhang et al. (2017) have shown that a standard convolutional neural network (CNN) can easily fit the entire training data with any ratio of noisy labels and eventually leads to very poor generalization on the test data.Thus, it is challenging to train a DNN robustly even when noisy labels exist in the training data.A popular approach to dealing with noisy labels is \"sample selection\" that selects true-labeled samples from the noisy training data (Jiang et al., 2018;Ren et al., 2018;Han et al., 2018;Yu et al., 2019;Song et al., 2019).Here, (1-\u03c4 )\u00d7100% of small-loss training samples are treated as true-labeled ones and then used to update a DNN robustly, where \u03c4 \u2208 [0, 1] is a noise rate.This loss-based separation is well known to be justified by the memorization effect (Arpit et al., 2017) that DNNs tend to learn easy patterns first and then gradually memorize all samples.In practice, Han et al. (2018) empirically proved that training on such small-loss samples yields a much better generalization performance under artificial noise scenarios.Despite its great success, Song et al. (2019) have recently argued that the performance of the lossbased separation becomes considerably worse depending on the type of label noise.For instance,  show those on CIFAR-100 with two types of synthetic noises of 40%, where \"symmetric noise\" flips a true label into other labels with equal probability, and \"pair noise\" flips a true label into a specific false label; (c) shows those on FOOD-101N (Lee et al., 2018) with real-world noise of 18.4%.in symmetric noise (Figure 1(a)), the loss-based approach well separates true-labeled samples from false-labeled ones because many true-labeled ones exhibit smaller loss than false-labeled ones.On the other hand, in pair and real-world noises (Figures 1(b) and 1(c)), many false-labeled samples are misclassified as true-labeled ones because the two distributions overlap closely; this overlap confirms that the loss-based separation still accumulates severe label noise from many misclassified cases, especially in real-world noise or pair noise which is regarded as more realistic than symmetric noise (Ren et al., 2018;Yu et al., 2019).This limitation definitely calls for a new approach that supports any type of label noise for practical use.In this regard, as shown in Figure 2(a), we thoroughly investigated the memorization effect of a DNN on the two types of noises and found two interesting properties as follows:\u2022 A noise type affects the memorization rate for false-labeled samples: The memorization rate for false-labeled samples is faster with pair noise than with symmetric noise.That is, the red portion in Figure 2(a) starts to appear earlier in pair noise than in symmetric noise.This observation supports the significant overlap of true-labeled and false-labeled samples in Figure 1(b).Thus, the loss-based separation performs well only if the false-labeled samples are scarcely learned at an early stage of training, as in symmetric noise.\u2022 There is a period where the network accumulates the label noise severely: Regardless of the noise type, the memorization of false-labeled samples significantly increases at a late stage of training.That is, the red portion in Figure 2(a) increases rapidly after the dashed line, in which we call the error-prone period.(See Section 3.2.1 for the details of estimating the error-prone period).We note that the training in that period brings no benefit.The generalization performance of \"Default\" deteriorates sharply, as shown in Figure 2(c).Based on these findings, we contend that eliminating this error-prone period should make a profound impact on robust optimization.In this paper, we propose a novel approach, called Prestopping, that achieves noise-free training based on the early stopping mechanism.Because there is no benefit from the error-prone period, Prestopping early stops training before that period begins.This early stopping effectively prevents a network from overfitting to false-labeled samples, and the samples memorized until that point are added to a maximal safe set because they are true-labeled (i.e., blue in Figure 2(a)) with high precision.Then, Prestopping resumes training the early stopped network only using the maximal safe set in support of noise-free training.Notably, our proposed merger of \"early stopping\" and \"learning from the maximal safe set\" indeed eliminates the error-prone period from the training process, as shown in Figure 2(b).As a result, the generalization performance of a DNN remarkably improves in both noise types, as shown in Figure 2(c).To validate the superiority of Prestopping, DenseNet (Huang et al., 2017) and VGG-19 (Simonyan & Zisserman, 2015) were trained on both simulated and real-world noisy data sets, including CIFAR-10, CIFAR-100, ANIMAL-10N, and Food-101N.Compared with four state-of-the-art methods, Prestopping significantly improved test error by up to 18.1ppfoot_1 in a wide range of noise rates."}
{"paper_id": 67, "abstract": "In this work, we embark on an exploration of Nesterov's accelerated gradient flows, delving into the intricate realm of probability spaces intertwined with information metrics. Our journey traverses two distinct metrics: the Fisher-Rao metric and the Wasserstein-$2$ metric. Focusing on the latter, we unveil the convergence properties of these accelerated gradient flows, providing clarity on their behavior within Gaussian families.   But we do not stop there. We introduce a robust discrete-time algorithm, crafted for particle implementations, which employs an innovative adaptive restart technique to enhance performance. At the heart of our approach lies a groundbreaking method for bandwidth selection, adept at learning the Wasserstein-$2$ gradient direction from samples influenced by Brownian motion.   Through rigorous experimentation, including applications in Bayesian inference, we demonstrate the formidable capabilities of our method, showcasing its superiority over existing state-of-the-art techniques. Join us as we unlock new pathways in the landscape of accelerated gradient flows, where theory meets practical application in a dance of mathematical elegance and computational prowess.", "introduction": "Recently, optimization problems on the space of probability and probability models attract increasing attentions from machine learning communities.These problems include variational inference (Blei et al., 2017), Bayesian inference (Liu & Wang, 2016), Generative Adversary Networks (GAN, Goodfellow et al. (2014)), and policy optimizations (Zhang et al., 2018), etc.For instance, variational inference methods approximate a target density by minimizing the Kullback-Leibler (KL) divergence as the loss (objective) function.Gradient descent methods with sampling efficient properties play essential roles to solve these optimization problems.Here the gradient descent direction often relies on the information metric over the probability space.This direction naturally reflects the change of the loss function with respect to the metric.In literature, two important information metrics, such as the Fisher-Rao metric and the Wasserstein-2 (in short, Wasserstein) metric, are of great interests (Amari, 1998;Otto, 2001;Lafferty, 1988).For the Fisher-Rao gradient, classical results including Adam (Kingma & Ba, 2014) and K-FAC (Martens & Grosse, 2015) demonstrate its effectiveness in probability models.For the Wasserstein gradient, many classical methods such as Markov chain Monte Carlo (MCMC) methods (Geman & Geman, 1987;Neal et al., 2011;Welling & Teh, 2011) and particle-based variational inference (ParVI) methods (Liu & Wang, 2016;Chen & Zhang, 2017;Chen et al., 2018) are based on this framework in the probability space.The strength of using the Wasserstein gradient is also shown in probability models such as GANs.(Arjovsky et al., 2017;Lin et al., 2018;Li et al., 2019).The Nesterov's accelerated method (Nesterov, 1983) is widely applied in accelerating the vanilla gradient descent under the Euclidean metric.It corresponds to a damped Hamiltonian flow, known as the accelerated gradient flow (Su et al., 2016).A natural question is whether there exists a counterpart of the accelerated gradient flow in the probability space under information metrics.For optimization problems on a Riemannian manifold, the accelerated gradient methods are studied by Liu et al. (2017); Zhang & Sra (2018).The probability space embedded with information metric can be viewed as a Riemannian manifold.Several previous works explore accelerated methods in this manifold under the Wasserstein metric.Liu et al. (2018;2019) propose an acceleration framework of ParVI methods based on manifold optimization.Taghvaei & Mehta (2019) introduce the accelerated flow from an optimal control perspective.On the other hand, Cheng et al. (2017); Ma et al. (2019) explore and analyze the acceleration on MCMC, based on the underdamped Langevin dynamics.In this paper, we present a unified framework of accelerated gradient flows in the probability space embedded with information metrics, named Accelerated Information Gradient (AIG) flows.From an information-differential-geometry perspective, we derive AIG flows by damping Hamiltonian flows, concerning both the Fisher-Rao metric and the Wasserstein metric.Then we focus on the Wasser-stein metric with the KL divergence loss function.In Gaussian families, we verify the existence of the solution to AIG flows.Here we show that the AIG flow corresponds to a well-posed ODE system in the space of symmetric positive definite matrices.We rigorously prove the convergence rate of AIG flows based on the geodesic convexity of the loss function.Here we note that our proof removes the unnecessary technical assumption in (Taghvaei & Mehta, 2019, Theorem 1).Besides, we handle two difficulties in numerical implementations of AIG flows.On the one hand, as pointed out by Taghvaei & Mehta (2019); Liu et al. (2019), the logarithm of density term (Wasserstein gradient of KL divergence) is hard to approximate in particle formulations.We propose a novel kernel selection method, whose bandwidth is learned by sampling from Brownian motions.We call it the BM method.On the other hand, we notice that the AIG flow can be a numerically stiff system, especially in high-dimensional sample spaces.This is because the solution of AIG flows can be close to the boundary of the probability space.To handle this issue, we propose an adaptive restart technique, which accelerates and stabilizes the AIG algorithm.Numerical results in toy examples, Gaussian measures and Bayesian Logistic regression indicate the validity of the BM method and the acceleration effects of the proposed AIG flow.This paper is organized as follows.Section 2 briefly reviews the information metrics and their corresponding gradient flows and Hamiltonian flows in the probability space.In Section 3, we formulate various forms of AIG flows and analyze W-AIG flows in Gaussian measures.We theoretically prove the convergence rate of W-AIG flows in Section 4. Section 5 presents the discrete-time algorithm for W-AIG flows, including the BM method and the adaptive restart technique.Section 6 provides numerical experiments."}
{"paper_id": 68, "abstract": "In the ever-evolving realm of Deep Reinforcement Learning, the tasks of localization, memorization, and planning within the intricate tapestry of partially observable 3D environments remain formidable challenges. Enter EgoMap, a groundbreaking neural memory architecture that reshapes the landscape of agent performance in these complex terrains. EgoMap is designed to empower deep reinforcement learning agents, enabling them to tackle demanding multi-step objectives with newfound agility.  At its core, the EgoMap architecture weaves together several powerful inductive biases, chief among them a differentiable inverse projection that translates CNN feature vectors into a top-down spatially structured map. This map is not static; it evolves in real-time, updated by ego-motion measurements through a seamless differentiable affine transform. Our experiments reveal that this innovative architecture consistently outperforms traditional recurrent agents as well as the leading contenders equipped with structured memory.  Remarkably, EgoMap allows agents to thrive on reward signals alone, sidestepping the costly process of gathering and annotating expert trajectories. Through a comprehensive ablation study, we illuminate the significance of the architecture's critical components. Moreover, our extensive qualitative analysis unveils the fascinating ways in which the agent leverages its structured internal memory, unlocking pathways to superior performance and mastery over its environment. In the grand tapestry of AI development, EgoMap stands as a beacon of progress, illuminating the path toward more capable and adaptable agents.", "introduction": "A critical part of intelligence is navigation, memory and planning.An animal that is able to store and recall pertinent information about their environment is likely to exceed the performance of an animal whose behavior is purely reactive.Many control problems in partially observed 3D environments involve long term dependencies and planning.Solving these problems requires agents to learn several key capacities: spatial reasoning -to explore the environment in an efficient manner and to learn spatio-temporal regularities and affordances.The agent needs to autonomously navigate, discover relevant objects, store their positions for later use, their possible interactions and the eventual relationships between the objects and the task at hand.Semantic mapping is a key feature in these tasks.A second feature is discovering semantics from interactions -while solutions exist for semantic mapping and semantic SLAM Civera et al. (2011); Tateno et al. (2017), a more interesting problem arises when the semantics of objects and their affordances are not supervised, but defined through the task and thus learned from reward.A typical approach for these types of problems are agents based on deep neural networks including recurrent hidden states, which encode the relevant information of the history of observations Mirowski et al. (2017); Jaderberg et al. (2017).If the task requires navigation, the hidden state will naturally be required to store spatially structured information.It has been recently reported that spatial structure as inductive bias can improve the performance on these tasks.In Parisotto & Salakhutdinov (2018), for instance, different cells in a neural map correspond to different positions of the agent.In our work, we go beyond structuring the agent's memory with respect to the agent's position.We use projective geometry as an inductive bias to neural networks, allowing the agent to structure its memory with respect to the locations of objects it perceives, as illustrated in Figure 1b.The model performs an inverse projection of CNN feature vectors in order to map and store observations in an egocentric (bird's eye view) spatially structured memory.The EgoMap is complementary to the hidden state vector of the agent and is read with a combination of a global convolutional read operation and an attention model allowing the agent to query the presence of specific content.We show that incorporating projective spatial memory enables the agent to learn policies that exceed the performance of a standard recurrent agent.Two different objects visible in the same input image (a) Visual features are mapped to the top-down egocentric map.Observations from a first-person viewpoint are passed through a perception module, extracted features are projected with the inverse camera matrix and depth buffer to their 3D coordinates.The operations are implemented in a differentiable manner, so error derivatives can be back-propagated to update the weights of the perception module.Different objects in the same image are mapped to different locations in the map.could be at very different places in the environment.In contrast to Parisotto & Salakhutdinov (2018), our model will map these observations to their respective locations, and not to cells corresponding to the agent's position, as shown in Figure 1a.The model bears a certain structural resemblance with Bayesian occupancy grids (BOG), which have been used in mobile robotics for many years Moravec (1988); Rummelhard et al. (2015).As in BOGs, we perform inverse projections of observations and dynamically resample the map to take into account ego-motion.However, in contrast to BOGs, our model does not require a handcrafted observation model and it learns semantics directly from interactions with the environment through reward.It is fully differentiable and trained end-to-end with backpropagation of derivatives calculated with policy gradient methods.Our contributions are as follows:\u2022 To our knowledge, we present the first method using a differentiable SLAM-like mapping of visual features into a top-down egocentric feature map using projective geometry while at the same time training this representation using RL from reward.\u2022 Our spatial map can be translated and rotated through a differentiable affine transform and read globally and through self-attention.\u2022 We show that the mapping, spatial memory and self-attention can be learned end-to-end with RL, avoiding the cost of labelling trajectories from domain experts, auxiliary supervision or pre-training specific parts of the architecture.\u2022 We demonstrate the improvement in performance over recurrent and spatial structured baselines without projective geometry.\u2022 We illustrate the reasoning abilities of the agent by visualizing the content of the spatial memory and the self-attention process, tying it to the different objects and affordances related to the task.\u2022 Experiments with noisy actions demonstrate the agent is robust to actions tolerances of up to 10%.The code will be made publicly available on acceptance.Reinforcement learning -In recent years the field of Deep Reinforcement Learning (RL) has gained attention with successes on board games Silver et al. (2018) and Atari games Mnih et al. (2015).One key component was the application of deep neural networks Lecun et al. (1998) to frames from the environment or game board states.Recent works that have applied Deep RL for the control of an agent in 3D environments such as maze navigation are Mirowski et al. (2017) and Jaderberg et al. (2017) which explored the use of auxiliary tasks such as depth prediction, loop detection and reward prediction to accelerate learning.Meta RL approaches for 3D navigation have been applied by Wang et al. (2016) and Lample & Chaplot (2017) also accelerated the learning process in 3D environments by prediction of tailored game features.There has also been recent work in the use of street-view scenes to train an agent to navigate in city environments Mirowski et al. (2018).In order to infer long term dependencies and store pertinent information about the partially observable environment; network architectures typically incorporate recurrent memory such as Gated Recurrent Units Chung et al. (2015) or Long Short-Term Memory Hochreiter & Schmidhuber (1997).Differentiable memory -Differentiable memory such as Neural Turing Machines Graves et al. (2014) and Differential Neural Computers Graves et al. (2016) have shown promise where long term dependencies and storage are required.Neural Networks augmented with these memory structures have been shown to learn tasks such as copying, repeating and sorting.Some recent works for control in 2D and 3D environments have included structured memory-based architectures and mapping of observations.Neural SLAM Zhang et al. (2017) aims to incorporate a SLAM-like mapping module as part of the network architecture, but uses simulated sensor data rather than RGB observations from the environment, so the agent is unable to extract semantic meaning from its observations.The experimental results focus on 2D environments and the 3D results are limited.Playing Doom with SLAM augmented memory Bhatti et al. (2016) implements a non-differentiable inverse projective mapping with a fixed feature extractor based on Faster-RCNN Ren et al. (2015), pre-trained in a supervised manner.A downside of this approach is that the network does not learn to extract features pertinent to the task at hand as it is not trained end-to-end with RL.Fang et al. (2019) replace recurrent memory with a transformer (Vaswani et al. (2017)) attention distribution over previous observation embeddings, to highlight that recurrent architectures can struggle to capture long term dependencies.The downside is the storage of previous observations grows linearly with each step in the environment and the agent cannot chose to discard redundant information.Grid cells -there is evidence that biological agents learn to encode spatial structure.Rats develop grid cells/neurons, which fire at different locations with different frequencies and phases, a discovery that led to the 2014 Nobel prize in medicine O' Keefe & Dostrovsky (1971); Hafting et al. (2005).A similar structure seems to emerge in artificial neural networks trained to localize themselves in a maze, discovered independently in 2018 by two different research groups Cueva & Wei (2018); Banino et al. (2018).Projective geometry and spatial memory -Our work encodes spatial structure directly into the agent as additional inductive bias.We argue that projective geometry is a strong law imposed on any vision system working from egocentric observations, justifying a fully differentiable model of perception.To our knowledge, we present the first method which uses projective geometry as inductive bias while at the same time learning spatial semantic features with RL from reward.The past decade has seen an influx of affordable depth sensors.This has led to a many works in the domain reconstruction of 3D environments, which can be incorporated into robotic systems.Seminal works in this field include Izadi et al. (2011) who performed 3D reconstruction scenes using a moving Kinect sensor and Henry et al. (2014) who created dense 3D maps using RGB-D cameras.Neural Map Parisotto & Salakhutdinov (2018) implements a structured 2D differentiable memory which was tested in both egocentric and world reference frames, but does not map observations in a SLAM-like manner and instead stores a single feature vector at the agent's current location.The agent's position is also discretized to fixed cells and orientation quantized to four angles (North, South, East, West).A further downside is that the movement of the memory is fixed to discrete translations and the map is not rotated to the agent's current viewpoint.MapNet Henriques & Vedaldi (2018) includes an inverse mapping of CNN features, is trained in a supervised manner to predict x,y position and rotation from human trajectories, but does not use the  2016Table 1: Comparison of key features of related works map for control in an environment.Visual Question Answering in Interactive Environments Gordon et al. (2018) creates semantic maps from 3D observations for planning and question answering and is applied in a discrete state space.Unsupervised Predictive Memory in a Goal-Directed Agent Wayne et al. (2018) incorporates a Differential Neural Computer in an RL agent's architecture and was applied to simulated memorybased tasks.The architecture achieves improved performance over a typical LSTM Hochreiter & Schmidhuber (1997) based RL agent, but does not include spatial structure or projective mapping.In addition, visual features and neural memory are learned through the reconstruction of observations and actions, rather than for a specific task.Cognitive Mapping and Planning for Visual Navigation Gupta et al. (2017) applies a differentiable mapping process on 3D viewpoints in a discrete grid-world, trained with imitation learning which provides supervision on expert trajectories.The downside of discretization is that affine sampling is trivial for rotations of 90-degree increments, and this motion is not representative of the real world.Their tasks are simple point-goal problems of up to 32 time-steps, whereas our work focused on complex multi-step objectives in a continuous state space.Their reliance on imitation learning highlights the challenge of training complex neural architectures with reward alone, particular on tasks with sparse reward such as the ones presented in this paper.Learning Exploration Policies for Navigation Chen et al. (2019), do not learn a perception module but instead map the depth buffer to a 2D map to provide a map-based exploration reward.Our work learns the features that can be mapped so the agent can query not only occupancy, but task-related semantic content.Our work greatly exceeds the performance of Neural Map Parisotto & Salakhutdinov (2018), by embedding a differentiable inverse projective transform and a continuous egocentric map into the agent's network architecture.The mapping of the environment is in the agent's reference frame, including translation and rotation with a differentiable affine transform.We demonstrate stable training with reinforcement learning alone, over several challenging tasks and random initializations, and do not require the expense of acquiring expert trajectories.We detail the key similarities and differences with related work in table 1."}
{"paper_id": 69, "abstract": "In the realm of machine learning, where the balance between exploration and representation learning is paramount, we delve into the fascinating world of neural-linear bandits. These ingenious constructs harness the formidable power of deep neural networks, merging their representational prowess with the efficient exploration strategies tailored for linear contextual bandits. However, as the learning process unfolds and the representation evolves, there arises a perilous challenge: the loss of critical information tied to \"old\" features, a phenomenon known as catastrophic forgetting.  To combat this formidable foe, we introduce a groundbreaking innovation\u2014the first limited-memory neural-linear bandit designed to withstand the ravages of forgetting. Through a series of comprehensive simulations across a diverse array of real-world challenges\u2014ranging from regression and classification to the nuanced realm of sentiment analysis\u2014we demonstrate that our algorithm not only excels in performance but also exhibits remarkable resilience against the encroaching shadows of catastrophic forgetting. With our approach, we pave a new path in the landscape of machine learning, ensuring that the lessons of the past are not easily lost in the pursuit of knowledge.", "introduction": "Deep neural networks (DNNs) can learn representations of data with multiple levels of abstraction and have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics (LeCun et al., 2015;Goodfellow et al., 2016).Using DNNs for function approximation in reinforcement learning (RL) enables the agent to generalize across states without domain-specific knowledge, and learn rich domain representations from raw, high-dimensional inputs (Mnih et al., 2015;Silver et al., 2016).Nevertheless, the question of how to perform efficient exploration during the representation learning phase is still an open problem.The -greedy policy (Langford & Zhang, 2008) is simple to implement and widely used in practice (Mnih et al., 2015).However, it is statistically suboptimal.Optimism in the Face of Uncertainty (Abbasi-Yadkori et al., 2011;Auer, 2002, OFU), and Thompson Sampling (Thompson, 1933;Agrawal & Goyal, 2013, TS) use confidence sets to balance exploitation and exploration.For DNNs, such confidence sets may not be accurate enough to allow efficient exploration.For example, using dropout as a posterior approximation for exploration does not concentrate with observed data (Osband et al., 2018) and was shown empirically to be insufficient (Riquelme et al., 2018).Alternatively, pseudo-counts, a generalization of the number of visits, were used as an exploration bonus (Bellemare et al., 2016;Pathak et al., 2017).Inspired by tabular RL, these ideas ignore the uncertainty in the value function approximation in each context.As a result, they may lead to inefficient confidence sets (Osband et al., 2018).Linear models, on the other hand, are considered more stable and provide accurate uncertainty estimates but require substantial feature engineering to achieve good results.Additionally, they are known to work in practice only with \"medium-sized\" inputs (with around 1, 000 features) due to numerical issues.A natural attempt at getting the best of both worlds is to learn a linear exploration policy on top of the last hidden layer of a DNN, which we term the neural-linear approach.In RL, this approach was shown to refine the performance of DQNs (Levine et al., 2017) and improve exploration when combined with TS (Azizzadenesheli et al., 2018) and OFU (O'Donoghue et al., 2018;Zahavy et al., 2018a).For contextual bandits, Riquelme et al. (2018) showed that neural-linear TS achieves superior performance on multiple data sets.A practical challenge for neural-linear bandits is that the representation (the activations of the last hidden layer) change after every optimization step, while the features are assumed to be fixed over time when used by linear contextual bandits.Riquelme et al. (2018) tackled this problem by storing the entire data set in a memory buffer and computing new features for all the data after each DNN learning phase.The authors also experimented with a bounded memory buffer, but observed a significant decrease in performance due to catastrophic forgetting (Kirkpatrick et al., 2017), i.e., a loss of information from previous experience.In this work, we propose a neural-linear bandit that uses TS on top of the last layer of a DNN (Fig. 1)foot_0 .Key to our approach is a novel method to compute priors whenever the DNN features change that makes our algorithm resilient to catastrophic forgetting.Specifically, we adjust the moments of the likelihood of the reward estimation conditioned on new features to match the likelihood conditioned on old features.We achieve this by solving a semi-definite program (Vandenberghe & Boyd, 1996, SDP) to approximate the covariance and using the weights of the last layer as prior to the mean.We present simulation results on several real-world and simulated data sets, including classification and regression, using Multi-Layered Perceptrons (MLPs).Our findings suggest that using our method to approximate priors improves performance when memory is limited.Finally, we demonstrate that our neural-linear bandit performs well in a sentiment analysis data set where the input is given in natural language (of size R 8k ) and we use a Convolution Neural Network (CNNs).In this regime, it is not feasible to use a linear method due to computational problems.To the best of our knowledge, this is the first neural-linear algorithm that is resilient to catastrophic forgetting due to limited memory."}
{"paper_id": 70, "abstract": "In the ever-evolving realm of neural networks, where every resource counts and efficiency reigns supreme, we embark on a quest to discover the optimal number of channels\u2014those vital conduits of information\u2014that can elevate accuracy while navigating the treacherous waters of constrained resources like FLOPs, latency, memory footprint, and model size. Enter AutoSlim, a novel and straightforward approach that defies the conventional wisdom of training countless network samples or relying on the whims of reinforcement learning. Instead, we forge a singular slimmable network, a master of adaptability, capable of approximating the accuracy across various channel configurations with remarkable precision.  Through a series of iterative evaluations, we wield this trained slimmable model, deftly trimming layers with the lightest touch\u2014those that promise minimal sacrifice in accuracy. This elegant, one-pass strategy reveals the optimized channel configurations tailored to diverse resource constraints, proving both efficient and effective.  Our experiments unfold across renowned architectures: MobileNet v1, MobileNet v2, ResNet-50, and the RL-searched MNasNet, all tested against the formidable challenge of ImageNet classification. The results are nothing short of transformative, showcasing significant advancements over their default channel settings. Notably, our AutoSlim-MobileNet-v2, operating at 305M FLOPs, achieves a remarkable 74.2% top-1 accuracy\u20142.4% higher than its default counterpart and even outshining the RL-searched MNasNet at 317M FLOPs by 0.2%. Similarly, our AutoSlim-ResNet-50, at 570M FLOPs and devoid of depthwise convolutions, surpasses MobileNet-v1 by 1.3%.  In this journey, we not only enhance performance but do so with a staggering 100X reduction in search cost compared to contemporary channel pruning and neural architecture search methods. Thus, we carve a new path forward, one where precision and resourcefulness intertwine seamlessly in the pursuit of neural network excellence.", "introduction": "The channel configuration (a.k.a.. filter numbers or channel numbers) of a neural network plays a critical role in its affordability on resource constrained platforms, such as mobile phones, wearables and Internet of Things (IoT) devices.The most common constraints (Liu et al., 2017b;Huang et al., 2017;Wang et al., 2017;Han et al., 2015a), i.e., latency, FLOPs and runtime memory footprint, are all bound to the number of channels.For example, in a single convolution or fully-connected layer, the FLOPs (number of Multiply-Adds) increases linearly by the output channels.The memory footprint can also be reduced (Sandler et al., 2018) by reducing the number of channels in bottleneck convolutions for most vision applications (Sandler et al., 2018;Howard et al., 2017;Ma et al., 2018;Zhang et al., 2017b).Despite its importance, the number of channels has been chosen mostly based on heuristics.LeNet-5 (LeCun et al., 1998) selected 6 channels in its first convolution layer, which is then projected to 16 channels after sub-sampling.AlexNet (Krizhevsky et al., 2012) adopted five convolutions with channels equal to 96, 256, 384, 384 and 256.A commonly used heuristic, the \"half size, double channel\" rule, was introduced in VGG nets (Simonyan & Zisserman, 2014), if not earlier.The rule is that when spatial size of feature map is halved, the number of filters is doubled.This heuristic has been more-or-less used in followup network architecture designs including ResNets (He et al., 2016;Xie et al., 2017), Inception nets (Szegedy et al., 2015;2016;2017), MobileNets (Sandler et al., 2018;Howard et al., 2017) and networks for many vision applications.Other heuristics have also been explored.For example, the pyramidal rule (Han et al., 2017;Zhang et al., 2017a) suggested to gradually increase the channels in all convolutions layer by layer, regardless of spatial size.Figure 1 visually summarizes these heuristics for setting channel numbers in a neural network.Beyond the macro-level heuristics across entire network, recent works (Sandler et al., 2018;He et al., 2016;Zhang et al., 2017a;Tan et al., 2018;Cai et al., 2018) have also digged into channel configuration for micro-level building blocks (a network building block is usually composed of  Han et al., 2017;Zhang et al., 2017a), and inside network building blocks ((a) -(f )) (Sandler et al., 2018;He et al., 2016;Han et al., 2017;Zhang et al., 2017a;Tan et al., 2018;Cai et al., 2018).several 1 \u00d7 1 and 3 \u00d7 3 convolutions).These micro-level heuristics have led to better speed-accuracy trade-offs.The first of its kind, bottleneck residual block, was introduced in ResNet (He et al., 2016).It is composed of 1 \u00d7 1, 3 \u00d7 3, and 1 \u00d7 1 convolutions, where the 1 \u00d7 1 layers are responsible for reducing and then restoring dimensions, leaving the 3 \u00d7 3 layer a bottleneck (4\u00d7 reduction).MobileNet v2 (Sandler et al., 2018), however, argued that the bottleneck design is not efficient and proposed the inverted residual block where 1 \u00d7 1 layers are used for expanding feature first (6\u00d7 expansion) and then projecting back after intermediate 3 \u00d7 3 depthwise convolution.Furthermore, MNasNet (Tan et al., 2018) and ProxylessNAS nets (Cai et al., 2018) included 3\u00d7 expansion version of inverted residual block into search space, and achieved even better accuracy under similar runtime latency.Apart from these human-designed heuristics, efforts on automatically optimizing channel configuration have been made explicitly or implicitly.A recent work (Liu et al., 2018c) suggested that many network pruning methods (Liu et al., 2017b;Li et al., 2016;Luo et al., 2017;He et al., 2017;Huang & Wang, 2018;Han et al., 2015b) can be thought of as performing network architecture search for channel numbers.Liu et al. (Liu et al., 2018c) showed that training these pruned architectures from scratch leads to similar or even better performance than fine-tuning and pruning from a large model.More recently, MNasNet (Tan et al., 2018) proposed to directly search network architectures, including filter sizes, using reinforcement learning algorithms (Schulman et al., 2017;Heess et al., 2017).Although the search is performed on the factorized hierarchical search space, massive network samples and computational cost (Tan et al., 2018) are required for an optimized network architecture.In this work, we study how to set channel numbers in a neural network to achieve better accuracy under constrained resources.To start, the first and the most brute-force approach came in mind is the exhaustive search: training all possible channel configurations of a deep neural network for full epochs (e.g., MobileNets (Sandler et al., 2018;Howard et al., 2017) are trained for approximately 480 epochs on ImageNet).Then we can simply select the best performers that are qualified for efficiency constraints.However, it is undoubtedly impractical since the cost of this brute-force approach is too high.For example, we consider a 8-layer convolutional networks and a search space limited to 10 candidates of channel numbers (e.g., 32, 64, ..., 320) for each layer.As a result, there are totally 10 8 candidate network architectures.To address this challenge, we present a simple and one-shot solution AutoSlim.Our main idea lies in training a slimmable network (Yu et al., 2018) to approximate the network accuracy of different channel configurations.Yu et al. (Yu et al., 2018;Yu & Huang, 2019) introduced slimmable networks that can run at arbitrary width with equally or even better performance than same architecture trained individually.Although the original motivation is to provide instant and adaptive accuracyefficiency trade-offs, we find slimmable networks are especially suitable as benchmark performance estimators for several reasons: (1) Training slimmable models (using the sandwich rule (Yu & Huang, 2019)) is much faster than the brute-force approach.(2) A trained slimmable model can execute at arbitrary width, which can be used to approximate relative performance among different channel configurations.(3) The same trained slimmable model can be applied on search of optimal channels for different resource constraints.In AutoSlim, we first train a slimmable model for a few epochs (e.g., 10% to 20% of full training epochs) to quickly get a benchmark performance estimator.We then iteratively evaluate the trained slimmable model and greedily slim the layer with minimal accuracy drop on validation set (for ImageNet, we randomly hold out 50K samples of training set as validation set).After this single pass, we can obtain the optimized channel configurations under different resource constraints (e.g., network FLOPs limited to 150M, 300M and 600M).Finally we train these optimized architectures individually or jointly (as a single slimmable network) for full training epochs.We experiment with various networks including MobileNet v1, MobileNet v2, ResNet-50 and RL-searched MNasNet on the challenging setting of 1000-class ImageNet classification.AutoSlim achieves better results (with much lower search cost) compared with three baselines: (1) the default channel configuration of these networks, (2) channel pruning methods on same network architectures (Luo et al., 2017;He et al., 2017;Yang et al., 2018) and (3) reinforcement learning based architecture search methods (He et al., 2018;Tan et al., 2018)."}
{"paper_id": 71, "abstract": "In the ever-evolving realm of neural architecture search (NAS), a powerful new contender emerges\u2014BigNAS. This innovative approach boldly strides into the arena, simplifying the intricate dance of model discovery while simultaneously scaling to accommodate a diverse array of model sizes. Traditionally, NAS methods have relied on a two-stage workflow, where a one-shot model, trained with shared weights, would necessitate a tedious retraining or fine-tuning of the best child model to avoid performance pitfalls. But BigNAS shatters those constraints.  With a keen understanding of the unique initialization and learning dynamics that differentiate small models from their larger counterparts, we introduce a series of groundbreaking techniques that allow us to train a single-stage model. This means that from one robust foundation, we can seamlessly slice off high-quality child models without the cumbersome need for retraining. Our experiments on the ImageNet dataset reveal a remarkable capability: a single set of shared weights gives rise to a family of child models, ranging from 200 to 1000 MFLOPs in size.  The results? Nothing short of extraordinary. The BigNASModels achieve top-1 accuracies that span from 76.5% to an impressive 80.9%, outpacing all existing state-of-the-art models within this spectrum, including the renowned EfficientNets. With BigNAS, we not only push the boundaries of what\u2019s possible in NAS but also redefine the very nature of model optimization in the quest for speed and accuracy.", "introduction": "Designing network architectures that are both accurate and efficient is crucial for deep learning on edge devices.It is well known that a single neural network architecture can require more than an order of magnitude more inference time if it is deployed on a slower device (Yu et al., 2018).This makes it appealing to not only search for architectures that are optimized for specific devices, but also to ensure that a range of models can be deployed effectively.In the past, Neural Architecture Search (NAS) methods (Zoph & Le, 2016;Zoph et al., 2018;Real et al., 2018) have shown to be excellent at optimizing for a single device and latency target (Tan et al., 2019).However, if we wish to target a large array of devices, it becomes prohibitively expensive and time-consuming to run a separate search for each one.A possible solution to this is to use basic scaling heuristics such as the EfficientNet family (Tan & Le, 2019).However, this loses out on opportunities to optimize models specialized for the diverse performance characteristics of individual devices.Another option would be to use efficient architecture search methods, e.g., Pham et al. (2018); Bender et al. (2018); Liu et al. (2018b).However, to target multiple devices, we must run many searches and retrain all of the searched models from scratch.In this work, we search over a big single-stage model that contains both small child models (\u223c200 MFLOPs, comparable to MobileNetV3) and big child models (\u223c1 GFLOPs, comparable to Effi-cientNet B2).Different from existing one-shot methods (Bender et al., 2018;Liu et al., 2018b;Brock et al., 2018;Pham et al., 2018), our trained single-stage model offers a much wider coverage of model capacities, and more importantly, all child models are trained in a way such that they simultaneously reach excellent performance at the end of the search phase, without requiring a separate retraining step.Architecture selection can be then carried out via a simple coarse-to-fine selection strategy.Once an architecture is selected, we can obtain a child model by slicing the single-stage model for instant deployment w.r.t. the given constraints such as memory footprint and/or runtime latency.The workflow is illustrated in Figure 1.Figure 1: Comparison with several existing workflows.We use nested squares to denote models with shared weights, and use the size of the square to denote the size of each model.Workflow in the middle refers the concurrent work from Cai et al. (2019), where submodels are sequentially induced through progressive distillation and channel sorting.We simultaneously train all child models in a single-stage model with proposed modifications, and deploy them without retraining or finetuning.The success of our method heavily relies on training a high-quality single-stage model, which is challenging on its own.For example, we find the training loss explodes if the single-stage model is not properly initialized, and bigger child models start to overfit before smaller ones plateau.It is particularly nontrivial to simultaneously retain good performance on every individual child model due to aggressive parameter sharing during architecture search.We address these challenges through a combination of techniques, including an improved sampling strategy and efficient inplace distillation, and substantially stabilize the single-stage model training through better initialization, learning rate schedule and regularization.Effectiveness of the proposed solutions is backed up by ablation studies.With the proposed techniques, we are able train a single-stage model on ImageNet and obtain a family of child models that simultaneously surpass all the state-of-the-art models in the range of 200 to 1000 MFLOPs, including EfficientNets B0-B2 (1.6% more accurate under 400 MFLOPs), without retraining or finetuning the child models upon the completion of search.One of our child models achieves 80.9% top-1 accuracy at 1G FLOPs (four times less computation than a ResNet-50)."}
{"paper_id": 72, "abstract": "In the ever-evolving realm of adversarial examples, a significant challenge persists: the creation of semantically meaningful perturbations that remain undetectable to the human eye. In this paper, we unveil a novel framework designed to generate adversarial examples that not only retain their semantic integrity but also confound existing defenses.   At the heart of our approach lies a manifold learning technique that meticulously captures the underlying semantics of input data. By employing statistical inference, we distill the high-dimensional complexity of our inputs into low-dimensional geometric representations, preserving their essential meaning.   Once we have established this manifold, we invoke the Gram-Schmidt process to carefully perturb its elements, ensuring that these modifications remain anchored within the manifold's structure. This delicate balance allows us to craft adversarial examples that are both subtle and semantically coherent.  Our efficient algorithm harnesses the rich semantics of the inputs as a guiding compass, enabling us to impose adversarial constraints while preserving the essence of the data. We rigorously test our framework across a variety of domains, including toy datasets, images, and text, demonstrating its prowess in generating adversarial examples that not only evade current defenses but also maintain their semantic fidelity. In a world where the line between adversary and ally blurs, our work stands as a beacon of innovation, paving the way for a deeper understanding of the adversarial landscape.", "introduction": "In response to the susceptibility of deep neural networks to small adversarial perturbations (Szegedy et al., 2014), several defenses have been proposed (Liu et al., 2019;Sinha et al., 2018;Raghunathan et al., 2018;Madry et al., 2017;Kolter & Wong, 2017).Recent attacks have, however, cast serious doubts on the robustness of these defenses (Athalye et al., 2018;Carlini & Wagner, 2016).A standard way to increase robustness is to inject adversarial examples into the training inputs (Goodfellow et al., 2014a).This method, known as adversarial training, is however sensitive to distributional shifts between the inputs and their adversarial examples (Ilyas et al., 2019).Indeed, distortions, occlusions or changes of illumination in an image, to name a few, do not always preserve the nature of the image.In text, slight changes to a sentence often alter its readability or lead to substantial differences in meaning.Constructing semantics preserving adversarial examples would provide reliable adversarial training signals to robustify deep learning models, and make them generalize better.However, several approaches in adversarial attacks fail to enforce the semantic relatedness that ought to exist between the inputs and their adversarial counterparts.This is due to inadequate characterizations of the semantics of the inputs and the adversarial examples - Song et al. (2018) and Zhao et al. (2018b) confine the distribution of the latents of the adversarial examples to a Gaussian.Moreover, the search for adversarial examples is customarily restricted to uniformly-bounded regions or conducted along suboptimal gradient directions (Szegedy et al., 2014;Kurakin et al., 2016;Goodfellow et al., 2014b).In this study, we introduce a method to address the limitations of previous approaches by constructing adversarial examples that explicitly preserve the semantics of the inputs.We achieve this by characterizing and aligning the low dimensional geometric summaries of the inputs and the adversarial examples.The summaries capture the semantics of the inputs and the adversarial examples.The alignment ensures that the adversarial examples reflect the unbiased semantics of the inputs.We decompose our attack mechanism into: (i.) manifold learning, (ii.) perturbation invariance, and (iii.)adversarial attack.The motivating principle behind step (i.) is to learn the low dimensional geometric summaries of the inputs via statistical inference.Thus, we present a variational inference technique that relaxes the rigid Gaussian prior assumption typically placed on VAEs encoder networks (Kingma & Welling, 2014) to capture faithfully such summaries.In step (ii.), we develop an approach around the manifold invariance concept of (Roussel, 2019) to perturb the elements of the learned manifold while ensuring the perturbed elements remain within the manifold.Finally, in step (iii.), we propose a learning algorithm whereby we leverage the rich semantics of the inputs and the perturbations as a source of knowledge upon which we impose adversarial constraints to produce adversarial examples.Unlike (Song et al., 2018;Carlini & Wagner, 2016;Zhao et al., 2018b;Goodfellow et al., 2014b) that resort to a costly search of adversarial examples, our algorithm is efficient and end-to-end.The main contributions of our work are thus: (i.) a variational inference method for manifold learning in the presence of continuous latent variables with minimal assumptions about their distribution, (ii.) an intuitive perturbation strategy that encourages perturbed elements of a manifold to remain within the manifold, (iii.) an end-to-end and computationally efficient algorithm that combines (i.) and (ii.) to generate adversarial examples in a black-box setting, and (iv.) illustration on toy data, images and text, as well as empirical validation against strong certified and non-certified adversarial defenses."}
{"paper_id": 73, "abstract": "In this paper, we embark on a quest to unravel the complexities of identifying samples that stray from the familiar training distribution\u2014those elusive out-of-distribution (OOD) samples that can confound classification tasks. While many have ventured into this realm, relying on the notion that low-confidence classifications signal OOD examples through the lens of deep neural networks (DNNs), a significant flaw emerges. In challenging datasets or with models that lack robust classification prowess, these methods often misinterpret in-distribution samples, especially those lingering near the decision boundary, as OOD. This misjudgment stems from their reliance on features nearest to the output layer, neglecting the rich tapestry of uncertainty woven throughout the network's layers.  To confront this challenge, we present a novel approach that delves deeper. By employing a reparameterization trick, we extract the uncertainties inherent in the features across each layer of the DNN, weaving them together into a more comprehensive understanding. Our experiments reveal a striking superiority over existing methodologies, as our method not only achieves but surpasses state-of-the-art detection performance across multiple datasets and classification models. For instance, we elevate the AUROC score from a respectable 83.8% to an astounding 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets. Join us as we illuminate this path forward, transforming the way we discern the known from the unknown in the realm of classification.", "introduction": "Deep neural networks (DNNs) have achieved high performance in many classification tasks such as image classification (Krizhevsky et al., 2012;Simonyan & Zisserman, 2014), object detection (Lin et al., 2017;Redmon & Farhadi, 2018), and speech recognition (Hinton et al., 2012;Hannun et al., 2014).However, DNNs tend to make high confidence predictions even for samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples (Hendrycks & Gimpel, 2016).Such errors can be harmful to medical diagnosis and automated driving.Because it is not generally possible to control the test data distribution in real-world applications, OOD samples are inevitably included in this distribution.Therefore, detecting OOD samples is important for ensuring the safety of an artificial intelligence system (Amodei et al., 2016).There have been many previous studies (Hendrycks & Gimpel, 2016;Liang et al., 2017;Lee et al., 2017;DeVries & Taylor, 2018;Lee et al., 2018;Hendrycks et al., 2018) that have attempted to solve this problem by regarding samples that are difficult to classify or samples with low classification confidence as OOD examples using DNNs.Their approaches work well and they are computationally efficient.The limitation of these studies is that, when using difficult datasets or models with low classification ability, the confidence of inputs will be low, even if the inputs are in-distribution samples.Therefore, these methods incorrectly regard such in-distribution samples as OOD samples, which results in their poor detection performance (Malinin & Gales, 2018), as shown in Figure 1.One cause of the abovementioned problem is that their approaches use only the features close to the output layer and the features are strongly related to the classification accuracy.Therefore, we use not only the features close to the output layer but also the features close to the input layer.We hypothesize that the uncertainties of the features close to the input layer are the uncertainties of the feature extraction and are effective for detecting OOD samples.For example, when using convolutional neural networks (CNNs), the filters of the convolutional layer close to the input layer extract features such as edges that are useful for in-distribution classification.In other words, indistribution samples possess more features that convolutional filters react to than OOD samples.Therefore, the uncertainties of the features will be larger when the inputs are in-distribution samples.Another cause of the abovementioned problem is that their approaches disregard the uncertainty of the features close to the output layer.We hypothesize that the uncertainties of the latent features close Baseline (Hendrycks & Gimpel, 2016) UFEL (ours) max softmax probability Baseline UFEL (ours) degree of uncertaintyFigure 1: Comparison of existing and proposed methods.We visualized scatter plots of the outputs of the penultimate layer of a CNN that can estimate the uncertainties of latent features using the SVHN dataset (Netzer et al., 2011).We used only classes 0, 1, and 2 for the training data.Classes 0, 1, 2, and OOD, indicated by red, yellow, blue, and black, respectively, were used for the validation data.We plot the contour of the maximum output of the softmax layer of the model.Left: Because the image of \"204\" includes the digits \"2\" and \"0,\" the maximum value of the softmax output decreases because the model does not know to which class the image belongs.Right: The sizes of points in the scatter plots indicate the value of the combined uncertainties of features.We can classify the image of \"204\" as an in-distribution image according to the value of the combined uncertainties.to the output layer are the uncertainties of classification and are also effective for detecting OOD samples.For example, in-distribution samples are embedded in the feature space close to the output layer to classify samples.In contrast, OOD samples have no fixed regions for embedding.Therefore, the uncertainties of the features of OOD samples will be larger than those of in-distribution samples.Based on the hypotheses, we propose a method that extracts the Uncertainties of Features in Each Layer (UFEL) and combines them for detecting OOD samples.Each uncertainty is easily estimated after training the discriminative model by computing the mean and the variance of their features using a reparameterization trick such as the variational autoencoder (Kingma & Welling, 2013) and variational information bottleneck (Alemi et al., 2016;2018).Our proposal is agnostic to the model architecture and can be easily combined with any regular architecture with minimum modifications.We visualize the maximum values of output probability and the combined uncertainties of the latent features in the feature space of the penultimate layer in Figure 1.The combined uncertainties of the features discriminate the in-distribution and OOD images that are difficult to classify.For example, although the images that are surrounded by the red line are in-distribution samples, they have low maximum softmax probabilities and could be regarded as OOD samples in prior work.Meanwhile, their uncertainties are smaller than those of OOD samples and they are regarded as in-distribution samples in our method.In experiments, we validate the hypothesis demonstrating that each uncertainty is effective for detecting OOD examples.We also demonstrate that UFEL can obtain state-of-the-art performance in several datasets including CIFAR-100, which is difficult to classify, and models including LeNet5 with low classification ability.Moreover, UFEL is robust to hyperparameters such as the number of in-distribution classes and the validation dataset."}
{"paper_id": 74, "abstract": "In the ever-evolving realm of machine learning, the quest for interpretability has emerged as a beacon of necessity, illuminating the path through the opaque labyrinth of black-box decision systems. Yet, as we delve into this intricate domain, we find ourselves at a crossroads: the need for explanations that are both succinct and thorough. Current methods often stumble, providing redundant insights that leave users adrift in a sea of information.   Enter the Variational Information Bottleneck for Interpretation\u2014VIBI, a groundbreaking approach that transcends the limitations of its predecessors. This system-agnostic framework harnesses the power of information theory, specifically the information bottleneck principle, to forge a new standard for interpretability. VIBI deftly navigates the balance between brevity and comprehensiveness, selecting key features that are not only maximally compressed around an input\u2014ensuring clarity\u2014but also rich in relevance to the decisions rendered by the enigmatic black-box.  In our exploration of VIBI, we subject it to rigorous testing across three diverse datasets, pitting it against the leading methods in the field. Through both human evaluations and quantitative metrics, we assess its interpretability and fidelity, revealing a promising new horizon for understanding the intricate workings of machine learning systems. Join us as we unravel the complexities of interpretation, armed with VIBI as our guide.", "introduction": "Interpretability is crucial in building and deploying black-box decision systems such as deep learning models.Interpretation of a black-box system helps decide whether or not to follow its decisions, or understand the logic behind the system.In recent years, the extensive use of deep learning black-box systems has given rise to interpretable machine learning approaches (Lipton, 2016;Doshi-Velez & Kim, 2017), which aim to explain how black-box systems work or why they reach certain decisions.In order to provide sufficient information while avoiding redundancy when explaining a black-box decision, we need to consider both briefness and comprehensiveness.However, existing approaches lack in-depth consideration for and fail to find both brief but comprehensive explanation.In order to obtain brief but comprehensive explanation, we adopt the information bottleneck principle (Tishby et al., 2000).This principle provides an appealing information theoretic perspective for learning supervised models by defining what we mean by a 'good' representation.The principle says that the optimal model transmits as much information as possible from its input to its output through a compressed representation called the information bottleneck.Then, the information bottleneck will maximally compress the mutual information (MI) with an input while preserving as much as possible MI with the output.Recently, it has been shown that the principle also applies to deep neural networks and each layer of a deep neural network can work as an information bottleneck (Tishby & Zaslavsky, 2015;Shwartz-Ziv & Tishby, 2017).Using this idea of information bottleneck principle, we define a brief but comprehensive explanation as maximally informative about the black-box decision while compressive about a given input.In this paper, we introduce the variational information bottleneck for interpretation (VIBI), a systemagnostic information bottleneck model that provides a brief but comprehensive explanation for every single decision made by a black-box model.VIBI is composed of two parts: explainer and approximator, each of which is modeled by a deep neural network.The explainer returns a probability whether a chunk of features such as a word, phrase, sentence or a group of pixels will be selected as an explanation or not for each instance, and an approximator mimics behaviour of a black-box model.Using the information bottleneck principle, we learn an explainer that favors brief explanations while enforcing that the explanations alone suffice for accurate approximations to a black-box model.i Under review as a conference paper at ICLR 2020 1.1 CONTRIBUTION Our main contribution is to provide a new framework that systematically defines and generates a 'good' (i.e.brief but comprehensive) explanation using the information bottleneck principle.Based on this principle, we develop VIBI that favors a brief but comprehensive explanation.In order to make the objective function of VIBI tractable, we derive a variational approximation to the objective.The benefits of our method are as follows.1) System-agnostic: VIBI can be applied to explain any black-box system.2) Post-hoc learning: VIBI is learned in a post-hoc manner, hence there is no tradeoff between task accuracy of a black-box system and interpretability of an explainer.3) Cognitive chunk: Cognitive chunk is defined as a group of raw features whose identity is understandable to human.VIBI groups non-cognitive raw features such as a pixel and letter into a cognitive chunk (e.g. a group of pixels, a word, a phrase, a sentence) and selects each unit as an explanation.4) Separate explainer and approximator: The explainer and approximator are designed for separated tasks so that we do not need to limit the approximator to have a simple structure, which may reduce the fidelity (the ability to imitate the behaviour of a black-box) of approximator."}
{"paper_id": 75, "abstract": "In the ever-evolving realm of Deep Neural Networks (DNNs), the pursuit of robustness and accuracy stands as both a fundamental challenge and a crucial endeavor, particularly when faced with the specter of semantically abnormal examples. Despite significant advancements in the field, a pivotal question remains largely uncharted: which training examples warrant our focus, and to what extent should we amplify their significance to foster resilient learning? This work embarks on a quest to unravel this mystery, introducing a novel approach we term gradient rescaling (GR). By deftly adjusting the magnitude of the logit vector\u2019s gradient, GR directs attention toward relatively easier training data points, especially in the presence of escalating noise. This mechanism serves as a form of explicit emphasis regularization, enhancing the generalization capabilities of DNNs. Beyond mere regularization, we weave GR into the fabric of example weighting and the design of robust loss functions. Our empirical findings reveal that GR exhibits remarkable resilience against anomalies, significantly surpassing state-of-the-art methods\u2014illustrated by a 7% performance boost on CIFAR100 amidst 40% noisy labels. Moreover, GR consistently outshines conventional regularizers in both pristine and challenging environments. To further illuminate the intricacies of GR, we present a series of comprehensive ablation studies, offering valuable insights for its application in real-world scenarios.", "introduction": "DNNs have been successfully applied in diverse applications (Socher et al., 2011;Krizhevsky et al., 2012;LeCun et al., 2015).However, their success is heavily reliant on the quality of training data, especially accurate semantic labels for learning supervision.Unfortunately, on the one hand, maintaining the quality of semantic labels as the scale of training data increases is expensive and almost impossible when the scale becomes excessively large.On the other hand, it has been demonstrated that DNNs are capable of memorising the whole training data even when all training labels are random (Zhang et al., 2017).Therefore, DNNs struggle to discern meaningful data patterns and ignore semantically abnormal examples 1 simultaneously (Krueger et al., 2017;Arpit et al., 2017).Consequently, it becomes an inevitable demand for DNNs to hold robustness when training data contains anomalies (Larsen et al., 1998;Natarajan et al., 2013;Sukhbaatar & Fergus, 2014;Xiao et al., 2015;Patrini et al., 2017;Vahdat, 2017;Veit et al., 2017;Li et al., 2017).Recently, great progress has been made towards robustness against anomalies when training DNNs (Krueger et al., 2017).There are three appealing perspectives in terms of their simplicity and effectiveness: 1) Examples weighting.For example, knowledge distilling from auxiliary models is popular for heuristically designing weighting schemes.However, it is challenging to select and train reliable auxiliary models in practice (Li et al., 2017;Malach & Shalev-Shwartz, 2017;Jiang et al., 2018;Ren et al., 2018;Han et al., 2018b).2) Robust loss functions (Van Rooyen et al., 2015;Ghosh et al., 2017;Zhang & Sabuncu, 2018;Wang et al., 2019b); 3) Explicit regularisation techniques (Arpit et al., 2017;Zhang et al., 2018a).Although designing robust losses or explicit regularisation is easier and more flexible in practice, the performance is not the optimal yet. 1 One training example is composed of an input and its corresponding label.A semantically abnormal example means the input is semantically unrelated to its label, which may come from corrupted input or label.For example, in Figure 3 in the supplementary material: 1) Out-of-distribution anomalies: An image may contain only background or an object which does not belong to any training class; 2) In-distribution anomalies: An image of class a may be annotated to class b or an image may contain more than one semantic object.Regarding examples weighting, there is a core research question which is not well answered yet:What training examples should be focused on and how large the emphasis spread should be?In this work, we present a thorough study of this practical question under different settings.For better analysis, we propose two basic and necessary concepts: emphasis focus and spread with explicit definition in Sec.3.2.They are conceptually introduced as follows:Emphasis focus.It is a common practice to focus on harder instances when training DNNs (Shrivastava et al., 2016;Lin et al., 2017).When a dataset is clean, it achieves faster convergence and better performance to emphasise on harder examples because they own larger gradient magnitude, which means more information and a larger update step for model's parameters.However, when severe noise exists, as demonstrated in (Krueger et al., 2017;Arpit et al., 2017), DNNs learn simple meaningful patterns first before memorising abnormal ones.In other words, anomalies are harder to fit and own larger gradient magnitude in the later stage.Consequently, if we use the default sample weighting in categorical cross entropy (CCE) where harder samples obtain higher weights, anomalies tend to be fitted well especially when a network has large enough capacity.That is why we need to move the emphasis focus towards relatively easier ones, which serves as emphasis regularisation.Emphasis spread.We term the weighting variance of training examples emphasis spread.The key concept is that we should not treat all examples equally, neither should we let only a few be emphasised and contribute to the training.Therefore, when emphasis focus changes, the emphasis spread should be adjusted accordingly.We integrate emphasis focus and spread into a unified example weighting framework.Emphasis focus defines what training examples own higher weights while emphasis spread indicates how large variance over their weights.Specifically, we propose gradient rescaling (GR), which modifies the magnitude of logit vector's gradient.The logit vector is the output of the last fully connected (FC) layer of a network.We remark that we do not design the weighting scheme heuristically from scratch.Instead, it is naturally motivated by the gradient analysis of several loss functions.Interestingly, GR can be naturally connected to examples weighting, robust losses, explicit regularisation: 1) The gradient magnitude of logit vector can be regarded as weight assignment that is built-in in loss functions (Gopal, 2016;Alain et al., 2016;Zhang et al., 2018b).Therefore, rescaling the gradient magnitude equals to adjusting the weights of examples; 2) A specific loss function owns a fixed gradient derivation.Adjusting the gradient can be treated as a more direct and flexible way of modifying optimisation objectives; 3) Instead of focusing on harder examplesfoot_0 by default, we can adjust emphasis focus to relative easier ones when noise is severe.GR serves as emphasis regularisation and is different from standard regularisers, e.g., L2 weight decay constraints on weight parameters and Dropout samples neural units randomly (Srivastava et al., 2014); GR is simple yet effective.We demonstrate its effectiveness on diverse computer vision tasks using different net architectures: 1) Image classification with clean training data; 2) Image classification with synthetic symmetric label noise, which is more challenging than asymmetric noise evaluated by (Vahdat, 2017;Ma et al., 2018); 3) Image classification with real-world unknown anomalies, which may contain open-set noise (Wang et al., 2018), e.g., images with only background, or outliers, etc.; 4) Video person re-identification, a video retrieval task containing diverse anomalies.Beyond, we show that GR is notably better than other standard regularisers, e.g., L2 weight decay and dropout.Besides, to comprehensively understand GR's behaviours, we present extensive ablation studies.Main contribution.Intuitively and principally, we claim that two basic factors, emphasis focus and spread, should be babysat simultaneously when it comes to examples weighting.To the best of our knowledge, we are the first to thoroughly study and analyse them together in a unified framework."}
{"paper_id": 76, "abstract": "In this paper, we unveil a novel approach we call Dimensional Reweighting Graph Convolutional Networks (DrGCNs), crafted to address the challenge of variance lurking within the dimensional information of node representations in Graph Convolutional Networks (GCNs). Through rigorous proof, we establish that DrGCNs possess the ability to mitigate this variance by anchoring our exploration in the rich soil of mean field theory. Yet, as we ventured into the practical realm, we discovered that the degrees to which DrGCNs exert their influence can fluctuate dramatically across different datasets.  In response to this variability, we took a step back and devised a new metric, denoted as K, to quantify the effectiveness of our approach. This measure not only illuminates when dimensional reweighting should be employed within GCNs but also delineates the extent of its potential benefits. Furthermore, it provides valuable insights into the mechanisms behind the enhancements achieved through DrGCNs.  The dimensional reweighting block we propose is both lightweight and adaptable, making it compatible with a wide array of GCN variants. Through meticulously designed experiments\u2014addressing issues such as duplicate entries, information leakage, and mislabeled instances within well-established node classification benchmark datasets\u2014we demonstrate that DrGCNs significantly outperform the current state-of-the-art methodologies. Notably, our findings reveal substantial improvements even within a large-scale industrial dataset, underscoring the robustness and versatility of our approach.", "introduction": "Deep neural networks (DNNs) have been widely applied in various fields, including computer vision (He et al., 2016;Hu et al., 2018), natural language processing (Devlin et al., 2019), and speech recognition (Abdel-Hamid et al., 2014), among many others.Graph neural networks (GNNs) is proposed for learning node presentations of networked data (Scarselli et al., 2009), and later be extended to graph convolutional network (GCN) that achieves better performance by capturing topological information of linked graphs (Kipf & Welling, 2017).Since then, GCNs begin to attract board interests.Starting from GraphSAGE (Hamilton et al., 2017) defining the convolutional neural network based graph learning framework as sampling and aggregation, many follow-up efforts attempt to enhance the sampling or aggregation process via various techniques, such as attention mechanism (Veli\u010dkovi\u0107 et al., 2018), mix-hop connection (Abu-El-Haija et al., 2019) and adaptive sampling (Huang et al., 2018).In this paper, we study the node representations in GCNs from the perspective of covariance between dimensions.Suprisingly, applying a dimensional reweighting process to the node representations may be very useful for the improvement of GCNs.As an instance, under our proposed reweighting scheme, the input covariance between dimensions can be reduced by 68% on the Reddit dataset, which is extremely useful since we also find that the number of misclassified cases reduced by 40%, compared with the previous SOTA method.We propose Dimensional reweighting Graph Convolutional Networks (DrGCNs), in which the input of each layer of the GCN is reweighted by global node representation information.Our discovery is that the experimental performance of GCNs can be greatly improved under this simple reweighting scheme.On the other hand, with the help of mean field theory (Kadanoff, 2009;Yang et al., 2019), this reweighting scheme is also proved to improve the stability of fully connected networks, provding insight to GCNs.To deepen the understanding to which extent the proposed reweighting scheme can help GCNs, we develop a new measure to quantify its effectiveness under different contexts (GCN variants and datasets).Experimental results verify our theoretical findings ideally that we can achieve predictable improvements on public datasets adopted in the literature over the state-of-the-art GCNs.While studying on these well-known benchmarks, we notice that two of them (Cora, Citeseer) suffer from duplicates and feature-label information leaks.We fix these problems and offer refined datasets for fair comparisons.To further validate the effectiveness, we deploy the proposed DrGCNs on A*foot_0 company's recommendation system and clearly demonstrate performance improvements via offline evaluations."}
{"paper_id": 77, "abstract": "In the realm of machine learning, predicting the emotional resonance of videos presents a formidable challenge, akin to navigating a treacherous landscape filled with hidden pitfalls. The critical stages of feature extraction, multi-modal fusion, and the intricate dance of temporal context fusion stand as towering obstacles in the quest to accurately gauge valence and arousal. Yet, these elements have remained largely untapped, like a forgotten treasure waiting to be unearthed.  In this paper, we unveil a groundbreaking framework, meticulously crafted with innovative model structures and a multi-modal fusion strategy that defies convention. We embark on a selective journey, identifying the most fitting modalities for our valence and arousal tasks, each one meticulously extracted using modality-specific pre-trained deep models honed on vast, generic datasets. To capture the ebb and flow of video content and the emotional undercurrents that accompany it, we introduce two distinct time-scale structures\u2014one dedicated to the nuances within individual clips and the other to the broader narrative woven between them.  But our journey does not end there. To weave together the rich tapestry of complementary information from multiple modalities, we propose a novel, efficient residual-based progressive training strategy. This method allows us to incrementally integrate each modality into our multi-modal model, filling in the gaps left by incomplete features with precision and grace.  With this comprehensive approach, our prediction framework not only meets the challenge head-on but also surpasses the state-of-the-art with remarkable performance, carving a new path in the landscape of emotional video analysis.", "introduction": "Affective video content analysis aims at predicting the videos' emotional impact on audiences.It plays important roles in understanding the videos' content, highlight detection, and of a fundamental support for several advanced applications such as multi-modal search with sentimental queries.Predicting the audiences' emotional evolvement when watching movies is also an important way to help both online media-server providers or filmmakers to invest movies, evaluate on-line effect as well as distribute them more efficiently.In the affective computing community, human emotions can be categorically or continuously defined (Izard, 2007;Barrett et al., 2007).Emotion categories are happy, sad, angry, surprise, disgust, neutral, which are commonly used in emotion recognition or classification (Cowie et al., 2001).Compared to the categorical definition, continuous definition describes the emotions continuously in two dimensions: Valence (positive vs. negative) and Arousal (active vs. calm).Any human emotion can be located in the space spanned by the two dimensions, which is more fine-grained than the categorical definition.The goal of our task is to predict the audiences' emotional states based on the movie content, i.e. the valence and arousal values with the movie going on.Finding discriminative features from raw videos for predicting valence and arousal values is far away from an easy task.Video is the typical multi-modal media involving both audio and visual modalities.Even if in visual content, human facial expressions, pose behaviors, scenes, etc. can also be regarded as modalities.Audiences' emotions can be triggered by any modality such as the actors' expressions or actions, the movies' scenes (environment, atmosphere) as well as background music.Therefore, the mainstream of affective video content analysis is to extract multi-modal features and combine all those features.Feature fusion is another challenging step.Multi-modal features are always complementary and the importance of each modality dynamically changes over time.For example, some movie clips' emotional impact can be captured by audio content while others may rely on visual features.Current studies of affective video content analysis mainly adopt either decision-level fusion (Dobri\u0161ek et al., 2013) or feature-level fusion (Wimmer et al., 2008).The former combines results from each modality through voting or weighted average methods.Each modality-specific model is trained independently which can't exploit the complementary information between modalities.The latter concatenates multi-modal features and learn parameters for all modalities at the same time which can easily lead to overfitting.We design a progressive training algorithm where each modality is trained and fine-tuned stepwisely.Each modality is only responsible for completing the missing parts of features extracted from previous modalities, thus the most discriminative modalities can be dynamically selected for each movie clip and the complementarity of multi-modal features can be fully utilized.The overfitting risk can also be suppressed since fewer parameters are learned at each step.Besides all the above, we also investigate how to utilize the temporal context of videos for sentiment prediction, which is lacking in most of the related works, where they simply apply LSTMs (Hochreiter & Schmidhuber, 1997) or GRUs (Cho et al., 2014) for temporal dependency.We propose two-time-scale model structures considering the video's long-short temporal context.For the short-time context, LSTMS are used for each modality.For the long-time dependency of the valence task, a structure that is similar to the temporal segment network (TSN) (Wang et al., 2016) is used to capture the long temporal context.For the arousal task, a moving mean post-processing method is adopted to utilize the trend of previous emotions.The contributions of this paper are as follows: We propose an effective multi-modal fusion network and design a two-time-scale model structure considering the video's long-short temporal context for affective impact prediction.Also, a residual-based progressive training strategy is used to train the fusion network to fully utilize the complementary and representative capability of each modality.Our model and training strategy achieve a new state-of-art on several related tasks."}
{"paper_id": 78, "abstract": "In the realm of reinforcement learning, the vision of an agent capable of mastering behaviors through mere observation is a tantalizing one. Yet, the quest to craft rewards that nurture this ambition within the RL framework remains fraught with challenges. In this work, we tackle this conundrum using the elegance of Siamese networks, designed to gauge the distance between the observed behaviors and those of the agent itself. By harnessing these networks, we can generate a reward signal for the RL agent, derived from the disparity between the desired motion and the agent's own execution.  Our exploration centers on a recurrent neural network (RNN)-based comparator model, adept at measuring distances in both spatial and temporal dimensions between motion clips, all while training the RL policy to minimize this very distance. Through rigorous experimentation, we\u2019ve uncovered that incorporating multi-task data alongside an additional image encoding loss significantly bolsters temporal consistency. These dual elements strike a delicate balance between rewarding the agent for accurately replicating a specific instance of behavior and encouraging a broader understanding of that behavior as a whole.  Moreover, we shine a light on a particularly daunting aspect of this endeavor: the one-shot learning scenario, where the agent must learn from a solitary demonstration of a task. Our findings showcase the efficacy of our approach on humanoid agents, both in a 2D environment with 10 degrees of freedom and a more complex 3D setting featuring 38 degrees of freedom.", "introduction": "Imitation learning and Reinforcement Learning (RL) often intersect when the goal is to imitate with incomplete information, for example, when imitating from motion capture data (mocap) or video.In this case, the agent needs to search for actions that will result in observations similar to the expert.However, formulating a metric that will provide a reasonable distance between the agent and the expert is difficult.Robots and people plan using types of internal and abstract pose representations that can have reasonable distances; however, typically when animals observe others performing tasks, only visual information is available.Using distances in pose-space is ill-suited for imitation as changing some features can result in drastically different visual appearance.In order to understand how to perform tasks from visual observation a mapping/transformation is used which allows for the minimization of distance in appearance.Even with a method to transform observations to a similar pose space, each person has different capabilities.Because of this, people are motivated to learn transformations in space and time where they can reproduce the behaviour to the best of their own ability.How can we learn a representation similar to this latent space?An essential detail of imitating demonstrations is their sequential and causal nature.There is both an ordering and speed in which a demonstration is performed.Most methods require the agent to learn to imitate the temporal and spatial structure at the same time creating a potentially narrow solution space.When the agent becomes desynchronized with the demonstration, the agent will receive a low reward.Consider the case when a robot has learned to stand when its goal is to walk.Standing is spatially close to the demonstration and actions that help the robot stand, as opposed to falling, should be encouraged.How can such latent goals be encouraged?If we consider a phase-based reward function r = R(s, a, \u03c6) where \u03c6 indexes the time in the demonstration and s and a is the agent state and action.As the demonstration timing \u03c6, often controlled by the environment, and agent diverge, the agent receives less reward, even if it is visiting states that exist elsewhere in the demonstration.The issue of determining if an agent is displaying outof-phase behaviour can understood as trying to find the \u03c6 that would result in the highest reward \u03c6 = max \u03c6 R(s, a, \u03c6) and the distance \u03c6 -\u03c6 is an indicator of how far away in time or out-ofphase the agent is.This phase-independent form can be seen as a form of reward shaping.However, this naive description ignores the ordered property of demonstrations.What is needed is a metric that gives reward for behaviour that is in the proper order, independent of phase.This ordering motivates the creation of a recurrent distance metric that is designed to understand the context between two motions.For example, does this motion look like a walk, not, does this motion look precisely like that walk.Our proposed Visual Imitation with Reinforcement Learning (VIRL) method uses Recurrent Siamese Networks (RSNs) and has similarities to both Inverse Reinforcement Learning (IRL) (Abbeel & Ng, 2004) and Generative Advisarial Imitation Learning (GAIL) (Ho & Ermon, 2016).The process of learning a cost function that understands the space of policies to find an optimal policy given a demonstration is fundamentally IRL.While using positive examples from the expert and negative examples from the policy is similar to the method GAIL uses to train a discriminator to recognize in distribution examples.In this work, we build upon these techniques by constructing a method that can learn policies using noisy visual data without action information.Considering the problem's data sparsity, we include data from other tasks to learn a more robust distance function in the space of visual sequence.We also construct a cost function that takes into account the demonstration ordering as well as pose using a recurrent Siamese network.Our contribution consists of proposing and exploring these forms of recurrent Siamese networks as a way to address a critical problem in defining reward structure for imitation learning from the video for deep RL agents and accomplishing this on simulated humanoid robots for the challenging single shot learning setting."}
{"paper_id": 79, "abstract": "In the vast realm of data collection, the quest for large-scale annotated datasets often leads to an unavoidable foe: label noise, the insidious presence of incorrect class labels. This challenge looms large, casting shadows over the development of robust deep learning models that must navigate the treacherous waters of training set inaccuracies while striving for high performance on unseen data.   In response to this formidable challenge, we unveil a groundbreaking approach that seeks to cleanse the labels, forging a path toward the creation of a model of unparalleled quality. Our method draws upon the powerful foundations of statistical principles to rectify erroneous data labels, bolstered by a theoretical guarantee of correctness. At the heart of our strategy lies the likelihood ratio test (LRT), a tool we wield to judiciously flip the labels of our training data.  Through rigorous proof, we demonstrate that our LRT label correction algorithm possesses the remarkable ability to adjust labels, aligning them with the true Bayesian optimal decision rule with a high degree of probability. By seamlessly integrating our label correction technique into the training regimen of deep neural networks, we empower these models to achieve exceptional performance across a spectrum of public datasets. In doing so, we not only confront the challenge of label noise but emerge victorious, illuminating the path for future endeavors in the realm of machine learning.", "introduction": "Label noise is ubiquitous in real world data.It may be caused by unintentional mistakes of manual or automatic annotators (Yan et al., 2014;Veit et al., 2017).It may also be introduced by malicious attackers (Steinhardt et al., 2017).Noisy labels impair the performance of a model (Smyth et al., 1994;Brodley & Friedl, 1999), especially a deep neural network, which tends to have strong memorization power (Frnay & Verleysen, 2014;Zhang et al., 2017).Improving the robustness of a model to label noise is a crucial yet challenging task in many applications (Mnih & Hinton, 2012;Wu et al., 2018).Existing methods mainly follow two directions, probabilistic reasoning and data selecting.Probabilistic methods explicitly model a noise transition matrix, namely, the probability of one label being corrupted into another (Goldberger & Ben-Reuven, 2017;Patrini et al., 2017).The transition matrix is often estimated from the data, and is used to re-calibrate the training loss or to correct the prediction.Explicit estimation of the transition matrix can be problematic due to the large variation of noise patterns, e.g., uniform noise, asymmetric noise, or mixtures.Furthermore, the transition matrix size is quadratic to the number of classes, making the estimation task prohibitive when the data has hundreds or even thousands of classes.Data-selecting methods are agnostic of the underlying noise pattern.These methods gradually collect clean data whose labels are trustworthy (Malach & Shalev-Shwartz, 2017;Jiang et al., 2018;Han et al., 2018).As more clean data are collected, the quality of the trained models improves.The major issue of these methods is the lack of a quantitative control of the quality of the collected clean data.Without a principled guideline, it is hard to find the correct data collection pace.An aggressive selection can unknowingly accumulate irreversible errors.On the other hand, an overly-conservative strategy can be very slow in training, or stops with insufficient clean data and mediocre models.We propose a novel method with the benefit from both the probabilistic and the data-selecting approaches.Similar to data-selecting methods, our method continuously improves the purity of the data labels by correcting the noise-corrupted ones.Meanwhile, we improve the classifier using the updated labels.Our label correction algorithm is based on statistical principles and is theoretically guaranteed to deliver a high quality label set.Instead of explicitly estimating the transition matrix, the correction algorithm only depends on the prediction of the current model, denoted as f .Using an f -based likelihood ratio test, we determine whether the current label of each data should be corrected.Our main theorem proves that the label correction algorithm will clean a majority of noisy labels with high probability.In practice, we incorporate the label correction algorithm into the training of deep neural networks.Our method iteratively updates the labels of the data while continuously training a deep neural network.To ensure the deep neural network does not overfit with noise labels that are yet to be corrected, we introduce a new retroactive loss term that regulates the model by enforcing its consistency with models in previous epochs.The rationale is that the model in an earlier training stage tends to fit the true signal rather than noise, although its overall performance is sub-optimal.Through experiments on various datasets with various noise patterns and levels, we show that our method produces robust neural network models with superior performance.To the best of our knowledge, our method is the first to correct labels with theoretical guarantees.It is has advantages over both probabilistic methods and data-selecting methods.Compared with other data-selecting methods, it has a better quantitative control of the label quality and thus is less brittle when generalizing to different datasets and different noise patterns.Also note that we are not selecting clean data.Instead, we correct labels and always use the whole training set to train.This brings an additional advantage of fully leveraging the data.Compared with other probabilistic methods, our correction algorithm assumes a rather general family of underlying noise patterns and avoids an explicit estimation of the transition matrix."}
{"paper_id": 80, "abstract": "In the ever-evolving realm of deep artificial neural networks, we have witnessed remarkable feats\u2014models that possess an astounding number of parameters, far exceeding the available training examples. To navigate the treacherous waters of overfitting, these models often rely on regularization techniques. These strategies can be categorized as either implicit, like the stochastic gradient descent and the parameter sharing found in convolutional layers, or explicit. The latter, which includes methods such as weight decay and dropout, has proven its worth in enhancing generalization. Yet, they come with a price: a blind reduction in the model's effective capacity, the introduction of sensitive hyper-parameters, and a need for deeper, wider architectures to compensate for this loss.  Enter data augmentation\u2014a technique that leverages domain knowledge to expand the training dataset without diminishing the model's capacity or introducing additional parameters tied to the model itself. In this paper, we embark on a systematic exploration, pitting data augmentation against explicit regularization across three popular neural architectures and three distinct datasets. Our findings reveal a striking truth: data augmentation, standing alone, can match or even surpass the performance of regularized models while exhibiting remarkable adaptability to architectural changes and variations in the volume of training data. In this clash of methodologies, it becomes clear that the path forward may lie in the augmentation of our data rather than the constriction of our models.", "introduction": "One of the central issues in machine learning research and application is finding ways of improving generalization.Regularization, loosely defined as any modification applied to a learning algorithm that helps prevent overfitting, plays therefore a key role in machine learning (Girosi et al., 1995;M\u00fcller, 2012).In the case of deep learning, where neural networks tend to have several orders of magnitude more parameters than training examples, statistical learning theory (Vapnik & Chervonenkis, 1971) indicates that regularization becomes even more crucial.Accordingly, a myriad of techniques have been proposed as regularizers: weight decay (Hanson & Pratt, 1989) and other L p penalties; dropout (Srivastava et al., 2014) and stochastic depth (Huang et al., 2016), to name a few examples.Moreover, whereas in simpler machine learning algorithms the regularizers can be easily identified as explicit terms in the objective function, in modern deep neural networks the sources of regularization are not only explicit, but implicit (Neyshabur et al., 2014).In this regard, many techniques have been studied for their regularization effect, despite not being explicitly intended as such.That is the case of unsupervised pre-training (Erhan et al., 2010), multi-task learning (Caruana, 1998), convolutional layers (LeCun et al., 1990), batch normalization (Ioffe & Szegedy, 2015) or adversarial training (Szegedy et al., 2013).In sum, there are multiple elements in deep learning that contribute to reduce overfitting and thus improve generalization.Driven by the success of such techniques and the efficient use of GPUs, considerable research effort has been devoted to finding ways of training deeper and wider networks with larger capacity (Simonyan & Zisserman, 2014;He et al., 2016;Zagoruyko & Komodakis, 2016).Ironically, the increased representational capacity is eventually reduced in practice by the use of explicit regularization, most commonly weight decay and dropout.It is known, for instance, that the gain in generalization provided by dropout comes at the cost of using larger models and training for longer (Goodfellow et al., 2016).Hence, it seems that with these standard regularization methods deep networks are wasting capacity (Dauphin & Bengio, 2013).Unlike explicit regularization, data augmentation improves generalization without reducing the capacity of the model.Data augmentation, that is synthetically expanding a data set by apply-ing transformations on the available examples, has been long used in machine learning (Simard et al., 1992) and identified as a critical component of many recent successful models, like AlexNet (Krizhevsky et al., 2012), All-CNN (Springenberg et al., 2014) or ResNet (He et al., 2016), among others.Although it is most popular in computer vision, data augmentation has also proven effective in speech recognition (Jaitly & Hinton, 2013), music source separation (Uhlich et al., 2017) or text categorization (Lu et al., 2006).Today, data augmentation is an almost ubiquitous technique in deep learning, which can also be regarded as an implicit regularizer for it improves generalization.Recently, the deep learning community has become more aware of the importance of data augmentation (Hern\u00e1ndez-Garc\u00eda & K\u00f6nig, 2018b) and new techniques, such as cutout (DeVries & Taylor, 2017a) or augmentation in the feature space (DeVries & Taylor, 2017b), have been proposed.Very interestingly, a promising avenue for future research has been set by recently proposed models that automatically learn the data transformations (Hauberg et al., 2016;Lemley et al., 2017;Ratner et al., 2017;Antoniou et al., 2017).Nonetheless, another study by Perez & Wang (2017) analyzed the performance of different techniques for object recognition and concluded that one of the most successful techniques so far is still the traditional data augmentation carried out in most studies.However, despite its popularity, the literature lacks, to our knowledge, a systematic analysis of the impact of data augmentation on convolutional neural networks compared to explicit regularization.It is a common practice to train the models with both explicit regularization, typically weight decay and dropout, and data augmentation, assuming they all complement each other.Zhang et al. (2017) included data augmentation in their analysis of generalization of deep networks, but it was questionably considered an explicit regularizer similar to weight decay and dropout.To our knowledge, the first time data augmentation and explicit regularization were systematically contrasted was the preliminary study by Hern\u00e1ndez-Garc\u00eda & K\u00f6nig (2018b).The present work aims at largely extending that work both with more empirical results and a theoretical discussion.Our specific contributions are the following:\u2022 Propose definitions of explicit and implicit regularization that aim at solving the ambiguity in the literature (Section 2).\u2022 A theoretical discussion based on statistical learning theory about the differences between explicit regularization and data augmentation, highlighting the advantages of the latter (Section 3).\u2022 An empirical analysis of the performance of models trained with and without explicit regularization, and different levels of data augmentation on several benchmarks (Sections 4 and 5).Further, we study their adaptability to learning from fewer examples (Section 5.2) and to changes in the architecture (Section 5.3).\u2022 A discussion on why encouraging data augmentation instead of explicit regularization can benefit both theory and practice in deep learning (Section 6).2 EXPLICIT AND IMPLICIT REGULARIZATION Zhang et al. (2017) raised the thought-provoking idea that \"explicit regularization may improve generalization performance, but is neither necessary nor by itself sufficient for controlling generalization error.\"The authors came to this conclusion from the observation that turning off the explicit regularizers of a model does not prevent the model from generalizing reasonably well.This contrasts with traditional machine learning involving convex optimization, where regularization is necessary to avoid overfitting and generalize (Vapnik & Chervonenkis, 1971).Such observation led the authors to suggest the need for \"rethinking generalization\" in order to understand deep learning.We argue it is not necessary to rethink generalization if we instead rethink regularization and, in particular, data augmentation.Despite their thorough analysis and relevant conclusions, Zhang et al. (2017) arguably underestimated the role of implicit regularization and considered data augmentation an explicit form of regularization much like weight decay and dropout.This illustrates that the terms explicit and implicit regularization have been used subjectively and inconsistently in the literature before.In order to avoid the ambiguity and facilitate the discussion, we propose the following definitions of explicit and implicit regularizationfoot_0 :\u2022 Explicit regularization techniques are those which reduce the representational capacity of the model they are applied on.That is, given a model class H 0 , for instance a neural network architecture, the introduction of explicit regularization will span a new hypothesis set H 1 , which is a proper subset of the original set, i.e.H 1 H 0 .\u2022 Implicit regularization is the reduction of the generalization error or overfitting provided by means other than explicit regularization techniques.Elements that provide implicit regularization do not reduce the representational capacity, but may affect the effective capacity of the model, that is the achievable set of hypotheses given the model, the optimization algorithm, hyperparameters, etc.One of the most common explicit regularization techniques in machine learning is L p -norm regularization, of which weight decay is a particular case, widely used in deep learning.Weight decay sets a penalty on the L 2 norm of the learnable parameters, thus constraining the representational capacity of the model.Dropout is another common example of explicit regularization, where the hypothesis set is reduced by stochastically deactivating a number of neurons during training.Similar to dropout, stochastic depth, which drops whole layers instead of neurons, is also an explicit regularization technique.There are multiple elements in deep neural networks that implicitly regularize the models.Note, in this regard, that the above definition, contrary to explicit regularization, does not refer to techniques, but to a regularization effect, as it can be provided by elements of very different nature.For instance, stochastic gradient descent (SGD) is known to have an implicit regularization effect without constraining the representational capacity.Batch normalization does not either reduce the capacity, but it improves generalization by smoothing the optimization landscape Santurkar et al. (2018).Of quite a different nature, but still implicit, is the regularization effect provided by early stopping, which does not reduce the representational, but the effective capacity.By analyzing the literature, we identified some previous pieces of work which, lacking a definition of explicit and implicit regularization, made a distinction apparently based on the mere intention of the practitioner.Under such notion, data augmentation has been considered in some cases an explicit regularization technique, as in Zhang et al. (2017).Here, we have provided definitions for explicit and implicit regularization based on their effect on the representational capacity and argue that data augmentation is not explicit, but implicit regularization, since it does not affect the representational capacity of the model."}
{"paper_id": 81, "abstract": "In the realm of artificial intelligence, where the quest for mastery over imitation learning often leads to perilous pitfalls, we unveil a groundbreaking approach: Support-guided Adversarial Imitation Learning, or SAIL. This innovative framework weaves together the intricate threads of expert policy support estimation with the robust tapestry of Adversarial Imitation Learning (AIL) algorithms. SAIL boldly confronts two formidable challenges that have long plagued AIL\u2014the insidious reward bias and the lurking specter of training instability. Through rigorous exploration, we reveal that SAIL stands not only as an equal to traditional AIL in efficiency but often surpasses it. Our extensive evaluations illuminate SAIL's prowess in mitigating reward bias, showcasing a remarkable enhancement in both performance and stability across a diverse array of benchmark control tasks. In this journey of discovery, SAIL emerges as a beacon of hope for those navigating the turbulent waters of imitation learning.", "introduction": "The class of Adversarial Imitation Learning (AIL) algorithms learns robust policies that imitate an expert's actions from a small number of expert trajectories, without further access to the expert or environment signals.AIL iterates between refining a reward via adversarial training, and reinforcement learning (RL) with the learned adversarial reward.For instance, Generative Adversarial Imitation Learning (GAIL) (Ho & Ermon, 2016) shows the equivalence between some settings of inverse reinforcement learning and Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), and recasts imitation learning as distribution matching between the expert and the RL agent.Similarly, Adversarial Inverse Reinforcement Learning (AIRL) (Fu et al., 2017) modifies the GAIL discriminator to learn a reward function robust to changes in dynamics or environment properties.AIL mitigates the issue of distributional drift from behavioral cloning (Ross et al., 2011), a classical imitation learning algorithm, and demonstrates good performance with only a small number of expert demonstrations.However, AIL has several important challenges, including implicit reward bias (Kostrikov et al., 2019), potential training instability (Salimans et al., 2016;Brock et al., 2018), and potential sample inefficiency with respect to environment interaction (Sasaki et al., 2019).In this paper, we propose a principled approach towards addressing these issues.Wang et al. (2019) demonstrated that imitation learning is also feasible by constructing a fixed reward function via estimating the support of the expert policy.Since support estimation only requires expert demonstrations, the method sidesteps the training instability associated with adversarial training.However, we show in Section 4.2 that the reward learned via support estimation deteriorates when expert data is sparse, and leads to poor policy performances.Support estimation and adversarial reward represent two different yet complementary RL signals for imitation learning, both learnable from expert demonstrations.We unify both signals into Supportguided Adversarial Imitation Learning (SAIL), a generic imitation learning framework.SAIL leverages the adversarial reward to guide policy exploration and constrains the policy search to the estimated support of the expert policy.It is compatible with existing AIL algorithms, such as GAIL and AIRL.We also show that SAIL is at least as efficient as standard AIL.In an extensive evaluation, we demonstrate that SAIL mitigates the implicit reward bias and achieves better performance and training stability against baseline methods over a series of benchmark control tasks."}
{"paper_id": 82, "abstract": "In the realm of machine learning, stochastic neural networks, characterized by their use of discrete random variables, stand as a powerful testament to the balance of expressivity and interpretability. However, the path to harnessing their full potential is fraught with challenges; traditional methods of differentiation and backpropagation falter in their presence. Instead, researchers have turned to Monte Carlo gradient estimation techniques, seeking to navigate the turbulent waters of training these intricate models.   Yet, the prevalent stochastic gradient estimators, such as the Straight-Through and Gumbel-Softmax methods, reveal their limitations when faced with the complexity of deeper architectures\u2014often faltering under the weight of multiple stochastic layers.   In this endeavor, we delve into the depths of stochastic networks laden with layers of Boolean latent variables. Our exploration employs the elegant framework of harmonic analysis, specifically tailored for Boolean functions, allowing us to unearth the roots of bias lurking within the Straight-Through estimator. From this analytical foundation, we introduce \\emph{FouST}, a novel gradient estimation algorithm that employs three straightforward bias reduction techniques.   Our extensive experiments reveal that FouST not only outperforms the current state-of-the-art biased estimators but also does so with remarkable speed, eclipsing the sluggishness of unbiased alternatives. To our knowledge, FouST marks a significant milestone as the first gradient estimator capable of efficiently training exceptionally deep stochastic neural networks, boasting up to 80 deterministic layers alongside 11 stochastic ones. In this unfolding narrative of discovery and innovation, we stand on the precipice of a new era in the training of complex stochastic models.", "introduction": "Stochastic neural networks with discrete latent variables have been an alluring class of models for their expressivity and interpretability, dating back to foundational work on Helmholtz machines (Dayan et al., 1995) and sigmoid belief nets (Neal, 1992).Since they are not directly differentiable, discrete random variables do not mesh well with the workhorse of modern Deep Learning, that is the backpropagation algorithm.Monte Carlo gradient estimation is an effective solution where, instead of computing the true gradients, one can sample gradients from some distribution.The sample estimates can be either biased or unbiased.Unbiased gradient estimates like score function estimators (Williams, 1992) come typically at the cost of high variance leading to slow learning.In contrast, biased gradient estimates such Straight-Through (Bengio et al., 2013), while efficient, run the risk of convergence to poor minima and unstable training.To this end several solutions have recently been proposed that either reduce variance in unbiased estimators (Mnih & Gregor, 2014;Gu et al., 2015;Tucker et al., 2017;Rezende et al., 2014;Grathwohl et al., 2017) or control bias in biased estimators (Jang et al., 2016;Maddison et al., 2016).These methods, however, have difficulty scaling up to complex neural networks with multiple stochastic layers: low-variance unbiased estimators are too expensivefoot_0 , while the compounded bias from the continuous relaxations on multiple stochastic layers leads to poor minima.In this work we focus on biased estimators.Our goal in this paper is a gradient estimator for Boolean random variables that works for any complex -deep or wide-neural network architecture.We resort to the term Boolean instead of binary to emphasize that we work directly on the Boolean space {-1, +1}, without any continuous relaxations or quantizations.With this in mind we re-purpose the framework of harmonic analysis of Boolean functions, widely used in computational learning and computational complexity theory (O'Donnell, 2014;Linial et al., 1993;Mossel et al., 2003;Mansour, 1994).We cast stochastic neural networks as Boolean functions f (z) over Boolean latent variables z sampled from probability Algorithm 1 FouST Gradient Estimator Require: Parameters \u03b8 \u2208 R K , Bernoulli Representation {-a, a}, Interval Parameter b \u2208 [0, a],Constant scaling parameter \u03b3 1: Sample x i \u223c p \u03b8i (x i ), i = 1, ..., K This and all steps below have constant complexity.2: Sample y i \u223c Unif(b, a) 3: Set y i := x i * y i 4: Compute ST gradient \u2202 \u03b8i := \u2202 yi f (y) 5: Importance reweighing \u2202 \u03b8i := 0.5/p(x i ) \u2022 \u2202 \u03b8i 6: return partial gradient \u03b3\u2202 \u03b8i , i = 1, ..., K distributions p(z).We then use harmonic analysis to determine that the bias in the Straight-Through gradient estimates corresponds to the weighted sum of higher-order Taylor coefficients of f (z).The direct consequence is that one can control the bias in the Straight-Through estimator by manipulating the higher-order Taylor coefficients of f (z).Building upon the harmonic analysis of existing gradient estimators, we present an algorithm, FouST, that admits low bias gradient estimates for Boolean latent variable models.In experiments, we were able to scale up the stochastic depth and width of neural networks, training deep stochastic residual networks with up to 80 deterministic and 11 stochastic layers with little difficulty.We summarize FouST in Algorithm 1.With this work we make the following three contributions.1. We introduce the framework of harmonic analysis of Boolean functions to analyze discrete stochastic neural networks and their REINFORCE and Straight-Through gradients.We show that stochastic gradients compute Fourier coefficients.2. Based on the above harmonic analysis we present FouST -a low-bias gradient estimator for Boolean latent variables based on three bias reduction steps.As a side contribution, we show that the gradient estimator employed with DARN (Gregor et al., 2013), originally proposed for autoregressive models, is a strong baseline for gradient estimation in large and complex models with many stochastic layers.3. We show that FouST is amenable to complex stochastic neural networks with Boolean random variables.To the best of our knowledge, FouST is the first gradient estimate algorithm that can train very deep stochastic neural networks with Boolean latent variables.The practical outcome is a simple gradient estimate algorithm that can be plugged in complex stochastic neural networks with multiple layers of Boolean random variables."}
{"paper_id": 83, "abstract": "In the ever-evolving realm of artificial intelligence, where the boundaries between human perception and machine understanding blur, we embark on a quest to unravel the mysteries of deep convolutional neural networks (CNNs). These intricate architectures, pre-trained on vast datasets, have become the cornerstone of image quality assessment, super-resolution, and a myriad of image-to-image translation challenges. Yet, beneath their polished surfaces lies a deeper connection waiting to be explored.  In this groundbreaking exploration, we forge a bridge between the fundamental principles of human visual perception and the enigmatic characteristics of learned deep CNN representations. By meticulously examining the frequency and orientation tuning of channels within renowned object detection networks\u2014such as VGG-16\u2014we employ a series of grating stimuli, each varying in spatial frequency and orientation, to peel back the layers of abstraction.   Our findings reveal a fascinating parallel: the behavior of CNN channels, acting as spatial frequency and orientation selective filters, mirrors the very essence of human visual processing. This revelation allows us to construct a novel theoretical framework, shedding light on how these deep representations can serve as perceptual quality features. We conclude that the sensitivity of CNN channels to spatial frequencies with lower contrast masking thresholds\u2014coupled with a pronounced orientation selectivity\u2014emerges as a crucial determinant of perceptual quality. In doing so, we not only enhance our understanding of deep CNNs but also illuminate the path toward more sophisticated and human-like image processing algorithms.", "introduction": "Quantifying human perception of image quality has been a subject of significant research for quite some time.Full-reference objective metrics such as the PSNR (Peak Signal to Noise Ratio) and SSIM (Structural Similarity Index) (Wang et al. (2004)), being fair metrics of distortion between two images, are not a satisfactory metrics to measure differences in perceptual quality.Considering the recent interest in the applications of deep CNNs in perception-oriented problems such as superresolution, image-restoration, frame-interpolation and style-transfer etc, research into effective loss metrics that quantify perceptual quality and help train CNNs in delivering better perceptual quality has become paramount.The perceptual loss proposed by Johnson et al. (2016) was one of the first to demonstrate how effective the feature representations of pre-trained image classification CNNs could be as features of full-reference perceptual quality, especially when incorporated into loss functions for image restoration.The perceptual loss is now popularly adopted in many image restoration problems such as super-resolution, style transfer, denoising etc. (Ledig et al. (2017), Wang et al. (2018), Gatys et al. (2016)).Zhang et al. (2018) and Blau & Michaeli (2018) further demonstrate how effective deep CNN representations can be as features of perceptual quality, but without any analysis into their characteristics.More recently, Mechrez et al. (2018) proposed a variation of the perceptual loss called the contextual loss, which still employs deep CNN features as perceptual quality features but uses an approximation of the KL-divergence to quantify distance.The contextual loss has been demonstrated to be quite effective in maintaining natural image statistics during SISR (Single-Image Super-Resolution).The recent PIRM Super-Resolution Challenge Report (Blau et al. (2018)) clearly iterates that the perceptual loss and the contextual loss are the most widely used loss functions for CNN based perceptual image Super-Resolution.Nevertheless, like most applications of deep learning, there has been little or no effort to understand and interpret the role of deep CNN representations as effective perceptual quality features.This is quite understandable, as it is difficult to find a direction to approach this problem from.Neural networks are non-linear, which makes a tractable analysis tricky.Furthermore, human perception of quality is also something that is still not understood completely.Most of our basic understanding of human visual perception of quality is in the frequency domain, with models such as the Contrast Sensitivity Function (CSF) (de Faria et al. (1998)).To make a connection between deep CNN features and human perception, it is important to realize that deep CNN channels are essentially complex spatial frequency and orientation selective filters.We stimulate pre-trained image classification CNNs with sinusoidal grating stimuli, record the response in the form of mean activation of each channel as function of spatial frequency/orientation of input grating, thus quantifying the frequency and orientation selectivity of different channels.This approach makes it significantly easier to establish a connection between perception models such as the CSF with learned deep feature representations.We hypothesize that two attributes are important for deep CNN channels that are good perceptual quality features.The attributes are based on visual masking in human visual perception, which refers to human ability to perceive distortions and contrast in visual stimulus.The first attribute is sensitivity to spatial frequencies at which there is minimal contrast masking in human visual perception (Nadenau et al. (2000)), making the CNN channel sensitive to highly perceivable distortions.The second attribute being a definite and strong orientation selectivity, which helps the channel respond better to image regions with less pattern complexity, where there is less masking for distortions from a perceptual standpoint (Wu et al. (2017)).We verify our hypothesis by designing an Objective Quality Assessment (OQA) experiment (Sheikh et al. (2006)).OQA experiments correlate the performance of any quality metric with human perception of quality, which is an accepted and standard experimental technique.We group the set of channels in different CNN layers into subsets on the basis of our hypothesis and demonstrate that the group which has channels with our described attributes, delivers a much better as a set of perceptual quality features.We repeat our experiment across multiple layers of many pre-trained image classification networks such as the VGG-16 (Simonyan & Zisserman (2014)), AlexNet (Krizhevsky & Hinton (2012)), ShuffleNet (Zhang et al. (2017)) and SqueezeNet (Iandola et al. (2017))."}
{"paper_id": 84, "abstract": "In the ever-evolving realm of reinforcement learning (RL), a vibrant tapestry of research has emerged, weaving together intricate algorithms that empower agents to tackle tasks with remarkable efficiency. Yet, amid this flourishing landscape, a crucial thread often goes unnoticed: the treasure trove of prior experiences that could illuminate the path to mastery. For many real-world challenges, the notion of an agent starting from a blank slate is not only daunting but computationally prohibitive. Instead, we posit that harnessing the wisdom of previous encounters can transform these formidable tasks into manageable endeavors.  In this paper, we unveil a groundbreaking framework designed to exploit existing experiences through the cultivation of reusable options. Our findings reveal that once an agent has navigated a handful of problems, the trajectories forged from these journeys can be harnessed to craft reusable options\u2014powerful tools that enable the agent to swiftly conquer novel and related challenges. With this approach, we unlock the potential for accelerated learning, paving the way for a future where agents rise not just as solitary learners but as wise practitioners, drawing upon the echoes of their past to illuminate the unknown.", "introduction": "Reinforcement learning (RL) techniques have experienced much of their success in simulated environments, such as video games (Mnih et al., 2015) or board games (Silver et al., 2016;Tesauro, 1995).One of the main reasons why RL has worked so well in these applications is that we are able simulate millions of interactions with the environment in a relatively short period of time.In many real world applications, however, where the agent interacts with the physical world, it might not be easy to generate such a large number of interactions.The time and cost associated with training such systems could render RL an unfeasible approach for training in large scale.As a concrete example, consider training a large number of humanoid robots (agents) to move quickly, as in the Robocup competition (Farchy et al., 2013).Although the agents have similar dynamics, subtle variations mean that a single policy shared across all agents would not be an effective solution.Furthermore, learning a policy from scratch for each agent is too data-inefficient to be practical.As shown by Farchy et al. (2013), this type of problem can be addressed by leveraging the experience obtained from solving a related task (e.g., walking) to quickly learn a policy for each individual agent that is tailored to a new task (e.g., running).These situations also occurs in industry, such as robots tasked with sorting items in fulfillment centers.A simple approach, like using PD controllers, would fail to adapt to the forces generated from picking up objects with different weight distributions, causing the arm to drop the objects.RL is able to mitigate this problem by learning a policy for each arm that is able to make corrections quickly, which is tailored to the robot's dynamics.However, training a new policy for each agent would be far too costly to be a practical solution.In these scenarios, it is possible to use a small number of policies learned a subset of the agents, and then leverage the experience obtained from learning those policies to allow the remaining agents to quickly learn their corresponding policies.This approach can turn problems that are prohibitively expensive to solve into relatively simple problems.To make use of prior experience and improve learning on new related problems in RL, several lines of work, which are complementary to each other, have been proposed and are actively being studied.Transfer learning (Taylor & Stone, 2009) refers to the problem of adapting information acquired while solving one task to another.One might consider learning a mapping function that allows for a policy learned in one task to be used in a different task (Ammar et al., 2015) or simply learn a mapping of the value function learned in one task to another (Taylor et al., 2007).These techniques can be quite effective, but are also limited in that they consider mapping information from one source task to another target task.Another approach to reusing prior knowledge is through meta learning or learning to learn (Schmidhuber, 1995;Schmidhuber et al., 1998).In the context of RL, the goal under this framework for an agent to be exposed to a number of tasks where it can learn some general behavior that generalizes to new tasks (Finn et al., 2017).One last technique to leverage prior experience, and the one this paper focuses on, is through temporally extended actions or temporal abstractions (McGovern & Sutton, 1998;Sutton et al., 1999).While in the standard RL framework the agent has access to a set of primitive actions (i.e., actions that last for one time step), temporally extended actions allow an agent to execute actions that last for several time-steps.They introduce a bias in the behavior of the agent which, if appropriate for the problem at hand, results in dramatic improvements in how quickly the agent learns to solve a new task.A popular representation for temporally extended actions is the options framework (Sutton & Precup, 1998;Sutton et al., 1999) (formally introduced in the next section), which is the focus of this work.It has been shown that options learned in a specific task or set of tasks, can be reused to improve learning on new tasks (Machado et al., 2017;Bacon et al., 2017); however, this often requires knowledge from the user about which options or how many options are appropriate for the type of problems the agent will face.In this paper, we propose learning reusable options for a set of related tasks with minimal information provided by the user.Throughout this paper, we refer as (near)-optimal policies to those policies that were learned to solve a particular task, but are not strictly speaking optimal.We consider the scenario where the agent must solve a large numbers of tasks and show that after learning a (near)-optimal policy for a small number of problems, we can learn an appropriate number of options that facilitates learning in a remaining set of tasks.To do so, we propose learning a set of options that minimize the expected number of decisions needed to represent trajectories generated from the (near)-optimal policies learned by the agent, while also maximizing the probability of generating those trajectories.Unlike techniques that learn options to rach bottleneck states (McGovern & Barto, 2001) or states deemed of high value (Machado et al., 2017), our method seeks to learn options that are able to generate trajectories known to perform well.This does not necessarily lead to learn options that reach states one might consider \"interesting\"."}
{"paper_id": 85, "abstract": "In the realm of artificial intelligence, knowledge serves as the lifeblood that fuels innovation and discovery. Enter TechKG, a monumental creation in the landscape of knowledge graphs\u2014an intricate tapestry woven from the threads of technology and research. While many knowledge graphs have emerged, predominantly in Western contexts and for general purposes, TechKG stands apart as a beacon for the Chinese academic landscape.  Crafted with precision, TechKG draws its strength from an extensive corpus of technical papers published across a multitude of Chinese academic journals. Through the application of meticulously designed heuristic rules, we have extracted a treasure trove of high-quality entities and relationships, culminating in a staggering collection of over 260 million triplets. These triplets are anchored in more than 52 million distinct entities, spanning 38 diverse research domains.  Our initial explorations reveal that TechKG is not merely a repository of information; it possesses remarkable adaptability, poised to serve as a foundational dataset for a wide array of AI-driven applications. In this way, TechKG not only enriches the field of knowledge graphs but also paves the way for future advancements in technology and research.", "introduction": "Generally, a knowledge graph (KG) stores factual knowledge in the form of structural triplet like <h, r, t>, in which h and t are entities (called head entity and tail entity respectively) and r denotes the relation type from h to t. KG is a kind of valuable knowledge base and is important for many different kinds of AI-related applications.Nowadays, great achievements have been made in building large scale KGs.And there are many available KGs like FreeBase Bollacker et al. (2008), WordNet Miller (1995), YAGO Suchanek et al. (2007), etc. Usually there are millions of entities and billions of relational facts in these KGs.With them, great progress have been made in lots of AI-related applications, such as knowledge graph embedding (KGE), distantly supervised relation extraction (RE), knowledge based question and answering (KBQA), etc.However, there are still following two challenges inherited in these KG-dependent applications.First, almost all of these KGs can only be used in English-related applications.But the fact is that each language has its own characteristics.It is uncertain whether a model that performs well in an English-related KG-dependent task could still perform well in the corresponding task of other language.Thus there is a pressing need to build KGs of different languages other than English.Second, most of existing KGs are designed for general purpose.It is also uncertain whether a model that performs well in an application of general domain could still perform well in applications of specific domains, such as in the technical domains liking medicine, biology, electronics, etc.It is necessary to evaluate whether there are some new kinds of characteristics hidden in KGs that haven't been noticed by researchers up to now.To address these issues, this work introduces TechKG, a large scale Chinese KG built from massive technical papers that are published in Chinese academic journals of different research domains.Compared with existing KGs, TechKG has following important characteristics.First, TechKG is a large scale Chinese KG that is technology-oriented.Totally, it comprises of over 280 million triplets that are built upon more than 60 million entities which come from more than 30 research domains.To the best of our knowledge, this is the first Chinese KG that is built by using multi-domain technical papers.Second, TechKG is a high quality KG.Both of its entities and relations are acquired in two ways.One is directly taking some components of a paper as entities and building relations between entities by simply re-organizing a paper's components.The other is taking carefully selected domain terminologies as entities and building relations between them by some carefully designed heuristic rules.These two ways guarantee TechKG have high accuracies at both entity and relation levels.Third, TechKG can provide more kinds of information associated with the original papers besides common triplets.This makes it can be easily taken as a benchmark dataset for many diverse AIrelated applications, such as KGE, distantly supervised RE, NER, KBQA, text classification(TC), machine translation(MT), among others.Fourth, TechKG can be continually updated as long as there are new available published papers.As the needed data source is readily available in almost all languages, researchers can easily build similar KGs for other languages.In the rest of this paper, we first introduce the building process of TechKG.Then we analyze the characteristics of TechKG in detail.We also conduct some primary experiments using TechKG as a dataset to three diverse tasks (see the Appendix part for detail).Both our analyses and experimental results show that lots of characteractics distinguish TechKG from existing KGs and it raises many new challenges for lots of existing applications."}
{"paper_id": 86, "abstract": "In the ever-evolving realm of deep learning, the quest for optimal neural architectures has led many to embrace differentiable neural architecture search (DNAS) as a powerful tool. However, this promising methodology often grapples with a fundamental flaw\u2014its instability, which hinders its application across expansive search spaces and its adaptability to diverse scenarios. In this paper, we delve into DARTS, the reigning champion among differentiable search algorithms, and uncover a critical source of its instability: the imprecise approximation of gradients for architectural parameters. This inadequacy can lead the optimization process astray, converging on points that yield significant inaccuracies during the re-training phase.  Armed with this insight, we introduce a novel corrective term for calculating architectural gradients, harnessing a direct property of the optimality inherent in network parameter optimization. Our approach mathematically ensures that gradient estimation is directed along a more reliable path, guiding the search process toward more promising architectures. The beauty of our solution lies not only in its theoretical foundation but also in its practical implementation; it integrates seamlessly into existing DARTS frameworks with minimal effort.  Through rigorous experimentation on CIFAR and ImageNet datasets, we demonstrate that our method not only enhances accuracy but also empowers DARTS-based techniques to venture into vast, uncharted search spaces, previously deemed too daunting to explore. In this way, we pave the way for a new era of discovery in neural architecture search, where the limits of exploration are pushed further than ever before.", "introduction": "Neural architecture search (NAS) has been an important topic in the research area of automated machine learning (AutoML).The idea is to replace the manual way of designing neural network architectures with an automatic algorithm, by which deep learning methods become more flexible in fitting complex data distributions, e.g., large-scale image datasets.Early efforts of NAS involved using heuristic search methods such as reinforcement learning (Zoph & Le, 2017;Zoph et al., 2018) and evolutionary algorithms (Real et al., 2017;Xie & Yuille, 2017) to sample networks from a large search space, and optimizing each sampled network individually to evaluate its quality.Despite notable successes obtained by this methodology, it often requires a vast amount of computation, which obstacles its applications in the scenarios of limited resources.Inspired by the idea of reusing and sharing parameters among trained networks, DARTS (Liu et al., 2019b) was designed as a 'one-shot' solution of NAS.The major difference from the aforementioned methods is a differentiable formulation of architecture search and an end-to-end mechanism which optimizes model weights (such as convolution) and architectural weights simultaneously.Recently, improvements upon DARTS were made in various aspects (Chen et al., 2019;Xu et al., 2019;Liang et al., 2019), making it a reasonable tradeoff between search cost and performance.Despite its broad applications, the current pipeline of DARTS (or, generally speaking, differentiable NAS approaches) suffers a critical weakness known as instability.Researchers reported (Liang et al., 2019) that DARTS-based algorithms can sometimes generate weird architectures that produce considerably worse accuracy than those generated in other individual runs, or even significantly worse than randomly generated architectures.There indeed exist tricks designed by human expertise (Chen et al., 2019;Nayman et al., 2019;Liang et al., 2019) to alleviate this issue, but we point out that these approaches violated the ideology of NAS, which is to maximally prevent human interventions.Moreover, even with such add-ons, a dramatic property of DARTS persists and has not been studied carefully in prior work.When DARTS, as well as its variants, gets trained for a longer time, e.g., from the default number of 50 epochs to 200 epochs, we surprisingly observe that all these approaches converge to very similar architectures, in which almost all edges are occupied by skip-connect (a.k.a., identity).These architectures, with fewer trainable parameters, are often far from producing high accuracy in particular on large datasets like ImageNet, but they somehow produce sufficiently high validation accuracy in the search stage.In other words, convergence in search often leads to bad performance in re-training.This is why some previous DARTS-based approaches advocated for early termination (Liang et al., 2019), a practical but non-essential solution.Also, we conjecture that early termination also contributes to the lack of stability and, more importantly, trustfulness, of DARTS-based approaches.This paper delves deep into the inconsistency between convergence and performance.We show that the devil lies in optimizing the loss function of the super-network, L val (\u03c9 (\u03b1) , \u03b1) (\u03c9 and \u03b1 denote network and architectural parameters, respectively, and \u03c9 (\u03b1) is the global optimum of \u03c9 given \u03b1), in which \u03c9 and \u03b1 get updated alternately.Following the chain rule,, in which the first term is easy to compute while the second term is not, mainly because \u03c9 (\u03b1) is difficult to estimate, and so is the term of \u2207 \u03b1 \u03c9 (\u03b1)| \u03b1=\u03b1t .DARTS-based approaches performed inaccurate approximation for this purpose, in which the first-order version of DARTS directly discarded the second term -but this term is often numerically significant, and the second-order version of DARTS applied an approximation to this term which is not mathematically guaranteed (see Section 3.4).Consequently, the accuracy of \u2207 \u03b1 L val (\u03c9 (\u03b1) , \u03b1)| \u03b1=\u03b1t cannot be guaranteed, and hence the update of \u03b1 can be problematic.To the best of our knowledge, this issue is not studied by existing DARTS-based approaches.To deal with this problem, we propose an alternative way of computing \u2207 \u03b1 \u03c9 (\u03b1)| \u03b1=\u03b1t .We make use of an important property, i.e., \u2207 \u03c9 L train (\u03c9, \u03b1)| \u03c9=\u03c9 (\u03b1 \u2020 ),\u03b1=\u03b1 \u2020 \u2261 0 holds for any \u03b1 \u2020 , which directly comes from the optimality of \u03c9 (\u03b1).Differentiating both sides with respect to \u03b1 \u2020 , we obtain a new equality which enables computing derives \u2207 \u03b1 \u03c9 (\u03b1)| \u03b1=\u03b1t with the inverse of the Hesse matrix, \u2207 2 \u03c9 L train (\u03c9, \u03b1) \u03c9=\u03c9 (\u03b1),\u03b1=\u03b1t .This idea enables us to achieve a more accurate approximation on \u2207 \u03b1 L val (\u03c9 (\u03b1) , \u03b1)| \u03b1=\u03b1t when \u03c9 (\u03b1) is not available.Mathematically, we prove that when we have \u03c9 est \u2248 \u03c9 (\u03b1 t ), the inner angle between the second term and our approximate term is smaller than 90 degrees.Note that this property does not hold in existing DARTS-based algorithms.Our final solution involves using the amended second term of \u2207 \u03b1 L val (\u03c9 (\u03b1) , \u03b1)| \u03b1=\u03b1t meanwhile keeping the first term unchanged, which goes one step further in optimizing the supernetwork, which reflects in a higher validation accuracy in the search stage.Our approach is very easily implemented.The overall computational overhead is comparable to the second-order version of DARTS.Experiments are performed on image classification, with popular datasets including CIFAR and ImageNet being used.In all experiments, we allow the search stage to come to a complete convergence and report competitive accuracy among current state-of-thearts.The stability of our approach also enables us to close the gap between hyper-parameters of search and evaluation, as well as explore more complex search spaces, which are believed to be correct directions of NAS but existing DARTS-based approaches would mostly fail.Therefore, we believe our algorithm can expand the application scenario of differentiable NAS methods in particular DARTS-based approaches.The remainder of this paper is organized as follows.We briefly review related work in Section 2, and illustrate our approach of amending architectural gradients in Section 3.After experiments are shown in Section 4, we conclude this work in Section 5."}
{"paper_id": 87, "abstract": "In the grand tapestry of multi-agent systems, where the threads of cooperation must interweave to create a cohesive whole, we embark on a journey to explore the art of communication among agents. In this endeavor, we delve into a pivotal collaborative task where agents must learn to converse effectively within the framework of multi-agent reinforcement learning (MARL). Despite the many paths trodden by previous researchers, the quest for scalable global cooperation remains fraught with challenges. Many existing algorithms falter under the weight of their own complexity\u2014scalability issues and excessive communication demands render them less effective as the population of agents swells.  Enter our proposed framework, Learning Structured Communication (LSC), a beacon of innovation that not only embraces scalability but also champions the cause of high-quality communication. At its core, LSC empowers agents to dynamically forge a hierarchical communication structure, enabling them to harness the power of graph neural networks (GNNs) to distill and exchange valuable information with their neighbors. Through the introduction of novel techniques, we weave together the threads of communication structure learning, GNN optimization, and MARL tasks into a harmonious whole.  Our extensive experiments reveal that the LSC framework excels in communication efficiency, scalability, and the ability to foster global cooperation\u2014transforming the landscape of multi-agent interactions. Thus, we stand on the precipice of a new era in cooperative learning, ready to usher in a future where agents not only communicate but thrive together in their shared pursuits.", "introduction": "Reinforcement learning (RL) has achieved remarkable success in solving single-agent sequential decision problems under interactive and complicated environments, such as games (Mnih et al., 2015;Silver et al., 2016) and robotics (Lillicrap et al., 2016).In many real world applications such as intelligent transportation systems (Adler & Blue, 2002) and unmanned systems (Semsar-Kazerooni & Khorasani, 2009), not only one, but usually a large number of agents are involved in the learning tasks.Such a setting naturally leads to the popular multi-agent reinforcement learning (MARL) problems, where the key research challenges include how to design scalable and efficient learning schemes under an unstationary environment (caused by partial observation and/or the dynamics of other agents' policies), with large and/or dynamic problem dimension, and complicated and uncertain relationship between agents.Learning to communicate among agents has been regarded as an effective manner to strengthen the inter-agent collaboration and ultimately improve the quality of policies learned by MARL.Various communication-based MARL algorithms have been devised recently, e.g., DIAL (Foerster et al., 2016), CommNet (Sukhbaatar et al., 2016), ATOC (Jiang & Lu, 2018), IC3Net (Singh et al., 2019) and TarMAC (Das et al., 2019).These schemes aim to improve the inter-agent collaboration by learning communication strategy to exchange information between agents.However, there are still two bottlenecks unresolved, especially when faced a large number of agents.One bottleneck lies in that achieving effective communication and global collaboration is difficult with limited resources, such as narrow communication bandwidth and energy.In particular, DIAL and TarMAC require each agent to communicate with all the other agents, i.e., a fully-connected communication network (Figure 1(a)), which is not feasible for large scale scenarios with geographically apart agents.CommNet and IC3 assume a star network (Figure 1(b)) with a central node coordinating the global collaboration of agents, which again does not allow large scale scenarios with long range communications.ATOC introduces an interesting attention scheme to build a tree (a) Fully-connected (b) StarStep 1Step 2Step 3(c) Tree (d) Hierarchical .While the tree network can be scaled, global collaboration has to be realized through inefficient multi-hop and sequential communications.In a word, improper communication topologies will limit the cooperation ability in large scale scenarios.Another bottleneck is the difficulty of extracting essential information to exchange between agents for achieving high-performance MARL, especially when the number of agents grows.Most of the existing works simply concatenate, take the mean or use the LSTM to extract information to be exchanged.First two lack in considering the inter-relationship between agents, and LSTM assumes that there is a fixed sequence of message passing between agents, that is, the relationship between agents is predefined.Recently, TarMAC utilized an attention scheme to aggregate messages by considering the relationship from each agent to all others.However, the improper communication topology still hinders the information extraction.The communication structure needs to be jointly designed with the information extraction scheme to achieve further improved learning performance.For instance, the agent \"G\" finds the target (red square), then it will be possible to get a higher weight \"4\" and become the central.Right: The importance weight generation step and network construction step will be repeated iteratively.After communication and action procedures, agents will generate their new communication importance weights, and determine to keep or change their roles respectively.Further, the structured communication network will be re-established.To address the above two issues, we propose a novel structured communication-based algorithm, called learning structured communication (LSC).Our LSC combines a structured communication network module and a communication-based policy module, which aims to establish a scalable hierarchically structured network and information exchange scheme for large scale MARL.In particular, a hierarchically structured communication network (Figure 1(d)) is dynamically learned based on local partial observations of agents.In the hierarchically structured network, all agents are grouped into clusters, where global collaboration can be achieved via intra-group and inter-group communications.In contrast to the other three types in Figure 1, the proposed hierarchical communication network is more flexible and scalable, with fewer resources needed to achieve long-range and global collaboration.The procedure to establish such a hierarchically structured communication network is shown in Figure 2. To better utilize the relationship between agents given the hierarchically structured communication network and obtain more effective information extraction, graph neural network (GNN) (Scarselli et al., 2008) is employed.In GNN, each communication step involves information embedding and aggregation.Benefiting from the unordered aggregation power and the dynamic graph adaptability of GNN, the proposed LSC algorithm can extract valuable information effectively.The GNN-based information extraction procedure is depicted in Figure 3.This paper is devoted to the learning of communication structure among agents.To our knowledge, this is the first work of hierarchical structured learning to communication for MARL.It allows to learn communication structure adaptively instead of using predefined forms.Specifically: i) To improve scalability for a large number of agents, a hierarchical structure is devised that divides the agents into higher-level central agents and sub-level normal ones.As such, the communication network is sparsified.While it still allows for more effective global cooperation via message passing among the central agents, compared with the star/tree structures.ii) For effective communication and global cooperation, the message representation learning is deeply integrated into the information aggregating and permeating through the network, via graph neural network (GNN), which is a natural combination with the hierarchical communication structure.iii) Extensive experiments on both MAgent and StarCraft2 show our approach achieves state-of-theart scalability and effectiveness on large-scale MARL problems."}
{"paper_id": 88, "abstract": "In the realm of optimization, where the quest for efficiency often feels like navigating a labyrinth, Evolutionary Strategies (ES) emerge as a beacon of promise. These black-box, zeroth-order algorithms harness the power of search distributions to deftly traverse the landscape of objective functions. In this paper, we delve into an exciting frontier: the integration of highly flexible search distributions into ES algorithms, challenging the traditional reliance on standard forms like the Gaussian.  To breathe life into this vision, we employ Generative Neural Networks (GNNs) as our architects, crafting distributions that are not only expressive but also capable of accelerating the stochastic search process. Our innovation manifests in a new ES algorithm that seamlessly integrates these flexible distributions, acting as a transformative plug-in for virtually any existing ES framework.  Through rigorous empirical exploration, we unveil the compelling advantages of our approach across a diverse array of objective functions, illuminating the path forward in the optimization landscape. Join us as we embark on this journey, where the fusion of evolutionary strategies and advanced neural networks heralds a new era of optimization prowess.", "introduction": "We are interested in the global minimization of a black-box objective function, only accessible through a zeroth-order oracle.In many instances of this problem the objective is expensive to evaluate, which excludes brute force methods as a reasonable mean of optimization.Also, as the objective is potentially non-convex and multi-modal, its global optimization cannot be done greedily but requires a careful balance between exploitation and exploration of the optimization landscape (the surface defined by the objective).The family of algorithms used to tackle such a problem is usually dictated by the cost of one evaluation of the objective function (or equivalently, by the maximum number of function evaluations that are reasonable to make) and by a precision requirement.For instance, Bayesian Optimization (Jones et al., 1998;Shahriari et al., 2016) targets problems of very high evaluation cost, where the global minimum must be approximately discovered after a few hundreds of function evaluations.When aiming for a higher precision and hence having a larger budget (e.g.thousands of function evaluations), a popular algorithm class is the one of Evolutionary Strategies (ES) (Rechenberg, 1978;Schwefel, 1977), a family of heuristic search procedures.ES algorithms rely on a search distribution, which role is to propose queries of potentially small value of the objective function.This search distribution is almost always chosen to be a multivariate Gaussian.It is namely the case of the Covariance Matrix Adaptation Evolution Strategies (CMA-ES) (Hansen & Ostermeier, 2001), a state-of-the-art ES algorithm made popular in the machine learning community by its good results on hyper-parameter tuning (Friedrichs & Igel, 2005;Loshchilov & Hutter, 2016).It is also the case for Natural Evolution Strategies (NES) (Wierstra et al., 2008) algorithms, which were recently used for direct policy search in Reinforcement Learning (RL) and shown to compete with state-of-the-art MDP-based RL techniques (Salimans et al., 2017).Occasionally, other distributions have been used; e.g.fat-tails distributions like the Cauchy were shown to outperform the Gaussian for highly multi-modal objectives (Schaul et al., 2011).We argue in this paper that in ES algorithms, the choice of a standard parametric search distribution (Gaussian, Cauchy, ..) constitutes a potentially harmful implicit constraint for the stochastic search of a global minimum.To overcome the limitations of classical parametric search distributions, we propose using flexible distributions generated by bijective Generative Neural Networks (GNNs), with computable and differentiable log-probabilities.We discuss why common existing optimization methods in ES algorithms cannot be directly used to train such models and design a tailored algorithm that efficiently train GNNs for an ES objective.We show how this new algorithm can readily incorporate existing ES algorithms that operates on simple search distributions, Algorithm 1: Generic ES procedure input: zeroth-order oracle on f , distribution \u03c0 0 , population size \u03bb repeat (Sampling) Sample x 1 , . . ., x \u03bb i.i.d \u223c \u03c0 t (Evaluation) Evaluate f (x 1 ), . . ., f (x n ).(Update) Update \u03c0 t to produce x of potentially smaller objective values.until convergence; like the Gaussian.On a variety of objective functions, we show that this extension can significantly accelerate ES algorithms.We formally introduce the problem and provide background on Evolutionary Strategies in Section 2. We discuss the role of GNNs in generating flexible search distributions in Section 3. We explain why usual algorithms fail to train GNNs for an ES objective and introduce a new algorithm in Section 4. Finally we report experimental results in Section 5."}
{"paper_id": 89, "abstract": "In the ever-evolving realm of machine learning, domain adaptation emerges as a powerful tool, bridging the chasm between a richly labeled source domain and a target domain that often lacks sufficient labels. Recent strides in domain-adversarial training (DAT) have illuminated a path toward crafting a domain-invariant feature space, skillfully manipulating the gradient flow of a domain classifier. Yet, this approach is not without its pitfalls. It grapples with challenges such as the instability of training\u2014often a consequence of the domain classifier's overpowering discriminative prowess during adversarial training. Moreover, it imposes restrictive feature-level alignment and falls short in providing a clear, systematic understanding of the learned feature space.  In this paper, we unveil a groundbreaking approach: Max-margin Domain-Adversarial Training (MDAT), anchored by the innovative Adversarial Reconstruction Network (ARN). By substituting the traditional domain classifier with a reconstruction network, MDAT stabilizes the gradient reversal process within ARN, enabling simultaneous feature-level and pixel-level domain alignment without the burden of additional network architectures. Our ARN showcases remarkable resilience across a diverse array of hyperparameter settings, significantly easing the model selection dilemma that often plagues practitioners.  Through extensive empirical evaluations, we demonstrate that our MDAT method not only surpasses existing state-of-the-art domain alignment techniques but also provides an intuitive visualization of reconstructed target samples. This visualization enhances our understanding of the domain-invariant feature space, aligning seamlessly with our theoretical expectations. In a world where clarity and robustness are paramount, our approach stands as a beacon of innovation in the quest for effective domain adaptation.", "introduction": "Deep neural networks have gained great success on a wide range of tasks such as visual recognition and machine translation (LeCun et al., 2015).They usually require a large number of labeled data that can be prohibitively expensive to collect, and even with sufficient supervision their performance can still be poor when being generalized to a new environment.The problem of discrepancy between the training and testing data distribution is commonly referred to as domain shift (Shimodaira, 2000).To alleviate the effect of such shift, domain adaptation sets out to obtain a model trained in a label-rich source domain to generalize well in an unlabeled target domain.Domain adaptation has benefited various applications in many practical scenarios, including but not limited to object detection under challenging conditions (Chen et al., 2018), cost-effective learning using only synthetic data to generalize to real-world imagery (Vazquez et al., 2013), etc.Prevailing methods for unsupervised domain adaptation (UDA) are mostly based on domain alignment which aims to learn domain-invariant features by reducing the distribution discrepancy between the source and target domain using some pre-defined metrics such as maximum mean discrepancy (Tzeng et al., 2014).Recently, Ganin & Lempitsky (2015) proposed to achieve domain alignment by domainadversarial training (DAT) that reverses the gradients of a domain classifier to maximize domain confusion.Having yielded remarkable performance gain, DAT was employed in many subsequent UDA methods (Long et al., 2018;Shu et al., 2018).Even so, there still exist three critical issues of DAT that hinder its performance: (1) as the domain classifier has high-capacity to discriminate two domains, the unbalanced adversarial training cannot continuously provide effective gradients, which is usually overcome by manually adjusting the weights of adversarial training according to specific tasks; (2) DAT-based methods cannot deal with pixel-level domain shift (Hoffman et al., 2018); (3) the domain-invariant features learned by DAT are only based on intuition but difficult to interpret, which impedes the investigation of the underlying mechanism of adversarial domain adaptation.To overcome the aforementioned difficulties, we propose an innovative DAT approach, namely Max-margin Domain-Adversarial Training (MDAT), to realize stable and comprehensive domain alignment.To demonstrate its effectiveness, we develop an Adversarial Reconstruction Network (ARN) that only utilizes MDAT for UDA.Specifically, ARN consists of a shared feature extractor, a label predictor, and a reconstruction network (i.e.decoder) that serves as a domain classifier.Supervised learning is conducted on source domain, and MDAT helps learn domain-invariant features.In MDAT, the decoder only focuses on reconstructing samples on source domain and pushing the target domain away from a margin, while the feature extractor aims to fool the decoder by learning to reconstruct samples on target domain.In this way, three critical issues can be solved by MDAT: (1) the max-margin loss reduces the discriminative capacity of domain classifier, leading to balanced and thus stable adversarial training; (2) without involving new network structures, MDAT achieves both pixel-level and feature-level domain alignment; (3) visualizing the reconstructed samples reveals how the source and target domains are aligned.We evaluate ARN with MDAT on five visual and non-visual UDA benchmarks.It achieves significant improvement to DAT on all tasks with pixel-level or higher-level domain shift.We also observe that it is insensitive to the choices of hyperparameters and as such is favorable for replication in practice.In principle, our approach is generic and can be used to enhance any UDA methods that leverage domain alignment as an ingredient."}
{"paper_id": 90, "abstract": "In the ever-evolving landscape of deep learning, where triumphs abound in realms like image processing, voice recognition, and graph analysis, there lies a shadowy expanse of untapped potential\u2014domains devoid of clear structural relationships among features. Consider, for instance, a tabular dataset brimming with diverse demographic and clinical factors, where the intricate dance of feature interactions remains cloaked in ambiguity. Herein, we unveil the Group-Connected Multilayer Perceptron (GMLP), a beacon of innovation designed to illuminate the path of deep representation learning in these challenging territories.  At its core, GMLP embraces the notion of crafting expressive combinations of features\u2014what we term \"groups\"\u2014and deftly harnesses these to streamline network complexity through localized group-wise operations. During the training odyssey, GMLP embarks on a quest to uncover a sparse feature grouping matrix, employing a temperature-annealing softmax approach coupled with an entropy loss term to foster the desired sparsity. Moreover, we propose a novel architecture reminiscent of binary trees, where each group-wise operation is followed by pooling mechanisms that amalgamate information, artfully reducing the number of groups as the network delves deeper into its layers.  To validate our groundbreaking approach, we ventured into the realm of empirical experimentation, traversing five distinct real-world datasets that span a multitude of application domains. Our findings, bolstered by visualizations from the MNIST dataset and synthesized data, reveal that GMLP not only adeptly learns and exploits these expressive feature combinations but also achieves state-of-the-art classification performance across various datasets. In this endeavor, we have taken a significant step toward unlocking the potential of deep learning in uncharted territories.", "introduction": "Deep neural networks have been quite successful across various machine learning tasks.However, this advancement has been mostly limited to certain domains.For example in image and voice data, one can leverage domain properties such as location invariance, scale invariance, coherence, etc. via using convolutional layers (Goodfellow et al., 2016).Alternatively, for graph data, graph convolutional networks were suggested to leverage adjacency patterns present in datasets structured as a graph (Kipf & Welling, 2016;Xu et al., 2019).However, there has been little progress in learning deep representations for datasets that do not follow a particular known structure in the feature domain.Take for instance the case of a simple tabular dataset for disease diagnosis.Such a dataset may consist of features from different categories such as demographics (e.g., age, gender, income, etc.), examinations (e.g., blood pressure, lab results, etc.), and other clinical conditions.In this scenario, the lack of any known structure between features to be used as a prior would lead to the use of a fully-connected multilayer perceptron network (MLP).Nonetheless, it has been known in the literature that MLP architectures, due to their huge complexity, do not usually admit efficient training and generalization for networks of more than a few layers.In this paper, we propose Group-Connected Multiplayer Perceptron (GMLP) networks.The main idea behind GMLP is to learn and leverage expressive feature subsets, henceforth referred to as feature groups.A feature group is defined as a subset of features that provides a meaningful representation or high-level concept that would help the downstream task.For instance, in the disease diagnosis example, the combination of a certain blood factor and age might be the indicator of a higher level clinical condition which would help the final classification task.Furthermore, GMLP leverages feature groups limiting network connections to local group-wise connections and builds a feature hierarchy via merging groups as the network grows in depth.GMLP can be seen as an architecture that learns expressive feature combinations and leverages them via group-wise operations.The main contributions of this paper are as follows: (i) proposing a method for end-to-end learning of expressive feature combinations, (ii) suggesting a network architecture to utilize feature groups and local connections to build deep representations, (iii) conducting extensive experiments demonstrating the effectiveness of GMLP as well as visualizations and ablation studies for better understanding of the suggested architecture.We evaluated the proposed method on five different real-world datasets in various application domains and demonstrated the effectiveness of GMLP compared to state-of-the-art methods in the literature.Furthermore, we conducted ablation studies and comparisons to study different architectural and training factors as well as visualizations on MNIST and synthesized data.To help to reproduce the results and encouraging future studies on group-connected architectures, we made the source code related to this paper available onlinefoot_0 ."}
{"paper_id": 91, "abstract": "In the realm of machine learning, the advent of graph convolutional networks (GCNs) has heralded a new epoch in the classification of nodes and entire graphs. These powerful tools have found a unique ally in active learning (AL), where a carefully curated selection of labeled examples can illuminate the path forward. Yet, a curious dichotomy has emerged in this landscape: most AL-GCN approaches lean heavily on the uncertainty of individual samples, leaving the rich tapestry of the graph itself largely untouched. Meanwhile, representative sampling delves into the graph\u2019s structure but neglects the predictive insights that the model provides.  In our quest to bridge this divide, we present a dual-faceted approach that harnesses both the uncertainty within the graph and the predictive power of GCNs. The first of our innovative methods, dubbed \"Regional Uncertainty,\" builds upon the classical entropy measure. Rather than merely seeking nodes with high entropy, we aim to unearth those nodes ensconced amidst a diverse array of neighboring classes, or those steeped in ambiguity\u2014nodes that pulse with potential insights.  The second method, which we call \"Adaptive Page-Rank,\" extends the venerable page-rank algorithm. Here, we target nodes that exhibit a low likelihood of being reached by random walks originating from labeled nodes, deftly selecting those that lie in the shadows of uncertainty. Our findings reveal that this method shines brightest when the proportion of labeled nodes is sparse. Yet, as this fraction approaches the average degree of the graph, Regional Uncertainty emerges as the superior strategy, eclipsing all existing methodologies.  While our exploration has focused on the intricate world of graphs, the principles underlying these methods possess a versatility that allows them to transcend their initial confines, applicable to any classification challenge where a meaningful distance can be articulated between input samples. Thus, we stand on the precipice of a new frontier in active learning, armed with tools that promise to illuminate the dark corners of uncertainty.", "introduction": "Relational information is often presented as graphs or multi-graphs, where nodes represent entities and edges represent relations between these entities.Such relations can be used to predict the class of the nodes, using two main principles.The first and most used method is based on node class homophily, where neighboring nodes belong to the same class with a high probability (Ji & Han, 2012;Berberidis & Giannakis, 2018;Zhu et al., 2003a;b;Sindhwani et al., 2005;Belkin & Niyogi, 2004).This has been used in many propagation-based algorithms where the class of a node is predicted using the class of neighboring nodes.The second approach presumes a correlation between the topological attributes (e.g.degree, centrality, clustering coefficient...) of nodes and their class (Shi & Malik, 2000;Yang et al., 2013;Rosen & Louzoun, 2015;Naaman et al., 2018;Cannistraci et al., 2013).These two principles are combined in Graph Convolutional Networks (GCN).Such networks received much interest over the last decade, and especially following the works of Kipf & Welling (2016), where they have produced higher accuracies than other label propagation methods.The main formalism proposed in such networks is the weighted combination of the input from previous layers in neighboring nodes:with W K being the weights of the k layer, X k the input to this layer and A a matrix derived from the adjacency matrix (e.g.A = D -1/2 [A + I]D -1/2 for undirected graphs).X 0 is usually external information about the nodes.In the absence of such information, the identity matrix (Schlichtkrull et al., 2018), topological features of nodes or the frequency of neighbors belonging to each class in the training set (Benami et al., 2019) have been proposed.In the absence of a predefined set of classified nodes, and when the nodes composing the training set can be chosen, active learning can be used to query the class of nodes that would produce the highest precision (as defined through the prediction accuracy or any other measure on the entire dataset) using the minimal number of classified samples.Many Active Learning (AL) methods have been proposed (Lewis & Catlett, 1994;Culotta & McCallum, 2005;Settles & Craven, 2008).The most frequently used approaches are uncertainty sampling and representative sampling (Settles, 2009).Uncertainty sampling is a general framework for measuring informativeness (Lewis & Catlett, 1994), where a learner queries the instance whose class is the most uncertain.Culotta & McCallum (2005) employed a simple uncertainty-based strategy for sequence models called least confidence (LC): \u03c6 LC (x) = 1 -P (y * |x; \u0398).Here, y * is the most likely label.This approach queries the instance for which the current model has the least confidence in its most probable label.Scheffer et al. (2001) proposed another uncertainty strategy, which queries the instance with the smallest margin between the posteriors for its two most probable labels: \u03c6 M (x) = P (y * 1 |x; \u0398) -P (y * 2 |x; \u0398), where y * 1 and y * 2 are the first and second best labels, respectively.Another uncertainty-based measure of informativeness is entropy (see Shannon (1948)).For a discrete random variable Y , the entropy is given by H(Y ) = -(P (y i )log(P (y i ))).A different approach to uncertainty involves several independent models and looks for disagreement among them (Seung et al., 1992).In representative sampling, one assumes that informative instances are \"representative\" of the underlying distribution, and the query is based on the properties of the nodes in contrast with the uncertainty sampling, where the predicted scores for each label are used and not the samples themselves.Measures of the distribution include the KullbackLeibler (KL) divergence similarity (McCallum & Nigam) or clustering (Xu et al., 2007), where the goal is to obtain representative labeled data samples.Fujii et al. (1998) considered a query strategy for nearest-neighbor methods that select queries that are (i) least similar to the labeled instances, and (ii) most similar to the unlabeled instances.Nguyen & Smeulders (2004) proposed a density-based approach that first clusters instances and then avoids querying outliers by propagating label information to instances in the same cluster.Settles and Craven (2008) suggested a general density-weighting technique combining both uncertainty and representative sampling.They query instances as follows:where \u03c6 A (X) represents the informativeness of x according to some \"base\" query strategy A, and U are the unlabeled samples.The second term weights the informativeness of x by its average similarity to all other instances in the input distribution (as approximated by U ), subject to a parameter \u03b2 that controls the relative importance of the density term (Settles & Craven, 2008).Zhu et al. (2009) also proposed sampling by a combination of uncertainty and density to solve the outliers problem emerging in some uncertainty techniques.Another frequently used measure is the influence of the unlabeled samples on the model, using varying methods, such as length of gradients (Settles et al., 2008), expected change or Fisher information ratio (Cohn et al., 1996).Finally, there are many hybrid methods that combine different criteria (Settles & Craven, 2008).When applied to graphs, uncertainty methods were based on the properties of the classifier scores and did not explicitly use the graph information to select nodes.However, since neighboring nodes share classes more often than non-neighboring nodes, the graph itself can be used not only to predict the class of nodes but also to predict the diffusion of uncertainty.Similarly, representative sampling takes advantage of the graph but ignores the information on the nodes class.Assume, for example, two nodes: 1) One node that is a distinct connectivity component, where the current classifier gives the same score for both classes in binary classification.Checking the class of such a node, would probably not improve the accuracy of other nodes.2) Similarly, a node with a predicted probability of 99 % to the first class would also not be of interest, even if this node has a very high degree, and checking its class would be of limited use since we already know it with a high probability.The interesting nodes to classify would be nodes that combine uncertainty and connection to many other nodes.We here propose two methods to combine uncertainty with graph properties and show that combining the graph within the AL leads to significantly higher accuracies than all current AL methods on standard datasets."}
{"paper_id": 92, "abstract": "In the ever-evolving realm of machine learning, the quest for understanding the enigmatic black-box models is akin to deciphering the intricate patterns of a vast tapestry woven with the threads of data. As these models gain traction, the need for clarity becomes paramount. Yet, crafting universally interpretable models that illuminate the entirety of their behavior presents a formidable challenge.   In response to this conundrum, we unveil a groundbreaking approach: Reinforcement Learning-based Locally Interpretable Modeling, or RL-LIM. This innovative method harnesses the power of reinforcement learning to meticulously select a select few samples, distilling the complex predictions of the black-box model into a simpler, low-capacity framework that resonates with human intuition. The training process is elegantly guided by a reward system, where success is measured by the harmony between the predictions of our locally interpretable model and those of the elusive black-box.  RL-LIM not only closely mirrors the predictive prowess of its black-box counterparts but also grants a level of interpretability that feels inherently human. In a decisive leap forward, it eclipses existing state-of-the-art locally interpretable models, achieving superior performance and fidelity. Thus, we embark on a new chapter in the narrative of machine learning, where understanding and insight become accessible to all.", "introduction": "Artificial Intelligence (AI) is advancing at a rapid pace, particularly with recent advances in deep neural networks and ensemble methods (Goodfellow et al., 2016;He et al., 2016;Chen & Guestrin, 2016;Ke et al., 2017).This progress has been fueled by 'black-box' machine learning models where the decision making is controlled by complex non-linear interactions between many parameters that are difficult for humans to understand and interpret.However, in many real-world applications AI systems are not only expected to perform well but are also required to be interpretable: doctors need to understand why a particular treatment is recommended, and financial institutions need to understand why a loan was declined.Use cases of model interpretability vary across applications: it can provide trust to users by showing rationales behind decisions, enable detection of systematic failure cases, and provide actionable feedback for improving models (Rudin, 2018).Many studies have suggested a trade-off between performance and interpretability (Vir\u00e1g & Nyitrai, 2014;Johansson et al., 2011).This is correct in that globally interpretable models, which attempt to explain the entire model behavior, typically yield considerably worse performance than 'blackbox' models (Lipton, 2016).To go beyond the performance limitations of globally interpretable models, another promising direction is locally interpretable models, which instead of explaining the entire model explain a single prediction (Ribeiro et al., 2016).Methodologically, while a globally interpretable model fits a single inherently interpretable model (such as a linear model or a shallow decision tree) to the entire training set, locally interpretable models aim to fit an inherently interpretable model locally, i.e. for each instance individually, by distilling knowledge from a high performance black-box model.Such locally interpretable models are very useful for real-world AI deployments to provide succinct and human-like explanations to users.They can be used to identify systematic failure cases (e.g. by seeking common trends in input dependence for failure cases), detect biases (e.g. by quantifying feature importance for a particular variable), and provide actionable feedback to improve a model (e.g.understand failure cases and what training data to collect).To be useful in practice, locally interpretable models need to maximize two objectives: (i) the overall prediction performance (how well it predicts compared to the ground truth labels) -for the model to be accurate, and (ii) fidelity (how well it approximates the 'black-box' model predictions) -to ensure the model is reliably approximating the black-box model's predictions in the neighborhood of interest (Plumb et al., 2019;Lakkaraju et al., 2019).To this end, a few methods have recently been proposed for locally interpretable modeling: Local Interpretable Model-agnostic Explanations (LIME) (Ribeiro et al., 2016), Supervised Local modeling methods (SILO) (Bloniarz et al., 2016), and Model Agnostic Supervised Local Explanations (MAPLE) (Plumb et al., 2018).LIME in particular has gained notable popularity and has been deployed in many applications due to its simplicity.However, the overall prediction performance and fidelity metrics are not reaching desired levels in many cases (Alvarez-Melis & Jaakkola, 2018;Zhang et al., 2019;Ribeiro et al., 2018;Lakkaraju et al., 2017).Indeed, as we show in our experiments, there are frequent cases where existing locally interpretable models even underperform commonly low-performing globally interpretable models.One of the fundamental challenges to fit a locally interpretable model is the representational capacity difference while applying distillation.Black-box machine learning models, such as deep neural networks or ensemble models, have much larger representational capacity than locally interpretable models.This can result in underfitting with conventional distillation techniques, leading to suboptimal performance (Hinton et al., 2015;Wang et al., 2019).We address this fundamental challenge by proposing a novel Reinforcement Learning-based method to fit Locally Interpretable Models which we call RL-LIM.RL-LIM efficiently utilizes the small representational capacity of locally interpretable models by training with a small number of samples that are determined to have the highest value contribution to the fitting of a locally interpretable model.In order to select these highest-value instances, we train instance-wise weight estimators (modeled with deep neural networks) using a reinforcement signal that quantifies the fidelity metric (i.e.how well does the model approximate the black-box model predictions).The contributions of this paper can be summarized as:1. We introduce the first method that tackles interpretability through data-weighted training, and show that reinforcement learning is highly effective for end-to-end training of such a model.2. We show that distillation of a black-box model into a low-capacity interpretable model can be significantly improved by fitting with a small subset of relevant samples that is controlled efficiently by our method.3. On various classification and regression datasets, we demonstrate that RL-LIM significantly outperforms alternative models (LIME, SILO and MAPLE) in overall prediction performance and fidelity metrics -in most cases, the overall performance of locally interpretable models obtained by RL-LIM is very similar to complex black-box models."}
{"paper_id": 93, "abstract": "In the ever-evolving realm of machine learning, a recent breakthrough has illuminated the intricate relationship between strongly overparametrized neural networks (NNs) and kernel methods. This connection unveils a new perspective on the enigmatic behaviors of NNs, particularly their convergence and generalization traits. In this exploration, we delve into the impact of initialization bias on these overparametrized NNs when subjected to gradient descent.  We establish a compelling theorem: fully-connected wide ReLU-NNs, when trained with squared loss, can be understood as the sum of two distinct components. The first component represents the minimum complexity solution derived from an interpolating kernel method, a foundation of elegant simplicity. The second, however, is a more capricious entity, influencing test error significantly and intricately tied to the network's initialization.  This duality yields two profound insights: First, when the initialization variance is small, the second component's influence diminishes, allowing us to carry over generalization bounds from minimum complexity interpolating kernel methods to NNs seamlessly. Second, in stark contrast, when the initialization variance is large, we observe a troubling phenomenon: the test error of wide NNs escalates dramatically, even as they continue to interpolate the training data flawlessly.  Our findings challenge the prevailing notion that initialization schemes are inconsequential. Instead, they reveal a potent truth: the choice of initialization wields considerable power over generalization performance. This insight offers a fresh lens through which to evaluate and refine initialization strategies, guiding us toward more effective neural network architectures.", "introduction": "Neural networks (NNs) have celebrated many successes over the past decade and achieved state of the art performance across various domains and tasks.From a theoretical standpoint, however, many aspects of neural networks are not well understood, as for example illustrated by Zhang et al. (2016).Neural networks seem to contradict classical learning theory as, in many scenarios, they are able to fit random labels perfectly while still generalizing well when trained on the true labels.In addition, overparametrized neural networks frequently exhibit even improved test performance when the number of parameters is increased further (Belkin et al., 2019).NN models thus often seem to avoid overfitting.Very recently there have been advances in understanding the training and evaluation behavior of neural networks in the infinitely wide limit (Jacot et al., 2018;Hayou et al., 2019) and also in the strongly overparametrized regime (Du et al., 2018b;Li & Liang, 2018;Allen-Zhu et al., 2018a), which is close to the infinite limit NN.Both lines of work express the behavior of the neural network in terms of the so-called neural tangent kernel (NTK).In particular, Du et al. (2018b) showed in this way that, under mild conditions, strongly over-parametrized neural networks converge to a global minimum of zero training error.In another line of research, Belkin et al. (2018b) suggested a new picture of learning in the overparametrized regime, introducing an \"interpolating kernel method\" which successfully learns in this regime.This method selects the least complex function that interpolates all data points perfectly, as opposed to traditional methods which balance the function's complexity against its goodness-of-fit.It is suggested that this picture of overparametrized learning could help understand neural networks.If overparametrized NNs are indeed linked to such kernel methods, then zero (or small) training error alone might not tell us much about the test performance, as the measure of function complexity could be determined by the NN architecture, initialization, and the optimization method.A bad NN design may lead to an unfavorable complexity measure that could result in a large generalization gap.The question therefore remains in which way exactly are overparametrized neural networks linked to interpolating kernel methods?And how can this connection help us better understand neural networks and their puzzling features?1.1 OUR CONTRIBUTIONS In this paper we answer the first question by making the link to interpolating kernel methods explicit for strongly overparametrized NNs.After that, we exhibit two implications for NNs which this connection allows us to draw.First, for strongly overparametrized ReLU-NNs we make the bias explicit that is imposed by the NN initialization and by training with discrete gradient descent steps on the squared loss.We achieve this by decomposing the learned NN function as well as the test error into two terms: The first term corresponds to a minimum complexity interpolating kernel method, whereas the second term is proportional to the initialized weights and gives a non-zero contribution only if the test input is not part of the training set.Second, we are able to bound the difference between a NN with standard initialization of small variance and the solution of a minimum complexity interpolating kernel method based on the NTK.This bound provides a new way to transfer existing test error bounds from interpolating kernel methods to NNs, and vice versa.Our third contribution shows that without putting any constraints on the NN initialization, low training error does not imply low test error.This is because the second term in the above test error decomposition grows as a power of the initialization variance, as we show, whereas the first term is independent of this variance.Thus, the test error grows dramatically with increasing initialization variance, despite (close to) zero training error.Our findings provide an additional theoretical way to analyze initialization schemes, besides the existing heuristics and intention to prevent vanishing and exploding gradients (He et al., 2015;Glorot & Bengio, 2010).Our theoretical bounds and insights are nicely corroborated in several experimental settings."}
{"paper_id": 94, "abstract": "In the vast and intricate realm of machine learning, understanding the true worth of data stands as a pivotal challenge. This quest for data valuation unfolds a tapestry of vital applications: from illuminating insights into the learning journey to facilitating seamless domain adaptation, unveiling corrupted samples, and fortifying robust learning. To navigate this complex landscape, we introduce a novel meta-learning framework we call Data Valuation using Reinforcement Learning (DVRL). This framework embarks on a journey to learn data values in harmony with the predictive model tasked with the ultimate goal.  At the heart of DVRL lies a data value estimator, crafted through the architecture of deep neural networks, designed to gauge the likelihood of each datum's contribution to the training of our predictor. The training of this estimator is guided by a reinforcement signal derived from the rewards earned on a carefully curated validation set, reflecting the model's performance on the target task. Our findings reveal that DVRL not only excels in providing more accurate data value estimates than existing methods across a variety of datasets but also shines in a multitude of application scenarios.  Moreover, when it comes to discovering corrupted samples, DVRL approaches an optimal performance level, as if it had prior knowledge of the noisy data. In the realms of domain adaptation and robust learning, our framework outstrips the state-of-the-art by impressive margins of 14.6% and 10.8%, respectively. Thus, DVRL stands not merely as a tool but as a beacon for future explorations in the ever-evolving landscape of machine learning.", "introduction": "Data is an essential ingredient in machine learning.Machine learning models are well-known to improve when trained on large-scale and high-quality datasets (Hestness et al., 2017;Najafabadi et al., 2015).However, collecting such large-scale and high-quality datasets is costly and challenging.One needs to determine the samples that are most useful for the target task and then label them correctly.Recent work (Toneva et al., 2019) suggests that not all samples are equally useful for training, particularly in the case of deep neural networks.In some cases, similar or even higher test performance can be obtained by removing a significant portion of training data, i.e. low-quality or noisy data may be harmful (Ferdowsi et al., 2013;Frenay & Verleysen, 2014).There are also some scenarios where train-test mismatch cannot be avoided because the training dataset only exists for a different domain.Different methods (Ngiam et al., 2018;Zhu et al., 2019) have demonstrated the importance of carefully selecting the most relevant samples to minimize this mismatch.Accurately quantifying the value of data has a great potential for improving model performance for real-world training datasets which commonly contain incorrect labels, and where the input samples differ in relatedness, sample quality, and usefulness for the target task.Instead of treating all data samples equally, lower priority can be assigned for a datum to obtain a higher performance modelfor example in the following scenarios:1. Incorrect label (e.g.human labeling errors).2. Input comes from a different distribution (e.g.different location or time).3. Input is noisy or low quality (e.g.noisy capturing hardware).4. Usefulness for target task (label is very common in the training dataset but not as common in the testing dataset).In addition to improving performance in such scenarios, data valuation also enables many new use cases.It can suggest better practices for data collection, e.g.what kinds of additional data would the model benefit the most from.For organizations that sell data, it determines the correct value-based pricing of data subsets.Finally, it enables new possibilities for constructing very large-scale training datasets in a much cheaper way, e.g. by searching the Internet using the labels and filtering away less valuable data.How does one evaluate the value of a single datum?This is a crucial and challenging question.It is straightforward to address at the full dataset granularity: one could naively train a model on the entire dataset and use its prediction performance as the value.However, evaluating the value of each datum is far more difficult, especially for complex models such as deep neural networks that are trained on large-scale datasets.In this paper, we propose a meta learning-based data valuation method which we name Data Valuation using Reinforcement Learning (DVRL).Our method integrates data valuation with the training of the target task predictor model.DVRL determines a reward by quantifying the performance on a small validation set, and uses it as a reinforcement signal to learn the likelihood of each datum being using in training of the predictor model.In a wide range of use cases, including domain adaptation, corrupted sample discovery and robust learning, we demonstrate significant improvements compared to permutation-based strategies (such as Leave-one-out and Influence Function (Koh & Liang, 2017)) and game theory-based strategies (such as Data Shapley (Ghorbani & Zou, 2019)).The main contributions can be summarized as follows:1. We propose a novel meta learning framework for data valuation that is jointly optimized with the target task predictor model.2. We demonstrate multiple use cases of the proposed data valuation framework and show DVRL significantly outperforms competing methods on many image, tabular and language datasets.3. Unlike previous methods, DVRL is scalable to large datasets and complex models, and its computational complexity is not directly dependent on the size of the training set."}
{"paper_id": 95, "abstract": "In the realm of neural networks, a pivotal innovation emerges with our equivarification method, a tool that operates independently of the intricate workings of any given layer. This remarkable flexibility allows it to be seamlessly integrated into any feedforward neural network, regardless of its architecture. Even as the network expands and evolves, the resulting equivariant neural network maintains the same parameter count as its predecessor, deftly sidestepping the common pitfalls of increased complexity. To illustrate the power of this approach, we crafted an equivariant neural network tailored for image classification, transforming a conventional convolutional neural network through the lens of equivarification. The results are striking: our method not only streamlines the design and training processes but also upholds, if not enhances, the learning performance, particularly in terms of accuracy. In this way, we forge a path toward more efficient and effective neural network design, marrying simplicity with sophistication.", "introduction": "One key issue in deep neural network training is the difficulty of tuning parameters, especially when the network size grows larger and larger Han et al. (2015).In order to reduce the complexity of the network, many techniques have been proposed by analyzing the structural characteristics of data, for example, sparsity Wen et al. (2016), invariance in movement Goodfellow et al. (2009).One of the most important structural characteristics of data is symmetry.By utilizing the translation symmetry of an object in the photo, convolutional neural network (CNN) (Krizhevsky et al. (2012)) uses shared filters to reduce the number of parameters compared with fully connected networks.However, to handle the case of rotation and reflection, people usually use the data augmentation approach to generate additional input data that has a bunch of training images with different rotation angles of the original images.In contrast to data augmentation approach, another idea is to design more sophisticated neural networks, such that the input data with certain symmetries can be trained together and applied to reduce the training complexity.Recent attempts have been made in the equivariant CNN Cohen & Welling (2016a); Cohen et al. (2018); Weiler et al. (2018).These existing works target at special cases and cannot be easily generalized to arbitrary networks and arbitrary symmetries.In this paper, we take advantage of the symmetry of the data set and provide a general method to modify an arbitrary neural network so that it preserves the symmetry.The process is called equivarification.A key feature is that our equivarification method can be applied without detailed knowledge of a layer in a neural network, and hence, can be generalized to any feedforward neural networks.Another feature is that the number of parameters in the new neural network that we need to train is the same as the original one, if we equivarify the original network globally (see the second paragraph in Section 4 for details).In addition, we can also output how each data instance is changed from the canonical form (for example, in an image classification problem, additionally we can also output how many degrees an image is rotated from original upside-up image) using the same network.We rigorously prove that our equivarification method produces truely equivariant neural networks.Practically, we equivarify a CNN as an illustration.We conduct experiments using the MNIST data set where the resulting equivariant neural network predicts the number and also the angle.If we forget the angle and keep only the number, we get an invariant neural network."}
{"paper_id": 96, "abstract": "In the intricate realm of graph-based learning, two prominent contenders stand at the forefront: Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN). Both wield the power of message passing to tackle the formidable challenge of node classification, yet they embark on their journeys in markedly different ways. LPA deftly sends ripples of node label information cascading along the edges of the graph, while GCN transforms and disseminates node features with a touch of mathematical elegance. Despite their conceptual kinship, the theoretical ties that bind LPA and GCN remain largely uncharted territory.  In this exploration, we delve into the nuanced relationship between LPA and GCN through two critical lenses. First, we examine the phenomenon of feature and label smoothing\u2014analyzing how the essence of one node subtly permeates its neighbors. Second, we investigate the influence wielded by a node's initial feature or label on the eventual outcome of another. Our theoretical insights pave the way for a groundbreaking end-to-end model that harmonizes the strengths of both GCN and LPA for the task of node classification.  In this unified framework, the edges themselves become dynamic, learnable entities, while LPA acts as a guiding force, a regularization mechanism that aids GCN in mastering the art of edge weight optimization. This innovative approach can be interpreted as a method of learning attention weights that are intricately tied to node labels, steering us toward a more task-centric paradigm than traditional feature-based attention models. Through a series of rigorous experiments on real-world graphs, our model emerges victorious, outshining state-of-the-art GCN-based methods in the quest for node classification accuracy.", "introduction": "Consider the problem of node classification in a graph, where the goal is to learn a mapping M : V \u2192 L from nodes V to labels L. Solution to this problem is widely applicable to various scenarios, e.g., inferring income of users in a social network or classifying scientific articles in a citation network.Different from a generic machine learning problem where samples are independent from each other, nodes are connected by edges in the graph, which provide additional information and require more delicate modeling.To capture the graph information, researchers have mainly designed models on the assumption that labels and features vary smoothly over the edges of the graph.In particular, on the label side L, node labels are propagated and aggregated along edges in the graph, which is known as Label Propagation Algorithm (LPA) (Zhu et al., 2005;Zhou et al., 2004;Zhang & Lee, 2007;Wang & Zhang, 2008;Karasuyama & Mamitsuka, 2013;Gong et al., 2017;Liu et al., 2019a); On the node side V, node features are propagated along edges and transformed through neural network layers, which is known as Graph Convolutional Neural Networks (GCN) (Kipf & Welling, 2017;Hamilton et al., 2017;Li et al., 2018;Xu et al., 2018;Liao et al., 2019;Xu et al., 2019;Qu et al., 2019).GCN and LPA are related in that they propagate features and labels on the two sides of the mapping M, respectively.However, the relationship between GCN and LPA has not yet been investigated.Specifically, what is the theoretical relationship between GCN and LPA, and how can they be combined to develop a more accurate model for node classification in graphs?Here we study the theoretical relationship between GCN and LPA from two viewpoints: (1) Feature/label smoothing, where we show that the intuition behind GCN/LPA is smoothing features/labels of nodes across the edges of the graph, i.e., one node's feature/label equals the weighted average of features/labels of its neighbors.We prove that if edge weights smooth the node features, they also smooth the node labels with guaranteed upper bound on the approximation error.And, (2) feature/label influence, where we quantify how much the initial feature/label of node v b influences the output feature/label of node v a in GCN/LPA by studying the Jacobian/gradient of node v b with respect to node v a , and then we also prove their quantitative relationship.Based on the above theoretical analysis, we propose a unified model GCN-LPA for node classification.We show that the key to improving the performance of GCN is to enable nodes within the same class/label to connect more strongly with each other by making edge weights/strengths trainable.Then we prove that increasing the strength of edges between the nodes of the same class is equivalent to increasing the accuracy of LPA's predictions.Therefore, we can first learn the optimal edge weights by minimizing the loss of predictions in LPA, then plug the optimal edge weights into a GCN to learn node representations and do final classification.In GCN-LPA, we further combine the two steps together and train the whole model in an end-to-end fashion, where the LPA part serves as regularization to assist the GCN part in learning proper edge weights that benefit the separation of different classes.It is worth noticing that GCN-LPA can also be seen as learning attention weights for edges based on node label information, which requires less handcrafting and is more task-oriented than existing work that learns attention weights based on node feature similarity (Veli\u010dkovi\u0107 et al., 2018;Thekumparampil et al., 2018;Zhang et al., 2018;Liu et al., 2019b).Experiments on five datasets indicate that our model outperforms state-of-the-art methods in terms of classification accuracy."}
{"paper_id": 97, "abstract": "In this paper, we embark on a quest to forge a straightforward yet powerful reinforcement learning algorithm, one that harnesses the prowess of conventional supervised learning techniques as its foundational tools. Our ambition is to craft an algorithm that relies solely on simple, convergent maximum likelihood loss functions, all while adeptly utilizing off-policy data to enhance its learning journey.  We introduce our creation, the Advantage-Weighted Regression (AWR), a method that unfolds in two distinct yet harmonious supervised learning steps. The first step involves regressing toward target values to refine a value function, while the second focuses on regressing toward weighted target actions to optimize the policy itself. This approach is not only elegant and versatile, accommodating both continuous and discrete actions, but it can also be seamlessly implemented in just a few lines of code, building upon existing supervised learning frameworks.  Within the pages of this work, we provide a robust theoretical underpinning for AWR and delve into its characteristics when integrating off-policy data drawn from experience replay. Our evaluations span a diverse array of standard OpenAI Gym benchmark tasks, revealing that AWR stands shoulder to shoulder with a host of established state-of-the-art reinforcement learning algorithms. Notably, AWR excels in crafting more effective policies than many off-policy strategies, even when learning from static datasets devoid of any further environmental interaction.  Moreover, we push the boundaries of our algorithm by applying it to challenging continuous control tasks, navigating the intricacies of highly complex simulated characters. In doing so, we demonstrate not only the versatility of AWR but also its potential to redefine the landscape of reinforcement learning.", "introduction": "Model-free reinforcement learning can be a general and effective methodology for training agents to acquire sophisticated behaviors with minimal assumptions on the underlying task (Mnih et al., 2015;Heess et al., 2017;Pathak et al., 2017).However, reinforcement learning algorithms can be substantially more complex to implement and tune than standard supervised learning methods.Arguably the simplest reinforcement learning methods are policy gradient algorithms (Sutton et al., 2000), which directly differentiate the expected return and perform gradient ascent.Unfortunately, these methods can be notoriously unstable and are typically on-policy (or nearly on-policy), often requiring a substantial number of samples to learn effective behaviors.Our goal is to develop a reinforcement learning algorithm that is simple, easy to implement, and can readily incorporate off-policy experience data.In this work, we propose advantage-weighted regression (AWR), a simple off-policy algorithm for model-free RL.Each iteration of the AWR algorithm simply consists of two supervised regression steps: one for training a value function baseline via regression onto cumulative rewards, and another for training the policy via weighted regression.The complete algorithm is shown in Algorithm 1. AWR can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods.Despite its simplicity, we find that AWR achieves competitive results when compared to commonly used on-policy and off-policy RL algorithms, and can effectively incorporate fully off-policy data, which has been a challenge for other RL algorithms.Our derivation presents an interpretation of AWR as a constrained policy optimization procedure, and provides a theoretical analysis of the use of off-policy data with experience replay.We first revisit the original formulation of reward-weighted regression, an on-policy RL method that utilizes supervised learning to perform policy updates, and then propose a number of new design decisions that significantly improve performance on a suite of standard continuous control benchmark tasks.We then provide a theoretical analysis of AWR, including the capability to incorporate off-policy data with experience replay.Although the design of AWR involves only a few simple design decisions, we show experimentally that these additions provide for a large improvement over previous methods for regression-based policy search, such as reward-weighted regression (RWR) (Peters & Schaal, 2007), while also being substantially simpler than more modern methods, such as MPO (Abdolmaleki et al., 2018).We show that AWR achieves competitive performance when compared to several well-established state-of-the-art on-policy and off-policy algorithms.We further demonstrate our algorithm on challenging control tasks with complex simulated characters."}
{"paper_id": 98, "abstract": "In the realm of style transfer, recent advancements have illuminated pathways through artistic, semantic, and photo-realistic domains, yet an inherent rigidity persists. Current methodologies often find themselves shackled by the constraints of domain-specific style representations, which not only curtail their adaptability but also diminish the overall stylistic quality. To break these chains, we introduce Cascade Style Transfer\u2014a framework both elegant and potent, designed to enhance the quality and versatility of style transfer by seamlessly merging multiple existing techniques.  At the heart of our cascade lies two innovative architectures: Serial Style Transfer (SST) and Parallel Style Transfer (PST). The SST operates by taking the stylized output from one method and using it as the input content for subsequent processes, thereby enriching the stylistic quality with each iteration. Meanwhile, the PST employs a shared backbone and a unified loss module to concurrently optimize the loss functions of various methods, fostering improvements in both quality and flexibility. This dual approach not only elevates the stylistic output but also guides us toward discovering domain-independent strategies.  Our rigorous experiments span three pivotal style transfer domains\u2014artistic, semantic, and photo-realistic\u2014and the results are compelling. In each arena, our cascade framework demonstrates a marked superiority over the current state-of-the-art methods, heralding a new era in the quest for adaptable and high-quality style transfer.", "introduction": "Given the content and style images, the goal of style transfer is to synthesize an image that preserves some notion of the content but carries characteristics of the style.Recently, the seminal work of Gatys et al. (2015) firstly captured the style of artistic images and transferred it to other images using Convolutional Neural Networks (CNNs).Since then, various Neural Style Transfer (NST) Jing et al. (2017) methods have been advanced and obtained visually pleasing results.Despite the recent rapid progress, these existing works often limited to one or few specific domains (in this paper, we mainly focus on three domains: artistic, semantic and photo-realistic).For instance, Li et al. (2017b); Huang & Belongie (2017); Gatys et al. (2016) can transfer the artistic styles well, but they perform poorly on the style transfer of photographs and corresponding semantics.Luan et al. (2017); Li et al. (2018) specialize in photo-realistic style transfer, and Li & Wand (2016); Champandard (2016) mainly target semantic style transfer.Fortunately, there are some multi-domain approaches which can perform well on multiple domains, e.g., Liao et al. (2017) can perform well on semantic and photo-realistic style transfer, Gu et al. (2018) is suitable for artistic and semantic style transfer, and Li et al. (2019) can adapt to artistic and photo-realistic style transfer.Nevertheless, they still have some limitations, and the quality could be further improved (see Fig. 1 (a)).That is to say, nowadays, it is still inconvenient to users that have to choose the appropriate methods for specific domains.In this sense, finding a common approach which could perform well in all style transfer domains is extremely hard but significant.As a coin has two sides, every existing NST method has both advantages and shortcomings.Fig. 1 (b) shows some typical examples, we can observe that using Gram matrices Gatys et al. (2016) to transfer the artistic styles performs well on global color, but fails to capture enough local patterns (e.g., circles and droplets).Patch-based method Li & Wand (2016) can alleviate this problem, but may cause insufficient color.Is there a way to combine the advantages of both and overcome their shortcomings?Obviously, redesigning a new algorithm is difficult, why not use some simpler ways, such as combining existing methods directly through some general architectures?Based on the above analyses, we propose Cascade Style Transfer (CST) mainly for two targets, i.e., higher quality and higher flexibility, and design two architectures, i.e., the Serial Style Transfer (SST) and the Parallel Style Transfer (PST) for these targets.In this work, we first revisit and demonstrate the impact of different initialization strategies on style transfer, and inspired by this, design our SST for higher quality domain-specific style transfer.Moreover, we develop upon this and further propose our PST for more flexible style transfer, this could guide us to create domainindependent approaches.As far as we know, this is the first paper to propose domain-independent style transfer (note that this kind of approach is flexible for arbitrary images in arbitrary domains, while existing so-called arbitrary style transfer methods are only flexible for arbitrary images in specific domains), and also the first attempt to combine multiple existing approaches directly to improve the quality and flexibility of style transfer.The main contributions of our work are:\u2022 We revisit the initialization of style transfer, and demonstrate that initialization can play an important role in improving the quality of style transfer.\u2022 We propose a serial architecture for cascade style transfer, it is simple yet effective, which could help improve the quality of domain-specific style transfer.\u2022 We first propose domain-independent style transfer, and design a parallel architecture to help improve the quality and flexibility of style transfer."}
{"paper_id": 99, "abstract": "In this paper, we unveil FoveaBox, a groundbreaking framework for object detection that stands apart for its precision, adaptability, and, most notably, its complete departure from traditional anchor-based methods. While the vast majority of cutting-edge object detectors rely on predefined anchors to navigate the complex landscape of potential object locations, scales, and aspect ratios, they often find their performance shackled by the very designs they depend upon. FoveaBox, however, boldly breaks this mold by directly learning the likelihood of object presence and the coordinates of bounding boxes\u2014free from the constraints of anchor references.  Our approach unfolds in two pivotal ways: first, by predicting category-sensitive semantic maps that illuminate the probability of object existence, and second, by generating category-agnostic bounding boxes for each position that might harbor an object. The scales of these target boxes seamlessly align with feature pyramid representations, allowing for a natural integration of multi-scale information.  Through rigorous evaluation on standard benchmarks, we demonstrate the formidable effectiveness of FoveaBox, achieving state-of-the-art performance with a single model on the COCO detection benchmark. Remarkably, this achievement comes without the computational overhead and hyper-parameter tuning typically associated with anchor boxes\u2014elements often fraught with sensitivity that can undermine detection accuracy.   We envision that this straightforward yet powerful methodology will not only serve as a robust baseline but will also pave the way for future advancements in the realm of object detection. With FoveaBox, we invite researchers to explore a new horizon, one unencumbered by the limitations of the past.", "introduction": "Object detection requires the solution of two main tasks: recognition and localization.Given an arbitrary image, an object detection system needs to determine whether there are any instances of semantic objects from predefined categories and, if present, to return the spatial location and extent.To add the localization functionality to generic object detection systems, sliding window approaches have been the method of choice for many years (Lampert et al., 2008;Felzenszwalb et al., 2010;Liu et al., 2018).Recently, deep learning techniques have emerged as powerful methods for learning feature representations automatically from data (Simonyan & Zisserman, 2014;He et al., 2016;Huang et al., 2017a).For object detection, the anchor-based Region Proposal Networks (Ren et al., 2015) are widely used to serve as a common component for searching possible regions of interest for modern object detection frameworks (Liu et al., 2016;He et al., 2017;Lin et al., 2018).In short, anchor method suggests dividing the box space into discrete bins and refining the object box in the corresponding bin.Most state-of-the-art detectors rely on anchors to enumerate the possible locations, scales, and aspect ratios for target objects (Liu et al., 2018).Anchors are regression references and classification candidates to predict proposals for two-stage detectors or final bounding boxes for single-stage detectors.Nevertheless, anchors can be regarded as a feature-sharing sliding window scheme to cover the possible locations of objects.IoU-based matching with ground-truth box Training target However, anchors must be carefully designed and used in object detection frameworks.(a) One of the most important factors in designing anchors is how densely it covers the instance location space.To achieve a good recall rate, anchors are carefully designed based on the statistics computed from the training/validation set (Lin et al., 2018).(b) One design choice based on a particular dataset is not always applicable to other applications, which harms the generality (Yang et al., 2018).(c) At training phase, anchor-methods rely on the intersection-over-union (IoU) to define the positive/negative samples, which introduces additional computation and hyper-parameters for an object detection system (Wang et al., 2019).In contrast, our human vision system can recognize the instance in space and predict the boundary given the visual cortex map, without any pre-defined shape template (Bear et al., 2007).In other words, we human naturally recognize the object in the visual scene without enumerating the candidate boxes.Inspired by this, an intuitive question to ask is, is the anchor scheme the optimal way to guide the search of objects?And further, could we design an accurate object detection framework without anchors or candidate boxes?Without anchors, one may expect a complex method is required to achieve comparable performance.However, we show that a surprisingly simple and flexible system can match, even surpass the prior state-of-the-art object detection results without any requirement of candidate boxes.To this end, we present FoveaBox, a completely anchor-free framework for object detection.Fove-aBox is motivated from the fovea of human eyes: the center of the vision field is with the highest visual acuity (Fig. 2 left), which is necessary for activities where visual detail is of primary importance (Iwasaki & Inomata, 1986).FoveaBox jointly predicts the locations where the object's center area is likely to exist as well as the bounding box at each valid location.In FoveaBox, each target object is predicted by category scores at center area, associated with 4-d bounding box, as shown in Fig. 2 right.At training phase, we do not need to utilize anchors, or IoU matching to generate training target.Instead, the training target is directly generated by ground-truth boxes.To demonstrate the effectiveness of the proposed detection scheme, we combine the recent progress of feature pyramid networks and our detection head to form the framework of FoveaBox.Without bells and whistles, FoveaBox gets state-of-the-art single-model results on the COCO object detection task.Compared with the anchor-based RetinaNet, FoveaBox gets 2.2 AP gains, which also surpasses most of previously published anchor based single-model results.We believe the simple training/inference manner of FoveaBox, together with the flexibility and accuracy, will benefit future research on object detection and relevant topics."}
{"paper_id": 100, "abstract": "In the ever-evolving realm of deep learning, the quest for powerful and efficient neural networks stands as a pivotal endeavor. Imagine, if you will, the architecture of a neural network as a grand tapestry woven from the threads of a directed acyclic graph. Here, each node represents a layer's transformative magic, while the edges signify the flow of information\u2014an intricate dance of data. Yet, amidst the careful selection of micro node operations, it is the overarching connections\u2014the very topology of the network\u2014that wields significant influence over the optimization journey.  In this exploration, we embark on a fresh perspective, reimagining residual connections through a novel topological lens. This new view reveals the profound advantages that dense connections can offer, enhancing the optimization process in ways previously uncharted. Inspired by these revelations, we introduce a groundbreaking method to refine the topology of neural networks. We define our optimization landscape as a complete graph, where learnable weights illuminate the significance of each connection. Thus, the intricate task of optimizing topology transforms into the art of learning a set of continuous variables that govern the edges.  To traverse the vast expanses of larger search spaces, we unveil a new series of networks, aptly named TopoNet. These innovative structures are designed not only to spotlight critical edges but also to bolster generalization capabilities within the densely connected realms. To achieve this, we incorporate an auxiliary sparsity constraint, skillfully guiding the distribution of edges.  Our rigorous experiments with classical networks illuminate the effectiveness of our topological optimization approach. Yet, the true test lies in the TopoNets, which demonstrate both their robustness and adaptability across diverse tasks\u2014spanning image classification, object detection, and face recognition. In this journey, we uncover the transformative potential of optimized network topology, paving the way for future advancements in the deep learning landscape.", "introduction": "Deep learning successfully transits the feature engineering from manual to automatic design.As a tendency, seeking effective neural networks gradually becomes an important and practical direction.Topologically, the architecture of a network can be expressed as a direct acyclic graph, whose nodes denote the transformation of layers and edges represent information flow.Corresponding to micro operations within a layer/block, the macro connections (Atwood & Towsley, 2016;P\u00e9rez-R\u00faa et al., 2018) between layers also experienced a series of evolutions.And we denote the macro connections as topology in this work.In initial literature, AlexNet (Krizhevsky et al., 2012), VGGNet (Simonyan & Zisserman, 2014) with plain topology were proposed.Due to the problems of gradient vanishing and exploding, extending the network to a deeper level for better representation is nearly difficult.To better adapt the optimization process of gradient descent, GoogleNet (Szegedy et al., 2015) adopted parallel modules, and Highway networks (Srivastava et al., 2015) utilized gating units to regular the flow of information, resulting in multipath and elastic topologies.Driven by the significance of depth, the residual block consisted of residual mapping and shortcut was raised in ResNet (He et al., 2016).Topological changes in neural networks successfully scaled up neural networks to hundreds or even thousands of layers.The residual topology was widely approved and applied in the following works, e.g.MobileNet (Sandler et al., 2018;Howard et al., 2019) and ShuffleNet (Zhang et al., 2018).Divergent from relative sparse topologies, DenseNet (Huang et al., 2017) wired densely within a block to reuse features fully.Recent advances in computer vision also explore neural architecture search (NAS) methods (Zoph et al., 2018;Liu et al., 2019b;Tan et al., 2019) to jointly search micro operations and topology.To trade-off efficiency and performance, their topologies are handdesigned stacked patterns and constrained in limited searching spaces.Orthogonally, randomly wired networks (Xie et al., 2019) explored random topology as architecture by different generators, and obtained competitive performance.These trends reflect the great impact of topology on the Figure 1: From natural view to topological view of networks with residual connections.We map both addition and unary layer to nodes, and data flow to edges.Red node denotes input x and green one means output y.Red arrows give an example of this mapping for a node with in-degree of 3.optimization of neural networks.Echoing this perspective, we wonder: Can topology itself in neural networks be optimized?What is the suitable route to do this?To answer these questions, we start by understanding the most representative residual connection in terms of topology.It formulates x + F(x) as a basic component, in which x represents the identity shortcut, and F(x) denotes the residual mapping.Figure 1 presents two residual architectures with a interval of 1 and 2 respectively.And the number of intervals represents the number of layers that make up a block.If we map both combining (e.g., addition) and transformation (e.g., 3 \u00d7 3 conv) to a node, and flow of features to an edge, then the architecture can be expressed as a directed acyclic graph (DAG).From the topological view, the residual connections are rather denser than the natural perspective.This novel representation illustrates residual networks perform multiple feed-forward paths instead of a single deep network.When the interval degrades to 1, its topology evolves into a complete DAG.For a complete DAG with n nodes, the number of available paths from input to output is (n -1)!.As a result, most layers are directly connected to the input and output, resulting in direct access to the gradients and the original input.So we make a hypothesis that dense wirings in topology benefit the optimization process.To verify the hypothesis and explore the effects of differently wired topologies, toy experiments on CIFAR-100 (Krizhevsky et al., 2009) are conducted.To ensure universality, three types of layer are selected respectively, including 3 \u00d7 3 conv, 3 \u00d7 3 inverted bottleneck (Sandler et al., 2018) and 3 \u00d7 3 separable depthwise conv (Howard et al., 2017).Defining the total number of nodes, we establish three series of networks under different intervals.Loss curves during training and test performance are given in Figure 2. It demonstrates an obvious topology-induced impact on optimization.First, it shows denser networks (intervals of 1 and 2) can achieve lower training losses in the majority of cases.Second, the optimal topology is different for different node types (2 for conv, 1 for depthwise and inverted bottleneck).It is possible due to that not all connections are beneficial, and the aggregation of features should be discretionary.So we modify the hypothesis as the topology with dense and effective connections are necessary for networks that are compatible with the optimization process.Regardless of the node types, the optimal structure is always a subset of the complete graph.Naturally, finding the optimal topology in a full search space is equivalent to finding the optimal sub-graph in the complete graph.To focus on the optimization of topology itself, in this work, we exclude the influence of the mixture of different layers/nodes and select the complete DAG as the search space.Through assigning learnable weights which reflect the importance of connections of edges, the topology can be optimized through gradient descent.Then task-related architectures can be achieved conveniently.Besides, this optimization method is also compatible with existing networks.Given a basic network, optimization of topology can lead to improvement without additional computing burdens.To verify the effectiveness of the optimization of topology in larger search spaces, we also propose a welldesigned network named TopoNet.To solve the optimization problem in a large space and promote generalization, we attach auxiliary sparsity regularization to the distribution of edges, resulting in critical connections and better generalization.Our contributions are as follows:\u2022 We first rethink the residual connections via a new topological view and observe the benefits provided by dense connections to the optimization.\u2022 We propose an innovative method to optimize the topology of a neural network.The optimization space is defined as a complete graph, through assigning learnable weights which reflect the importance of connections, the optimization of topology is transformed into learning a set of continuous variables of edges.\u2022 The proposed optimization method of topology is effective for existing networks.\u2022 When extended to larger search space, TopoNets and optimization using sparsity constraint verify the availability and transferability in different tasks, including image classification on ImageNet, object detection on COCO and face recognition on MegaFace."}
{"paper_id": 101, "abstract": "In the ever-shifting landscape of user preferences, temporal information emerges as a beacon of insight for recommendation systems. As we navigate the complexities of human choice, recent strides in deep learning\u2014particularly the advent of innovative attention mechanisms and cutting-edge architectures\u2014have unlocked new dimensions in harnessing the temporal sequences of user interactions. Among these advancements, the SASRec model, drawing inspiration from the acclaimed Transformer architecture, has set a new benchmark in performance. Yet, like its predecessor, SASRec remains a unidimensional entity, lacking the nuanced touch of personalized user embeddings.  To address this critical shortcoming, we present the Personalized Transformer (SSE-PT), a model that elevates the art of recommendation by nearly 5% in NDCG@10 across five diverse real-world datasets. Our exploration into the engagement histories of various users reveals that SSE-PT not only enhances interpretability but also zeroes in on recent interaction patterns unique to each individual. Moreover, in a further refinement of our approach, the SSE-PT++ variant emerges, adeptly managing exceptionally long sequences while maintaining a competitive training speed, thus achieving a harmonious balance between performance and efficiency.  At the heart of our innovation lies the novel application of Stochastic Shared Embeddings (SSE) regularization, a cornerstone of our personalization strategy. We invite the community to explore our findings, with all code and data freely accessible at https://github.com/SSE-PT/SSE-PT. In this journey through the intricate tapestry of user preferences, we stand on the precipice of a new era in recommendation systems, where personalization reigns supreme.", "introduction": "The sequential recommendation problem has been an important open research question, yet using temporal information to improve recommendation performance has proven to be challenging.SASRec, proposed by (Kang and McAuley, 2018) for sequential recommendation problems, has achieved stateof-the-art results and enjoyed more than 10x speed-up when compared to earlier CNN/RNN-based methods.However, the model used in SASRec is the standard Transformer which is inherently an un-personalized model.In practice, it is important to include a personalized Transformer in SASRec especially for recommender systems, but (Kang and McAuley, 2018) found that adding additional personalized embeddings did not improve the performance of their Transformer model, and postulate that the failure of adding personalization is due to the fact that they already use the user history and the user embeddings only contribute to overfitting.In this work, we propose a novel method, Personalized Transformer (SSE-PT), that successfully introduces personalization into self-attentive neural network architectures.Introducing user embeddings into the standard transformer model is intrinsically difficult with existing regularization techniques, as unavoidably a large number of user parameters are introduced, which is often at the same scale of the number of training data.But we show that personalization can greatly improve ranking performance with a recent regularization technique called Stochastic Shared Embeddings (SSE) (Wu et al., 2019).The personalized Transformer (SSE-PT) model with SSE regularization works well for all 5 real-world datasets we consider without overfitting, outperforming previous state-of-the-art algorithm SASRec by almost 5% in terms of NDCG@10.Furthermore, after examining some random users' engagement history, we find our model is not only more interpretable but also able to focus on recent engagement patterns for each user.Moreover, our SSE-PT model with a slight modification, which we call SSE-PT++, can handle extremely long sequences and outperform SASRec in ranking results with comparable training speed, striking a balance between performance and speed requirements."}
{"paper_id": 102, "abstract": "In the realm of neural networks, where the battle between robustness and vulnerability rages on, adversarial training stands as a beacon of hope. It offers a structured approach to forge resilient classifiers amidst the chaos of adversarial attacks. At its core, this adversarial training embodies a daunting minimax optimization challenge: the outer layer seeks to craft a steadfast classifier, while the inner layer strives to conjure up the most cunning adversarial samples. Yet, the path is fraught with difficulty, for this minimax problem lacks the elegant convex-concave structure that would make it manageable.  In this work, we unveil a novel adversarial training method, rooted in a versatile learning-to-learn (L2L) framework. Rather than relying on traditional, hand-crafted algorithms to tackle the inner optimization challenge, we take a bold step forward by training an optimizer\u2014one that is elegantly parameterized as a convolutional neural network. Concurrently, we cultivate a robust classifier, designed to withstand the onslaught of adversarial attacks generated by our learned optimizer.  Our experiments, conducted on the formidable CIFAR-10 and CIFAR-100 datasets, reveal a remarkable truth: the L2L approach not only surpasses existing adversarial training methods in classification accuracy but also excels in computational efficiency. Furthermore, the adaptability of our L2L framework extends beyond mere classification; it holds the potential to enhance generative adversarial imitation learning, bringing stability to the training process. In this unfolding narrative of machine learning, we stand on the brink of a new era, where learning to learn becomes the key to unlocking the true power of robust neural networks.", "introduction": "This decade has witnessed great breakthroughs in deep learning in a variety of applications, such as computer vision (Taigman et al., 2014;Girshick et al., 2014;He et al., 2016;Liu et al., 2017).Recent studies (Szegedy et al., 2013), however, show that most of these deep learning models are very vulnerable to adversarial attacks.Specifically, by injecting a small perturbation to a normal sample, one can obtain an adversarial example.Although the adversarial example is semantically indistinguishable from the normal one, it can fool deep learning models and undermine the security of deep learning, causing reliability problems in autonomous driving, biometric authentication, etc.Researchers have devoted many efforts to studying efficient adversarial attack and defense (Szegedy et al., 2013;Goodfellow et al., 2014b;Nguyen et al., 2015;Zheng et al., 2016;Madry et al., 2017;Carlini and Wagner, 2017).There is a growing body of work on generating adversarial examples, e.g., fast gradient sign method (FGSM, Goodfellow et al. (2014b)), projected gradient method (PGM, Kurakin et al. (2016)), Carlini-Wagner (CW, Paszke et al. (2017)) etc.As for defense, Goodfellow et al. (2014b) propose to robustify the network by adversarial training, which trains over the adversarial examples and still requires the network to output the correct label.Further, Madry et al. (2017) formalize the adversarial training as the following minimax optimization problem:where {(x i , y i )} n i=1 \u2282 R d \u00d7 Y are n pairs of input feature and the corresponding label, denotes a loss function, f (\u2022; \u03b8) denotes the neural network with parameter \u03b8, and \u03b4 i \u2208 B denotes the perturbation for x i in constraint B. The existing literature on the optimization also refers to \u03b8 as the primal variable and \u03b4 i 's as the dual variables.Different from the well-studied convex-concave problemfoot_0 , problem (1) is very challenging since is nonconvex in \u03b8 and nonconcave in \u03b4.As a result, there may be many equilibria, and majority of them are unstable.In the existing optimization literature, there is no algorithm to converge to a stable equilibrium with theoretical guarantees.Empirically, the existing primal-dual algorithms may perform poorly for solving (1).Minimax formulation (1) naturally provides us with a unified perspective on prior works of adversarial training.Such a minimax problem consists of two optimization problems, an inner maximization problem and an outer minimization problem: The inner problem targets on finding an optimal attack for a given data point (x, y) maximizing the loss, which essentially is the adversarial attack; The outer problem aims to find a \u03b8 so that the loss given by the inner problem is minimized.For solving (1), Goodfellow et al. (2014b) propose to use FGSM to solve the inner problem.Kurakin et al. (2016) then find that FGSM with true label predicted suffers from a \"label leaking\" effect, which can ruin the adversarial training.Madry et al. (2017) further suggest to solve the inner problem by PGM and obtain a better result than FGSM, since FGSM essentially is one iteration PGM.However, adversarial training needs to find a \u03b4 i for each (x i , y i ), thus the dimension of the overall search space for all data is substantial, which makes the computation expensive.Instead, we propose a new learning-to-learn (L2L) framework that provides a more principled and efficient way for adversarial training.Specifically, we parameterize the optimizer of the inner maximization problem by a neural network denoted by g(A(x, y, \u03b8); \u03c6), where A(x, y, \u03b8) denotes the input of the optimizer g.We also call the optimizer as the attacker network.Since the neural network is very powerful in function approximation, our parameterization ensures that g is able to yield strong adversarial perturbations.Under our framework, instead of directly solving \u03b4 i , we update the parameter \u03c6 of g.Our training procedure becomes updating the parameters of two neural networks, which is very similar to generative adversarial network (GAN, Goodfellow et al. (2014a)).The proposed L2L is a generic framework and can be extended to other minimax optimization problems, e.g., generative adversarial imitation learning, which is studied in Section 4.Different from the hand-designed methods that compute the adversarial perturbation for each individual sample using gradients from backpropagation, our methods generate the perturbations for all samples through the shared attacker g.This enables the attacker g to learn potential common structures of the perturbations.Therefore, our method is capable of yielding strong perturbations and accelerating the training process.Furthermore, L2L framework is very flexible: we can either choose different input A(x, y, \u03b8), or use different attacker architecture.For example, we can include gradient information in A(x, y, \u03b8) and use a recurrent neural network (RNN) to mimic multi-step gradient-type methods.Instead of simply computing the high order information with finite difference approximation or multiple gradients, by parameterizing the algorithm as a neural network, our proposed methods can capture this information in a much smarter way (Finn et al., 2017).Our experiments demonstrate that our proposed methods not only outperform existing adversarial training methods, e.g., PGM training, but also enjoy the computational efficiency over CIFAR-10 and CIFAR-100 datasets (Krizhevsky and Hinton, 2009).The research on L2L has a long history (Schmidhuber, 1987;1992;1993;Younger et al., 2001;Hochreiter et al., 2001;Andrychowicz et al., 2016).The basic idea is that one first models the updating formula of complicated optimization algorithms in a parametric form, and then uses some simple algorithms, e.g., stochastic gradient algorithm to learn the parameters of the optimizer.Among existing works, Hochreiter et al. (2001) propose a system allowing the output of backpropagation from one network to feed into an additional learning network, with both networks trained jointly; Based on this, Andrychowicz et al. (2016) further show that the design of an optimization algorithm can be cast as a learning problem.Specifically, they use long short-term memory RNNs to model the algorithm and allow the RNNs to exploit structure in the problems of interest in an automatic way, which is undoubtedly one of the most popular methods for learning-to-learn.However, there are two major drawbacks of the existing L2L methods: (1) It requires a large amount of datasets (or a large number of tasks in multi-task learning) to guarantee the learned optimizer to generalize, which significantly limits their applicability (most of the related works only consider the image encoding as the motivating application); (2) The number of layers/iterations in RNNs for modeling algorithms cannot be large to avoid significant computational burden in backpropagation.Our contribution is that we fill the blank of the L2L framework in solving the minimax problem, and our proposed methods do not suffer from the aforementioned drawbacks: (1) The attacker g with a different \u03c6 essentially generates a different task/dataset.Therefore, for adversarial training, we have sufficiently many tasks for learning-to-learn; (2) The inner problem does not need a large scale RNN, and we use a convolutional neural network (CNN) or a length-two RNN (the sequence of length equals 2) as our attacker network, which eases the computation.Our work is also related to GAN and dual-embedding (Dai et al., 2016).All three solve minimax problems and share some common ground.We discuss these works in Section 5.Notations.Given a \u2208 R, denote (a) + as max(a, 0).Given x, y \u2208 R d , denote x i as the i-th element of x, ||x|| \u221e = max i |x i | as \u221e -norm of x, and x\u2022y = [x 1 y 1 , \u2022 \u2022 \u2022 , x d y d ] as element-wise product.This paper focuses on \u221e -norm attack.We define the \u221e -ball with center 0 and radius by B( ) = {\u03b4 \u2208 R d : ||\u03b4|| \u221e \u2264 } and the corresponding projection as follows:"}
{"paper_id": 103, "abstract": "In the ever-evolving realm of graph-structured data, we unveil a groundbreaking method known as Deep MinCut (DMC). This innovative approach transcends traditional boundaries by harnessing the power of unsupervised learning to craft node embeddings that resonate with the intricate tapestry of community membership. No longer do we require the cumbersome step of separate node clustering; DMC elegantly captures the essence of graph structure through its embeddings, offering profound insights into the relationships that bind nodes together.  At the heart of DMC lies a dual pursuit: the simultaneous learning of both node embeddings and the communities they inhabit, achieved through the meticulous minimization of mincut loss. This loss function deftly quantifies the connections between communities, ensuring that our method not only identifies clusters but also respects the delicate interplay of their interactions.   In our quest for scalability, we introduce a training regimen grounded in the use of minibatches, allowing DMC to flourish even in the face of vast datasets. Empirical evidence supports our claims, as we demonstrate that the communities unearthed by DMC are not only coherent but also rich in meaning. Furthermore, the node embeddings produced stand tall against a variety of node classification benchmarks, proving their competitive edge. With DMC, we embark on a new chapter in the understanding of graph structures, one where the mysteries of connectivity and community are unveiled with clarity and precision.", "introduction": "Graphs are a natural representation of relations between entities in complex systems, such as social networks or information networks.To enable inference on graphs, a graph embedding may be learned.It comprises node embeddings, each being a vector-based representation of a graph's node that incorporates its relations to other nodes (Goyal & Ferrara, 2018;Hamilton et al., 2017b).While supervised node embeddings have received a lot of attention, most real-world graphs are not labelled, which calls for unsupervised learning techniques.The main principle in unsupervised learning of node embeddings is that \"similar\" nodes have close embeddings in the embedding space.The similarity of nodes is often defined based on their distance in a graph, e.g., based on their co-occurrence probability in a random walk (Goyal & Ferrara, 2018;Perozzi et al., 2014;Grover & Leskovec, 2016).Recently, it was also argued that two nodes should be similar, if they are similar to a graph summary representation (Veli\u010dkovi\u0107 et al., 2018).In this work, we argue that node embeddings shall not only be of high quality for inference tasks, but shall also be meaningful.That is, they shall directly provide insights into interesting structures in a graph in order to avoid a potentially biased post-analysis step, e.g., through clustering of the embeddings.We therefore assess node similarity from the perspective of node communities, where the dimensions of embeddings are some unknown communities instead of some unknown latent features as in traditional techniques.Considering a community as a set of densely connected nodes with sparse connections to outside nodes, the homophily principle is restated as follows: Nodes with similar community membership characteristics shall have close embeddings.Specifically, for each node, we incorporate membership information as a probability distribution over a set of communities.Then, nodes are similar, if they are both likely and unlikely to be part of the same communities.Since communities are generally unknown, we propose to minimize the mincut loss for unsupervised learning of communities and node embeddings simultaneously.Mincut loss leverages the principle that communities are well-separated if there are few connections between them (Fortunato, 2010).It is theoretically motivated as its optimal closed-form solution can be found, while its variant, the normalized cut, is a well-studied problem.Aiming at a realisation of the above idea, we propose Deep MinCut (DMC), a neural network approach to minimize mincut loss.We learn node embeddings to sample one-hot vectors that represent the assignment of nodes to communities.The vectors are drawn from distributions parameterized by continuous node embeddings using Gumbel-Softmax (Jang et al., 2016;Maddison et al., 2016).This renders the process differentiable and, thus, enables joint learning of embeddings and communities.We demonstrate the applicability of DMC in various applications.In node classification, our node embeddings turn out to outperform traditional embedding techniques (Grover & Leskovec, 2016;Perozzi et al., 2014;Veli\u010dkovi\u0107 et al., 2018), while also revealing the graph's community structure.In community detection, by stacking mincut losses, we are able to learn a hierarchy of communities, e.g., when generating word embeddings, we can link words to topics, and topics to abstract themes."}
{"paper_id": 104, "abstract": "In the realm of news summarization, a curious phenomenon known as lead bias often emerges, where the opening lines of an article tend to house its most critical insights. While numerous algorithms have sought to harness this tendency for summary generation, the unintended consequence is a diminished capacity for models to discern and extract vital information from the broader text. However, we propose a novel approach that turns this lead bias into an ally. By pretraining abstractive news summarization models on a vast, unlabelled corpus, we focus on the task of predicting leading sentences based on the remainder of the article. Through meticulous data cleaning and filtering, our transformer-based pretrained model achieves astonishing results across various news summarization challenges, all without the need for finetuning. Yet, when we do apply finetuning, the performance soars even higher, eclipsing numerous competitive baseline models. Notably, our pretrained model outstrips the pointer-generator network on the CNN/DailyMail dataset. And in a remarkable display of prowess, the finetuned version achieves an impressive 3.2% increase in ROUGE-1, 1.6% in ROUGE-2, and 2.1% in ROUGE-L scores compared to the leading baseline on the XSum dataset. Thus, we unveil a powerful new methodology that not only leverages lead bias but transforms it into a formidable tool for advancing news summarization technology.", "introduction": "The goal of text summarization is to condense a piece of text into a shorter version that contains the salient information.Due to the prevalence of news articles and the need to provide succinct summaries for readers, a majority of existing datasets for summarization come from the news domain (Hermann et al., 2015;Sandhaus, 2008;Narayan et al., 2018).However, according to journalistic conventions, the most important information in a news report usually appears near the beginning of the article (Kedzie et al., 2018;Jung et al., 2019).While it facilitates faster and easier understanding of the news for readers, this lead bias causes undesirable consequences for summarization models.The output of these models is inevitably affected by the positional information of sentences.Furthermore, the simple baseline of using the top few sentences as summary can achieve a stronger performance than many sophisticated models (See et al., 2017).It can take a lot of effort for models to overcome the lead bias Kedzie et al. (2018).Additionally, most existing summarization models are fully supervised and require time and laborintensive annotations to feed their insatiable appetite for labeled data.For example, the New York Times Annotated Corpus (Sandhaus, 2008) contains 1.8 million news articles, with 650,000 summaries written by library scientists.Therefore, some recent work (Gusev, 2019) explores the effect of domain transfer to utilize datasets other than the target one.But this method may be affected by the domain drift problem and still suffers from the lack of labelled data.The recent promising trend of pretraining models (Devlin et al., 2018;Radford et al., 2018) proves that a large quantity of data can be used to boost NLP models' performance.Therefore, we put forward a novel method to leverage the lead bias of news articles in our favor to conduct large-scale pretraining of summarization models.The idea is to leverage the top few sentences of a news article as the target summary and use the rest as the content.The goal of our pretrained model is to generate an abstractive summary given the content.Coupled with careful data filtering and cleaning, the lead bias can provide a delegate summary of sufficiently good quality, and it immediately renders the large quantity of unlabeled news articles corpus available for training news summarization models.We employ this pretraining idea on a three-year collection of online news articles.We conduct thorough data cleaning and filtering.For example, to maintain a quality assurance bar for using leading sentences as the summary, we compute the ratio of overlapping non-stopping words between the top 3 sentences and the rest of the article.As a higher ratio implies a closer semantic connection, we only keep articles for which this ratio is higher than a threshold.We end up with 21.4M articles based on which we pretrain a transformer-based encoder-decoder summarization model.We conduct thorough evaluation of our models on five benchmark news summarization datasets.Our pretrained model achieves a remarkable performance on various target datasets without any finetuning.This shows the effectiveness of leveraging the lead bias to pretrain on large-scale news data.We further finetune the model on target datasets and achieve better results than a number of strong baseline models.For example, the pretrained model without finetuning obtains state-of-the-art results on DUC-2003 andDUC-2004.The finetuned model obtains 3.2% higher ROUGE-1, 1.6% higher ROUGE-2 and 2.1% higher ROUGE-L scores than the best baseline model on XSum dataset (Narayan et al., 2018).Human evaluation results also show that our models outperform existing baselines like pointer-generator network.The rest of paper is organized as follows.We introduce related work in news summarization and pretraining in Section 2. We describe the details of pretraining using lead bias in Section 3. We introduce the transformer-based summarization model in Section 4. We show the experimental results in Section 5 and conclude the paper in Section 6."}
{"paper_id": 105, "abstract": "In the realm of visual perception, natural images serve as mere shadows of the intricate 3D objects that cast them upon a 2D canvas. While contemporary 2D generative models, such as GANs, have achieved remarkable feats in capturing the essence of this visual manifold, a lingering question remains: Do they truly grasp the underlying 3D structures that give rise to these images? More importantly, how might we harness this latent knowledge to unveil the hidden shapes of the objects contained within?   In this work, we embark on a pioneering quest to extract 3D geometric insights directly from a standard 2D GAN, one that has been trained solely on RGB images. Our exploration reveals a surprising truth: this pre-trained GAN is imbued with a wealth of 3D understanding, empowering us to reconstruct 3D shapes from a solitary 2D image in an unsupervised fashion. At the heart of our framework lies an iterative strategy, deftly navigating the diverse landscapes of viewpoint and lighting variations present in the GAN\u2019s image manifold. Remarkably, our approach does not rely on 2D keypoints, 3D annotations, or stringent assumptions about object symmetry. Yet, it adeptly recovers 3D shapes with exceptional precision for a variety of subjects, including human faces, cats, cars, and buildings.   The 3D shapes we unveil pave the way for high-quality image manipulations, such as relighting and object rotation. We substantiate the efficacy of our method through rigorous quantitative comparisons with prior techniques in both 3D shape reconstruction and face rotation. For those eager to delve deeper, our code awaits at https://github.com/XingangPan/GAN2Shape.", "introduction": "Generative adversarial networks (GANs) (Goodfellow et al., 2014) are capable of modeling the 2D natural image manifold (Zhu et al., 2016) of diverse object categories with high fidelity.Recall the fact that natural images are actually the projections of 3D objects to the 2D plane, an ideal 2D image Starting with an initial ellipsoid 3D shape (viewed in surface normal), our approach renders various 'pseudo samples' with different viewpoints and lighting conditions.GAN inversion is applied to these samples to obtain the 'projected samples', which are used as the ground truth of the rendering process to refine the initial 3D shape.This process is repeated until more precise results are obtained.manifold should be able to reflect some underlying 3D geometrical properties.For example, it is shown that a GAN could shift the object in its generated images (e.g., human faces) in a 3D rotation manner, as a direction in the GAN image manifold may correspond to viewpoint change (Shen et al., 2020).This phenomenon motivates us to ask -\"Is it possible to reconstruct the 3D shape of a single 2D image by exploiting the 3D-alike image manipulation effects produced by GANs?\"Despite its potential to serve as a powerful method to learn 3D shape from unconstrained RGB images, this problem remains much less explored.Some previous attempts (Lunz et al., 2020;Henzler et al., 2019;Szab\u00f3 et al., 2019) also adopt GANs to learn 3D shapes from images, but they rely on explicitly modeling 3D representation and rendering during training (e.g.3D voxels, 3D models).Due to either heavy memory consumption or additional training difficulty brought by the rendering process, the qualities of their generated samples notably lag behind their 2D GAN counterparts.Another line of works (Wu et al., 2020;Goel et al., 2020;Tulsiani et al., 2020;Li et al., 2020) for unsupervised 3D shape learning generally learns to infer the viewpoint and shape for each image in an 'analysis by synthesis' manner.Despite their impressive results, these methods often assume object shapes are symmetric (symmetry assumption) to prevent trivial solutions, which is hard to generalize to asymmetric objects such as 'building'.We believe that existing pre-trained 2D GANs, without above specific designs, already capture sufficient knowledge for us to recover the 3D shapes of objects from 2D images.Since the 3D structure of an instance could be inferred from images of the same instance with multiple viewpoint and lighting variations, our insight is that we may create these variations by exploiting the image manifold captured by 2D GANs.However, the main challenge is to discover well-disentangled semantic directions in the image manifold that control viewpoint and lighting in an unsupervised manner, as manually inspect and annotate the samples in the image manifold is laborious and time-consuming.To tackle the above challenge, we observe that for many objects such as faces and cars, a convex shape prior like ellipsoid could provide a hint on the change of their viewpoints and lighting conditions.Inspired by this, given an image generated by GAN, we employ an ellipsoid as its initial 3D object shape, and render a number of unnatural images, called 'pseudo samples', with various randomly-sampled viewpoints and lighting conditions as shown in Fig. 2. By reconstructing them using the GAN, these pseudo samples could guide the original image towards the sampled viewpoints and lighting conditions in the GAN manifold, producing a number of natural-looking images, called 'projected samples'.These projected samples could be adopted as the ground truth of the differentiable rendering process to refine the prior 3D shape (i.e. an ellipsoid).To achieve more precise results, we further regard the refined shape as the initial shape and repeat the above steps to progressively refine the 3D shape.With the proposed approach, namely GAN2Shape, we show that existing 2D GANs trained on images only are sufficient to accurately reconstruct the 3D shape of a single image for many object categories such as human faces, cars, buildings, etc.Our method thus is an effective approach for unsupervised 3D shape reconstruction from unconstrained 2D images without any 2D keypoint or 3D annotations.With an improved GAN inversion strategy, our method works not only for GAN samples, but also for real natural images.On the BFM benchmark (Paysan et al., 2009), our method outperforms a recent strong baseline designed specifically for 3D shape learning (Wu et al., 2020).We also show high-quality 3D-aware image manipulations using the semantic latent directions discovered by our approach, which achieves more accurate human face rotation than other competitors.Our contributions are summarized as follows.1) We present the first attempt to reconstruct the 3D object shapes using GANs that are pre-trained on 2D images only.Our work shows that 2D GANs inherently capture rich 3D knowledge for different object categories, and provides a new perspective for 3D shape generation.2) Our work also provides an alternative unsupervised 3D shape learning method, and does not rely on the symmetry assumption of object shapes.3) We achieve highly photo-realistic 3D-aware image manipulations including rotation and relighting without using any external 3D models."}
{"paper_id": 106, "abstract": "In the realm of understanding complex data, the quest to unravel the generative factors hidden within has long been confined to meticulously crafted scenarios, like a hero's journey set in an intricately designed world. But we propose a new path, one that ventures into the wilds of natural data. Our exploration reveals that the very statistics of this data provide a robust framework for disentanglement, both in theory and practice.  Specifically, we uncover an intriguing phenomenon: objects in natural movies often transition in ways that are subtle, punctuated by occasional, significant leaps\u2014much like the unpredictable twists of fate in a grand epic. To tackle this revelation, we present a groundbreaking proof, relying on a sparse prior that focuses on temporally adjacent observations, allowing us to recover the true latent variables, albeit with some permutations and sign flips. This result is not merely an incremental improvement; it stands as a testament to our advancements over prior work.  Equipped with our innovative prior, practical estimation methods often surpass the current state-of-the-art across several established benchmark datasets, all without relying on impractical assumptions, such as prior knowledge of the number of generative factors at play. Moreover, we introduce two new benchmarks\u2014Natural Sprites and KITTI Masks\u2014that weave the measured dynamics of the natural world into the fabric of disentanglement evaluation, pushing the boundaries of realism in our datasets.  Through these benchmarks, we rigorously test our theories, showcasing improved performance while also uncovering the hidden challenges that current methods face when scaling to more natural domains. In essence, our work illuminates critical issues in the realm of disentanglement research, guiding the way toward a future where we can more effectively navigate the complexities of natural settings.", "introduction": "Natural scene understanding can be achieved by decomposing the signal into its underlying factors of variation.An intuitive approach for this problem assumes that a visual representation of the world can be constructed via a generative process that receives factors as input and produces natural signals as output (Bengio et al., 2013).This analogy is justified by the fact that our world is composed of distinct entities that can vary independently, but with regularity imposed by physics.What makes the approach appealing is that it formalizes representation learning by directly comparing representations to underlying ground-truth states, as opposed to the indirect evaluation of benchmarking against heuristic downstream tasks (e.g.object recognition).However, the core issue with this approach is non-identifiability, which means a set of possible solutions may all appear equally valid to the model, while only one identifies the true generative factors.Our work is motivated by the question of whether the statistics of natural data will allow for the formulation of an identifiable model.Our core observation that enables us to make progress in addressing this question is that generative factors of natural data have sparse transitions.To estimate these generative factors, we compute statistics on measured transitions of area and position for object masks from large-scale, natural, unstructured videos.Specifically, we extracted over 300,000 object segmentation mask transitions from YouTube-VOS (Xu et al., 2018;Yang et al., 2019) and KITTI-MOTS (Voigtlaender et al., 2019;Geiger et al., 2012;Milan et al., 2016) (discussed in detail in Appendix D).We fit generalized Laplace distributions to the collected data (Eq.2), which we indicate with orange lines in Fig. 1.We see empirically that all marginal distributions of temporal transitions are highly sparse and that there exist complex dependencies between natural factors (e.g.motion typically affects both position and apparent size).In this study, we focus on the sparse marginals, which we believe constitutes an important advance that sets the stage for solving further issues and eventually applying the technology to real-world problems.With this information at hand, we are able to provide a stronger proof for capturing the underlying generative factors of the data up to permutations and sign flips that is not covered by previous work (Hyv\u00e4rinen and Morioka, 2016;2017;Khemakhem et al., 2020a).Thus, we present the first work, to the best of our knowledge, which proposes a theoretically grounded solution that covers the statistics observed in real videos.Figure 1: Statistics of Natural Transitions.The histograms show distributions over transitions of segmented object masks from natural videos for horizontal and vertical position as well as object size.The red lines indicate fits of generalized Laplace distributions (Eq.2) with shape value \u03b1.Data shown is for object masks extracted from YouTube videos.See Appendix G for 2D marginals and corresponding analysis from the KITTI self-driving car dataset.Our contributions are: With measurements from unstructured natural video annotations we provide evidence that natural generative factors undergo sparse changes across time.We provide a proof of identifiability that relies on the observed sparse innovations to identify nonlinearly mixed sources up to a permutation and sign-flips, which we then validate with practical estimation methods for empirical comparisons.We leverage the natural scene information to create novel datasets where the latent transitions between frames follow natural statistics.These datasets provide a benchmark to evaluate how well models can uncover the true latent generative factors in the presence of realistic dynamics.We demonstrate improved disentanglement over previous models on existing datasets and our contributed ones with quantitative metrics from both the disentanglement (Locatello et al., 2018) and the nonlinear ICA community (Hyv\u00e4rinen and Morioka, 2016).We show via numerous visualization techniques that the learned representations for competing models have important differences, even when quantitative metrics suggest that they are performing equally well."}
{"paper_id": 107, "abstract": "In the realm of machine learning, the task of gleaning insights from a scant few samples often feels like navigating a treacherous landscape, where the specter of overfitting looms large, lurking in the shadows cast by a biased distribution forged from mere handfuls of training examples. In this paper, we embark on a quest to recalibrate the distribution of these underrepresented classes, drawing upon the robust statistics of their more populous counterparts. By doing so, we forge a pathway to generate an adequate number of examples from this newly calibrated distribution, thereby enriching the inputs available to our classifier.  We posit that each dimension of our feature representation adheres to a Gaussian distribution, allowing us to borrow the mean and variance from similar classes\u2014those blessed with a wealth of samples and better-defined statistics. Remarkably, our approach seamlessly integrates with existing pretrained feature extractors and classification models, requiring no additional parameters to unleash its potential.   In our trials, a straightforward logistic regression classifier, trained on features sampled from our calibrated distribution, eclipses the state-of-the-art benchmarks across three distinct datasets, boasting an impressive ~5% enhancement in accuracy on miniImageNet over its nearest rival. The visualizations of these generated features stand as testament to the precision of our calibrated distribution, illuminating the path forward in a landscape often fraught with uncertainty.", "introduction": "Learning from a limited number of training samples has drawn increasing attention due to the high cost of collecting and annotating a large amount of data.Researchers have developed algorithms to improve the performance of models that have been trained with very few data.Finn et al. (2017); Snell et al. (2017) train models in a meta-learning fashion so that the model can adapt quickly on tasks with only a few training samples available.Hariharan & Girshick (2017); Wang et al. (2018) try to synthesize data or features by learning a generative model to alleviate the data insufficiency problem.Ren et al. (2018) propose to leverage unlabeled data and predict pseudo labels to improve the performance of fewshot learning.While most previous works focus on developing stronger models, scant attention has been paid to the property of the data itself.It is natural that when the number of data grows, the ground truth distribution can be more accurately uncovered.Models trained with a wide coverage of data can generalize well during evaluation.On the other hand, when training a model with only a few training data, the model tends to overfit on these few samples by minimizing the training loss over these samples.These phenomena are illustrated in Figure 1.This biased distribution based on a few examples can damage the generalization ability of the model since it is far from mirroring the ground truth distribution from which test cases are sampled during evaluation.Here, we consider calibrating this biased distribution into a more accurate approximation of the ground truth distribution.In this way, a model trained with inputs sampled from the calibrated distribution can generalize over a broader range of data from a more accurate distribution rather than only fitting itself to those few samples.Instead of calibrating the distribution of the original data space, we try to calibrate the distribution in the feature space, which has much lower dimensions and is easier to calibrate (Xian et al. (2018)).We assume every dimension in the feature vectors follows a Gaussian distribution and observe that similar classes usually have similar mean and variance of the feature representations, as shown in Table 1.Thus, the mean and variance of the Gaussian distribution can be transferred across similar classes (Salakhutdinov et al. (2012)).Meanwhile, the statistics can be estimated more accurately when there are adequate samples for this class.Based on these observations, we reuse the statistics from many-shot classes and transfer them to better estimate the distribution of the few-shot classes according to their class similarity.More samples can be generated according to the estimated distribution which provides sufficient supervision for training the classification model.In the experiments, we show that a simple logistic regression classifier trained with our strategy can achieve state-of-the-art accuracy on three datasets.Our distribution calibration strategy can be paired with any classifier and feature extractor with no extra learnable parameters.Training with samples selected from the calibrated distribution can achieve 12% accuracy gain compared to the baseline which is only trained with the few samples given in a 5way1shot task.We also visualize the calibrated distribution and show that it is an accurate approximation of the ground truth that can better cover the test cases."}
{"paper_id": 108, "abstract": "In the realm of adversarial machine learning, a prevailing notion has long suggested that the pursuit of robustness and accuracy are at odds with one another, akin to two rival factions vying for dominance in a world of complexity. Yet, recent investigations have illuminated a path that defies this assumption, revealing that it is indeed possible to bolster accuracy while maintaining robustness. However, the reverse scenario\u2014preserving accuracy while enhancing robustness\u2014presents a far more intriguing challenge, for it is a fundamental truth that robust accuracy must always trail behind standard accuracy in any model.  In this paper, we embark on an exploration of this promising avenue. Our first revelation is that even deep networks, which are often deemed over-parameterized, may still lack the necessary capacity to withstand adversarial onslaughts, primarily due to the overwhelming smoothing effect induced by adversarial training. This insight leads us to a second, critical argument: in the face of limited model capacity, not all adversarial data points should be regarded with equal weight. From a geometric perspective, a natural data point's proximity to the class boundary dictates its robustness\u2014those closer are inherently more secure, while those further away are more vulnerable. Thus, we propose that the corresponding adversarial data points should be weighted accordingly, with greater emphasis placed on those that are harder to attack.  To bring this concept to fruition, we introduce a novel approach: geometry-aware instance-reweighted adversarial training. In this framework, the weights assigned reflect the difficulty of launching an attack on a given natural data point. Our experimental results demonstrate that this method significantly enhances the robustness of standard adversarial training. By intertwining these two strategies, we achieve a remarkable synergy, ultimately improving both the robustness and accuracy of adversarial training, paving the way for a new understanding in the landscape of machine learning.", "introduction": "Crafted adversarial data can easily fool the standard-trained deep models by adding humanimperceptible noise to the natural data, which leads to the security issue in applications such as medicine, finance, and autonomous driving (Szegedy et al., 2014;Nguyen et al., 2015).To mitigate this issue, many adversarial training methods employ the most adversarial data maximizing the loss for updating the current model such as standard adversarial training (AT) (Madry et al., 2018), TRADES (Zhang et al., 2019), robust self-training (RST) (Carmon et al., 2019), and MART (Wang et al., 2020b).The adversarial training methods seek to train an adversarially robust deep model whose predictions are locally invariant to a small neighborhood of its inputs (Papernot et al., 2016).By leveraging adversarial data to smooth the small neighborhood, the adversarial training methods acquire adversarial robustness against adversarial data but often lead to the undesirable degradation of standard accuracy on natural data (Madry et al., 2018;Zhang et al., 2019).Thus, there have been debates on whether there exists a trade-off between robustness and accuracy.For example, some argued an inevitable trade-off: Tsipras et al. (2019) showed fundamentally different representations learned by a standard-trained model and an adversarial-trained model; Zhang et al. (2019) and Wang et al. (2020a) proposed adversarial training methods that can trade off standard accuracy for adversarial robustness.On the other hand, some argued that there is no such the trade-off: Raghunathan et al. (2020) showed infinite data could eliminate this trade-off; Yang et al. (2020) showed benchmark image datasets are class-separated.overfitting (Rice et al., 2020), meanwhile leading to the improved robustness with zero or little degradation of accuracy (in Section 4.1 and Appendix C.1). Besides, we use Wide ResNets (Zagoruyko & Komodakis, 2016) to corroborate the efficacy of our geometry-aware instance-reweighted methods: Our GAIRAT significantly boosts the robustness of standard AT; combined with FAT, our GAIR-FAT improves both the robustness and accuracy of standard AT (in Section 4.2).Consequently, we conjecture no inevitable trade-off between robustness and accuracy."}
{"paper_id": 109, "abstract": "In the realm of machine learning, the study of determinantal point processes (DPPs) has emerged as a beacon of innovation, illuminating the path to better modeling of subsets from vast collections of items. Recent explorations have unveiled the power of nonsymmetric DPP (NDPP) kernels, which stand tall against their symmetric counterparts, offering enhanced modeling capabilities and superior predictive prowess. Yet, a shadow looms over these advancements: the existing algorithms for learning and inference in NDPPs demand memory that scales quadratically with the size of the item collection, $M$, and their runtime spirals to cubic levels for learning and quadratic for inference. This imposes a formidable barrier for many practical subset selection tasks.  In this work, we embark on a quest to shatter these constraints. We unveil a groundbreaking learning algorithm that operates with both space and time complexities that are linear in $M$, achieved through an innovative NDPP kernel decomposition. But our journey does not end there; we also derive a maximum a posteriori (MAP) inference algorithm for NDPPs that boasts linear complexity, applicable to both our novel kernel and those of previous works. Through rigorous evaluation on real-world datasets, we demonstrate that our algorithms not only scale with remarkable efficiency but also match the predictive performance of their predecessors, heralding a new era in subset selection methodologies.", "introduction": "Determinantal point processes (DPPs) have proven useful for numerous machine learning tasks.For example, recent uses include summarization (Sharghi et al., 2018), recommender systems (Wilhelm et al., 2018), neural network compression (Mariet & Sra, 2016), kernel approximation (Li et al., 2016), multi-modal output generation (Elfeki et al., 2019), and batch selection, both for stochastic optimization (Zhang et al., 2017) and for active learning (B\u0131y\u0131k et al., 2019).For subset selection problems where the ground set of items to select from has cardinality M , the typical DPP is parameterized by an M \u00d7 M kernel matrix.Most prior work has been concerned with symmetric DPPs, where the kernel must equal its transpose.However, recent work has considered the more general class of nonsymmetric DPPs (NDPPs) and shown that these have additional useful modeling power (Brunel, 2018;Gartrell et al., 2019).In particular, unlike symmetric DPPs, which can only model negative correlations between items, NDPPs allow modeling of positive correlations, where the presence of item i in the selected set increases the probability that some other item j will also be selected.There are many intuitive examples of how positive correlations can be of practical importance.For example, consider a product recommendation task for a retail website, where a camera is found in a user's shopping cart, and the goal is to display several other items that might be purchased.Relative to an empty cart, the presence of the camera probably increases the probability of buying an accessory like a tripod.Although NDPPs can theoretically model such behavior, the existing approach for NDPP learning and inference (Gartrell et al., 2019) is often impractical in terms of both storage and runtime requirements.These algorithms require memory quadratic in M and time quadratic (for inference) or cubic (for learning) in M ; for the not-unusual M of 1 million, this requires storing 8TB-size objects in memory, with runtime millions or billions of times slower than that of a linear-complexity method.In this work, we make the following contributions:For intuition about the kernel parameters, notice that the probabilities of singletons {i} and {j} are proportional to L ii and L jj , respectively.Hence, it is common to think of L's diagonal as representing item qualities.The probability of a pair {i, j} is proportional to det(L {i,j} ) = L ii L jj -L ij L ji .Thus, if -L ij L ji < 0, this indicates i and j interact negatively.Similarly, if -L ij L ji > 0, then i and j interact positively.Therefore, off-diagonal terms determine item interactions.(The vague term \"interactions\" can be replaced by the more precise term \"correlations\" if we consider the DPP's marginal kernel instead; see Gartrell et al. (2019, Section 2.1) for an extensive discussion.)In order to ensure that P L defines a probability distribution, all principal minors of L must be non-negative: det(L Y ) \u2265 0. Matrices that satisfy this property are called P 0 -matrices (Fang, 1989, Definition 1).There is no known generative method or matrix decomposition that fully covers the space of all P 0 matrices, although there are many that partially cover the space (Tsatsomeros, 2004).One common partial solution is to use a decomposition that covers the space of symmetric P 0 matrices.By restricting to the space of symmetric matrices, one can exploit the fact that L \u2208 P 0 if L is positive semidefinite (PSD) * (Prussing, 1986).Any symmetric PSD matrix can be written as the Gramian matrix of some set of vectors: L := V V , where V \u2208 R M \u00d7K .Hence, the V V decomposition provides an easy means of generating the entire space of symmetric P 0 matrices.It also has a nice intuitive interpretation: we can view the i-th row of V as a length-K feature vector describing item i.Unfortunately, the symmetry requirement limits the types of correlations that a DPP can capture.A symmetric model is able to capture only nonpositive interactions between items, since L ij L ji = L 2 ij \u2265 0, whereas a nonsymmetric L can also capture positive correlations.(Again, see Gartrell et al. (2019, Section 2.1) for more intuition.)To expand coverage to nonsymmetric matrices in P 0 , it is natural to consider nonsymmetric PSD matrices.In what follows, we denote by P + 0 the set of all nonsymmetric (and symmetric) PSD matrices.Any nonsymmetric PSD matrix is in P 0 (Gartrell et al., 2019, Lemma 1), so P + 0 \u2286 P 0 .However, unlike in the symmetric case, the set of nonsymmetric PSD matrices does not fully cover the set of nonsymmetric P 0 matrices.For example, considerStill, nonsymmetric PSD matrices cover a large enough portion of the P 0 space to be useful in practice, as evidenced by the experiments of Gartrell et al. (2019).This work covered the P + 0 space by using the following decomposition: L := S + A, with S := V V for V \u2208 R M \u00d7K , and A := BC -CB for B, C \u2208 R M \u00d7K .This decomposition makes use of the fact that any matrix L can be decomposed uniquely as the sum of a symmetric matrix S = (L + L T )/2 and a skew-symmetric matrix A = (L -L T )/2.All skew-symmetric matrices A are trivially PSD, sincex Ax = 0 for all x \u2208 R M .Hence, the L here is guaranteed to be PSD simply because its S uses the standard Gramian decomposition V V .In this work we will also only consider P + 0 , and leave to future work the problem of finding tractable ways to cover the rest of P 0 .We propose a new decomposition of L that also covers the P + 0 space, but allows for more scalable learning.As in prior work, our decomposition has inner dimension K that could be as large as M , but is usually much smaller in practice.Our algorithms work well for modest values of K.In cases where the natural K is larger (e.g., natural language processing), random projections can often be used to significantly reduce K (Gillenwater et al., 2012a)."}
{"paper_id": 110, "abstract": "In a world where the vast expanses of data fuel the engines of machine learning, the quest for efficiency becomes paramount. The burden of storing colossal datasets and the expense of training intricate models can weigh heavily on even the most resourceful of researchers. In this paper, we unveil a groundbreaking technique known as Dataset Condensation\u2014a method that distills the essence of large datasets into a compact collection of insightful synthetic samples, perfect for training deep neural networks from the ground up.  We approach this challenge as a grand puzzle of gradient matching, aligning the gradients of deep neural network weights trained on both the original and our synthesized data. Through rigorous experimentation across a series of demanding computer vision benchmarks, we reveal that our method not only stands tall among its peers but decisively outshines the current state-of-the-art techniques.  But our journey does not end there. We delve into the realms of continual learning and neural architecture search, discovering that our technique holds untapped potential, yielding remarkable advancements even when faced with the constraints of limited memory and computational resources. Join us as we chart a new course in the landscape of data-efficient learning, where the power of synthesis meets the promise of innovation.", "introduction": "Large-scale datasets, comprising millions of samples, are becoming the norm to obtain state-ofthe-art machine learning models in multiple fields including computer vision, natural language processing and speech recognition.At such scales, even storing and preprocessing the data becomes burdensome, and training machine learning models on them demands for specialized equipment and infrastructure.An effective way to deal with large data is data selection -identifying the most representative training samples -that aims at improving data efficiency of machine learning techniques.While classical data selection methods, also known as coreset construction (Agarwal et al., 2004;Har-Peled & Mazumdar, 2004;Feldman et al., 2013), focus on clustering problems, recent work can be found in continual learning (Rebuffi et al., 2017;Toneva et al., 2019;Castro et al., 2018;Aljundi et al., 2019) and active learning (Sener & Savarese, 2018) where there is typically a fixed budget in storing and labeling training samples respectively.These methods commonly first define a criterion for representativeness (e.g. in terms of compactness (Rebuffi et al., 2017;Castro et al., 2018), diversity (Sener & Savarese, 2018;Aljundi et al., 2019), forgetfulness (Toneva et al., 2019)), then select the representative samples based on the criterion, finally use the selected small set to train their model for a downstream task.Unfortunately, these methods have two shortcomings: they typically rely on i) heuristics (e.g.picking cluster centers) that does not guarantee any optimal solution for the downstream task (e.g.image classification), ii) presence of representative samples, which is neither guaranteed.A recent method, Dataset Distillation (DD) (Wang et al., 2018) goes beyond these limitations by learning a small set of informative images from large training data.In particular, the authors model the network parameters as a function of the synthetic training data and learn them by minimizing the training loss over the original training data w.r.t.synthetic data.Unlike in the coreset methods, the synthesized data are directly optimized for the downstream task and thus the success of the method does not rely on the presence of representative samples.Inspired from DD (Wang et al., 2018), we focus on learning to synthesize informative samples that are optimized to train neural networks for downstream tasks and not limited to individual samples in original dataset.Like DD, our goal is to obtain the highest generalization performance with a model trained on a small set of synthetic images, ideally comparable performance to that of a model trained on the original images (see Figure 1(a)).In particular, we investigate the following Figure 1: Dataset Condensation (left) aims to generate a small set of synthetic images that can match the performance of a network trained on a large image dataset.Our method (right) realizes this goal by learning a synthetic set such that a deep network trained on it and the large set produces similar gradients w.r.t.its weights.The synthetic data can later be used to train a network from scratch in a small fraction of the original computational load.CE denotes Cross-Entropy.questions.Is it possible to i) compress a large image classification dataset into a small synthetic set, ii) train an image classification model on the synthetic set that can be further used to classify real images, iii) learn a single set of synthetic images that can be used to train different neural network architectures?To this end, we propose a Dataset Condensation method to learn a small set of \"condensed\" synthetic samples such that a deep neural network trained on them obtains not only similar performance but also a close solution to a network trained on the large training data in the network parameter space.We formulate this goal as a minimization problem between two sets of gradients of the network parameters that are computed for a training loss over a large fixed training set and a learnable condensed set (see Figure 1(b)).We show that our method enables effective learning of synthetic images and neural networks trained on them, outperforms (Wang et al., 2018) and coreset methods with a wide margin in multiple computer vision benchmarks.In addition, learning a compact set of synthetic samples also benefits other learning problems when there is a fixed budget on training images.We show that our method outperforms popular data selection methods by providing more informative training samples in continual learning.Finally, we explore a promising use case of our method in neural architecture search, and show that -once our condensed images are learned -they can be used to train numerous network architectures extremely efficiently.Our method is related to knowledge distillation (KD) techniques (Hinton et al., 2015;Bucilu\u01ce et al., 2006;Ba & Caruana, 2014;Romero et al., 2014) that transfer the knowledge in an ensemble of models to a single one.Unlike KD, we distill knowledge of a large training set into a small synthetic set.Our method is also related to Generative Adversarial Networks (Goodfellow et al., 2014a;Mirza & Osindero, 2014;Radford et al., 2015) and Variational AutoEncoders (Kingma & Welling, 2013) that synthesize high-fidelity samples by capturing the data distribution.In contrast, our goal is to generate informative samples for training deep neural networks rather than to produce \"real-looking\" samples.Finally our method is related to the methods that produce image patches by projecting the feature activations back to the input pixel space (Zeiler & Fergus, 2014), reconstruct the input image by matching the feature activations (Mahendran & Vedaldi, 2015), recover private training images for given training gradients (Zhu et al., 2019;Zhao et al., 2020), synthesize features from semantic embeddings for zero-shot learning (Sariyildiz & Cinbis, 2019).Our goal is however to synthesize a set of condensed training images not to recover the original or missing training images.In the remainder of this paper, we first review the problem of dataset condensation and introduce our method in section 2, present and analyze our results in several image recognition benchmarks in section 3.1, showcase applications in continual learning and network architecture search in section 3.2, and conclude the paper with remarks for future directions in section 4."}
{"paper_id": 111, "abstract": "In the realm of neural networks, where the quest for efficiency often collides with the need for performance, we embark on a daring exploration of Post-training Quantization (PTQ)\u2014a formidable challenge that seeks to refine models without the arduous journey of end-to-end retraining. Traditionally, PTQ has relied on a modest subset of training data, yielding quantized models that, while functional, fall short of the prowess exhibited by their Quantization-Aware Training (QAT) counterparts.   Enter BRECQ, our groundbreaking framework that dares to push the limits of quantization further than ever before, achieving the audacious feat of downscaling to INT2 bitwidth for the very first time. With a meticulous approach, BRECQ dissects neural networks into their fundamental components, reconstructing each element with precision and care. Through a rigorous theoretical examination of second-order error, we unveil how BRECQ deftly balances the intricate interplay of cross-layer dependencies with the demands of generalization error.  But we do not stop there. To harness the full potential of quantization, we weave in the mixed precision technique, skillfully approximating both inter-layer and intra-layer sensitivities. Our extensive experiments span a diverse array of handcrafted and optimized neural architectures, tackling both image classification and object detection challenges head-on. In a remarkable revelation, we demonstrate that PTQ can produce 4-bit versions of ResNet and MobileNetV2 that rival the capabilities of QAT, all while achieving a staggering 240-fold acceleration in the production of quantized models.  For those eager to delve deeper into this innovative approach, our codes await at https://github.com/yhhhli/BRECQ. Join us as we venture into this new frontier, where the balance of power and efficiency redefines what is possible in the world of neural networks.", "introduction": "The past decade has witnessed the rapid development of deep learning in many tasks, such as computer vision, autonomous driving, etc.However, the issue of huge computation cost and memory footprint requirements in deep learning has received considerable attention.Some works such as neural architecture search (Zoph & Le, 2016) try to design and search a tiny network, while others, like quantization (Hubara et al., 2017), and network pruning (Han et al., 2015) are designed to compress and accelerate off-the-shelf well-trained redundant networks.Many popular quantization and network pruning methods follow a simple pipeline: training the original model and then finetune the quantized/pruned model.However, this pipeline requires a full training dataset and many computation resources to perform end-to-end backpropagation, which will greatly delay the production cycle of compressed models.Besides, not all training data are always ready-to-use considering the privacy problem.Therefore, there is more demand in industry for quantizing the neural networks without retraining, which is called Post-training Quantization.Although PTQ is fast and light, it suffers from severe accuracy degeneration when the quantization precision is low.For example, DFQ (Nagel et al., 2019) can quantize ResNet-18 to 8-bit without accuracy loss (69.7% top-1 accuracy) but in 4-bit quantization, it can only achieve 39% top-1 accuracy.The primary reason is the approximation in the parameter space is not equivalent to the approximation in model space thus we cannot assure the optimal minimization on the final task loss.Recent works like (Nagel et al., 2020) recognized the problem and analyzed the loss degradation by Taylor series expansion.Analysis of the second-order error term indicates we can reconstruct each layer output to approximate the task loss degeneration.However, their work cannot further quantize the weights into INT2 because the cross-layer dependency in the Hessian matrix cannot be ignored when the perturbation on weight is not small enough.In this work, we analyze the second-order error based on the Gauss-Newton matrix.We show that the second-order error can be transformed into network final outputs but suffer from bad generalization.To achieve the best tradeoff, we adopt an intermediate choice, block reconstruction.In addition, our contributions are threefold:1. Based on the second-order analysis, we define a set of reconstruction units and show that block reconstruction is the best choice with the support from theoretical and empirical evidence.We also use Fisher Information Matrix to assign each pre-activation with an importance measure during reconstruction.2. We incorporate genetic algorithm and the well-defined intra-block sensitivity measure to generate latency and size guaranteed mixed precision quantized neural networks, which fulfills a general improvement on both specialized hardware (FPGA) and general hardware (ARM CPU).3. We conduct extensive experiments to verify our proposed methods.We find that our method is applicable to a large variety of tasks and models.Moreover, we show that post-training quantization can quantize weights to INT2 without significant accuracy loss for the first time."}
{"paper_id": 112, "abstract": "In the realm of machine learning, where models often stand as impenetrable fortresses, the quest for crafting black-box adversarial examples presents a formidable challenge. Adversaries must navigate the intricate landscape of the victim model, carefully leveraging its feedback while grappling with the daunting specter of high query complexity\u2014especially when only the top-1 decision, or hard-label prediction, is at their disposal.   In this paper, we unveil a groundbreaking approach: the Policy-Driven Attack. This innovative hard-label black-box attack is designed to streamline the process of generating adversarial examples, significantly reducing the number of queries needed. At the heart of our strategy lies a meticulously crafted policy network, which learns to identify promising search directions within the adversarial landscape. By framing our approach through a novel reinforcement learning lens, we ensure that each query becomes a calculated step toward our goal, rather than a random stumble.  Our experimental results speak volumes, showcasing a dramatic reduction in query complexity when compared to existing state-of-the-art hard-label black-box attacks across a variety of image classification benchmark datasets. For those eager to delve deeper into our findings, we invite you to explore our code and models, available at https://github.com/ZiangYan/pda.pytorch. Join us as we push the boundaries of adversarial machine learning, forging a path toward more efficient and effective attacks.", "introduction": "It is widely known that deep neural networks (DNNs) are vulnerable to adversarial examples, which are crafted via perturbing clean examples to cause the victim model to make incorrect predictions.In a white-box setting where the adversaries have full access to the architecture and parameters of the victim model, gradients w.r.t.network inputs can be easily calculated via back-propagation, and thus first-order optimization techniques can be directly applied to craft adversarial examples in this setting (Szegedy et al., 2014;Goodfellow et al., 2015;Carlini & Wagner, 2017;Madry et al., 2018;Rony et al., 2019).However, in black-box settings, input gradients are no longer readily available since all model internals are kept secret.Over the past few years, the community has made massive efforts in developing black-box attacks.In order to gain high attack success rates, delicate queries to the victim model are normally required.Recent methods can be roughly categorized into score-based attacks (Chen et al., 2017;Ilyas et al., 2018;Nitin Bhagoji et al., 2018;Ilyas et al., 2019;Yan et al., 2019;Li et al., 2020b;Tu et al., 2019;Du et al., 2019;Li et al., 2019;Bai et al., 2020) and hard-label attacks (a.k.a, decision-based attacks) (Brendel et al., 2018;Cheng et al., 2019;Dong et al., 2019;Shi et al., 2019;Brunner et al., 2019;Chen et al., 2020;Rahmati et al., 2020;Li et al., 2020a;Shi et al., 2020;Chen & Gu, 2020), based on the amount of information exposed to the adversaries from the output of victim model.When the prediction probabilities of the victim model are accessible, an intelligent adversary would generally prefer score-based attacks, while in a more practical scenario where only the top-1 class prediction is available, the adversaries will have to resort to hard-label attacks.Since less information is exposed from such feedback of the victim model, hard-label attacks often bare higher query complexity than that of score-based attacks, making their attack process costly and time intensive.In this paper, we aim at reducing the query complexity of hard-label black-box attacks.We cast the problem of progressively refining the candidate adversarial example (by skillfully querying the victim model and analyzing its feedback) into a reinforcement learning formulation.At each iteration, we search along a set of chosen directions to see whether there exists any new candidate adversarial example that is perceptually more similar to its benign counterpart, i.e., in the sense of requiring less distortion.A reward is assigned to each of such search directions (treated as actions), based on the amount of distortion reduction yielded after updating the adversarial example along that direction.Such a reinforcement learning formulation enables us to learn the non-differentiable mapping from search directions to their potential of refining the current adversarial example, directly and precisely.The policy network is expected to be capable of providing the most promising search direction for updating candidate adversarial examples to reduce the required distortion of the adversarial examples from their benign counterparts.As we will show, the proposed policy network can learn from not only the queries that had been performed following the evolving policy but also peer experience from other black-box attacks.As such, it is possible to pre-train the policy network on a small number of query-reward pairs obtained from the performance log of prior attacks (with or without policy) to the same victim model.Experiments show that our policy-driven attack (PDA) can achieve significantly lower distortions than existing state-of-the-arts under the same query budgets."}
{"paper_id": 113, "abstract": "In the realm of automatic speech recognition (ASR), two distinct paths diverge: the swift, responsive nature of streaming ASR, which strives to deliver each word as it is spoken, and the contemplative, full-context ASR, which waits patiently for a complete utterance before revealing its insights. In this endeavor, we unveil a groundbreaking approach\u2014a unified framework we call Dual-mode ASR. This innovative model harnesses the power of shared weights to seamlessly bridge the gap between streaming and full-context recognition, training them as one cohesive entity.  Our findings reveal that the synergy of weight sharing and joint training, particularly through the application of inplace knowledge distillation, bestows significant advantages upon the latency and accuracy of streaming ASR. The versatility of the Dual-mode ASR framework allows it to integrate with cutting-edge convolutional and transformer-based ASR architectures.   To validate our approach, we conducted extensive experiments utilizing two state-of-the-art networks\u2014ContextNet and Conformer\u2014across two diverse datasets: the well-regarded LibriSpeech and the expansive MultiDomain. The results of our experiments, bolstered by thorough ablation studies, demonstrate that Dual-mode ASR not only streamlines the training and deployment process for both streaming and full-context models but also markedly enhances the emission speed and recognition precision of streaming ASR.  With Dual-mode ASR, we proudly announce the achievement of new state-of-the-art results for streaming ASR on both LibriSpeech and MultiDomain, setting a new benchmark for accuracy and latency in this rapidly evolving field.", "introduction": "\"Ok Google.Hey Siri.Hi Alexa.\" have featured a massive boom of smart speakers in recent years, unveiling a trend towards ubiquitous and ambient Artificial Intelligence (AI) for better daily lives.As the communication bridge between human and machine, low-latency streaming ASR (a.k.a., online ASR) is of central importance, whose goal is to emit each hypothesized word as quickly and accurately as possible on the fly as they are spoken.On the other hand, there are some scenarios where full-context ASR (a.k.a., offline ASR) is sufficient, for example, offline video captioning on video-sharing platforms.While low-latency streaming ASR is generally preferred in most of the speech recognition scenarios, it often has worse prediction accuracy as measured in Word Error Rate (WER), due to the lack of future context compared with full-context ASR.Improving both WER and emission latency has been shown to be highly challenging (He et al., 2019;Li et al., 2020a;Sainath et al., 2020) in streaming ASR systems.Since the acoustic, pronunciation, and language model (AM, PM, and LM) of a conventional ASR system have been evolved into a single end-to-end (E2E) all-neural network, modern streaming and full-context ASR models share most of the neural architectures and training recipes in common, such as, Mel-spectrogram inputs, data augmentations, neural network meta-architectures, training objectives, model regularization techniques and decoding methods.The most significant difference is that streaming ASR encoders are auto-regressive models, with the prediction of the current timestep conditioned on previous ones (no future context is permitted).Specifically, let x and y be the input and output sequence, t as frame index, T as total length of frames.Streaming ASR encoders model the output y t as a function of input x 1:t while full-context ASR encoders model the output y t as a function of input x 1:T .Streaming ASR encoders can be built with uni-directional LSTMs, causal convolution and left-context attention layers in streaming ASR encoders (Chiu & Raffel, 2018;Fan et al., 2018;Han et al., 2020;Gulati et al., 2020;Huang et al., 2020;Moritz et al., 2020;Miao et al., 2020;Tsunoo et al., 2020;Zhang et al., 2020;Yeh et al., 2019).Recurrent Neural Network Transducers (RNN-T) (Graves, 2012) are commonly used as the decoder in both streaming and fullcontext models, which predicts the token of the current input frame based on all previous tokens using uni-directional recurrent layers.Figure 1 illustrates a simplified example of the similarity and difference between streaming and full-context ASR models with E2E neural networks.Albeit the similarities, streaming and full-context ASR models are usually developed, trained, and deployed separately.In this work, we propose Dual-mode ASR, a framework to unify streaming and full-context speech recognition networks with shared weights.Dual-mode ASR comes with many immediate benefits, including reduced model download and storage on devices and simplified development and deployment workflows.To accomplish this goal, we first introduce Dual-mode Encoders, which can run in both streaming mode and full-context mode.Dual-mode encoders are designed to reuse the same set of model weights for both modes with zero or near-zero parameters overhead.We propose the design principles of a dual-mode encoder and show examples on how to design dual-mode convolution, dual-mode pooling, and dual-mode attention layers.We also investigate into different training algorithms for Dual-mode ASR, specifically, randomly sampled training and joint training.We show that joint training significantly outperforms randomly sampled training in terms of model quality and training stability.Moreover, motivated by Inplace Knowledge Distillation (Yu & Huang, 2019b) in which a large model is used to supervise a small model, we propose to distill knowledge from the full-context mode (teacher) into the streaming mode (student) on the fly during the training within the same Dual-mode ASR model, by encouraging consistency of the predicted token probabilities.We demonstrate that the emission latency and prediction accuracy of streaming ASR significantly benefit from weight sharing and joint training of its full-context mode, especially with inplace knowledge distillation during the training.We present extensive experiments with two state-of-theart ASR networks, convolution-based ContextNet (Han et al., 2020) and conv-transformer hybrid Conformer (Gulati et al., 2020), on two datasets, a widely used public dataset LibriSpeech (Panay-otov et al., 2015) (970 hours of English reading speech) and a large-scale dataset MultiDomain (Narayanan et al., 2018) (413,000 hours speech of a mixture across multiple domains including Voice Search, Farfield Speech, YouTube and Meetings).For each proposed technique, we also present ablation study and analysis to demonstrate and understand the effectiveness.With Dual-mode ASR, we achieve new state-of-the-art streaming ASR results on both LibriSpeech and MultiDomain in terms of accuracy and latency."}
{"paper_id": 114, "abstract": "In the realm of deep neural networks, the quest for an optimal balance between performance and compression has led to the intriguing exploration of mixed-precision quantization. Yet, this journey has been fraught with challenges, as existing methods often tread a narrow path\u2014either confined to a limited, manually crafted search space or ensnared in the labyrinth of cumbersome neural architecture searches. These approaches, while noble in intent, fall short of efficiently uncovering the true potential of quantization schemes.  Enter our innovative approach: Bit-Level Sparsity Quantization (BSQ). This method reframes the problem, viewing each bit of quantized weights as an independent, trainable variable. By introducing a differentiable bit-sparsity regularizer, BSQ empowers the model to induce all-zero bits within groups of weight elements, facilitating dynamic precision reduction. In doing so, we unlock a mixed-precision quantization scheme that elegantly adapts to the original model's architecture.  What sets BSQ apart is its ability to navigate the expansive mixed-precision landscape through a singular gradient-based optimization process, requiring only one hyperparameter to balance performance and compression. The results speak for themselves: BSQ not only achieves superior accuracy but also enhances bit reduction across various model architectures on the CIFAR-10 and ImageNet datasets, outshining previous methods in both efficiency and effectiveness. This work heralds a new era in mixed-precision quantization, where the intricacies of deep learning are met with innovative solutions that push the boundaries of what is possible.", "introduction": "Numerous deep neural network (DNN) models have been designed to tackle real-world problems and achieved beyond-human performance.DNN models commonly demand extremely high computation cost and large memory consumption, making the deployment and real-time processing on embedded and edge devices difficult (Han et al., 2015b;Wen et al., 2016).To address this challenge, model compression techniques, such as pruning (Han et al., 2015b;Wen et al., 2016;Yang et al., 2020), factorization (Jaderberg et al., 2014;Zhang et al., 2015) and fixed-point quantization (Zhou et al., 2016;Wu et al., 2019;Dong et al., 2019), have been extensively studied.Among them, fixed-point quantization works directly on the data representation by converting weight parameters originally in the 32-bit floating-point form to low-precision values in a fixed-point format.For a DNN model, its quantized version requires much less memory for weight storage.Moreover, it can better utilize fixed-point processing units in mobile and edge devices to run much faster and more efficiently.Typically, model compression techniques aim to reduce a DNN model size while maintaining its performance.The two optimization objectives in this tradeoff, however, have a contrary nature: the performance can be formulated as a differentiable loss function L(W ) w.r.t. the model's weights W ; yet the model size, typically measured by the number of non-zero parameters or operations, is a discrete function determined mainly by the model architecture.To co-optimize the performance and model size, some previous pruning and factorization methods relax the representation of model size as a differentiable regularization term R(W ).For example, group Lasso (Wen et al., 2016) and DeepHoyer (Yang et al., 2020) induce weight sparsity for pruning, and the attractive force regularizer (Wen et al., 2017) and nuclear norm (Xu et al., 2018) are utilized to induce low rank.The combined objective L(W ) + \u03b1R(W ) can be directly minimized with a gradient-based optimizer for optimizing the performance and model size simultaneously.Here, the hyperparameter \u03b1 controls the strength of the regularization and governs the performance-size tradeoff of the compressed model.Unlike for pruning and factorization, there lacks a well-defined differentiable regularization term that can effectively induce quantization schemes.Early works in quantization mitigate the tradeoff exploration complexity by applying the same precision to the entire model.This line of research focuses on improving the accuracy of ultra low-precision DNN models, e.g., quantizing all the weights to 3 or less bits (Zhou et al., 2016;Zhang et al., 2018), even to 1-bit (Rastegari et al., 2016).These models commonly incur significant accuracy loss, even after integrating emerging training techniques like straight-through estimator (Bengio et al., 2013;Zhou et al., 2016), dynamic range scaling (Polino et al., 2018) and non-linear trainable quantizers (Zhang et al., 2018).As different layers of a DNN model present different sensitivities with performance, a mixed-precision quantization scheme would be ideal for the performance-size tradeoff (Dong et al., 2019).There have also been accelerator designs to support the efficient inference of mixed-precision DNN models (Sharma et al., 2018).However, to achieve the optimal layer-wise precision configuration, it needs to exhaustively explore the aforementioned discrete search space, the size of which grows exponentially with the number of layers.Moreover, the dynamic change of each layer's precision cannot be formulated into a differentiable objective, which hinders the efficiency of the design space exploration.Prior studies (Wu et al., 2019;Wang et al., 2019) utilize neural architecture search (NAS), which suffers from extremely high searching cost due to the large space of mixed-precision quantization scheme.Recently, Dong et al. (2019) propose to rank each layer based on the corresponding Hessian information and then determine the relative precision order of layers based on their ranking.The method, however, still requires to manually select the precision level for each layer.Here, we propose to revisit the fixed-point quantization process from a new angle of bit-level sparsity: decreasing the precision of a fixed-point number can be taken as forcing one or a few bits, most likely the least significant bit (LSB), to be zero; and reducing the precision of a layer is equivalent to zeroing out a specific bit of all the weight parameters of the layer.In other words, the precision reduction can be viewed as increasing the layer-wise bit-level sparsity.By considering the bits of fixed-point DNN parameters as continuous trainable variables during DNN training, we can utilize a sparsity-inducing regularizer to explore the bit-level sparsity with gradient-based optimization, dynamically reduce the layer precision and lead to a series of mixed-precision quantization schemes.More specific, we propose Bit-level Sparsity Quantization (BSQ) method with the following contributions:\u2022 We propose a gradient based training algorithm for bit-level quantized DNN models.The algorithm considers each bit of quantized weights as an independent trainable variable and enables the gradient-based optimization with straight-through estimator (STE).\u2022 We propose a bit-level group Lasso regularizer to dynamically reduce the weight precision of every layer and therefore induce mixed-precision quantization schemes.\u2022 BSQ uses only one hyperparameter, the strength of the regularizer, to trade-off the model performance and size, making the exploration more efficient.This work exclusively focuses on layer-wise mixed-precision quantization, which is the granularity considered in most previous works.However, the flexibility of BSQ enables it to explore mixedprecision quantization of any granularity with the same cost regardless of the search space size."}
{"paper_id": 115, "abstract": "In the ever-expanding realm of machine learning, we embark on an ambitious quest: the creation of disentangled representations that capture the essence of object shape and appearance across diverse domains\u2014think dogs and cars, each with their own unique visual language. Our objective is to forge a generative model that crafts an intermediate distribution, skillfully weaving together characteristics from each domain, allowing us to conjure images that defy the boundaries of their origins.  This formidable challenge demands a precise disentanglement of the intricate elements of shape, appearance, and background inherent to each domain. Only then can we achieve the alchemy of interchanging these factors, merging the essence of one with the form of another. To tackle this, we enhance an existing methodology that excels at disentangling factors within a single domain but falters when faced with the complexity of multiple realms.  Our pivotal contribution lies in the innovative representation of object appearance through a differentiable histogram of visual features. By optimizing our generator, we ensure that two images sharing the same latent appearance factor\u2014yet differing in their latent shape factors\u2014yield remarkably similar histograms. Through rigorous experimentation across various multi-domain datasets, we unveil a method that not only achieves accurate and consistent transfers of appearance and shape across domains but also pushes the boundaries of what is possible in the fusion of visual characteristics. In this way, we step boldly into the future of generative modeling, where the magic of transformation knows no limits.", "introduction": "Humans possess the incredible ability of being able to combine properties from multiple image distributions to create entirely new visual concepts.For example, Lake et al. (2015) discussed how humans can parse different object parts (e.g., wheels of a car, handle of a lawn mower) and combine them to conceptualize novel object categories (a scooter).Fig. 2 illustrates another example from a different angle; it is easy for us humans to imagine how the brown car would look if its appearance were borrowed from the blue and red bird.To model a similar ability in machines, a precise disentanglement of shape and appearance features, and the ability to combine them across different domains are needed.In this work, we seek to develop a framework to do just that, where we define domains to correspond to \"basic-level categories\" (Rosch, 1978).(intra-domain) (inter-domain) (intra-domain)I 1 I 2 I 3 I 4 shape appearance shape appearanceFigure 2: Each domain can be represented with e.g., a set of object shapes (X A/B ) and appearances (Y A/B ).The ability to generate images of the form I AA/BB requires the system to learn intra-domain disentanglement (Singh et al., 2019) of latent factors, whereas the ability to generate images of the form I AB (appearance/shape from domain A/B, respectively) requires inter-domain disentanglement of factors, which is the goal of this work.Disentangling the factors of variation in visual data has received significant attention (Chen et al., 2016;Higgins et al., 2017;Denton & Birodkar, 2017;Singh et al., 2019), in particular with advances in generative models (Goodfellow et al., 2014;Radford et al., 2016;Zhang et al., 2018;Karras et al., 2019;Brock et al., 2019).The premise behind learning disentangled representations is that an image can be thought of as a function of, say two independent latent factors, such that each controls only one human interpretable property (e.g., shape vs. appearance).The existence of such representations enables combining latent factors from two different source images to create a new one, which has properties of both.Prior generative modeling work (Hu et al., 2018;Singh et al., 2019;Li et al., 2020) explore a part of this idea, where the space of latent factors being combined is limited to one domain (e.g., combining a sparrow's appearance with a duck's shape within the domain of birds; I AA in Fig. 2), a scenario which we refer to as intra-domain disentanglement of latent factors.This work, focusing on shape and appearance as factors, generalizes this idea to inter-domain disentanglement: combining latent factors from different domains (e.g., appearance from birds, shape from cars) to create a new breed of images which does not exist in either domain (I AB in Fig. 2).The key challenge to this problem is that there is no ground-truth distribution for the hybrid visual concept that spans the two domains.Due to this, directly applying a single domain disentangled image generation approach to the multi-domain setting does not work, as the hybrid concept would be considered out of distribution (we provide more analysis in Sec. 3).Despite the lack of ground-truth, as humans, we would deem certain combinations of factors to be better than others.For example, if two domains share object parts (e.g., dog and leopard), we would prefer a transfer of appearance in which local part appearances are preserved.For the ones that don't share object parts (e.g., bird and car), we may prefer a transfer of appearance in which the overall color/texture frequency is preserved (e.g.Fig. 2, I 2 and I AB ), which has been found to be useful in object categorization at the coarse level in a neuroimaging study (Rice et al., 2014).Our work formulates this idea as a training process, where any two images having the same latent appearance are constrained to have similar frequency of those low-level features.These features in turn are learned (as opposed to being hand-crafted), using contrastive learning (Hadsell et al., 2006;Chen et al., 2020), to better capture the low-level statistics of the dataset.The net effect is an accurate transfer of appearance, where important details remain consistent across domains in spite of large shape changes.Importantly, we achieve this by only requiring bounding box annotations to help disentangle object from background, without any other labels, including which domain an image comes from.To our knowledge, our work is the first to attempt combining factors from different data distributions to generate abstract visual concepts (e.g., car with dog's texture).We perform experiments on a variety of multi-modal datasets, and demonstrate our method's effectiveness qualitatively, quantitatively, and through user studies.We believe our work can open up new avenues for art/design; e.g., a customer could visualize how sofas would look with an animal print or a fashion/car designer could create a new space of designs using the appearance from arbitrary objects.Finally, we believe that the task introduced in this work offers better scrutiny of the quality of disentanglement learned by a method: if it succeeds in doing so within a domain but not in the presence of multiple ones, that in essence indicates some form of entanglement of factors with the domain's properties.Learning disentangled representations for image generation has been studied in both the supervised (relying on e.g., keypoints and object masks) (Peng et al., 2017;Balakrishnan et al., 2018;Ma et al., 2018) and unsupervised settings (Li et al., 2018;Shu et al., 2018).Recent work disentangle object shape, appearance, pose, and background with only bounding box annotations (Singh et al., 2019;Li et al., 2020).All prior work, however, focus on disentangling and combining factors within a single domain (e.g., birds), and cannot be directly extended to the multi-domain setting since hybrid images would be considered out of distribution (i.e., fake).We present a framework that addresses this limitation, and which works equally well in both single and multi-domain settings.Another potential angle to tackle the task at hand is through style-content disentanglement (Gatys et al., 2015;Johnson et al., 2016;Ulyanov et al., 2016).However, an object's appearance and shape in complex datasets do not necessarily align with those of style and content (e.g., color of background dominating the style rather than object's appearance).Unsupervised image-to-image translation works (Zhu et al., 2017;Kim et al., 2017;Huang et al., 2018;Gonzalez-Garcia et al., 2018;Choi et al., 2020) translate an image from domain A to domain B, such that the resulting image preserves the property common to domains A and B (e.g., structure), and property exclusive to B (e.g., appearance/style).However, if the domains don't have anything in common (e.g., cars \u2194 dogs: different structure and appearance), the translated images typically become degenerate, and no longer preserve properties from different domains.In contrast, our method can combine latent factors across arbitrary domains that have no part-level correspondences.Moreover, when part-level correspondences do exist (e.g., dogs \u2194 tiger), it combines appearance and shape in a way which preserves them.Lee et al. (2018) extended the multimodal image-to-image translation setting by conditioning the translation process on both a content image as well as a query attribute image, so that the resulting output preserves the content and attributes of the respective images.However, this application was explored in settings where both the content and attribute image share similar content/structure (e.g., natural and sketch images of face domain as content/attribute images respectively), which is different from our setting in which the factors to be combined come from entirely different domains (e.g., cars vs birds)."}
{"paper_id": 116, "abstract": "In the ever-evolving realm of neural network verification, the quest for tight and efficient bounding looms large, akin to a hero striving to conquer an insurmountable mountain. Recent advancements have unveiled a cadre of specialized dual solvers, each promising efficiency in bounding neural networks. Yet, like many ambitious heroes, they often falter when faced with the more formidable properties, their bounds proving too loose for the challenge at hand. This shortcoming, we discover, is intricately tied to the limitations of the relaxations employed\u2014typically linear programs whose size grows linearly with the number of neurons, akin to a hero weighed down by excessive armor.  Amidst this struggle, a glimmer of hope emerges in the form of a tighter linear relaxation designed for piecewise linear activations. However, this promising solution bears the burden of an exponential number of constraints, leaving a void where an efficient solver should reside. Here, we step into the fray with a novel dual algorithm that embraces the full potential of this new relaxation, deftly navigating a small active set of dual variables. Our method harnesses the strengths of this relaxation in the dual space, achieving both tightness and the power of a linear separation oracle.  Yet, we do not abandon the advantages of previous dual approaches that leverage weaker relaxations. Our algorithm retains the virtues of massive parallelism, GPU implementation, low iteration costs, and the ability to provide valid bounds at any moment. The result? We achieve bounds that surpass those of conventional solvers, all while consuming only a fraction of their time. Moreover, when computational resources are limited, we deftly recover the speed-accuracy balance characteristic of looser dual solvers. Through our efforts, we unveil a pathway to significant speed-ups in formal verification, propelling the field forward like a hero charging into the fray, ready to face whatever challenges lie ahead.", "introduction": "Verification requires formally proving or disproving that a given property of a neural network holds over all inputs in a specified domain.We consider properties in their canonical form (Bunel et al., 2018), which requires us to either: (i) prove that no input results in a negative output (property is true); or (ii) identify a counter-example (property is false).The search for counter-examples is typically performed by efficient methods such as random sampling of the input domain (Webb et al., 2019), or projected gradient descent (Carlini & Wagner, 2017).In contrast, establishing the veracity of a property requires solving a suitable convex relaxation to obtain a lower bound on the minimum output.If the lower bound is positive, the given property is true.If the bound is negative and no counter-example is found, either: (i) we make no conclusions regarding the property (incomplete verification); or (ii) we further refine the counter-example search and lower bound computation within a branch-and-bound framework until we reach a concrete conclusion (complete verification).The main bottleneck of branch and bound is the computation of the lower bound for each node of the enumeration tree via convex optimization.While earlier works relied on off-the-shelf solvers (Ehlers, 2017;Bunel et al., 2018), it was quickly established that such an approach does not scale-up elegantly with the size of the neural network.This has motivated researchers to design specialized dual solvers (Dvijotham et al., 2019;Bunel et al., 2020a), thereby providing initial evidence that verification can be realised in practice.However, the convex relaxation considered in the dual solvers is itself very weak (Ehlers, 2017), hitting what is now commonly referred to as the \"convex barrier\" (Salman et al., 2019).In practice, this implies that either several properties remain undecided in incomplete verification, or take several hours to be verified exactly.Multiple works have tried to overcome the convex barrier for piecewise linear activations (Raghunathan et al., 2018;Singh et al., 2019).Here, we focus on the single-neuron Linear Programming (LP) relaxation by Anderson et al. (2020).Unfortunately, its tightness comes at the price of exponentially many (in the number of variables) constraints.Therefore, existing dual solvers (Dvijotham et al., 2018;Bunel et al., 2020a) are not easily applicable, limiting the scaling of the new relaxation.We address this problem by presenting a specialized dual solver for the relaxation by Anderson et al. (2020), which realises its full potential by meeting the following desiderata:\u2022 By keeping an active set of dual variables, we obtain a sparse dual solver that recovers the strengths of the original primal problem (Anderson et al., 2020) in the dual domain.In line with previous dual solvers, our approach yields valid bounds at anytime, leverages convolutional network structure and enjoys massive parallelism within a GPU implementation, resulting in better bounds in an order of magnitude less time than off-the-shelf solvers (Gurobi Optimization, 2020).\u2022 We present a unified dual treatment that includes both a linearly sized LP relaxation (Ehlers, 2017) and the tighter formulation.As a consequence, our solver provides a wide range of speedaccuracy trade-offs: (i) it is competitive with dual approaches on the looser relaxation (Dvijotham et al., 2018;Bunel et al., 2020a); and (ii) it yields much tighter bounds if a larger computational budget is available.Owing to this flexibility, we show that our dual algorithm yields large complete verification gains compared to primal approaches (Anderson et al., 2020) and previous dual algorithms."}
{"paper_id": 117, "abstract": "In the realm of machine learning, knowledge distillation emerges as a powerful technique, allowing us to harness the wisdom of a well-trained network\u2014or an ensemble of them, known as the teacher\u2014to nurture the growth of a student network. This process involves using the outputs of the teacher as soft labels, guiding the student\u2019s training journey. Recent explorations (M\u00fcller et al., 2019; Yuan et al., 2020) have unveiled a fascinating characteristic of these soft labels: they act as a robust form of regularization for the student network. While regularization traditionally seeks to tame variance, the interplay of bias and variance in the context of soft labels remains shrouded in mystery.  In this work, we delve into the nuances of the bias-variance tradeoff that arises during distillation with soft labels. Our findings reveal that this tradeoff fluctuates on a sample-wise basis throughout training. Intriguingly, under a consistent distillation temperature, we discover a negative correlation between distillation performance and a subset of samples\u2014aptly termed \"regularization samples.\" These samples paradoxically contribute to an increase in bias while simultaneously reducing variance. Yet, our empirical investigations reveal a critical insight: completely discarding these regularization samples can lead to a decline in distillation performance.  Motivated by these revelations, we introduce a novel concept of weighted soft labels, empowering the network to adaptively navigate the sample-wise bias-variance tradeoff. Our experimental results on standard evaluation benchmarks substantiate the efficacy of our approach. For those curious to explore our methodology further, our code is available in the supplementary materials.", "introduction": "For deep neural networks (Goodfellow et al., 2016), knowledge distillation (KD) (Ba & Caruana, 2014;Hinton et al., 2015) refers to the technique that uses well-trained networks to guide the training of another network.Typically, the well-trained network is named as the teacher network while the network to be trained is named as the student network.For distillation, the predictions from the teacher network are leveraged and referred to as the soft labels (Balan et al., 2015;M\u00fcller et al., 2019).Soft labels generated by the teacher network have been proven effective in large-scale empirical studies (Liang et al., 2019;Tian et al., 2020;Zagoruyko & Komodakis, 2017;Romero et al., 2015) as well as recent theoretical studies (Phuong & Lampert, 2019).However, the reason why soft labels are beneficial to the student network is still not well explained.Giving a clear theoretical explanation is challenging: The optimization details of a deep network with the common one-hot labels are still not well-studied (Nagarajan & Kolter, 2019), not to mention training with the soft labels.Nevertheless, two recent studies (M\u00fcller et al., 2019;Yuan et al., 2020) shed light on the intuitions about how the soft labels work.Specifically, label smoothing, which is a special case of soft labels based training, is shown to regularize the activations of the penultimate layer to the network (M\u00fcller et al., 2019).The regularization property of soft labels is further explored in (Yuan et al., 2020).They hypothesize that in KD, one main reason why the soft labels work is the regularization introduced by soft labels.Based on the assumption, the authors design a teacher-free distillation method by turning the predictions of the student network into soft labels.Considering that soft labels are targets for distillation, the evidence of the regularization brought by soft labels drives us to rethink soft labels for KD: Soft labels are both supervisory signals and regularizers.Meanwhile, it is known that there is a tradeoff between fitting the data and imposing regularizations, i.e., the bias-variance dilemma (Kohavi & Wolpert, 1996;Bishop, 2006), but it is unclear how bias and variance change for distillation with soft labels.Since the bias-variance tradeoff is an important issue in statistical learning, we investigate whether the bias-variance tradeoff exists for soft labels and how the tradeoff affects distillation performance.We first compare the bias and variance decomposition of direct training with that of distillation with soft labels, noticing that distillation results in a larger bias error and a smaller variance.Then, we rewrite distillation loss into the form of a regularization loss adding the direct training loss.Through inspecting the gradients of the two terms during training, we notice that for soft labels, the biasvariance tradeoff varies sample-wisely.Moreover, by looking into a conclusion from (M\u00fcller et al., 2019), we observe that under the same temperature setting, the distillation performance is negatively associated with the number of some certain samples.These samples lead to bias increase and variance decrease and we name them as regularization samples.To investigate how regularization samples affect distillation, we first examine if we can design ad hoc filters for soft labels to avoid training with regularization samples.But completely filtering out regularization samples also deteriorates distillation performance, leading us to speculate that regularization samples are not well handled by standard KD.In the light of these findings, we propose weighted soft labels for distillation to handle the sample-wise bias-variance tradeoff, by adaptively assigning a lower weight to regularization samples and a larger weight to the others.To sum up, our contributions are:\u2022 For knowledge distillation, we analyze how the soft labels work from a perspective of biasvariance tradeoff.\u2022 We discover that the bias-variance tradeoff varies sample-wisely.Also, we discover that if we fix the distillation temperature, the number of regularization samples is negatively associated with the distillation performance.\u2022 We design straightforward schemes to alleviate negative impacts from regularization samples and then propose the novel weighted soft labels for distillation.Experiments on large scale datasets validate the effectiveness of the proposed weighted soft labels."}
{"paper_id": 118, "abstract": "In the ever-evolving realm of machine learning, a new frontier has emerged\u2014one that seeks to unravel the complexities of high-dimensional spatiotemporal phenomena through the lens of differential equations. In this article, we embark on an ambitious journey to forge a novel and versatile paradigm, inspired by the time-honored technique of separation of variables in partial differential equations. This approach not only breathes new life into the concept of spatiotemporal disentanglement but also offers a dynamic interpretation of how we can learn to capture the intricate dance between space and time.  At the heart of our model lies a principled framework that empowers us to disentangle spatial and temporal representations, allowing for the precise prediction of future observations. Through rigorous experimentation, we unveil the prowess of our method, showcasing its remarkable performance and wide-ranging applicability against the backdrop of existing state-of-the-art models, whether in the realm of physical phenomena or the intricacies of synthetic video datasets. Join us as we chart this new course, pushing the boundaries of what is possible in the prediction of spatiotemporal dynamics.", "introduction": "The interest of the machine learning community in physical phenomena has substantially grown for the last few years (Shi et al., 2015;Long et al., 2018;Greydanus et al., 2019).In particular, an increasing amount of works studies the challenging problem of modeling the evolution of dynamical systems, with applications in sensible domains like climate or health science, making the understanding of physical phenomena a key challenge in machine learning.To this end, the community has successfully leveraged the formalism of dynamical systems and their associated differential formulation as powerful tools to specifically design efficient prediction models.In this work, we aim at studying this prediction problem with a principled and general approach, through the prism of Partial Differential Equations (PDEs), with a focus on learning spatiotemporal disentangled representations.Prediction via spatiotemporal disentanglement was first studied in video prediction works, in order to separate static and dynamic information (Denton & Birodkar, 2017) for prediction and interpretability purposes.Existing models are particularly complex, involving either adversarial losses or variational inference.Furthermore, their reliance on Recurrent Neural Networks (RNNs) hinders their ability to model spatiotemporal phenomena (Y\u0131ld\u0131z et al., 2019;Ayed et al., 2020;Franceschi et al., 2020).Our proposition addresses these shortcomings with a simplified and improved model by grounding spatiotemporal disentanglement in the PDE formalism.Spatiotemporal phenomena obey physical laws such as the conservation of energy, that lead to describe the evolution of the system through PDEs.Practical examples include the conservation of energy for physical systems (Hamilton, 1835), or the equation describing constant illumination in a scene (Horn & Schunck, 1981) for videos that has had a longstanding impact in computer vision with optical flow methods (Dosovitskiy et al., 2015;Finn et al., 2016).We propose to model the evolution of partially observed spatiotemporal phenomena with unknown dynamics by leveraging a formal method for the analytical resolution of PDEs: the functional separation of variables (Miller, 1988).Our framework formulates spatiotemporal disentanglement for prediction as learning a separable solution, where spatial and dynamic information are represented in separate variables.Besides offering a novel interpretation of spatiotemporal disentanglement, it confers simplicity and performance compared to existing methods: disentanglement is achieved through the sole combination of a prediction objective and regularization penalties, and the temporal dynamics is defined by a learned Ordinary Differential Equation (ODE).We experimentally demonstrate the applicability, disentanglement capacity and forecasting performance of the proposed model on various spatiotemporal phenomena involving standard physical processes and synthetic video datasets against prior state-of-the-art models."}
{"paper_id": 119, "abstract": "In the realm of machine learning, where the vast expanse of data often feels like an uncharted cosmos, this paper embarks on a quest to illuminate the challenges of self-supervised learning, particularly for smaller models. Our exploration is sparked by empirical findings that reveal a stark truth: while the widely embraced contrastive self-supervised learning methods have propelled large models to remarkable heights, they falter when applied to their smaller counterparts.  To navigate this treacherous terrain, we introduce a novel paradigm\u2014Self-Supervised Distillation, affectionately dubbed **SEED**. In this innovative approach, we harness the wisdom of a larger network, our Teacher, to bestow its representational prowess upon a smaller architecture, our Student, in a self-supervised manner. Rather than merely learning from the unlabeled chaos of data, we train our Student encoder to replicate the nuanced similarity score distribution that the Teacher infers across a diverse set of instances.  The results of our endeavor are nothing short of transformative. **SEED** breathes new life into small networks, significantly enhancing their performance on downstream tasks. In a head-to-head comparison with existing self-supervised baselines, **SEED** propels the top-1 accuracy from a modest 42.2% to an impressive 67.6% on EfficientNet-B0 and from 36.3% to a staggering 68.2% on MobileNet-v3-Large, all within the expansive landscape of the ImageNet-1k dataset. Through this work, we not only shed light on the path forward for small models but also chart a new course in the ever-evolving saga of self-supervised learning.", "introduction": "The vertical axis is the top-1 accuracy and the horizontal axis is the number of learnable parameters for different network architectures.Directly applying self-supervised contrastive learning (MoCo-V2) does not work well for smaller architectures, while our method (SEED) leads to dramatic performance boost.Details of the setting can be found in Section 4.The burgeoning studies and success on self-supervised learning (SSL) for visual representation are mainly marked by its extraordinary potency of learning from unlabeled data at scale.Accompanying with the SSL is its phenomenal benefit of obtaining task-agnostic representations while allowing the training to dispense with prohibitively expensive data labeling.Major ramifications of visual SSL include pretext tasks (Noroozi & Favaro, 2016;Zhang et al., 2016;Gidaris et al., 2018;Zhang et al., 2019;Feng et al., 2019), contrastive representation learning (Wu et al., 2018;He et al., 2020;Chen et al., 2020a), online/offline clustering (Yang et al., 2016;Caron et al., 2018;Li et al., 2020;Caron et al., 2020;Grill et al., 2020), etc.Among them, several recent works (He et al., 2020;Chen et al., 2020a;Caron et al., 2020) have achieved comparable or even better accuracy than the supervised pre-training when transferring to downstream tasks, e.g.semi-supervised classification, object detection.The aforementioned top-performing SSL algorithms all involve large networks (e.g., ResNet-50 (He et al., 2016) or larger), with, however, little attention on small networks.Empirically, we find that existing techniques like contrastive learning do not work well on small networks.For instance, the linear probe top-1 accuracy on ImageNet using MoCo-V2 (Chen et al., 2020c) is only 36.3% with MobileNet-V3-Large (see Figure 1), which is much lower compared with its supervised training accuracy 75.2% (Howard et al., 2019).For EfficientNet-B0, the accuracy is 42.2% compared with its supervised training accuracy 77.1% (Tan & Le, 2019).We conjecture that this is because smaller models with fewer parameters cannot effectively learn instance level discriminative representation with large amount of data.To address this challenge, we inject knowledge distillation (KD) (Bucilu\u01ce et al., 2006;Hinton et al., 2015) into self-supervised learning and propose self-supervised distillation (dubbed as SEED) as a new learning paradigm.That is, train the larger, and distill to the smaller both in self-supervised manner.Instead of directly conducting self-supervised training on a smaller model, SEED first trains a large model (as the teacher) in a self-supervised way, and then distills the knowledge to the smaller model (as the student).Note that the conventional distillation is for supervised learning, while the distillation here is in the self-supervised setting without any labeled data.Supervised distillation can be formulated as training a student to mimic the probability mass function over classes predicted by a teacher model.In unsupervised knowledge distillation setting, however, the distribution over classes is not directly attainable.Therefore, we propose a simple yet effective self-supervised distillation method.Similar to (He et al., 2020;Wu et al., 2018), we maintain a queue of data samples.Given an instance, we first use the teacher network to obtain its similarity scores with all the data samples in the queue as well as the instance itself.Then the student encoder is trained to mimic the similarity score distribution inferred by the teacher over these data samples.The simplicity and flexibility that SEED brings are self-evident.1) It does not require any clustering/prototypical computing procedure to retrieve the pseudo-labels or latent classes.2) The teacher model can be pre-trained with any advanced SSL approach, e.g., MoCo-V2 (Chen et al., 2020c), SimCLR (Chen et al., 2020a), SWAV (Caron et al., 2020).3) The knowledge can be distilled to any target small networks (either shallower, thinner, or totally different architectures).To demonstrate the effectiveness, we comprehensively evaluate the learned representations on series of downstream tasks, e.g., fully/semi-supervised classification, object detection, and also assess the transferability to other domains.For example, on ImageNet-1k dataset, SEED improves the linear probe accuracy of EfficientNet-B0 from 42.2% to 67.6% (a gain over 25%), and MobileNet-V3 from 36.3% to 68.2% (a gain over 31%) compared to MoCo-V2 baselines, as shown in Figure 1 and Section 4.Our contributions can be summarized as follows:\u2022 We are the first to address the problem of self-supervised visual representation learning for small models.\u2022 We propose a self-supervised distillation (SEED) technique to transfer knowledge from a large model to a small model without any labeled data.\u2022 With the proposed distillation technique (SEED), we significantly improve the state-of-theart SSL performance on small models.\u2022 We exhaustively compare a variety of distillation strategies to show the validity of SEED under multiple settings."}
{"paper_id": 120, "abstract": "In this exploration, we unveil a groundbreaking approach known as Generative Cellular Automata\u2014a probabilistic 3D generative model that conjures a tapestry of diverse and strikingly intricate shapes. Picture, if you will, the shape generation process as a dance of chance, where we sample from the transition kernel of a Markov chain. This chain, through its intricate steps, gradually unfurls into the complete shape dictated by the learned distribution.  At the core of our method lies a clever application of cellular automata's local update rules. This approach artfully narrows the vast search space inherent in high-resolution 3D grids, leveraging the unique connectivity and inherent sparsity found within 3D shapes. Our generation process is progressive, honing in on a sparse collection of occupied voxels and their immediate neighbors. This focus not only streamlines the process but also allows us to harness the power of an expressive sparse convolutional network.  To ensure our model learns effectively, we introduce a robust training scheme designed to derive the local homogeneous rules of our generative cellular automata. This scheme utilizes sequences that, while subtly differing from the sampling chain, converge beautifully to the full shapes represented in our training data. Through extensive experimentation in both probabilistic shape completion and shape generation, we demonstrate that our method stands shoulder to shoulder with the most advanced techniques in the field, delivering performance that is nothing short of competitive.", "introduction": "Probabilistic 3D shape generation aims to learn and sample from the distribution of diverse 3D shapes and has applications including 3D contents generation or robot interaction.Specifically, learning the distribution of shapes or scenes can automate the process of generating diverse and realistic virtual environments or new object designs.Likewise, modeling the conditional distribution of the whole scene given partial raw 3D scans can help the decision process of a robot, by informing various possible outputs of occluded space.The distribution of plausible shapes in 3D space is diverse and complex, and we seek a scalable formulation of the shape generation process.Pioneering works on 3D shape generation try to regress the entire shape (Dai et al. (2017)) which often fail to recover fine details.We propose a more modular approach that progressively generates shape by a sequence of local updates.Our work takes inspiration from prior works on autoregressive models in the image domains, such as the variants of pixelCNN (van den Oord & Kalchbrenner (2016); van den Oord et al. (2016;2017)), which have been successful in image generation.The key idea of pixelCNN (van den Oord et al. (2016)) is to order the pixels, and then learn the conditional distribution of the next pixel given all of the previous pixels.Thus generating an image becomes the task of sampling pixel-by-pixel in the predefined order.Recently, PointGrow (Sun et al. (2020)) proposes a similar approach in the field of 3D generation, replacing the RGB values of pixels with the coordinates of points and sampling point-by-point in a sequential manner.While the work proposes a promising interpretable generation process by sequentially growing a shape, the required number of sampling procedures expands linearly with the number of points, making the model hard to scale to high-resolution data.We believe that a more scalable solution in 3D is to employ the local update rules of cellular automata (CA).CA, a mathematical model operating on a grid, defines a state to be a collection of cells that carries values in the grid (Wolfram (1982)).The CA repeatedly mutates its states based on the predefined homogeneous update rules only determined by the spatial neighborhood of the current cell.In contrast to the conventional CA where the rules are predefined, we employ a neural network to infer the stochastic sequential transition rule of individual cells based on Markov chain.The obtained homogeneous local rule for the individual cells constitutes the 3D generative model, named Generative Cellular Automata (GCA).When the rule is distributed into the group of occupied cells of an arbitrary starting shape, the sequence of local transitions eventually evolves into an instance among the diverse shapes from the multi-modal distribution.The local update rules of CA greatly reduce the search space of voxel occupancy, exploiting the sparsity and connectivity of 3D shapes.We suggest a simple, progressive training procedure to learn the distribution of local transitions of which repeated application generates the shape of the data distribution.We represent the shape in terms of surface points and store it within a 3D grid, and the transition rule is trained only on the occupied cells by employing a sparse CNN (Graham et al. (2018)).The sparse representation can capture the high-resolution context information, and yet learn the effective rule enjoying the expressive power of deep CNN as demonstrated in various computer vision tasks (Krizhevsky et al. (2012); He et al. (2017)).Inspired by Bordes et al. (2017), our model learns sequences that are slightly different from the sampling chain but converge to the full shapes in the training data.The network successfully learns the update rules of CA, such that a single inference samples from the distribution of diverse modes along the surface.The contributions of the paper are highlighted as follows: (1) We propose Generative Cellular Automata (GCA), a Markov chain based 3D generative model that iteratively mends the shape to a learned distribution, generating diverse and high-fidelity shapes.(2) Our work is the first to learn the local update rules of cellular automata for 3D shape generation in voxel representation.This enables the use of an expressive sparse CNN and reduces the search space of voxel occupancy by fully exploiting sparsity and connectivity of 3D shapes.(3) Extensive experiments show that our method has competitive performance against the state-of-the-art models in probabilistic shape completion and shape generation."}
{"paper_id": 121, "abstract": "In the realm of machine learning, the quest for robustness often leads practitioners down the path of adversarial training (AT), a strategy heralded for its prowess in fortifying models against cunning attacks. Yet, recent evaluations reveal a perplexing truth: many enhancements proposed for AT fall short, with the simple act of early stopping proving to be a more potent ally. This unexpected revelation sparks our curiosity, compelling us to delve into the intricate tapestry of implementation details across a multitude of AT methods.  What we uncover is both surprising and illuminating. The foundational settings\u2014weight decay, training schedules, and more\u2014exhibit a staggering inconsistency across these approaches. In our exploration, we conduct thorough evaluations on the CIFAR-10 dataset, shining a light on the often-neglected training tricks and hyperparameters that influence adversarially trained models. Our findings reveal a startling sensitivity of adversarial robustness to these basic configurations. For instance, a mere adjustment in weight decay can diminish a model's robust accuracy by over 7%, potentially eclipsing any gains promised by more elaborate techniques.  From these insights, we establish a baseline training framework and re-implement previous defenses, ultimately achieving new state-of-the-art results. Our work not only underscores the critical importance of these overlooked confounders in benchmarking defenses but also invites the community to reconsider the foundational elements of adversarial training. In a world where every detail matters, our journey reveals that the simplest choices can wield the greatest power.", "introduction": "Adversarial training (AT) has been one of the most effective defense strategies against adversarial attacks (Biggio et al., 2013;Szegedy et al., 2014;Goodfellow et al., 2015).Based on the primary AT frameworks like PGD-AT (Madry et al., 2018), many improvements have been proposed from different perspectives, and demonstrate promising results (detailed in Sec. 2).However, the recent benchmarks (Croce & Hein, 2020b;Chen & Gu, 2020) find that simply early stopping the training procedure of PGD-AT (Rice et al., 2020) can attain the gains from almost all the previously proposed improvements, including the state-of-the-art TRADES (Zhang et al., 2019b).This fact is somewhat striking since TRADES also executes early stopping (one epoch after decaying the learning rate) in their code implementation.Besides, the reported robustness of PGD-AT in Rice et al. (2020) is much higher than in Madry et al. (2018), even without early-stopping.This paradox motivates us to check the implementation details of these seminal works.We find that TRADES uses weight decay of 2 \u00d7 10 -4 , Gaussian PGD initialization as \u03b4 0 \u223c N (0, \u03b1I), and eval mode of batch normalization (BN) when crafting adversarial examples, while Rice et al. (2020) use weight decay of 5 \u00d7 10 -4 , uniform PGD initialization as \u03b4 0 \u223c U(-, ), and train mode of BN to generate adversarial examples.In our experiments on CIFAR-10 (e.g., Table 8), the two slightly different settings can differ the robust accuracy by \u223c 5%, which is significant according to the reported benchmarks.To have a comprehensive study, we further investigate the implementation details of tens of papers working on the AT methods, some of which are summarized in Table 1.We find that even using the same model architectures, the basic hyperparameter settings (e.g., weight decay, learning rate schedule, etc.) used in these papers are highly inconsistent and customized, which could affect the model performance and may override the gains from the methods themselves.Under this situation, if we directly benchmark these methods using their released code or checkpoints, some actually effective improvements would be under-estimated due to the improper hyperparameter settings.Our contributions.We evaluate the effects of a wide range of basic training tricks (e.g., warmup, early stopping, weight decay, batch size, BN mode, etc.) on the adversarially trained models.Our empirical results suggest that improper training settings can largely degenerate the model performance, while this degeneration may be mistakenly ascribed to the methods themselves.We provide a baseline recipe for PGD-AT on CIFAR-10 as an example, and demonstrate the generality of the recipe on training other frameworks like TRADES.As seen in Table 16, the retrained TRADES achieve new state-of-the-art performance on the AutoAttack benchmark (Croce & Hein, 2020b).Although our empirical conclusions may not generalize to other datasets or tasks, we reveal the facts that adversarially trained models could be sensitive to certain training settings, which are usually neglected in previous work.These results also encourage the community to re-implement the previously proposed defenses with fine-tuned training settings to better explore their potentials."}
{"paper_id": 122, "abstract": "In the ever-evolving realm of deep learning, a new contender has emerged: Neural Ordinary Differential Equations (Neural ODEs), a groundbreaking class of models characterized by their continuous depth. Yet, as with any powerful magic, challenges abound. The numerical estimation of gradients in this continuous landscape remains a formidable obstacle. Current implementations of the adjoint method often falter, plagued by inaccuracies in reverse-time trajectories. Meanwhile, simpler approaches, like the naive method and the adaptive checkpoint adjoint method (ACA), are burdened by a memory cost that swells with the passage of integration time.  Enter the Memory-efficient ALF Integrator (MALI), a solution forged from the innovative asynchronous leapfrog (ALF) solver. MALI stands as a beacon of efficiency, boasting a constant memory cost relative to integration time\u2014akin to the adjoint method\u2014while ensuring precision in reverse-time trajectories and, by extension, in gradient estimation.  Our validation of MALI spans a diverse array of tasks. In the realm of image recognition, MALI has achieved a historic milestone, enabling the feasible training of a Neural ODE on ImageNet and surpassing even the most finely-tuned ResNet models, where others have stumbled under the weight of memory demands or inaccuracies. In time series modeling, MALI decisively outshines the adjoint method. And when it comes to continuous generative models, MALI sets new benchmarks for performance.  For those eager to wield this powerful tool, we proudly present our PyPI package: [TorchDiffEqPack](https://jzkay12.github.io/TorchDiffEqPack). With MALI in your arsenal, the potential of Neural ODEs is not just a dream\u2014it's a reality waiting to be unlocked.", "introduction": "Recent research builds the connection between continuous models and neural networks.The theory of dynamical systems has been applied to analyze the properties of neural networks or guide the design of networks (Weinan, 2017;Ruthotto & Haber, 2019;Lu et al., 2018).In these works, a residual block (He et al., 2016) is typically viewed as a one-step Euler discretization of an ODE; instead of directly analyzing the discretized neural network, it might be easier to analyze the ODE.Another direction is the neural ordinary differential equation (Neural ODE) (Chen et al., 2018), which takes a continuous depth instead of discretized depth.The dynamics of a Neural ODE is typically approximated by numerical integration with adaptive ODE solvers.Neural ODEs have been applied in irregularly sampled time-series (Rubanova et al., 2019), free-form continuous generative models (Grathwohl et al., 2018;Finlay et al., 2020), mean-field games (Ruthotto et al., 2020), stochastic differential equations (Li et al., 2020) and physically informed modeling (Sanchez-Gonzalez et al., 2019;Zhong et al., 2019).Though the Neural ODE has been widely applied in practice, how to train it is not extensively studied.The naive method directly backpropagates through an ODE solver, but tracking a continuous trajectory requires a huge memory.Chen et al. (2018) proposed to use the adjoint method to determine the gradient in continuous cases, which achieves constant memory cost w.r.t integration time; however, as pointed out by Zhuang et al. (2020), the adjoint method suffers from numerical errors due to the inaccuracy in reverse-time trajectory.Zhuang et al. (2020) proposed the adaptive checkpoint adjoint (ACA) method to achieve accuracy in gradient estimation at a much smaller memory cost compared to the naive method, yet the memory consumption of ACA still grows linearly with integration time.Due to the non-constant memory cost, neither ACA nor naive method are suitable for large scale datasets (e.g.ImageNet) or high-dimensional Neural ODEs (e.g.FFJORD (Grathwohl et al., 2018)).In this project, we propose the Memory-efficient Asynchronous Leapfrog Integrator (MALI) to achieve advantages of both the adjoint method and ACA: constant memory cost w.r.t integration time and accuracy in reverse-time trajectory.MALI is based on the asynchronous leapfrog (ALF) integrator (Mutze, 2013).With the ALF integrator, each numerical step forward in time is reversible.Therefore, with MALI, we delete the trajectory and only keep the end-time states, hence achieve constant memory cost w.r.t integration time; using the reversibility, we can accurately reconstruct the trajectory from the end-time value, hence achieve accuracy in gradient.Our contributions are:1. We propose a new method (MALI) to solve Neural ODEs, which achieves constant memory cost w.r.t number of solver steps in integration and accuracy in gradient estimation.We provide theoretical analysis.2. We validate our method with extensive experiments: (a) for image classification tasks, MALI enables a Neural ODE to achieve better accuracy than a well-tuned ResNet with the same number of parameters; to our knowledge, MALI is the first method to enable training of Neural ODEs on a large-scale dataset such as ImageNet, while existing methods fail due to either heavy memory burden or inaccuracy.(b) In time-series modeling, MALI achieves comparable or better results than other methods.(c) For generative modeling, a FFJORD model trained with MALI achieves new state-of-the-art results on MNIST and Cifar10."}
{"paper_id": 123, "abstract": "In the ever-evolving landscape of deep learning, the recent advent of the DL compiler, coupled with the innovative approach of Learning to Compile, has emerged as a formidable force in the quest for optimizing deep learning models. Yet, a critical gap remains: current methodologies tend to hone in on accelerating the convergence of individual tensor operators, often neglecting the overarching convergence speed of the entire model. This oversight can lead to protracted optimization times, leaving practitioners grappling with delays in achieving their desired latency.  Enter DynaTune, a groundbreaking method that redefines the optimization landscape for deep neural networks. At the heart of DynaTune lies a clever application of the Multi-Armed Bandit (MAB) framework, tailored specifically for the challenges of tensor program optimization. By leveraging Upper Confidence Bound (UCB) strategies, we adeptly navigate the complexities of time-slot-based optimization. Furthermore, we introduce a sophisticated Bayesian belief model that not only predicts the potential performance gains of each operator but also quantifies the inherent uncertainties, steering the optimization journey with precision.  In our rigorous evaluations, DynaTune stands head and shoulders above the competition, demonstrating speeds that are an impressive 1.2 to 2.4 times faster than the leading DL compiler, all while maintaining an equivalent level of optimization quality across a diverse array of models and hardware architectures. With DynaTune, we are not just optimizing; we are revolutionizing the way deep learning models converge, ushering in a new era of efficiency and performance.", "introduction": "The enormous computational intensity of Deep Neural Network (DNN) models has attracted great interest in optimizing their performance.Popular deep learning (DL) frameworks such as Py-Torch (Paszke et al., 2019) and TensorFlow (Abadi et al., 2016) adopt custom optimized kernels such as Intel MKL-DNN or Nvidia cuDNN (Chetlur et al., 2014) as back-end.However, given the increasing complexity of tensor operations in DNNs and the volatility of DL algorithms, it calls for developing fast and automated compilation frameworks to handle the unprecedented amount of innovations.To imitate or even exceed the success of hand-optimized libraries, recent research has developed neural network compilers, such as XLA (Leary & Wang, 2017), Glow (Rotem et al., 2018), Tensor Comprehension (Vasilache et al., 2018), and TVM (Chen et al., 2018a).Among them, TVM has shown superior performance improvements using a technique called Learning to Compile (AutoTVM) (Chen et al., 2018b).AutoTVM optimizes the code by generating many versions of a tensor operator and chooses the best through a learned cost model and search over a large space of code transformation choices.While the Learning to Compile approach produces highly optimized code of DNN models, it suffers from excessively long optimization time.As an example, although AutoTVM is able to demonstrate close to 2\u00d7 performance improvement over TensorFlow on ResNet-18, the optimization time can take several hours or even tens of hours (Chen et al., 2018b).The long optimization time hinders the turnaround time and even puts the practical utility of the current compiler-based solutions into question.Recent works strive to reduce the optimization time by improving the search strategy for the code transformation plan and lowering the hardware measurement cost (Ahn et al., 2020;Adams et al., 2019).However, these approaches mostly focus on accelerating the convergence speed of optimization at the individual tensor operator level (e.g., Conv2D, batched GEMM), which do not necessarily solve the issue of slow convergence and long optimization time of the entire model, often containing tens of tensor operators.Different from existing methods, we introduce DynaTune, a DL code optimization algorithm that minimizes the sum of the execution time of all operators in a model as much as possible and as quickly as possible.Specifically, the contributions of our paper consist of (1) a preliminary analysis that reveals the challenges and opportunities from existing DL code optimization strategies, (2) a time-slot-based optimization scheme, which simultaneously explores different operators and learns in an online manner that allows to dynamically switch to optimizing more promising tensors operators.(3) a Bayesian belief model that predicts future performance gains of operators, which helps make better decisions and expedites the convergence speed.(4) a detailed evaluation of the proposed algorithm with modern DNNs (ResNet-18, VGG, SqueezeNet, Transformer) on both CPU and GPU.Compared with the leading framework, AutoTVM, DynaTune is 1.2-2.4\u00d7times faster to obtain the same levels of optimization."}
{"paper_id": 124, "abstract": "In the ever-evolving landscape of convolutional neural networks (CNNs), recent explorations into dynamic convolution have unveiled a remarkable potential for enhancing efficiency through the adaptive synthesis of K static convolution kernels. Yet, like any powerful tool, it comes with its own set of challenges. Two notable limitations emerge: first, the method multiplies the number of convolutional weights by K, and second, the intricate dance of optimizing both dynamic attention and static kernels proves to be a formidable task.  In this paper, we embark on a journey to reexamine dynamic convolution through the lens of matrix decomposition, uncovering a pivotal insight: the crux of the issue lies in the application of dynamic attention across channel groups after projecting into a higher-dimensional latent space. To navigate this complexity, we introduce a novel approach\u2014dynamic channel fusion. This innovative technique not only streamlines the dimensionality of the latent space but also alleviates the difficulties associated with joint optimization.   The result? A method that is not only easier to train but also demands significantly fewer parameters, all while maintaining the accuracy we strive for in our models. Join us as we unveil the intricacies of this approach, which promises to reshape our understanding of dynamic convolution. For those eager to delve deeper, the source code awaits at https://github.com/liyunsheng13/dcd.", "introduction": "Dynamic convolution (Yang et al., 2019;Chen et al., 2020c) has recently become popular for the implementation of light-weight networks (Howard et al., 2017;Zhang et al., 2018b).Its ability to achieve significant performance gains with negligible computational cost has motivated its adoption for multiple vision tasks (Su et al., 2020;Chen et al., 2020b;Ma et al., 2020;Tian et al., 2020).The basic idea is to aggregate multiple convolution kernels dynamically, according to an input dependent attention mechanism, into a convolution weight matrixwhere K convolution kernels {W k } are aggregated linearly with attention scores {\u03c0 k (x)}.Dynamic convolution has two main limitations: (a) lack of compactness, due to the use of K kernels, and (b) a challenging joint optimization of attention scores {\u03c0 k (x)} and static kernels {W k }.Yang et al. (2019) proposed the use of a sigmoid layer to generate attention scores {\u03c0 k (x)}, leading to a significantly large space for the convolution kernel W (x) that makes the learning of attention scores {\u03c0 k (x)} difficult.Chen et al. (2020c) replaced the sigmoid layer with a softmax function to compress the kernel space.However, small attention scores \u03c0 k output by the softmax make the corresponding kernels W k difficult to learn, especially in early training epochs, slowing training convergence.To mitigate these limitations, these two methods require additional constraints.For instance, Chen et al. (2020c) uses a large temperature in the softmax function to encourage nearuniform attention.In this work, we revisit the two limitations via matrix decomposition.To expose the limitations, we reformulate dynamic convolution in terms of a set of residuals, re-defining the static kernels as 3).It applies dynamic attention \u03a0(x) over channel groups in a high dimensional space (SV T x \u2208 R KC ).Right: proposed dynamic convolution decomposition, which applies dynamic channel fusion \u03a6(x) in a low dimensional space (Q T x \u2208 R L , L C), resulting in a more compact model.whereW k is the average kernel and \u2206W k = W k -W 0 a residual weight matrix.Further decomposing the latter with an SVD,whereand \u03a0(x) stacks attention scores diagonally as \u03a0(x) = diag(\u03c0 1 (x)I, . . ., \u03c0 K (x)I), where I is an identity matrix.This decomposition, illustrated in Figure 1, shows that the dynamic behavior of W (x) is implemented by the dynamic residual U \u03a0(x)SV T , which projects the input x to a higher dimensional space SV T x (from C to KC channels), applies dynamic attention \u03a0(x) over channel groups, and reduces the dimension back to C channels, through multiplication by U .This suggests that the limitations of vanilla dynamic convolution are due to the use of attention over channel groups, which induces a high dimensional latent space, leading to small attention values that may suppress the learning of the corresponding channels.To address this issue, we propose a dynamic convolution decomposition (DCD), that replaces dynamic attention over channel groups with dynamic channel fusion.The latter is based on a full dynamic matrix \u03a6(x), of which each element \u03c6 i,j (x) is a function of input x.As shown in Figure 1-(right), the dynamic residual is implemented as the product P \u03a6(x)Q T of \u03a6(x) and two static matrices P , Q, such that Q compresses the input into a low dimensional latent space, \u03a6(x) dynamically fuses the channels in this space, and P expands the number of channels to the output space.The key innovation is that dynamic channel fusion with \u03a6(x) enables a significant dimensionality reduction of the latent space (Q T x \u2208 R L , L C). Hence the number of parameters in P , Q is significantly reduced, when compared to U , V of Eq. 3, resulting in a more compact model.Dynamic channel fusion also mitigates the joint optimization challenge of vanilla dynamic convolution, as each column of P , Q is associated with multiple dynamic coefficients of \u03a6(x).Hence, a few dynamic coefficients of small value are not sufficient to suppress the learning of static matrices P , Q. Experimental results show that DCD both significantly reduces the number of parameters and achieves higher accuracy than vanilla dynamic convolution, without requiring the additional constraints of (Yang et al., 2019;Chen et al., 2020c)."}
{"paper_id": 125, "abstract": "In the realm of machine learning, where the quest for efficiency and precision is relentless, knowledge distillation emerges as a beacon of hope. This technique, wherein a student model diligently learns to emulate the wisdom of a teacher model, has proven itself a formidable ally in the dual battles of model compression and accuracy enhancement. Yet, as we delve deeper, we uncover a troubling truth: the majority of existing knowledge distillation methods, crafted primarily for the straightforward domain of image classification, falter when faced with the intricate challenges of object detection.  In this paper, we unveil the underlying reasons for this shortcoming. We argue that the struggles of knowledge distillation in object detection stem from two critical issues: (1) the stark imbalance between the myriad pixels of the foreground and the overwhelming expanse of the background, and (2) the neglect of the intricate relationships that bind different pixels together. With these insights in hand, we introduce two innovative approaches: attention-guided distillation and non-local distillation, each meticulously designed to tackle one of these challenges.  Attention-guided distillation harnesses the power of attention mechanisms to illuminate the vital pixels of foreground objects, compelling the student model to focus its learning on these essential features. Meanwhile, non-local distillation empowers students to transcend the limitations of isolated pixel learning, enabling them to grasp the complex interrelations among pixels through sophisticated non-local modules.   Our experiments yield compelling results, demonstrating substantial improvements in average precision (AP) across both one-stage and two-stage detectors, whether anchor-based or anchor-free. Notably, our approach elevates the performance of Faster RCNN with a ResNet101 backbone to an impressive 43.9 AP on the COCO2017 dataset, a remarkable 4.1 points above the baseline. For those eager to explore these advancements, our code has been made available on GitHub, inviting the community to join us in this exciting journey of discovery and innovation.", "introduction": "Recently, excellent breakthrough in various domains has been achieved with the success of deep learning (Ronneberger et al., 2015;Devlin et al., 2018;Ren et al., 2015).However, the most advanced deep neural networks always consume a large amount of computation and memory, which has limited their deployment in edge devices such as self-driving cars and mobile phones.To address this problem, abundant techniques are proposed, including pruning (Han et al., 2016;Zhang et al., 2018;Liu et al., 2018;Frankle & Carbin, 2018), quantization (Nagel et al., 2019;Zhou et al., 2017), compact model design (Sandler et al., 2018;Howard et al., 2019;Ma et al., 2018;Iandola et al., 2016) and knowledge distillation (Hinton et al., 2014;Bucilu\u01ce et al., 2006).Knowledge distillation, which is also known as teacher-student learning, aims to transfer the knowledge of an over-parameterized teacher to a lightweight student.Since the student is trained to mimic the logits or features of the teacher, the student can inherit the dark knowledge from the teacher, and thus often achieves much higher accuracy.Due to its simplicity and effectiveness, knowledge distillation has become a popular technique for both model compression and model accuracy boosting.As one of the most crucial challenges in computer vision, object detection has an urgent requirement of both accurate and efficient models.Unfortunately, most of the existing knowledge distillation methods in computer vision are designed for image classification and usually leads to trivial improvements on object detection (Li et al., 2017).In this paper, we impute the failure of knowledge distillation on object detection to the following two issues, which will be solved later, respectively.Imbalance between foreground and background.In an image to be detected, the background pixels are often more overwhelming than the pixels of the foreground objects.However, in previous knowledge distillation, the student is always trained to mimic the features of all pixels with the same priority.As a result, students have paid most of their attention to learning background pixels features, which suppresses student's learning on features of the foreground objects.Since foreground pixels are more crucial in detection, the imbalance hurts the performance of knowledge distillation severely.To overcome this obstacle, we propose the attention-guided distillation which distills only the crucial foreground pixels.Since the attention map can reflect the position of the important pixels (Zhou et al., 2016), we adopt the attention map as the mask for knowledge distillation.Concretely, the pixel with a higher attention value is regarded as a pixel of a foreground object and then is learned by the student model with a higher priority.Compared with the previous binary mask method (Wang et al., 2019), the mask generated by attention maps in our methods is more fine-grained and requires no additional supervision.Compared with the previous attention-based distillation methods (Zagoruyko & Komodakis, 2017), the attention map in our methods is not only utilized as the information to be distilled but also utilized as the mask signal for feature distillation.Lack of distillation on relation information.It is generally acknowledged that the relation between different objects contains valuable information in object detection.Recently, lots of researchers successfully improve the performance of detectors by enabling detectors to capture and make use of these relations, such as non-local modules (Wang et al., 2018) and relation networks (Hu et al., 2018).However, the existing object detection knowledge distillation methods only distill the information of individual pixels but ignore the relation of different pixels.To solve this issue, we propose the non-local distillation, which aims to capture the relation information of students and teachers with non-local modules and then distill them from teachers to students.Since the non-local modules and attention mechanism in our methods are only required in the training period, our methods don't introduce additional computation and parameters in the inference period.Besides, our methods are feature-based distillation methods which do not depend on a specific detection algorithm so they can be directly utilized in all kinds of detectors without any modification.On MS COCO2017, 2.9, 2.9 and 2.2 AP improvements can be observed on two-stage, one-stage, and anchor-free models on average, respectively.Experiments on Mask RCNN show that our methods can also improve the performance of instance segmentation by 2.0 AP, on average.We have conducted a detailed ablation study and sensitivity study to show the effectiveness and stability of each distillation loss.Moreover, we study the relation between teachers and students on object detection and find that knowledge distillation on object detection requires a high AP teacher, which is different from the conclusion in image classification where a high AP teacher may harm the performance of students (Mirzadeh et al., 2019;Cho & Hariharan, 2019).We hope that these results are worth more contemplation of knowledge distillation on tasks except for image classification.To sum up, the contribution of this paper can be summarized as follows.\u2022 We propose the attention-guided distillation, which emphasizes students' learning on the foreground objects and suppresses students' learning on the background pixels.\u2022 We propose the non-local distillation, which enables the students to learn not only the information of the individual pixel but also the relation between different pixels from teachers.\u2022 We show that a teacher with higher AP is usually a better teacher in knowledge distillation on object detection, which is different from the conclusion in image classification."}
{"paper_id": 126, "abstract": "In this paper, we embark on a journey into the realm of visual counting, a fascinating endeavor that seeks to predict the number of occurrences within a natural image based on a given query\u2014be it a question or a category. While many previous approaches have relied on complex, symbolic models that often prove computationally burdensome and struggle with generalization, we present a fresh perspective. Our solution revisits the concept of modulated convolutions, artfully blending the query and the image in a localized manner.  We introduce our method, MoVie\u2014short for Modulated conVolutional bottlenecks. This innovative approach is built upon the principles of residual bottlenecks, allowing MoVie to reason both implicitly and holistically, requiring only a single forward pass during inference. The results speak volumes: MoVie not only advances the state-of-the-art in counting-specific Visual Question Answering (VQA) tasks with remarkable efficiency, but also excels on challenging benchmarks like COCO for common object counting. Our method even propelled us to victory in the 2020 VQA challenge, where it was integrated as a module for addressing \u2018number\u2019 related inquiries within generic VQA models.  Moreover, we provide compelling evidence that modulated convolutions, as exemplified by MoVie, can transcend the boundaries of counting, serving as a versatile mechanism for a variety of reasoning tasks. In this exploration, we open new doors for future research and application, inviting others to join us on this exciting path.", "introduction": "We focus on visual counting: given a natural image and a query, it aims to predict the correct number of occurrences in the image corresponding to that query.The query is generic, which can be a natural language question (e.g.'how many kids are on the sofa') or a category name (e.g.'car').Since visual counting requires open-ended query grounding and multiple steps of visual reasoning (Zhang et al., 2018), it is a unique testbed to evaluate a machine's ability to understand multi-modal data.Mimicking how humans count, most existing counting modules (Trott et al., 2018) adopt an intuition-driven reasoning procedure, which performs counting iteratively by mapping candidate image regions to symbols and count them explicitly based on relationships (Fig. 1, top-left).While interpretable, modeling regions and relations repeatedly can be expensive in computation (Jiang et al., 2020).And more importantly, counting is merely a single visual reasoning task -if we consider the full spectrum of reasoning tasks (e.g.logical inference, spatial configuration), it is probably infeasible to manually design specialized modules for every one of them (Fig. 1, bottom-left).In this paper, we aim to establish a simple and effective alternative for visual counting without explicit, symbolic reasoning.Our work is built on two research frontiers.First, on the synthetic CLEVR dataset (Johnson et al., 2017), it was shown that using queries to directly modulate convolutions can lead to major improvements in the reasoning power of a Convolutional Network (ConvNet) (e.g.achieving near-perfect 94% on counting) (Perez et al., 2018).However, it was difficult to transfer this finding to natural images, partially due to the dominance of bottom-up attention features that represent images with regions (Anderson et al., 2018).Interestingly, recent analysis discovered that plain convolutional features can be as powerful as region features (Jiang et al., 2020), which becomes a second step-stone for our approach to compare fairly against region-based counting modules.Motivated by fusing multi-modalities locally for counting, the central idea behind our approach is to revisit convolutions modulated by query representations.Following ResNet (He et al., 2016), we choose bottleneck as our basic building block, with each bottleneck being modulated once.Multiple bottlenecks are stacked together to form our final module.Therefore, we call our method MoVie: Modulated conVolutional bottlenecks.Inference for MoVie is performed by a simple, feed-forward pass holistically on the feature map, and reasoning is done implicitly (Fig. 1, top-right).MoVie demonstrates strong performance.First, it improves the state-of-the-art on several VQAbased counting benchmarks (HowMany-QA (Trott et al., 2018) and TallyQA (Acharya et al., 2019)), while being more efficient.It also works well on counting common objects, significantly outperforming all previous approaches on challenging datasets like COCO (Lin et al., 2014).Furthermore, we show MoVie can be easily plugged into generic VQA models and improve the 'number' category on VQA 2.0 (Goyal et al., 2017) -and with the help of MoVie, we won the first place of 2020 VQA challenge, achieving 76.36% overall accuracy on the VQA v2.0 test-challenge server. 1 To better understand this implicit model, we present detailed ablative analysis and visualizations of MoVie, and notably find it improves upon its predecessor FiLM (Perez et al., 2018) across all the counting benchmarks we experimented, with a similar computation cost.Finally, we validate the feasibility of MoVie for reasoning tasks beyond counting (Fig. 1, bottomright) by its near-perfect accuracy on CLEVR and competitive results on GQA (Hudson & Manning, 2019a).These evidences suggest that modulated convolutions such as MoVie can potentially serve as a general mechanism for visual reasoning.Code will be made available."}
{"paper_id": 127, "abstract": "In the realm of machine learning, the quest for vast annotated datasets often feels like a hero's arduous journey\u2014one fraught with challenges and bottlenecks that can stall even the most ambitious projects. Enter the realm of weak supervision, a beacon of hope that illuminates the path to generating labeled datasets without the arduous need for ground truth annotations. Instead, it conjures probabilistic labels from a tapestry of multiple, albeit noisy, heuristics. This innovative approach scales effortlessly to vast datasets and has already showcased its prowess across diverse landscapes, from the intricate halls of healthcare to the bustling marketplaces of e-commerce.  Yet, within this promising landscape lies a practical dilemma: the creation of user-generated heuristics demands not only creativity and foresight but also a deep well of domain expertise. It\u2019s a process that can be as tedious as it is subjective, often leaving practitioners yearning for a more efficient solution.   In response, we unveil a groundbreaking framework for interactive weak supervision, where our method boldly proposes heuristics and learns from the invaluable feedback provided by users on each suggestion. Our experiments reveal a remarkable truth: with only a handful of feedback iterations, we can train models that achieve test set performance rivaling that of their fully annotated counterparts, all without the need for ground truth labels. Furthermore, our user studies illuminate a bright future, demonstrating that users can effectively engage with and refine these heuristics, with test set results mirroring the performance of simulated oracles. In this new age of machine learning, we stand at the brink of a revolution, where the synergy of human insight and algorithmic ingenuity paves the way for unprecedented advancements.", "introduction": "The performance of supervised machine learning (ML) hinges on the availability of labeled data in sufficient quantity and quality.However, labeled data for applications of ML can be scarce, and the common process of obtaining labels by having annotators inspect individual samples is often expensive and time consuming.Additionally, this cost is frequently exacerbated by factors such as privacy concerns, required expert knowledge, and shifting problem definitions.Weak supervision provides a promising alternative, reducing the need for humans to hand label large datasets to train ML models (Riedel et al., 2010;Hoffmann et al., 2011;Ratner et al., 2016;Dehghani et al., 2018).A recent approach called data programming (Ratner et al., 2016) combines multiple weak supervision sources by using an unsupervised label model to estimate the latent true class label, an idea that has close connections to modeling workers in crowd-sourcing (Dawid & Skene, 1979;Karger et al., 2011;Dalvi et al., 2013;Zhang et al., 2014).The approach enables subject matter experts to specify labeling functions (LFs)-functions that encode domain knowledge and noisily annotate subsets of data, such as user-specified heuristics or external knowledge bases-instead of needing to inspect and label individual samples.These weak supervision approaches have been used on a wide variety of data types such as MRI sequences and unstructured text, and in various domains such as healthcare and e-commerce (Fries et al., 2019;Halpern et al., 2014;Bach et al., 2019;R\u00e9 et al., 2020).Not only does the use of multiple sources of weak supervision provide a scalable framework for creating large labeled datasets, but it can also be viewed as a vehicle to incorporate high level, conceptual feedback into the data labeling process.In data programming, each LF is an imperfect but reasonably accurate heuristic, such as a pre-trained classifier or keyword lookup.For example, for the popular 20 newsgroups dataset, an LF to identify the class 'sci.space' may look for the token 'launch' in documents and would be right about 70% of the time.While data programming can be very effective when done right, experts may spend a significant amount of time designing the weak supervision sources (Varma & R\u00e9, 2018) and must often inspect samples at random to generate ideas (Cohen-Wang et al., 2019).In our 20 newsgroups example, we may randomly see a document mentioning 'Salman Rushdie' and realize that the name of a famous atheist could be a good heuristic to identify posts in 'alt.atheism'.While such a heuristic seems obvious after the fact, we have to chance upon the right documents to generate these ideas.In practice, coming up with effective LFs becomes difficult after the first few.Substantial foresight (Ramos et al., 2020) is required to create a new function that applies to a non-negligible subset of given data, is novel, and adds predictive value.We propose a new approach for training supervised ML models with weak supervision through an interactive process, supporting domain experts in fast discovery of good LFs.The method queries users in an active fashion for feedback about candidate LFs, from which a model learns to identify LFs likely to have good accuracy.Upon completion, our approach produces a final set of LFs.We use this set to create an estimate of the latent class label via an unsupervised label model and train a final, weakly supervised end classifier using a noise aware loss function on the estimated labels as in Ratner et al. (2016).The approach relies on the observation that many applications allow for heuristics of varying quality to be generated at scale (similar to Varma & R\u00e9 (2018)), and that experts can provide good judgment by identifying some LFs that have reasonable accuracy.The full pipeline of the proposed approach, termed Interactive Weak Supervision (IWS)foot_0 , is illustrated in Fig. 1.Our contributions are:1. We propose, to the best of our knowledge, the first interactive method for weak supervision in which queries to be annotated are not data points but labeling functions.This approach automates the discovery of useful data labeling heuristics.2. We conduct experiments with real users on three classification tasks, using both text and image datasets.Our results support our modeling assumptions, demonstrate competitive test set performance of the downstream end classifier, and show that users can provide accurate feedback on automatically generated LFs. 3. In our results, IWS shows superior performance compared to standard active learning, i.e. we achieve better test set performance with a smaller number of queries to users.In text experiments with real users, IWS achieves a mean test set AUC after 200 LF annotations that requires at least three times as many active learning iterations annotating data points.In addition, the average user response time for LF queries was shorter than for the active learning queries on data points."}
{"paper_id": 128, "abstract": "In the intricate realm of object recognition, shape and texture stand as twin pillars, each offering vital insights into the identity of an object. Yet, the powerful engines of Convolutional Neural Networks often find themselves swayed by one cue over the other, a bias that emerges from the training datasets themselves. Our exploration reveals that this imbalance can significantly hinder model performance. Driven by this revelation, we have crafted a straightforward yet effective algorithm designed to harmonize shape and texture in the learning process.  To counteract the tendency of models to latch onto a singular cue, we introduce a novel approach: augmenting the training dataset with images that present conflicting shape and texture cues\u2014imagine a chimpanzee adorned in the vibrant skin of a lemon. This technique is further enhanced by providing dual supervision from both shape and texture, ensuring that the model learns to balance these critical features in its representations.  Our experiments demonstrate that this method not only elevates model performance across a spectrum of image recognition benchmarks but also fortifies adversarial robustness. For instance, when applied to ResNet-152 on ImageNet, our approach yields notable gains: a +1.2% improvement on standard ImageNet, +5.2% on ImageNet-A, +8.3% on ImageNet-C, and an impressive +11.1% on Stylized-ImageNet. Furthermore, it enhances defenses against FGSM adversarial attacks on ImageNet by +14.4%. Notably, our technique seamlessly integrates with advanced data augmentation strategies like Mixup and CutMix. For those interested in delving deeper, the code is readily accessible here: https://github.com/LiYingwei/ShapeTextureDebiasedTraining.", "introduction": "It is known that both shape and texture serve as essential cues for object recognition.A decade ago, computer vision researchers had explicitly designed a variety of hand-crafted features, either based on shape (e.g., shape context (Belongie et al., 2002) and inner distance shape context (Ling & Jacobs, 2007)) or texture (e.g., textons (Malik et al., 2001)), for object recognition.Moreover, researchers found that properly combining shape and texture can further recognition performance (Shotton et al., 2009;Zheng et al., 2007), demonstrating the superiority of possessing both features.Nowadays, as popularized by Convolutional Neural Networks (CNNs) (Krizhevsky et al., 2012), the features used for object recognition are automatically learned, rather than manually designed.This change not only eases human efforts on feature engineering, but also yields much better performance on a wide range of visual benchmarks (Simonyan & Zisserman, 2015;He et al., 2016;Girshick et al., 2014;Girshick, 2015;Ren et al., 2015;Long et al., 2015;Chen et al., 2015).But interestingly, as pointed by Geirhos et al. (2019), the features learned by CNNs tend to bias toward either shape or texture, depending on the training dataset.We verify that such biased representation learning (towards either shape or texture) weakens CNNs' performance. 1 Nonetheless, surprisingly, we also find (1) the model with shape-biased representations and the model with texture-biased representations are highly complementary to each other, e.g., they focus on completely different cues for predictions (an example is provided in Figure 1); and(2) being biased towards either cue may inevitably limit model performance, e.g., models may not be able to tell the difference between a lemon and an orange without texture information.These observations altogether deliver a promising message-biased models (e.g., ImageNet trained (texturebiased) CNNs (Geirhos et al., 2019) or (shape-biased) CNNs (Shi et al., 2020)) are improvable.Test Image Label: Fur Coat \u2713Shape \u2715Texture \u2715Shape \u2713Texture \u2713Shape \u2713Texture Models pay attention to\u2026 Shape-biased Model Texture-biased Model Debiased Model (ours) Poncho \u2715 Egyptian cat \u2715 Fur Coat \u2713 Models' prediction As shown above, when classifying this fur coat image, the shape-biased model is confounded by the cloth-like shape therefore predict it as a poncho, and the texture-biased model confuses it as an Egyptian cat because of the misleading texture.Nonetheless, our debiased model can successfully recognize it as a fur coat by leveraging both shape and texture.To this end, we hereby develop a shape-texture debiased neural network training framework to guide CNNs for learning better representations.Our method is a data-driven approach, which let CNNs automatically figure out how to avoid being biased towards either shape or texture from their training samples.Specifically, we apply style transfer to generate cue conflict images, which breaks the correlation between shape and texture, for augmenting the original training data.The most important recipe of training a successful shape-texture debiased model is that we need to provide supervision from both shape and texture on these generated cue conflict images, otherwise models will remain being biased.Experiments show that our proposed shape-texture debiased neural network training significantly improves recognition models.For example, on the challenging ImageNet dataset (Russakovsky et al., 2015), our method helps ResNet-152 gain an absolute improvement of 1.2%, achieving 79.8% top-1 accuracy.Additionally, compared to its vanilla counterpart, this debiased ResNet-152 shows better generalization on ImageNet-A (Hendrycks et al., 2019) (+5.2%),ImageNet-C (Hendrycks & Dietterich, 2019) (+8.3%) and Stylized ImageNet (Geirhos et al., 2019) (+11.1%), and stronger robustness on defending against FGSM adversarial attacker on ImageNet (+14.4%).Our shape-texture debiased neural network training is orthogonal to other advanced data augmentation strategies, e.g., it further boosts CutMix-ResNeXt-101 (Yun et al., 2019) by 0.7% on ImageNet, achieving 81.2% top-1 accuracy."}
{"paper_id": 129, "abstract": "In the realm of video action recognition, mastering the flow of time is akin to wielding a powerful magic\u2014one that can elevate the accuracy of recognizing dynamic movements. Yet, the true art lies not just in understanding this temporal tapestry, but in deftly eliminating redundancy and harnessing the wisdom of past features. Enter the AdaFuse, our adaptive temporal fusion network, a creation designed to weave together the threads of current and historical feature maps with remarkable agility. By melding essential elements from earlier convolutional maps with streamlined present features, we aim to forge a path that enhances both recognition precision and computational efficiency. Moreover, our innovative skipping operation acts as a clever shortcut, slashing the computational burden of action recognition. Through rigorous experimentation on datasets such as SomethingV1 & V2, Jester, and Mini-Kinetics, we demonstrate that AdaFuse can achieve a remarkable 40% reduction in computation while maintaining accuracy that stands shoulder to shoulder with the leading methods in the field. For those intrigued by our journey, further details can be uncovered at https://mengyuest.github.io/AdaFuse/.", "introduction": "Over the last few years, video action recognition has made rapid progress with the introduction of a number of large-scale video datasets (Carreira & Zisserman, 2017;Monfort et al., 2018;Goyal et al., 2017).Despite impressive results on commonly used benchmark datasets, efficiency remains a great challenge for many resource constrained applications due to the heavy computational burden of deep Convolutional Neural Network (CNN) models.Motivated by the need of efficiency, extensive studies have been recently conducted that focus on either designing new lightweight architectures (e.g., R(2+1)D (Tran et al., 2018), S3D (Xie et al., 2018), channel-separated CNNs (Tran et al., 2019)) or selecting salient frames/clips conditioned on the input (Yeung et al., 2016;Wu et al., 2019b;Korbar et al., 2019;Gao et al., 2020).However, most of the existing approaches do not consider the fact that there exists redundancy in CNN features which can significantly save computation leading to more efficient action recognition.In particular, orthogonal to the design of compact models, the computational cost of a CNN model also has much to do with the redundancy of CNN features (Han et al., 2019).Furthermore, the amount of redundancy depends on the dynamics and type of events in the video: A set of still frames for a simple action (e.g.\"Sleeping\") will have a higher redundancy comparing to a fast-changed action with rich interaction and deformation (e.g.\"Pulling two ends of something so that it gets stretched\").Thus, based on the input we could compute just a subset of features, while the rest of the channels can reuse history feature maps or even be skipped without losing any accuracy, resulting in large computational savings compared to computing all the features at a given CNN layer.Based on this intuition, we present a new perspective for efficient action recognition by adaptively deciding what channels to compute or reuse, on a per instance basis, for recognizing complex actions.In this paper, we propose AdaFuse, an adaptive temporal fusion network that learns a decision policy to dynamically fuse channels from current and history feature maps for efficient action recognition.Specifically, our approach reuses history features when necessary (i.e., dynamically decides which channels to keep, reuse or skip per layer and per instance) with the goal of improving both recognition accuracy and efficiency.As these decisions are discrete and non-differentiable, we rely on a Gumbel Softmax sampling approach (Jang et al., 2016) to learn the policy jointly with the network parameters through standard back-propagation, without resorting to complex reinforcement learning as in (Wu et al., 2019b;Fan et al., 2018;Yeung et al., 2016).We design the loss to achieve both competitive performance and resource efficiency required for action recognition.Extensive experiments on multiple benchmarks show that AdaFuse significantly reduces the computation without accuracy loss.The main contributions of our work are as follows:\u2022 We propose a novel approach that automatically determines which channels to keep, reuse or skip per layer and per target instance for efficient action recognition.\u2022 Our approach is model-agnostic, which allows this to be served as a plugin operation for a wide range of 2D CNN-based action recognition architectures.\u2022 The overall policy distribution can be seen as an indicator for the dataset characteristic, and the block-level distribution can bring potential guidance for future architecture designs.\u2022 We conduct extensive experiments on four benchmark datasets (Something-Something V1 (Goyal et al., 2017), Something-Something V2 (Mahdisoltani et al., 2018), Jester (Materzynska et al., 2019) and Mini-Kinetics (Kay et al., 2017)) to demonstrate the superiority of our proposed approach over state-of-the-art methods."}
{"paper_id": 130, "abstract": "In the realm of real-time point cloud applications, particularly those constrained by the limitations of edge devices, we unveil BiPointNet\u2014a groundbreaking approach to model binarization designed for the efficient deployment of deep learning on point clouds. Our journey reveals that the significant performance decline observed in binarized models arises primarily from two formidable challenges: the aggregation-induced feature homogenization, which dulls the richness of information entropy, and the scale distortion that complicates optimization and disrupts scale-sensitive structures.   With a foundation built on rigorous theoretical insights and detailed analysis, BiPointNet introduces two pivotal innovations: Entropy-Maximizing Aggregation (EMA), which skillfully modulates the distribution prior to aggregation to ensure maximum information entropy, and Layer-wise Scale Recovery (LSR), a technique that adeptly restores the capacity of feature representation. Our extensive experimental evaluations demonstrate that BiPointNet not only surpasses existing binarization methods by substantial margins but also achieves performance levels that rival those of full precision models.  We assert that the methodologies we present are versatile, promising significant enhancements across a variety of fundamental tasks and popular backbones. Remarkably, BiPointNet delivers a staggering 14.7\u00d7 increase in processing speed and an 18.9\u00d7 reduction in storage requirements on real-world devices constrained by resource limitations. This work stands as a testament to the potential of innovative techniques in transforming the landscape of edge-device applications.", "introduction": "With the advent of deep neural networks that directly process raw point clouds (PointNet (Qi et al., 2017a) as the pioneering work), great success has been achieved in learning on point clouds (Qi et al., 2017b;Li et al., 2018;Wang et al., 2019a;Wu et al., 2019;Thomas et al., 2019;Liu et al., 2019b;Zhang et al., 2019b).Point cloud applications, such as autonomous driving and augmented reality, often require real-time interaction and fast response.However, computation for such applications is usually deployed on resource-constrained edge devices.To address the challenge, novel algorithms, such as Grid-GCN (Xu et al., 2020b), RandLA-Net (Hu et al., 2020), and PointVoxel (Liu et al., 2019d), have been proposed to accelerate those point cloud processing networks.While significant speedup and memory footprint reduction have been achieved, these works still rely on expensive floating-point operations, leaving room for further optimization of the performance from the model quantization perspective.Model binarization (Rastegari et al., 2016;Bulat & Tzimiropoulos, 2019;Hubara et al., 2016;Wang et al., 2020;Zhu et al., 2019;Xu et al., 2019) emerged as one of the most promising approaches to optimize neural networks for better computational and memory usage efficiency.Binary Neural Networks (BNNs) leverage 1) compact binarized parameters that take small memory space, and 2) highly efficient bitwise operations which are far less costly compared to the floating-point counterparts.Despite that in 2D vision tasks (Krizhevsky et al., 2012;Simonyan & Zisserman, 2014;Szegedy et al., 2015;Girshick et al., 2014;Girshick, 2015;Russakovsky et al., 2015;Wang et al., 2019b;  EMA consists of the transformation unit and the aggregation unit for maximizing the information entropy of feature after binarization.LSR with the learnable layer-wise scaling factor \u03b1 is applied to address the scale distortion of bi-linear layers (which form the BiMLPs), flexibly restore the distorted output to reasonable values Zhang et al., 2021) has been studied extensively by the model binarization community, the methods developed are not readily transferable for 3D point cloud networks due to the fundamental differences between 2D images and 3D point clouds.First, to gain efficiency in processing unordered 3D points, many point cloud learning methods rely heavily on pooling layers with large receptive field to aggregate point-wise features.As shown in PointNet (Qi et al., 2017b), global pooling provides a strong recognition capability.However, this practice poses challenges for binarization.Our analyses show that the degradation of feature diversity, a persistent problem with binarization (Liu et al., 2019a;Qin et al., 2020b;Xie et al., 2017), is significantly amplified by the global aggregation function (Figure 2), leading to homogenization of global features with limited discriminability.Second, the binarization causes immense scale distortion at the point-wise feature extraction stage, which is detrimental to model performance in two ways: the saturation of forward-propagated features and backward-propagated gradients hinders optimization, and the disruption of the scale-sensitive structures (Figure 3) results in the invalidation of their designated functionality.In this paper, we provide theoretical formulations of the above-mentioned phenomenons and obtain insights through in-depth analysis.Such understanding allows us to propose a method that turns fullprecision point cloud networks into extremely efficient yet strong binarized models (see the overview in Figure 1).To tackle the homogenization of the binarized features after passing the aggregation function, we study the correlation between the information entropy of binarization features and the performance of point cloud aggregation functions.We thus propose Entropy-Maximizing Aggregation (EMA) that shifts the feature distribution towards the statistical optimum, effectively improving expression capability of the global features.Moreover, given maximized information entropy, we further develop Layer-wise Scale Recovery (LSR) to efficiently restore the output scale that enhances optimization, which allows scale-sensitive structures to function properly.LSR uses only one learnable parameter per layer, leading to negligible storage increment and computation overhead.Our BiPointNet is the first binarization approaches to deep learning on point clouds, and it outperforms existing binarization algorithms for 2D vision by convincing margins.It is even almost on par (within \u223c 1-2%) with the full-precision counterpart.Although we conduct most analysis on the PointNet baseline, we show that our methods are generic and can be readily extendable to other popular backbones, such as PointNet++ (Qi et al., 2017b), PointCNN (Li et al., 2018), DGCNN (Wang et al., 2019a), and PointConv (Wu et al., 2019), which are the representatives of mainstream categories of point cloud feature extractors.Moreover, extensive experiments on multiple fundamental tasks on the point cloud, such as classification, part segmentation, and semantic segmentation, highlight that our BiPointNet is task-agnostic.Besides, we highlight that our EMA and LSR are efficient and easy to implement in practice: in the actual test on popular edge devices, BiPointNet achieves 14.7\u00d7 speedup and 18.9\u00d7 storage savings compared to the full-precision PointNet.Our code is released at https://github.com/htqin/BiPointNet.et al., 2018;Yu et al., 2020), and binarization.Among these methods, binarization enjoys compact binarized parameters and highly efficient bitwise operations for extreme compression and acceleration (Rastegari et al., 2016;Qin et al., 2020a).In general, the forward and backward propagation of binarized models in the training process can be formulated as:where x denotes the element in floating-point weights and activations, b denotes the element in binarized weights B w and activations B a .g x , and g b donate the gradient \u2202C \u2202x and \u2202C \u2202b , respectively, where C is the cost function for the minibatch.In forward propagation, sign function is directly applied to obtain the binary parameters.In backward propagation, the Straight-Through Estimator (STE) (Bengio et al., 2013) is used to obtain the derivative of the sign function, avoiding getting all zero gradients.The existing binarization methods are designed to obtain accurate binarized networks by minimizing the quantization error (Rastegari et al., 2016;Zhou et al., 2016;Lin et al., 2017), improving loss function (Ding et al., 2019;Hou et al., 2017), reducing the gradient error (Liu et al., 2018;2020), and designing novel structures and pipelines (Martinez et al., 2020).Unfortunately, we show in Sec 3 that these methods, designed for 2D vision tasks, are not readily transferable to 3D point clouds.Deep Learning on Point Clouds.PointNet (Qi et al., 2017a) is the first deep learning model that processes raw point clouds directly.The basic building blocks proposed by PointNet such as MLP for point-wise feature extraction and max pooling for global aggregation (Guo et al., 2020) have become the popular design choices for various categories of newer backbones: 1) the pointwise MLP-based such as PointNet++ (Qi et al., 2017b); 2) the graph-based such as DGCNN (Xu et al., 2020b); 3) the convolution-based such as PointCNN (Li et al., 2018), PointConv (Wu et al., 2019) RS-CNN (Liu et al., 2019c) and KP-Conv (Thomas et al., 2019).Recently, methods are proposed for efficient deep learning on point clouds through novel data structuring (Xu et al., 2020b), faster sampling (Hu et al., 2020), adaptive filters (Xu et al., 2020a), efficient representation (Liu et al., 2019d) or convolution operation (Zhang et al., 2019b) .However, they still use expensive floatingpoint parameters and operations, which can be improved by binarization."}
{"paper_id": 131, "abstract": "In the ever-evolving landscape of neural network optimization, a fascinating avenue has emerged: the art of pruning networks right from their inception. Our investigation delves into several pioneering approaches\u2014SNIP, GraSP, SynFlow, and the traditional magnitude pruning\u2014seeking to uncover their strengths and weaknesses. While these techniques certainly outshine the rudimentary strategy of random pruning, they still fall short of the performance achieved through magnitude pruning post-training. This raises a critical question: what lies at the heart of this discrepancy?  Through our exploration, we unveil a compelling insight: when we shuffle the weights targeted for pruning within each layer or sample new initial values, we often find that accuracy is not merely preserved, but can actually improve. This revelation leads us to reconsider the per-weight decisions made by these methods. Instead, we propose a paradigm shift\u2014what if we approached pruning from a per-layer perspective, determining only the fraction of weights to eliminate?  This discovery hints at deeper challenges, not only with the pruning heuristics themselves but also with the very desire to prune at the outset. As we navigate this complex terrain, we open the door to new possibilities in the quest for more efficient and effective neural networks.", "introduction": "Since the 1980s, we have known that it is possible to eliminate a significant number of parameters from neural networks without affecting accuracy at inference-time (Reed, 1993;Han et al., 2015).Such neural network pruning can substantially reduce the computational demands of inference when conducted in a fashion amenable to hardware (Li et al., 2017) or combined with libraries (Elsen et al., 2020) and hardware designed to exploit sparsity (Cerebras, 2019;NVIDIA, 2020;Toon, 2020).When the goal is to reduce inference costs, pruning often occurs late in training (Zhu & Gupta, 2018;Gale et al., 2019) or after training (LeCun et al., 1990;Han et al., 2015).However, as the financial, computational, and environmental demands of training (Strubell et al., 2019) have exploded, researchers have begun to investigate the possibility that networks can be pruned early in training or even before training.Doing so could reduce the cost of training existing models and make it possible to continue exploring the phenomena that emerge at larger scales (Brown et al., 2020).There is reason to believe it may be possible to prune early in training without affecting final accuracy.Work on the lottery ticket hypothesis (Frankle & Carbin, 2019;Frankle et al., 2020a) shows that, from early in training (although often after initialization), there exist subnetworks that can train in isolation to full accuracy (Figure 1, red line).These subnetworks are as small as those found by inference-focused pruning methods after training (Appendix E; Renda et al., 2020), raising the prospect that it may be possible to maintain this level of sparsity for much or all of training.However, this work does not suggest a way to find these subnetworks without first training the full network.The pruning literature offers a starting point for finding such subnetworks efficiently.Standard networks are often so overparameterized that pruning randomly has little effect on final accuracy at lower sparsities (green line).Moreover, many existing pruning methods prune during training (Zhu & Gupta, 2018;Gale et al., 2019), even if they were designed with inference in mind (orange line).Recently, several methods have been proposed specifically for pruning at initialization.SNIP (Lee et al., 2019) aims to prune weights that are least salient for the loss.GraSP (Wang et al., 2020) aims to prune weights that most harm or least benefit gradient flow.SynFlow (Tanaka et al., 2020) aims to iteratively prune weights with the lowest \"synaptic strengths\" in a data-independent manner with the goal of avoiding layer collapse (where pruning concentrates on certain layers).In this paper, we assess the efficacy of these pruning methods at initialization.How do SNIP, GraSP, and SynFlow perform relative to each other and naive baselines like random and magnitude pruning?Method Early Pruning Methods Baseline Methods Ablations SNIP GraSP SynFlow Magnitude Random LTR Magnitude (After) Other Reinit Shuffle Invert SNIP ---GraSP --SynFlow -Figure 2: Comparisons in the SNIP, GraSP, and SynFlow papers.Does not include MNIST.SNIP lacks baselines beyond MNIST.GraSP includes random, LTR, and other methods; it lacks magnitude at init and ablations.SynFlow has other methods at init but lacks baselines or ablations."}
{"paper_id": 132, "abstract": "In the ever-evolving realm of machine learning, differentiable architecture search (DARTS) stands as a beacon of innovation, yet it grapples with a persistent instability that hampers its true potential. The quest for robust solutions has often led researchers to focus on the aftermath of performance dips, rather than delving into the root causes. Various indicators, like the ominous Hessian eigenvalues, have emerged as harbingers of impending failure, prompting the premature cessation of searches once these metrics breach predetermined thresholds. However, this reactive strategy can unwittingly cast aside promising architectures, especially when the thresholds are miscalibrated, all while navigating the inherent noise of the search process.  In this paper, we embark on a more nuanced journey to combat this collapse. We unveil a compelling truth: skip connections, enhanced by learnable architectural coefficients, possess a remarkable ability to rebound from unfavorable conditions, often eclipsing their peers in the process. Yet, we posit that this very advantage may be the catalyst for the collapse observed in the derived models. To level the playing field, we introduce an auxiliary skip connection, effectively mitigating the undue dominance of these advantageous paths and fostering a more equitable competition among all operations. Our extensive experiments across diverse datasets reveal that this approach significantly bolsters the robustness of DARTS. For those eager to explore our findings, our code awaits at https://github.com/Meituan-AutoML/DARTS.", "introduction": "Recent studies (Zela et al., 2020;Liang et al., 2019;Chu et al., 2020b) have shown that one critical issue for differentiable architecture search (Liu et al., 2019b) regarding the performance collapse due to superfluous skip connections.Accordingly, some empirical indicators for detecting the occurrence of collapse have been produced.R-DARTS (Zela et al., 2020) shows that the loss landscape has more curvatures (characterized by higher Hessian eigenvalues w.r.t.architectural weights) when the derived architecture generalizes poorly.By regularizing for a lower Hessian eigenvalue, Zela et al. (2020); Chen & Hsieh (2020) attempt to stabilize the search process.Meanwhile, by directly constraining the number of skip connections to a fixed number (typically 2), the collapse issue becomes less pronounced (Chen et al., 2019b;Liang et al., 2019).These indicator-based approaches have several main drawbacks.Firstly, robustness relies heavily on the quality of the indicator.An imprecise indicator either inevitably accepts poor models or mistakenly reject good ones.Secondly, indicators impose strong priors by directly manipulating the inferred model, which is somewhat suspicious, akin to touching the test set.Thirdly, extra computing cost (Zela et al., 2020) or careful tuning of hyper-parameters (Chen et al., 2019b;Liang et al., 2019) are required.Therefore, it's natural to ask the following questions:\u2022 Can we resolve the collapse without handcrafted indicators and restrictions to interfere with the searching and/or discretization procedure?\u2022 Is it possible to achieve robustness in DARTS without tuning extra hyper-parameters?To answer the above questions, we propose an effective and efficient approach to stabilize DARTS.Our contributions can be summarized as follows:New Paradigm to Stabilize DARTS.While empirically observing that current indicators (Zela et al., 2020;Chen & Hsieh, 2020) can avoid performance collapse at a cost of reduced exploration coverage in the search space, we propose a novel indicator-free approach to stabilize DARTS, referred to as DARTS-foot_0 , which involves an auxiliary skip connection (see Figure 1) to remove the unfair advantage (Chu et al., 2020b) in the searching phase.Strong Robustness and Stabilization.We conduct thorough experiments across seven search spaces and three datasets to demonstrate the effectiveness of our method.Specifically, our approach robustly obtains state-of-the-art results on 4 search space with 3\u00d7 fewer search cost than R-DARTS (Zela et al., 2020), which requires four independent runs to report the final performance.Seamless Plug-in Combination with DARTS Variants.We conduct experiments to demonstrate that our approach can work together seamlessly with other orthogonal DARTS variants by removing their handcrafted indicators without extra overhead.In particular, our approach is able to improve 0.8% accuracy for P-DARTS, and 0.25% accuracy for PC-DARTS on CIFAR-10 dataset."}
{"paper_id": 133, "abstract": "In the realm of continual learning, two formidable adversaries stand in our way: the specter of catastrophic forgetting and the ever-looming constraints of memory. To confront these challenges, we present a groundbreaking approach inspired by the intricacies of human cognition. Our method employs autoencoders, enhanced through the magic of Neural Style Transfer, to deftly encode and preserve images.  As we navigate the turbulent waters of learning new tasks, the reconstructed images from our encoded episodes serve as a lifeline, replayed strategically to shield our classifier from the perils of forgetting. We\u2019ve crafted a loss function for these reconstructions, carefully weighted to mitigate its influence during classifier training, thus countering the inevitable degradation of image quality.  When memory reaches its limits, a transformative process occurs: the encoded episodes are distilled into centroids and covariance matrices. These serve as the foundation for generating pseudo-images during classifier training, ensuring that our model maintains its performance even in the face of memory constraints. The result? A remarkable boost in classification accuracy\u2014between 13% to 17% over the leading methods currently in play\u2014while simultaneously slashing storage requirements by an impressive 78%. In this way, we carve a new path forward, merging the best of human-inspired learning with the relentless pursuit of efficiency in machine intelligence.", "introduction": "Humans continue to learn new concepts over their lifetime without the need to relearn most previous concepts.Modern machine learning systems, however, require the complete training data to be available at one time (batch learning) (Girshick, 2015).In this paper, we consider the problem of continual learning from the class-incremental perspective.Class-incremental systems are required to learn from a stream of data belonging to different classes and are evaluated in a single-headed evaluation (Chaudhry et al., 2018).In single-headed evaluation, the model is evaluated on all classes observed so far without any information indicating which class is being observed.Creating highly accurate class-incremental learning systems is a challenging problem.One simple way to create a class-incremental learner is by training the model on the data of the new classes, without revisiting the old classes.However, this causes the model to forget the previously learned classes and the overall classification accuracy decreases, a phenomenon known as catastrophic forgetting (Kirkpatrick et al., 2017).Most existing class-incremental learning methods avoid this problem by storing a portion of the training samples from the earlier learned classes and retraining the model (often a neural network) on a mixture of the stored data and new data containing new classes (Rebuffi et al., 2017;Hou et al., 2019).Storing real samples of the previous classes, however, leads to several issues.First, as pointed out by Wu et al. (2018b), storing real samples exhausts memory capacity and limits performance for real-world applications.Second, storing real samples introduces privacy and security issues (Wu et al., 2018b).Third, storing real samples is not biologically inspired, i.e. humans do not need to relearn previously known classes.This paper explores the \"strict\" class-incremental learning problem in which the model is not allowed to store any real samples of the previously learned classes.The strict class-incremental learning problem is more akin to realistic learning scenarios such as a home service robot that must learn continually with limited on-board memory.This problem has been previously addressed using generative models such as autoencoders (Kemker & Kanan, 2018) or Generative Adversarial Networks (GANs) (Ostapenko et al., 2019).Most approaches for strict class-incremental learning use GANs to generate samples reflecting old class data, because GANs generate sharp, fine-grained images (Ostapenko et al., 2019).The downside of GANs, however, is that they tend to generate images which do not belong to any of the learned classes, hurting classification performance.Autoencoders, on the other hand, always generate images that relate to the learned classes, but tend to produce blurry images that are also not good for classification.To cope with these issues, we propose a novel, cognitively-inspired approach termed Encoding Episodes as Concepts (EEC) for continual learning, which utilizes convolutional autoencoders to generate previously learned class data.Inspired by models of the hippocampus (Renoult et al., 2015), we use autoencoders to create compressed embeddings (encoded episodes) of real images and store them in memory.To avoid the generation of blurry images, we borrow ideas from the Neural Style Transfer (NST) algorithm proposed by Gatys et al. (2016) to train the autoencoders.For efficient memory management, we use the notion of memory integration, from hippocampal and neocortical concept learning (Mack et al., 2018), to combine similar episodes into centroids and covariance matrices eliminating the need to store real data.This paper contributes: 1) an autoencoder based approach to strict class-incremental learning which uses Neural Style Transfer to produce quality samples reflecting old class data (Sec.3.1); 2) a cognitively-inspired memory management technique that combines similar samples into a centroid/covariance representation, drastically reducing the memory required (Sec.3.2); 3) a data filtering and a loss weighting technique to manage image degradation of old classes during classifier training (Sec.3.3).We further show that EEC outperforms state-of-the-art (SOTA) approaches on benchmark datasets by significant margins while also using far less memory."}
{"paper_id": 134, "abstract": "In the realm of neural networks, where complexity often veils efficiency, network pruning emerges as a beacon of hope, adeptly trimming the excess from over-parameterized models to make them suitable for low-resource environments. Recent breakthroughs in retraining pruned networks\u2014particularly through techniques like weight rewinding and learning rate rewinding\u2014have demonstrated a remarkable ability to reclaim lost accuracy, eclipsing traditional fine-tuning methods (Renda et al., 2020). Yet, the underlying reasons for this newfound prowess have remained shrouded in mystery. In our exploration, we embark on a rigorous investigation of the enigmatic power of learning rate rewinding. Our findings reveal that the secret to its success lies in the application of a larger learning rate\u2014a revelation that echoes through various learning rate schedules, such as the 1-cycle learning rate schedule (Smith et al., 2019). By harnessing the right learning rate schedule during retraining, we unveil a surprising truth: randomly pruned networks can surpass their meticulously pruned counterparts, fine-tuned through conventional means. Our results underscore the pivotal role of the learning rate schedule in the retraining of pruned networks\u2014a nuance often neglected by practitioners caught in the intricacies of network pruning.", "introduction": "Training neural networks is an everyday task in the era of deep learning and artificial intelligence.Generally speaking, given data availability, large and cumbersome networks are often preferred as they have more capacity to exhibit good data generalization.In the literature, large networks are considered easier to train than small ones (Neyshabur et al., 2018;Arora et al., 2018;Novak et al., 2018;Brutzkus & Globerson, 2019).Thus, many breakthroughs in deep learning are strongly correlated to increasingly complex and over-parameterized networks.However, the use of large networks exacerbate the gap between research and practice since real-world applications usually require running neural networks in low-resource environments for numerous purposes: reducing memory, latency, energy consumption, etc.To adopt those networks to resourceconstrained devices, network pruning (LeCun et al., 1990;Han et al., 2015;Li et al., 2016) is often exploited to remove dispensable weights, filters and other structures from neural networks.The goal of pruning is to reduce overall computational cost and memory footprint without inducing significant drop in performance of the network.A common approach to mitigating performance drop after pruning is retraining: we continue to train the pruned models for some more epochs.In this paper, we are interested in approaches based on learning rate schedules to control the retraining.A well-known practice is fine-tuning, which aims to train the pruned model with a small fixed learning rate.More advanced learning rate schedules exist, which we generally refer to as retraining.The retraining step is a critical part in implementing network pruning, but it has been largely overlooked and tend to vary in each implementation including differences in learning rate schedules, retraining budget, hyperparameter choices, etc.Recently, Renda et al. (2020) proposed a state-of-the-art technique for retraining pruned networks namely learning rate rewinding (LRW).Specifically, instead of fine-tuning the pruned networks with a fixed learning rate, usually the last learning rate from the original training schedule (Han et al., 2015;Liu et al., 2019), the authors suggested using the learning rate schedule from the previous t epochs (i.e.rewinding).This seemingly subtle change in learning rate schedule led to an important result: LRW was shown to achieve comparable performance to more complex and computationally expensive pruning algorithms while only utilizing simple norm-based pruning.Unfortunately, the authors did not provide the analysis to justify the improvement.In general, it is intriguing to understand the importance of a learning rate schedule and how it affects the final performance of a pruned model.In this work, we study the behavior of pruned networks under different retraining settings.We found that the efficacy from retraining with learning rate rewinding is rooted in the use of a large learning rate, which helps pruned networks to converge faster after pruning.We demonstrate that the success of learning rate rewinding over fine-tuning is not exclusive to the learning rate schedule coupling with the original training process.Retraining with a large learning rate could outperform fine-tuning even with some modest retraining, e.g., for a few epochs, and regardless of network compression ratio.We argue that retraining is of paramount importance to regain the performance in network pruning and should not be overlooked when comparing two pruning algorithms.This is evidenced by our extensive experiments: (1) randomly pruned network can outperform methodically pruned network with only (hyper-parameters free) modifications of the learning rate schedule in retraining, and (2) a simple baseline such as norm-based pruning can perform as well as as other complex pruning methods by using a large learning rate restarting retraining schedule.The contributions of our work are as follows.\u2022 We document a thorough experiment on learning rate schedule for the retraining step in network pruning with different pruning configurations; \u2022 We show that learning rate matters: pruned models retrained with a large learning rate consistently outperform those trained by conventional fine-tuning regardless of specific learning rate schedules; \u2022 We present a novel and counter-intuitive result achieved by solely applying large learning rate retraining: a randomly pruned network and a simple norm-based pruned network can perform as well as networks obtained from more sophisticated pruning algorithms.Given the significant impact of learning rate schedule in network pruning, we advocate the following practices: learning rate schedule should be considered as a critical part of retraining when designing pruning algorithms.Rigorous ablation studies with different retraining settings should be made for a fair comparison of pruning algorithms.To facilitate reproducibility, we would release our implementation upon publication."}
{"paper_id": 135, "abstract": "In the realm of point cloud sequences, where chaos and order dance in a delicate balance, we find a unique challenge. These sequences, characterized by their irregular and unordered nature in the spatial dimension, reveal a remarkable consistency and rhythm in the temporal domain. Traditional grid-based convolutions, which have served well in the kingdom of conventional video processing, falter when faced with the intricacies of spatio-temporal modeling in raw point cloud sequences.   In this work, we unveil a novel approach: the Point Spatio-Temporal (PST) convolution. This innovative technique embarks on a journey to disentangle the threads of space and time woven within point cloud sequences. It begins by employing a spatial convolution to uncover the hidden structures of points within the three-dimensional expanse, capturing the essence of their local arrangements. Then, it seamlessly transitions to a temporal convolution, adeptly modeling the dynamic interplay of these spatial regions as they evolve over time.  To harness the power of our PST convolution, we introduce PSTNet, a deep network designed to extract features from point cloud sequences in a hierarchical fashion. Through rigorous experimentation across established datasets for 3D action recognition and 4D semantic segmentation, we demonstrate the formidable capabilities of PSTNet, proving its prowess in effectively modeling the complexities of point cloud sequences. In this endeavor, we not only push the boundaries of existing methodologies but also illuminate a path forward in the exploration of spatio-temporal phenomena.", "introduction": "Modern robotic and automatic driving systems usually employ real-time depth sensors, such as LiDAR, to capture the geometric information of scenes accurately while being robust to different lighting conditions.A scene geometry is thus represented by a 3D point cloud, i.e., a set of measured point coordinates {(x, y, z)}.Moreover, when RGB images are available, they are often used as additional features associated with the 3D points to enhance the discriminativeness of point clouds.However, unlike conventional grid based videos, dynamic point clouds are irregular and unordered in the spatial dimension while points are not consistent and even flow in and out over time.Therefore, existing 3D convolutions on grid based videos (Tran et al., 2015;Carreira & Zisserman, 2017;Hara et al., 2018) are not suitable to model raw point cloud sequences, as shown in Fig. 1.To model the dynamics of point clouds, one solution is converting point clouds to a sequence of 3D voxels, and then applying 4D convolutions (Choy et al., 2019) to the voxel sequence.However, directly performing convolutions on voxel sequences require a large amount of computation.Furthermore, quantization errors are inevitable during voxelization, which may restrict applications that require precise measurement of scene geometry.Another solution MeteorNet (Liu et al., 2019e) is extending the static point cloud method PointNet++ (Qi et al., 2017b) to process raw point cloud sequences by appending 1D temporal dimension to 3D points.However, simply concatenating coordinates and time together and treating point cloud sequences as unordered 4D point sets neglect the temporal order of timestamps, which may not properly exploit the temporal information and lead to inferior performance.Moreover, the scales of spatial displacements and temporal differences in point cloud sequences may not be compatible.Treating them equally is not conducive for network optimization.Besides, MeteorNet only considers spatial neighbors and neglects the local dependencies of neighboring frames.With the use of whole sequence length as its temporal receptive field, MeteorNet cannot construct temporal hierarchy.As points are not consistent and even flow in and out of the region, especially for long sequences and fast motion embedding points in a spatially local area along an entire sequence handicaps capturing accurate local dynamics of point clouds.In this paper, we propose a point spatio-temporal (PST) convolution to directly process raw point cloud sequences.As dynamic point clouds are spatially irregular but ordered in the temporal dimension, we Our PST convolution encodes an input to an output composed of a coordinate tensor (3 \u00d7 L \u00d7 N ) and a feature tensor (C \u00d7 L \u00d7 N ).Usually, L \u2264 L and N \u2264 N so that networks can model point cloud sequences in a spatio-temporally hierarchical manner.Note that points in different frames are not consistent, and thus it is challenging to capture the spatio-temporal correlation.decouple the spatial and temporal information to model point cloud sequences.Specifically, PST convolution consists of (i) a point based spatial convolution that models the spatial structure of 3D points and (ii) a temporal convolution that captures the temporal dynamics of point cloud sequences.In this fashion, PST convolution significantly facilitates the modeling of dynamic point clouds and reduces the impact of the spatial irregularity of points on temporal modeling.Because point cloud sequences emerge inconsistently across frames, it is challenging to perform convolution on them.To address this problem, we introduce a point tube to preserve the spatio-temporal local structure.To enhance the feature extraction ability, we incorporate the proposed PST convolution into a spatiotemporally hierarchical network, namely PSTNet.Moreover, we extend our PST convolution to a transposed version to address point-level prediction tasks.Different from the convolutional version, the PST transposed convolution is designed to interpolate temporal dynamics and spatial features.Extensive experiments on widely-used 3D action recognition and 4D semantic segmentation datasets demonstrate the effectiveness of the proposed PST convolution and the superiority of PSTNet in modeling point clouds sequences.The contributions of this paper are fourfold: \u2022 To the best of our knowledge, we are the first attempt to decompose spatial and temporal information in modeling raw point cloud sequences, and propose a generic point based convolutional operation, named PST convolution, to encode raw point cloud sequences.\u2022 We propose a PST transposed convolution to decode raw point cloud sequences via interpolating the temporal dynamics and spatial feature for point-level prediction tasks.\u2022 We construct convolutional neural networks based on the PST convolutions and transposed convolutions, dubbed PSTNet, to tackle sequence-level classification and point-level prediction tasks.To the best of our knowledge, our PSTNet is the first deep neural network to model raw point cloud sequences in a both spatially and temporally hierarchical manner.\u2022 Extensive experiments on four datasets indicate that our method improves the accuracy of 3D action recognition and 4D semantic segmentation."}
{"paper_id": 136, "abstract": "In the realm of understanding the intricate dance of neural responses to visual stimuli, deep neural networks (DNNs) have emerged as powerful allies, setting unprecedented benchmarks in predicting the reactions of neural populations. At the heart of these DNNs lies a convolutional network\u2014a core that serves as the foundation shared across all neurons. This core learns to distill the essence of neural computation within the visual cortex, while a neuron-specific readout deftly combines the relevant features into a coherent output.   This paper embarks on a quest to ascertain whether this learned representation is truly a universal hallmark of the visual cortex, one that transcends the boundaries of individual species. We delve into the factors that enable the emergence of such a generalizing core. To maximize the potential of our model, we introduce an innovative readout mechanism that dramatically reduces the number of parameters per neuron\u2014by as much as two orders of magnitude compared to previous paradigms. This is achieved through the clever utilization of retinotopy, allowing our model to learn a Gaussian distribution over the position of each neuron\u2019s receptive field.  Training our network on the neural responses from the primary visual cortex (V1) of mice, we unveil a remarkable 7% enhancement in performance over the previous state-of-the-art. But our adventure does not end there. We probe deeper to evaluate whether the convolutional core truly encapsulates the general features of the cortex by employing it in transfer learning across different species. The results are astonishing: when transferring a core trained on thousands of neurons from diverse animals, we surpass the performance of direct training on a new animal by 12%, and outshine the widely-used VGG16 core\u2014pre-trained on ImageNet\u2014by an impressive 33%.   Moreover, our data-driven core proves to be more data-efficient, achieving equivalent performance with merely 40% of the data needed for direct training. Thus, our model, with its groundbreaking readout, not only establishes a new pinnacle for predicting neural responses in the mouse visual cortex from natural images but also demonstrates a remarkable ability to generalize across species, capturing the quintessential characteristics of cortical features far better than conventional task-driven pre-training methods like VGG16.", "introduction": "A long lasting challenge in sensory neuroscience is to understand the computations of neurons in the visual system stimulated by natural images (Carandini et al., 2005).Important milestones towards this goal are general system identification models that can predict the response of large populations of neurons to arbitrary visual inputs.In recent years, deep neural networks have set new standards in predicting responses in the visual system (Yamins et al., 2014;Vintch et al., 2015;Antol\u00edk et al., 2016;Cadena et al., 2019a;Batty et al., 2016;Kindel et al., 2017;Klindt et al., 2017;Zhang et al., 2018;Ecker et al., 2018;Sinz et al., 2018) and the ability to yield novel response characterizations (Walker et al., 2019;Bashivan et al., 2019;Ponce et al., 2019;Kindel et al., 2019;Ukita et al., 2019).Such a general system identification model is one way for neuroscientists to investigate the computations of the respective brain areas in silico.Such in silico experiments exhibit the possibility to study the system at a scale and level of detail that is impossible in real experiments which have to cope with limited experimental time and adaptation effects in neurons.Moreover, all parameters, connections and weights in an in silico model can be accessed directly, opening up the opportunity to manipulate the model or determine its detailed tuning properties using numerical optimization methods.In order for the results of such analyses performed on an in silico model to be reliable, however, one needs to make sure that the model does indeed replicate the responses of its biological counterpart faithfully.This work provides an important step towards obtaining such a generalizing model of mouse V1.High performing predictive models need to account for the increasingly nonlinear response properties of neurons along the visual hierarchy.As many of the nonlinearities are currently unknown, one of the key challenges in neural system identification is to find a good set of characteristic nonlinear basis functions-so called representations.However, learning these complex nonlinearities from single neuron responses is difficult given limited experimental data.Two approaches have proven to be promising in the past: Task-driven system identification networks rely on transfer learning and use nonlinear representations pre-trained on large datasets for standard vision tasks, such as object recognition (Yamins & DiCarlo, 2016).Single neuron responses are predicted from a particular layer of a pre-trained network using a simple readout mechanism, usually an affine function followed by a static nonlinearity.Data-driven models share a common nonlinear representation among hundreds or thousands of neurons, and train the entire network end-to-end on stimulus response pairs from the experiment.Because the nonlinear representation is shared, it is trained via massive multi-task learning (one neuron-one task) and can be learned even from limited experimental data.Task-driven networks are appealing because they only need to fit the readout mechanisms on top of a given representation and thus are data-efficient in terms of the number of stimulus-response pairs needed to achieve good predictive performance (Cadena et al., 2019a).Moreover, as their representations are obtained independently of the neural data, a good predictive performance suggests that the nonlinear features are characteristic for a particular brain area.This additionally offers the interesting normative perspective that the functional representations in deep networks and biological vision could be aligned by common computational goals (Yamins & DiCarlo, 2016;Kell et al., 2018;Kubilius et al., 2018;Nayebi et al., 2018;Sinz et al., 2019;G\u00fc\u00e7l\u00fc & van Gerven, 2014;Kriegeskorte, 2015;Khaligh-Razavi & Kriegeskorte, 2014;Kietzmann et al., 2019).In order to quantify the fit of the normative hypothesis, it is important to compare a given representation to other alternatives (Schrimpf et al., 2018;Cadena et al., 2019a).However, while representations pre-trained on ImageNet are the state-of-the-art for predicting visual cortex in primates (Cadena et al., 2019a;Yamins & DiCarlo, 2016), recent work has demonstrated that pre-training on object categorization (VGG16) yields no benefits over random initialization for mouse visual cortex (Cadena et al., 2019b).Since random representation should not be characteristic for a particular brain area and other tasks that might yield more meaningful representations have not been found yet, this raises the questions whether there are better ways to obtain a generalizing nonlinear representation for mouse visual cortex.Here, we investigate whether such a generalizing representation can instead be obtained from datadriven networks.For this purpose, we develop a new data efficient readout which is designed to push non-linear computations into the core and test whether this core has learned general characteristic features of mouse visual cortex by applying the same criteria as for the task-driven approach: The ability to predict a population of unseen neurons in a new animal (transfer learning).Specifically, we make the following contributions: 1 We introduce a novel readout mechanism that keeps the number of per-neuron parameters at a minimum and learns a bivariate Gaussian distribution for the readout position from anatomical data using retinotopy.With this readout alone, we surpass the previous state-of-the-art performance in direct training by 7%. 2 We demonstrate that a representation pre-trained on thousands of neurons from various animals generalizes to neurons from an unseen animal (transfer learning).It exceeds the direct training condition by another 11%, setting the new state-of-the-art and outperforms a task-driven representation-trained on object recognition-by about 33%. 3 We then show that this generalization can be attributed to the representation and not the readout mechanism, indicating that the data-driven core indeed captures generalizing features of cortex: A representation trained on a single experiment (4.5k examples) in combination with a readout trained on anatomically matched neurons from four experiments (17.5k examples) did not achieve this performance.4 Lastly, we find that transfer learning with our data-driven core is more data-efficient than direct training, achieving the same performance with only 40% of the data.Each scan was performed on a specific set of neurons (rows) using a specific set of unique images (columns).Repeatedly presented test images were the same for all scans.Some scans were performed on the same neuron but with different image sets (first row).Colors indicate grouping of scans into training sets and match line colors in Fig. 5 to indicate which dataset a representation/core (not the readout) was trained on."}
{"paper_id": 137, "abstract": "In the realm of medical diagnostics, where lives hang in the balance, Convolutional Neural Networks (CNNs) have emerged as powerful allies, wielding the ability to automate disease diagnosis with remarkable precision. Yet, lurking in the shadows is a peril that threatens to undermine this technological marvel: adversarial attacks. These insidious manipulations can lead to catastrophic misdiagnoses, casting a dark pall over human healthcare. Thus, the quest begins\u2014a quest to fortify our deep medical diagnosis systems against these unseen foes.  The landscape of medical imaging is vast and varied, encompassing modalities such as CT scans, fundus photographs, and endoscopic visuals, each with its own unique characteristics. This diversity presents a formidable challenge; crafting adversarial perturbations that can traverse this complex terrain is no small feat. In this paper, we unveil a novel approach\u2014a method designed to generate consistent adversarial perturbations across the spectrum of medical images.  At the heart of our method lies a carefully crafted objective function, harmonizing two crucial components: the loss deviation term and the loss stabilization term. The loss deviation term serves to amplify the discord between the CNN\u2019s predictions for an adversarial example and its true label, while the loss stabilization term acts as a guiding force, ensuring that the CNN\u2019s predictions remain aligned with those of a smoothed version of the input. This duality allows for an exhaustive exploration of the perturbation landscape, deftly navigating away from local optima.  Delving deeper, we analyze the KL-divergence of our proposed loss function, revealing that the stabilization term not only directs the perturbations toward a fixed target but also deftly distances them from the ground truth. This intricate dance guarantees that our adversarial attacks retain their potency across various medical image types, all while maintaining a delicate balance of minimal variance in the perturbations produced.  Through rigorous experimentation on a range of medical image analysis benchmarks, including the contemporary COVID-19 dataset, we demonstrate the resilience and efficacy of our approach. In doing so, we take a significant step toward safeguarding the future of medical diagnostics against the lurking threats of adversarial manipulation.", "introduction": "Computer Aided Diagnosis (CADx) has been widely applied in the medical screening process.The automatic diagnosis benefits doctors to efficiently obtain health status to avoid disease exacerbation.Recently, Convolutional Neural Networks (CNNs) have been utilized in CADx to improve the diagnosis accuracy.The discriminative representations improve the performance of medical image analysis including lesion localization, segmentation and disease classification.However, recent advances in adversarial examples have revealed that the deployed CADx systems are usually fragile to adversarial attacks (Finlayson et al., 2019), e.g., small perturbations applied to the input images can deceive CNNs to have opposite conclusions.As mentioned in Ma et al. (2020), the vast amount of money in the healthcare economy may attract attackers to commit insurance fraud or false claims of medical reimbursement by manipulating medical reports.Moreover, image noise is a common issue during the data collection process and sometimes these noise perturbations could implicitly form adversarial attacks.For example, particle contamination of optical lens in dermoscopy and endoscopy and metal/respiratory artifacts of CT scans frequently deteriorate the quality of collected images.Therefore, there is a growing interest to investigate how medical diagnosis systems respond to adversarial attacks and what we can do to improve the robustness of the deployed systems.While recent studies of adversarial attacks mainly focus on natural images, the research of adversarial attacks in the medical image domain is desired as there are significant differences between Figure 1: Adversarial attacks on medical images.A clean fundus image is shown in (a) and correctly classified as \"None\" during diabetic retinopathy grading.The perturbations from FGSM (Goodfellow et al., 2014) attack successfully (i.e., grading as \"Mild\") in (b) while PGD (Madry et al., 2017) fails (i.e., grading still as \"None\").A clean CT slice is shown in (e) where the lung is correctly segmented.The perturbations from FGSM do not attack completely (i.e., cyan mask is still accurate) in (f) while PGD works in (g).A clean endoscopic image detection result is shown in (i).FGSM and PGD are not effective to fail the detector completely.The perturbations produced by SMIA decrease the analysis performance across different medical image datasets as shown in (d), (h) and (l).two domains.Beyond regular RGB cameras, there are various types of medical imaging equipments (e.g., Computed Tomography (CT) scanners, ultrasound transducers and fundus cameras) to generate dramatically different images.Fig. 1 shows three examples where an image captured from fundus camera is in (a), an image captured from the CT scanner is in (e) and an endoscopic video frame is in (i).As can be seen in the figure that these three images have little in common.The huge data variance across different modalities of medical images brings more challenges to develop a technology that works for all the modalities.In addition, existing investigations on medical adversarial attacks are limited.In Finlayson et al. (2019), adversarial examples are shown to deteriorate the diagnosis accuracy of deep learning based medical systems.These medical attack methods are mainly based on those from natural images (e.g., Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2014) and Project Gradient Descent (PGD) (Madry et al., 2017), which are insufficiently developed for different types of medical data.As shown in Fig. 1, the adversarial examples generated by FGSM and PGD do not consistently decrease the network's performance in (b), (c), (f), (g), (j) and (k).The data variance in (a) and (e) leads to the inconsistent attack results by existing methods.In this paper, we propose a medical image attack method to consistently produce adversarial perturbations that can fool deep medical diagnosis systems working with different medical data modalities.The perturbations are iteratively generated via taking partial derivatives of a well-defined objective function that is composed of a deviation loss term and a stabilized loss term with respect to the input.By maximizing the deviation loss term, the adversarial attack system enlarges the divergence between CNN predictions and the ground truth to have effective attack samples.To handle the aforementioned ubiquitous data noise issue in medical images, we propose a novel stabilization loss term as an extra regularization, which ensures a consistent deviation trajectory for the crafted attack samples.Meanwhile, the stabilization term avoids the local optima in the optimization process caused by the image noise.The proposed stabilization loss term is designed to measure the difference between two CNN predictions, where the first prediction is from the crafted adversarial sample and the second one is from the same sample processed with a Gaussian smoothing.Given an adversarial example A and its Gaussian smoothed result A, the loss stabilization term constrains the corresponding CNN predictions (i.e., f (A) and f ( A)) to be similar via a minimization process.The intuition from the scale space optimization (Lindeberg, 1992) indicates that the minimization of f (A) and f ( A) will exhaustively search the perturbation space to smooth the single spot for local optimum escape.We further analyze this stabilized loss term via KL-divergence and find that the CNN predictions are steered towards a fixed objective spot during iterations.This stabilization improves the attack effectiveness on different types of medical data including CT, fundus, and endoscopic images.We evaluate the proposed Stablized Medical Image Attack (SMIA) on several medical datasets (APT, 2019;EAD, 2019;Kag, 2015), including the recent COVID-19 (COV, 2019) lung CT.Thorough evaluations demonstrate that the proposed method is effective to produce perturbations that decrease the prediction accuracy of different medical diagnosis systems.Our investigation provides a guidance for strengthening the robustness of these medical systems towards adversarial attacks."}
{"paper_id": 138, "abstract": "In the realm of machine learning, convolutional image classifiers stand as titans of predictive accuracy. Yet, lurking in the shadows is a formidable challenge: the quantification of uncertainty. This challenge has proven to be a significant barrier, particularly when these classifiers are deployed in high-stakes environments where every decision matters. Existing methods, like Platt scaling, strive to refine the network's probability estimates, but they lack the formal guarantees that practitioners crave.  Enter our innovative algorithm, a beacon of clarity in this murky landscape. With the power to transform any classifier, it delivers predictive sets that confidently encompass the true label with a user-defined probability\u2014be it 90% or another threshold of your choosing. Simple and swift, akin to the elegance of Platt scaling, our approach boasts a robust finite-sample coverage guarantee applicable to any model or dataset.  At its core, our method enhances a conformal prediction algorithm, introducing a regularization technique that stabilizes the predictive sets by tempering the small scores of unlikely classes post-Platt scaling. In rigorous experiments conducted on both Imagenet and Imagenet-V2, utilizing the ResNet-152 architecture alongside other classifiers, our approach not only shines but eclipses existing methodologies. We achieve coverage with predictive sets that are often five to ten times smaller than those produced by traditional Platt scaling, marking a significant advancement in the quest for reliable uncertainty quantification in image classification.", "introduction": "Imagine you are a doctor making a high-stakes medical decision based on diagnostic information from a computer vision classifier.What would you want the classifier to output in order to make the best decision?This is not a casual hypothetical; such classifiers are already used in medical settings (e.g., Razzak et al., 2018;Lundervold & Lundervold, 2019;Li et al., 2014).A maximumlikelihood diagnosis with an accompanying probability may not be the most essential piece of information.To ensure the health of the patient, you must also rule in or rule out harmful diagnoses.In other words, even if the most likely diagnosis is a stomach ache, it is equally or more important to rule out stomach cancer.Therefore, you would want the classifier to give you-in addition to an estimate of the most likely outcome-actionable uncertainty quantification, such as a set of predictions that provably covers the true diagnosis with a high probability (e.g., 90%).This is called a prediction set (see Figure 1).Our paper describes a method for constructing prediction sets from any pre-trained image classifier that are formally guaranteed to contain the true class with the desired probability, relatively small, and practical to implement.Our method modifies a conformal predictor (Vovk et al., 2005) given in Romano et al. (2020) for the purpose of modern image classification in order to make it more stable in the presence of noisy small probability estimates.Just as importantly, we provide extensive evaluations and code for conformal prediction in computer vision.Formally, for a discrete response Y \u2208 Y = {1, . . ., K} and a feature vector X \u2208 R d , we desire an uncertainty set function, C(X), mapping a feature vector to a subset of {1, . . ., K} such thatfor a pre-specified confidence level \u03b1 such as 10%.Conformal predictors like our method can modify any black-box classifier to output predictive sets that are rigorously guaranteed to satisfy the desired coverage property shown in Eq. (1).For evaluations, we focus on Imagenet classification using convolutional neural networks (CNNs) as the base classifiers, since this is a particularly challenging testbed.In this setting, X would be the image and Y would be the class label.Note that the guarantee in Eq. ( 1) is marginal over X and Y -it holds on average, not for a particular image X.A first approach toward this goal might be to assemble the set by including classes from highest to lowest probability (e.g., after Platt scaling and a softmax function; see Platt et al., 1999;Guo et al., 2017) until their sum just exceeds the threshold 1 -\u03b1.We call this strategy naive and formulate it precisely in Algorithm 1.There are two problems with naive: first, the probabilities output by CNNs are known to be incorrect (Nixon et al., 2019), so the sets from naive do not achieve coverage.Second, image classification models' tail probabilities are often badly miscalibrated, leading to large sets that do not faithfully articulate the uncertainty of the model; see Section 2.3.Moreover, smaller sets that achieve the same coverage level can be generated with other methods.The coverage problem can be solved by picking a new threshold using holdout samples.For example, with \u03b1 =10%, if choosing sets that contain 93% estimated probability achieves 90% coverage on the holdout set, we use the 93% cutoff instead.We refer to this algorithm, introduced in Romano et al. ( 2020), as Adaptive Prediction Sets (APS).The APS procedure provides coverage but still produces large sets.To fix this, we introduce a regularization technique that tempers the influence of these noisy estimates, leading to smaller, more stable sets.We describe our proposed algorithm, Regularized Adaptive Prediction Sets (RAPS), in Algorithms 2 and 3 (with APS as a special case).As we will see in Section 2, both APS and RAPS are always guaranteed to satisfy Eq. ( 1)-regardless of model and dataset.Furthermore, we show that RAPS is guaranteed to have better performance than choosing a fixed-size set.Both methods impose negligible computational requirements in both training and evaluation, and output useful estimates of the model's uncertainty on a new image given, say, 1000 held-out examples.In Section 3 we conduct the most extensive evaluation of conformal prediction in deep learning to date on Imagenet and Imagenet-V2.We find that RAPS sets always have smaller average size than naive and APSsets.For example, using a ResNeXt-101, naive does not achieve coverage, while APS and RAPS achieve it almost exactly.However, APS sets have an average size of 19, while RAPS sets have an average size of 2 at \u03b1 = 10% (Figure 2 and Table 1).We will provide an accompanying codebase that implements our method as a wrapper for any PyTorch classifier, along with code to exactly reproduce all of our experiments."}
{"paper_id": 139, "abstract": "In the realm of data analysis, the quest for swift and precise classification of sequential data stands as a formidable challenge, particularly when the cost of sampling looms large. Enter the sequential probability ratio test (SPRT), a beacon of hope in this landscape, heralded for its Bayes-optimal nature\u2014an algorithm adept at minimizing the expected number of samples while adhering to a predetermined error threshold. Yet, the original SPRT is shackled by two crucial assumptions that hinder its practical application: first, it presumes samples are drawn independently and identically; second, it demands an exact calculation of the likelihood that data belongs to each class.  In response to these limitations, we unveil the SPRT-TANDEM, a groundbreaking algorithm that harnesses the power of deep neural networks to navigate these obstacles. The SPRT-TANDEM innovatively estimates the log-likelihood ratio of two competing hypotheses, employing a novel Loss function for Log-Likelihood Ratio estimation (LLLR) that accommodates correlations among the last \\(N\\) samples\u2014where \\(N\\) is a natural number. Through rigorous testing on a unique dataset alongside two well-established video databases\u2014Nosaic MNIST, UCF101, and SiW\u2014the SPRT-TANDEM not only outshines traditional baseline classifiers in terms of classification accuracy but does so with a notably reduced number of samples. For those eager to explore this advancement, the code and the Nosaic MNIST dataset are readily accessible at https://github.com/TaikiMiyagawa/SPRT-TANDEM.", "introduction": "The sequential probability ratio test, or SPRT, was originally invented by Abraham Wald, and an equivalent approach was also independently developed and used by Alan Turing in the 1940s (Good, 1979;Simpson, 2010;Wald, 1945).SPRT calculates the log-likelihood ratio (LLR) of two competing hypotheses and updates the LLR every time a new sample is acquired until the LLR reaches one of the two thresholds for alternative hypotheses (Figure 1).Wald and his colleagues proved that when sequential data are sampled from independently and identically distributed (i.i.d.) data, SPRT can minimize the required number of samples to achieve the desired upper-bounds of false positive and false negative rates comparably to the Neyman-Pearson test, known as the most powerful likelihood test (Wald & Wolfowitz, 1948) (see also Theorem (A.5) in Appendix A).Note that Wald used the i.i.d.assumption only for ensuring a finite decision time (i.e., LLR reaches a threshold within finite steps) and for facilitating LLR calculation: the non-i.i.d.property does not affect other aspects of the SPRT including the error upper bounds (Wald, 1947).More recently, Tartakovsky et al. verified that the non-i.i.d.SPRT is optimal or at least asymptotically optimal as the sample size increases (Tartakovsky et al., 2014), opening the possibility of potential applications of the SPRT to non-i.i.d.data series.About 70 years after Wald's invention, neuroscientists found that neurons in the part of the primate brain called the lateral intraparietal cortex (LIP) showed neural activities reminiscent of the SPRT (Kira et al., 2015); when a monkey sequentially collects random pieces of evidence to make a binary choice, LIP neurons show activities proportional to the LLR.Importantly, the time of the decision can be predicted from when the neural activity reaches a fixed threshold, the same as the SPRT's decision rule.Thus, the SPRT, the optimal sequential decision strategy, was re-discovered to be an the LLR every time a new sample (x (t) at time t) is acquired, until the LLR reaches one of the two thresholds.For data that is easy to be classified, the SPRT outputs an answer after taking a few samples, whereas for difficult data, the SPRT takes in numerous samples in order to make a \"careful\" decision.For formal definitions and the optimality in early classification of time series, see Appendix A.algorithm explaining primate brains' computing strategy.It remains an open question, however, what algorithm will be used in the brain when the sequential evidence is correlated, non-i.i.d.series.The SPRT is now used for several engineering applications (Cabri et al., 2018;Chen et al., 2017;Kulldorff et al., 2011).However, its i.i.d.assumption is too crude for it to be applied to other real-world scenarios, including time-series classification, where data are highly correlated, and key dynamic features for classification often extend across more than one data point, violating the i.i.d.assumption.Moreover, the LLR of alternative hypotheses needs to be calculated as precisely as possible, which is infeasible in many practical applications.In this paper, we overcome the above difficulties by using an SPRT-based algorithm that Treats data series As an N-th orDEr Markov process (SPRT-TANDEM), aided by a sequential probability density ratio estimation based on deep neural networks.A novel Loss function for Log-Likelihood Ratio estimation (LLLR) efficiently estimates the density ratio that let the SPRT-TANDEM approach close to asymptotic Bayes-optimality (i.e., Appendix A.4).In other words, LLLR optimizes classification speed and accuracy at the same time.The SPRT-TANDEM can classify non-i.i.d.data series with user-defined model complexity by changing N (\u2208 N), the order of approximation, to define the number of past samples on which the given sample depends.By dynamically changing the number of samples used for classification, the SPRT-TANDEM can maintain high classification accuracy while minimizing the sample size as much as possible.Moreover, the SPRT-TANDEM enables a user to flexibly control the speed-accuracy tradeoff without additional training, making it applicable to various practical applications.classification.The comprehensive review is left to Appendix B, while in the following, we introduce the SPRT, probability density estimation algorithms, and early classification of the time series.Sequential Probability Ratio Test (SPRT).The SPRT, denoted by \u03b4 * , is defined as the tuple of a decision rule and a stopping rule (Tartakovsky et al., 2014;Wald, 1947):Definition 2.1.Sequential Probability Ratio Test (SPRT).Let \u03bb t as the LLR at time t, and X (1,T )  as a sequential data X (1,T ) := {x (t) } T t=1 .Given the absolute values of lower and upper decision threshold, a 0 \u2265 0 and a 1 \u2265 0, SPRT, \u03b4 * , is defined aswhere the decision rule d * and stopping time \u03c4 * are\u03c4 * = inf{T \u2265 0|\u03bb T / \u2208 (-a 0 , a 1 )} .(3)We review the proof of optimality in Appendix A.4, while Figure 1 shows an intuitive explanation.Probability density ratio estimation.Instead of estimating numerator and denominator of a density ratio separately, the probability density ratio estimation algorithms estimate the ratio as a whole, reducing the degree of freedom for more precise estimation (Sugiyama et al., 2010;2012).Two of the probability density ratio estimation algorithms that closely related to our work are the probabilistic classification (Bickel et al., 2007;Cheng & Chu, 2004;Qin, 1998) and density fitting approach (Sugiyama et al., 2008;Tsuboi et al., 2009) algorithms.As we show in Section 4 and Appendix E, the SPRT-TANDEM sequentially estimates the LLR by combining the two algorithms.Early classification of time series.To make decision time as short as possible, algorithms for early classification of time series can handle variable length of data (Mori et al., 2018;Mori et al., 2016;Xing et al., 2009;2012) to minimize high sampling costs (e.g., medical diagnostics (Evans et al., 2015;Griffin & Moorman, 2001), or stock crisis identification (Ghalwash et al., 2014)).Leveraging deep neural networks is no exception in the early classification of time series (Dennis et al., 2018;Suzuki et al., 2018).Long short-term memory (LSTM) variants LSTM-s/LSTM-m impose monotonicity on classification score and inter-class margin, respectively, to speed up action detection (Ma et al., 2016).Early and Adaptive Recurrent Label ESTimator (EARLIEST) combines reinforcement learning and a recurrent neural network to decide when to classify and assign a class label (Hartvigsen et al., 2019)."}
{"paper_id": 140, "abstract": "In the ever-evolving landscape of deep learning, a shadow looms\u2014adversarial examples that threaten the very foundations of neural network integrity. These malicious perturbations have sparked a fervent quest for robust defenses, yet the solutions crafted thus far often falter, bolstering resilience against only specific types of attacks. While some recent advancements have ventured into the realm of multi-$\\ell_p$ ball defenses, they still leave much to be desired when faced with the diverse arsenal of adversarial perturbations.  To unravel this conundrum, we introduce the *multi-domain* hypothesis, a guiding principle that posits these adversarial perturbations hail from distinct domains, each with its own characteristics and challenges. With this insight, we unveil *Gated Batch Normalization (GBN)*\u2014a groundbreaking architectural innovation designed to fortify deep neural networks against a multitude of perturbation types.   At the heart of GBN lies a gated sub-network, a sentinel that discerns between various perturbation types, while a multi-branch batch normalization layer takes on the mantle of specialization. Each branch is dedicated to mastering the statistics of a single perturbation type, ensuring that the model learns to navigate the treacherous waters of adversarial attacks with domain-specific finesse. The culmination of this process yields features that are harmonized into domain-invariant representations, ready to empower the subsequent layers of the network.  Our rigorous evaluations across benchmark datasets\u2014MNIST, CIFAR-10, and Tiny-ImageNet\u2014reveal the prowess of GBN, as it decisively outstrips previous defenses against a spectrum of perturbation types, including $\\ell_1$, $\\ell_2$, and $\\ell_{\\infty}$, achieving remarkable improvements of 10-20%. In the battle against adversarial threats, GBN stands as a formidable ally, ushering in a new era of resilience for deep learning models.", "introduction": "Deep neural networks (DNNs) have achieved remarkable performance across a wide areas of applications (Krizhevsky et al., 2012;Bahdanau et al., 2014;Hinton et al., 2012), but they are susceptible to adversarial examples (Szegedy et al., 2013).These elaborately designed perturbations are imperceptible to humans but can easily lead DNNs to wrong predictions, threatening both digital and physical deep learning applications (Kurakin et al., 2016;Liu et al., 2019a).To improve model robustness against adversarial perturbations, a number of adversarial defense methods have been proposed (Papernot et al., 2015;Engstrom et al., 2018;Goodfellow et al., 2014).Many of these defense methods are based on adversarial training (Goodfellow et al., 2014;Madry et al., 2018), which augment training data with adversarial examples.However, most adversarial defenses are designed to counteract a single type of perturbation (e.g., small \u221e -noise) (Madry et al., 2018;Kurakin et al., 2017;Dong et al., 2018).These defenses offer no guarantees for other perturbations (e.g., 1 , 2 ), and sometimes even increase model vulnerability (Kang et al., 2019;Tram\u00e8r & Boneh, 2019).To address this problem, other adversarial training strategies have been proposed with the goal of simultaneously achieving robustness against multiple types of attacks, i.e., \u221e , 1 , and 2 attacks (Tram\u00e8r & Boneh, 2019;Maini et al., 2020).Although these methods improve overall model robustness against adversarial attacks in multiple p balls, the performance for each individual perturbation type is still far from satisfactory.In this work, we propose the multi-domain hypothesis, which states that different types of adversarial perturbation arise in different domains, and thus have separable characteristics.Training on data from multiple domains can be regarded as solving the invariant risk minimization problem (Ahuja et al., 2020), in which an invariant predictor is learnt to achieve the minimum risk for different environments.For a deep learning model, instance-related knowledge can be stored in the weight matrix of each layer, whereas domain-related knowledge can be represented by the batch normalization (BN) layer statistics (Li et al., 2017).Inspired by the multi-domain hypothesis, we propose to improve model robustness against multiple perturbation types by separating domain-specific information for different perturbation types, and using BN layer statistics to better align data from the mixture distribution and learn domain-invariant representations for multiple adversarial examples types.In particular, we propose a novel building block for DNNs, referred to as Gated Batch Normalization (GBN), which consists of a gated subnetwork and a multi-branch BN layer.GBN first learns to separate perturbations from different domains on-the-fly and then normalizes them by obtaining domain-specific features.Specifically, each BN branch handles a single perturbation type (i.e., domain).Then, features computed from different branches are aligned as domain-invariant representations that are aggregated as the input to subsequent layers.Extensive experiments on MNIST, CIFAR-10, and Tiny-ImageNet demonstrate that our method outperforms previous defense strategies by large margins, i.e., 10-20%."}
{"paper_id": 141, "abstract": "In the ever-evolving realm of distributed learning, where the promise of vast computational power and collaborative intelligence beckons, researchers have found themselves at a crossroads. The traditional paradigms of distributed learning, often built on the fragile assumption of flawless worker performance, stand vulnerable to the chaotic realities of the world\u2014communication failures, malicious attacks, and the unpredictable whims of technology. Enter Byzantine learning (BL), a domain that acknowledges these threats and seeks to fortify the foundations of distributed systems against the treachery of failure and sabotage.  Yet, even within this burgeoning field, a challenge remains. Most existing BL approaches operate under synchronous conditions, a constraint that proves impractical in scenarios where workers are heterogeneous or operate offline. Thus, the call for asynchronous Byzantine learning (ABL) has emerged, demanding innovative solutions that can adapt to the complexities of real-world applications.  In response to this challenge, we introduce a groundbreaking method: buffered asynchronous stochastic gradient descent (BASGD). To our knowledge, BASGD is the first ABL framework that defends against malicious attacks without the burden of storing instances on a central server, significantly reducing the risk of privacy breaches. Through rigorous analysis, we demonstrate that BASGD not only converges effectively but also stands resilient against failures and attacks.  Empirical evidence reveals that BASGD significantly outstrips traditional asynchronous stochastic gradient descent (ASGD) and other ABL benchmarks, particularly in the face of adversarial conditions. With BASGD, we embark on a new chapter in the saga of distributed learning, one where resilience and adaptability reign supreme amidst the chaos of the digital landscape.", "introduction": "Due to the wide application in cluster-based large-scale learning, federated learning (Konevcn\u1ef3 et al., 2016;Kairouz et al., 2019), edge computing (Shi et al., 2016) and so on, distributed learning has recently become a hot research topic (Zinkevich et al., 2010;Yang, 2013;Jaggi et al., 2014;Shamir et al., 2014;Zhang & Kwok, 2014;Ma et al., 2015;Lee et al., 2017;Lian et al., 2017;Zhao et al., 2017;Sun et al., 2018;Wangni et al., 2018;Zhao et al., 2018;Zhou et al., 2018;Yu et al., 2019a;b;Haddadpour et al., 2019).Most traditional distributed learning methods are based on stochastic gradient descent (SGD) and its variants (Bottou, 2010;Xiao, 2010;Duchi et al., 2011;Johnson & Zhang, 2013;Shalev-Shwartz & Zhang, 2013;Zhang et al., 2013;Lin et al., 2014;Schmidt et al., 2017;Zheng et al., 2017;Zhao et al., 2018), and typically assume no failure or attack on workers.However, in real distributed learning applications with multiple networked machines (nodes), different kinds of hardware or software failure may happen.Representative failure include bit-flipping in the communication media and the memory of some workers (Xie et al., 2019).In this case, a small failure on some machines (workers) might cause a distributed learning method to fail.In addition, malicious attack should not be neglected in an open network where the manager (or server) generally has not much control on the workers, such as the cases of edge computing and federated learning.Some malicious workers may behave arbitrarily or even adversarially.Hence, Byzantine learning (BL), which refers to distributed learning with failure or attack, has recently attracted much attention (Diakonikolas et al., 2017;Chen et al., 2017;Blanchard et al., 2017;Alistarh et al., 2018;Damaskinos et al., 2018;Xie et al., 2019;Baruch et al., 2019;Diakonikolas & Kane, 2019).Existing BL methods can be divided into two main categories: synchronous BL (SBL) methods and asynchronous BL (ABL) methods.In SBL methods, the learning information, such as the gradient in SGD, of all workers will be aggregated in a synchronous way.On the contrary, in ABL methods the learning information of workers will be aggregated in an asynchronous way.Existing SBL methods mainly take two different ways to achieve resilience against Byzantine workers which refer to those workers with failure or attack.One way is to replace the simple averaging aggregation operation with some more robust aggregation operations, such as median and trimmed-mean (Yin et al., 2018).Krum (Blanchard et al., 2017) and ByzantinePGD (Yin et al., 2019) take this way.The other way is to filter the suspicious learning information (gradients) before averaging.Representative examples include ByzantineSGD (Alistarh et al., 2018) and Zeno (Xie et al., 2019).The advantage of SBL methods is that they are relatively simple and easy to be implemented.But SBL methods will result in slow convergence when there exist heterogeneous workers.Furthermore, in some applications like federated learning and edge computing, synchronization cannot even be performed most of the time due to the offline workers (clients or edge servers).Hence, ABL is preferred in these cases.To the best of our knowledge, there exist only two ABL methods: Kardam (Damaskinos et al., 2018) and Zeno++ (Xie et al., 2020).Kardam introduces two filters to drop out suspicious learning information (gradients), which can still achieve good performance when the communication delay is heavy.However, when in face of malicious attack, some work finds that Kardam also drops out most correct gradients in order to filter all faulty (failure) gradients.Hence, Kardam cannot resist malicious attack (Xie et al., 2020).Zeno++ scores each received gradient, and determines whether to accept it according to the score.But Zeno++ needs to store some training instances on server for scoring.In practical applications, storing data on server will increase the risk of privacy leakage or even face legal risk.Therefore, under the general setting where server has no access to any training instances, there have not existed ABL methods to resist malicious attack.In this paper, we propose a novel method, called buffered asynchronous stochastic gradient descent (BASGD), for ABL.The main contributions of BASGD are listed as follows:\u2022 To the best of our knowledge, BASGD is the first ABL method that can resist malicious attack without storing any instances on server.Compared with those methods which need to store instances on server, BASGD takes less risk of privacy leakage.\u2022 BASGD is theoretically proved to be convergent, and be able to resist failure or attack.\u2022 Empirical results show that BASGD significantly outperforms vanilla ASGD and other ABL baselines when there exist failure or malicious attack on workers.In particular, BASGD can still converge under malicious attack, when ASGD and other ABL methods fail."}
{"paper_id": 142, "abstract": "In the realm of artificial intelligence, deep reinforcement learning strives to emulate human-level decision-making, often grappling with the intricate web of interconnected sub-tasks that define complex challenges. Traditional approaches falter in the face of these nuanced relationships, leaving a void for more innovative solutions. To illuminate this struggle, we have crafted a first-person shooting environment, replete with randomly generated spatial structures, serving as a vivid backdrop for our exploration.  In this dynamic landscape, an ideal agent must deftly navigate the dual demands of locating adversaries and executing precise shots\u2014each a sub-task that requires its own mastery. To tackle the intricacies of this environment, we introduce the Meta Soft Hierarchical reinforcement learning framework, affectionately dubbed MeSH. This framework empowers low-level sub-policies to hone in on specific sub-tasks while a high-level policy intelligently orchestrates their collaboration through the art of meta-gradients.  Through MeSH, we unravel the complexities of multiple sub-tasks, enabling the discovery of tailored low-level policies suited to a variety of scenarios. Our extensive series of comparative experiments demonstrates the framework's remarkable effectiveness and efficiency. In the spirit of collaboration and advancement, both the environment and the algorithm's code will be made available as open source, inviting the community to delve deeper into this exciting frontier of research.", "introduction": "With great breakthrough of deep reinforcement learning (DRL) methods (Mnih et al., 2015;Silver et al., 2016;Mnih et al., 2016;Schulman et al., 2015;Lillicrap et al., 2015), it is an urgent need to use DRL methods to solve more complex decision-making problems.The practical problem in real world is often a subtle combination of multiple sub-tasks, which may happen simultaneously and hard to disentangle by time series.For instance, in StarCraft games (Pang et al., 2019), agents need to consider building units and organizing battles, sub-tasks may change rapidly over the whole game process; sweeping robots tradeoff between navigating and collecting garbage; shooting agents should move to appropriate positions and launch attacks, etc.The relationship between sub-tasks is complex and subtle.Sometimes they compete with each other and need to focus on one task to gain key advantages; at other times, they need to cooperate with each other to maintain the possibility of global exploration.It is often time consuming and ineffective to learn simply by collecting experience and rewarding multiple objectives for different sub-tasks.A reasonable idea is to utilize deep hierarchical reinforcement learning (DHRL) methods (Vezhnevets et al., 2017;Igl et al., 2020), where the whole system is divided into a high-level agent and several low-level agents.Low-level agents learn sub-policies, which select atomic actions for corresponding sub-tasks.The high-level agent is responsible for a meta task in the abstract logic or coarser time granularity, guiding low-level agents by giving a goal, or directly selecting among subpolicies.However, DHRL methods face some inherent problems: due to the complex interaction between multi-level agents, there is no theoretical guarantee of convergence, and it shows unstable experimental performance.Most DHRL methods require heavy manual design, and end-to-end system lacks reasonable semantic interpretation.In addition, these agents are often constrained by specific tasks and are easy to overfit.Even transferring between similar tasks, they perform poorly and need a lot of additional adjustments.We introduce a first-person shooting (FPS) environment with random spatial structures.The game contains a 3D scene from human perspective.When the player defeats all enemies, the player wins the game.When the player drops to the ground or losses all health points, the player loses the game.It is very risky for the player to drop to the ground, thus environment contains two key tasks: navigation and combat.The terrain and enemies in the game are randomly generated.This ensures: 1) the agent cannot learn useful information by memorizing coordinates; 2) the possibility of overfitting is restrained and the generalization ability of learned policy is enhanced.The state information is expressed in the way of raycast.This representation of environment information requires much less computing resources than the raw image representation.It can be trained and tested even with only CPU machines, which makes us pay more attention to the reinforcement learning algorithm itself rather than the computing ablity related to image processing.For this environment, we propose a Meta Soft Hierarchical reinforcement learning framework (MeSH).The high-level policy is a differentiable meta parameter generator, and the low-level policy contains several sub-policies, which are in the same form and differentiated automatically in the training procedure.The high-level policy selects and combines sub-policies through the meta parameter and interacts with the environment.We find that the meta generator can adaptively combines sub-policies with the process of the task, and have strong interpretability in semantics.Compared with a series of baselines, the agent has achieved excellent performance in FPS environment.The main contributions of this work are as follows:\u2022 clarifying the complex relationship between multi-task composition.\u2022 a novel meta soft hierarchical reinforcement learning framework, MeSH, which uses differentiable meta generator to adaptively select sub-policies and shows strong interpretability.\u2022 a series of comparison experiments to show the effectiveness of the framework.\u2022 an open-sourced environment and code to encourage further research on multi-task RLfoot_0 .In this paper, we discuss the related work in Section 2. We introduce the details of the implemented environment in Section 3. We show our proposed framework in Section 4. We demonstrate details of our experiments in Section 5.At last, we conclude in Section 6."}
{"paper_id": 143, "abstract": "In the realm of machine learning, the quest for understanding the intricate threads that weave together the fabric of data is a noble one. Our exploration into Content and Style (C-S) disentanglement seeks to unravel these threads, breaking down the essence of objects into two distinct yet harmonious latent spaces. With the aim of achieving unsupervised disentanglement, we introduce a guiding principle to our approach: by assigning unique and independent roles to content and style, we can better approximate the complex distributions of real-world data.  In our method, the content embeddings of individual images are compelled to coalesce around a shared distribution, providing a common ground from which all images can draw. Meanwhile, the style embeddings, which capture the unique characteristics of each instance, serve to personalize this shared distribution, allowing for a rich tapestry of expression. Through rigorous experimentation across several esteemed datasets, we demonstrate that our approach not only achieves state-of-the-art disentanglement when pitted against other unsupervised methods, but also rivals, and in some cases surpasses, the performance of supervised techniques.  Moreover, we unveil a novel application of C-S disentanglement: the generation of multi-view images from a single perspective, paving the way for advancements in 3D reconstruction. In this endeavor, we harness the power of disentangled representations, illuminating new pathways in the intricate landscape of visual understanding.", "introduction": "The disentanglement task aims to recover the underlying explanatory factors of natural images into different dimensions of latent space, and provide an informative representation for tasks like image translation (Wu et al., 2019b;Kotovenko et al., 2019), domain adaptation (Li et al., 2019;Zou et al., 2020) and geometric attributes extraction (Wu et al., 2019c;Xing et al., 2019), etc.The previous methods (Kim & Mnih, 2018;Higgins et al., 2017;Burgess et al., 2018a;Kumar et al., 2017) learn disentangled factors by optimizing the total correlation in an unsupervised manner.However, Locatello et al. (2019) prove that unsupervised disentanglement is fundamentally impossible without inductive bias on both model and data.In this paper, we focus on content and style (C-S) disentanglement, where content and style represent two separate groups of factors.The main novelty of our work is that we assign different roles to the content and style in modeling the image distribution instead of treating the factors equally, which is the inductive bias introduced in our method.Most of the previous C-S disentanglement works (Denton & Birodkar, 2017;Jha et al., 2018;Bouchacourt et al., 2018;Gabbay & Hoshen, 2020) rely on supervision, which is hard to obtain for real data.E.g., Gabbay & Hoshen (2020) leverage group observation to achieve disentanglement by forcing images from the same group to share a common embedding.To our best knowledge, the only exception is Wu et al. (2019c).However, this method forces the content path to learn geometric structure limited by 2D landmarks.Our definition of content and style is similar to Gabbay & Hoshen (2020), where the content includes the information which can be transferred among groups and style is image-specific information.When group observation is not available, we define content includes the factors shared across the whole dataset, such as pose.Take the human face dataset CelebA (Liu et al., 2015) as an example, the content encodes pose, and style encodes identity, and multi-views of the same identity have the same style embeddings, but different content embeddings, i.e., poses.Based on the above definitions, we propose a new problem formulation and network architecture by introducing an inductive bias: assigning different and independent roles to content and style when approximating the real data distributions.Specifically, as shown in Figure 1, we force the content embeddings of individual images to share a common distribution, and the style embeddings are used to scale and shift the common distribution to match target image distribution via a generator.Content Latent Space Generator Style Latent Space                 \u00d7 +            Figure 1: Overview of our framework.c i , c j , c k labelled with different shapes are embeddings sampled from a shared distribution \u03a8. s i , s j , s k labelled with different colors are embeddings from the style latent space.f \u03c3 and f \u00b5 are two fully-connected layers predicting the statistics parameters to scale and shift \u03a8 respectively to approximate the target image distributions via a Generator.For each generated image from 3 \u00d7 3 grid, the content and style embeddings are from the column and row respectively.We follow Bojanowski et al. (2018) and Gabbay & Hoshen (2020) to apply latent optimization to optimize the embeddings and the parameters of the generator.We also propose to use instance discrimination as a complementary constraint to assist the disentanglement.Please note that we only use the image reconstruction loss as the supervision; no extra labeling is needed.As the content and style perform a different and independent role when modeling the data, they are disentangled to encode the shared and instance-specific features respectively after the optimization.The contributions of our work are as follows: we achieve unsupervised C-S disentanglement by introducing an inductive bias in our formulation: assign different and independent roles to content and style when modeling the real data distributions.Furthermore, we achieve better C-S disentanglement by leveraging instance discrimination.The experiments on several popular datasets demonstrate that our method achieves the state-of-the-art unsupervised C-S disentanglement and comparable or even better results than supervised methods.Besides, we propose to apply C-S disengagement to a new task: single view 3D reconstruction."}
{"paper_id": 144, "abstract": "In the realm of federated learning, a unique tapestry is woven from the disparate threads of local client data, each thread contributing to the creation of a collaborative prediction model through the art of secure aggregation. Yet, within this intricate system lies a crucial imperative: the preservation of client privacy. Each participant must safeguard their local training datasets, sharing only the distilled essence of their model updates with the central server. However, our exploration reveals a shadow lurking within this paradigm\u2014a vulnerability that can lead to a model's downfall, particularly when beset by corrupted data samples manipulated by adversaries post-deployment.  To forge a path toward greater resilience, we embark on a journey of understanding, dissecting the aggregation error faced by the central server into its fundamental components: bias and variance. From this analysis, we unveil a novel framework, Fed_BVA, designed to bolster model robustness. This framework harnesses the power of on-device adversarial training, utilizing bias-variance oriented adversarial examples communicated asymmetrically from the server to the clients.  Through rigorous experimentation across a variety of benchmark datasets and employing several prominent neural network architectures, our findings illuminate the strength of Fed_BVA. The results demonstrate its formidable defense against both white-box and black-box adversarial attacks, proving its mettle in both IID and non-IID scenarios. Thus, we stand on the brink of a new era in federated learning, where the harmony of collaboration and the shield of robustness work hand in hand to protect the integrity of our models.", "introduction": "The explosive amount of decentralized user data collected from the ever-growing usage of smart devices, e.g., smartphones, wearable devices, home sensors, etc., has led to a surge of interest in the field of decentralized learning.To protect the privacy-sensitive data of the clients, federated learning (McMahan et al., 2017;Yang et al., 2019) has been proposed.Federated learning only allows a group of clients to train local models using their own data, and then collectively merges the model updates on a central server using secure aggregation (Acar et al., 2018).Due to its high privacy-preserving property, federated learning has attracted much attention in recent years along with the prevalence of efficient light-weight deep models (Howard et al., 2017) and low-cost network communications (Wen et al., 2017;Kone\u010dn\u1ef3 et al., 2016).In federated learning, the central server only inspects the secure aggregation of the local models as a whole.Consequently, it is susceptible to clients' corrupted updates (e.g., system failures, etc).Recently, multiple robust federated learning models (Fang et al., 2019;Pillutla et al., 2019;Portnoy & Hendler, 2020;Mostafa, 2019) have been proposed.These works only focus on performing clientlevel robust training or designing server-level aggregation variants with hyper-parameter tuning for Byzantine failures.However, none of them have the ability to mitigate the federated learning's vulnerability when the adversarial manipulations are present during testing, which as we shown in Section 4.1 that is mainly due to the generalization error in the model aggregation.Our work bridges this gap by investigating the error incurred during the aggregation of federated learning from the perspective of bias-variance decomposition (Domingos, 2000;Valentini & Dietterich, 2004).Specifically, we show that the generalization error of the aggregated model on the central server can be decomposed as the combination of bias (triggered by the main prediction of these clients) and variance (triggered by the variations among clients' predictions).Next, we propose to perform the local robust training on clients by supplying them with a tiny amount of the bias-variance perturbed examples generated from the central server via asymmetrical communications.The experiments are conducted on neural networks with cross-entropy loss, however, other loss functions are also applicable as long as their gradients w.r.t.bias and variance are tractable to estimate.In this way, any gradient-based adversarial training strategies (Goodfellow et al., 2015;Madry et al., 2018) could be used.Compared with previous work, our major contributions include:\u2022 We provide the exact solution of bias-variance analysis w.r.t. the generalization error which is perfectly suitable for neural network based federated learning.As a comparison, performing adversarial attacks or training with conventional federated learning methods will only focus on the bias of the central model but ignore the variance.\u2022 We demonstrate that the conventional federated learning framework is vulnerable to the strong attacking methods with increasing communication rounds even if the adversarial training using the locally generated adversarial examples is performed on each client.\u2022 Without violating the clients' privacy, we show that providing a tiny amount of bias-variance perturbed data from the central server to the clients through asymmetrical communication could dramatically improve the robustness of the training model under various settings."}
{"paper_id": 145, "abstract": "In the realm of machine learning, the art of transfer learning has woven itself into the fabric of numerous transformative applications. Yet, much of the existing literature remains anchored in the static waters of traditional transfer learning, neglecting the dynamic currents of time-evolving target domains, such as the ever-shifting landscape of online movie reviews. In this paper, we embark on a quest to explore the uncharted territory of continuous transfer learning, where the target domain is not a fixed entity but a living, breathing organism that evolves over time.  One of the most formidable challenges we face in this endeavor is the shifting relationship between the source domain and the current target domain as it undergoes its metamorphosis. To illuminate this path, we first derive a comprehensive generalization error bound that adapts to the current target domain, employing flexible measures of domain discrepancy. Building upon this foundation, we introduce a groundbreaking concept: the label-informed C-divergence. This innovative metric captures the nuances of joint data distributions\u2014encompassing both input features and output labels\u2014across domains, allowing us to forge a tighter upper bound on the error in the continuous transfer learning setting.  With these tools in hand, we unveil CONTE, an adversarial Variational Auto-encoder algorithm designed to minimize the C-divergence-based error upper bound. Through rigorous experimentation across diverse datasets, we demonstrate the prowess of our CONTE algorithm, proving its effectiveness in navigating the turbulent waters of continuous transfer learning. Join us as we chart a course through this evolving landscape, illuminating the path for future explorations in the realm of machine learning.", "introduction": "Transfer learning has achieved significant success across multiple high-impact application domains (Pan & Yang, 2009).Compared to conventional machine learning methods assuming both training and test data have the same data distribution, transfer learning allows us to learn the target domain with limited label information by leveraging a related source domain with abundant label information (Ying et al., 2018).However, in many real applications, the target domain is constantly evolving over time.For example, the online movie reviews are changing over the years: some famous movies were not well received by the mainstream audience when they were first released, but became famous only years later (e.g., Citizen Cane, Fight Club, and The Shawshank Redemption); whereas the online book reviews typically do not have this type of dynamics.It is challenging to transfer knowledge from the static source domain (e.g., the book reviews) to the time evolving target domain (e.g., the movie reviews).Therefore, in this paper, we study the transfer learning setting with a static source domain and a continuously time evolving target domain (see Figure 1), which has not attracted much attention from the research community and yet is commonly seen across many real applications.The unique challenge for continuous transfer learning lies in the time evolving nature of the task relatedness between the static source domain and the time evolving target domain.Although the change in the target data distribution in consecutive time stamps might be small, over time, the cumulative change in the target domain might even lead to negative transfer (Rosenstein et al., 2005).Existing theoretical analysis on transfer learning (Ben-David et al., 2010;Mansour et al., 2009) showed that the target error is typically bounded by the source error, the domain discrepancy of marginal data distributions and the difference of labeling functions.However, it has been observed (Zhao et al., 2019;Wu et al., 2019) that marginal feature distribution alignment might not guarantee the minimization of the target error in real world scenarios.This indicates that in the context of continuous transfer learning, marginal feature distribution alignment would lead to the sub-optimal solution (or even negative transfer) with undesirable predictive performance when directly transferring from D S to the target domain D Tt at the t th time stamp.This paper aims to bridge the gap in terms of both the theoretical analysis and the empirical solutions for the target domain with a time evolving distribution, which lead to a novel continuous transfer learning algorithm as well as the characterization of negative transfer.The main contributions of this paper are summarized as follows: (1) We derive a generic error bound for continuous transfer learning setting with flexible domain divergence measures; (2) We propose a label-informed domain discrepancy measure (C-divergence) with its empirical estimate, which instantiates a tighter error bound for continuous transfer learning setting; (3) Based on the proposed C-divergence, we design a novel adversarial Variational Auto-encoder algorithm (CONTE) for continuous transfer learning; (4) Extensive experimental results on various data sets verify the effectiveness of the proposed CONTE algorithm.The rest of the paper is organized as follows.Section 2 introduces the notation and our problem definition.We derive a generic error bound for continuous transfer learning setting in Section 3. Then we propose a novel C-divergence in Section 4, followed by a instantiated error bound and a novel continuous transfer learning algorithm in Section 5.The experimental results are provided in Section 6.We summarize the related work in Section 7, and conclude the paper in Section 8."}
{"paper_id": 146, "abstract": "In the realm of deep learning, we have witnessed remarkable triumphs across diverse landscapes\u2014images, voices, and intricate graphs. Yet, a shadow lingers over the domains where the relationships between features remain cloaked in mystery, particularly in tabular datasets filled with demographic and clinical factors that lack a predefined structure. In this paper, we unveil the Group-Connected Multilayer Perceptron (GMLP) networks, a novel approach designed to illuminate the dark corners of deep representation learning in these challenging terrains.  At the heart of GMLP lies the concept of harnessing expressive feature combinations\u2014what we call groups\u2014and utilizing them to streamline network complexity through localized group-wise operations. As the network undergoes training, GMLP embarks on a journey to discover a sparse feature grouping matrix. This is achieved through the clever application of temperature annealing softmax, augmented by an entropy loss term that champions sparsity.  Moreover, we propose an architecture reminiscent of binary trees, where group-wise operations seamlessly intertwine with pooling operations to amalgamate information, gradually reducing the number of groups as the network delves deeper into its layers. To validate our approach, we embarked on a series of experiments across a spectrum of real-world datasets, spanning various applications. We also present captivating visualizations on the MNIST dataset and synthesized data that further illuminate our findings.  The results speak volumes: GMLP not only adeptly learns and capitalizes on expressive feature combinations but also achieves state-of-the-art classification performance across multiple datasets. In a world where complexity often reigns, GMLP stands as a beacon of innovation, guiding us toward deeper understanding and unprecedented success.", "introduction": "Deep neural networks have been quite successful across various machine learning tasks.However, this advancement has been mostly limited to certain domains.For example in image and voice data, one can leverage domain properties such as location invariance, scale invariance, coherence, etc. via using convolutional layers (Goodfellow et al., 2016).Alternatively, for graph data, graph convolutional networks were suggested to leverage adjacency patterns present in datasets structured as a graph (Kipf & Welling, 2016;Xu et al., 2019).However, there has been little progress in learning deep representations for datasets that do not follow a particular known structure in the feature domain.Take for instance the case of a simple tabular dataset for disease diagnosis.Such a dataset may consist of features from different categories such as demographics (e.g., age, gender, income, etc.), examinations (e.g., blood pressure, lab results, etc.), and other clinical conditions.In this scenario, the lack of any known structure between features to be used as a prior would lead to the use of a fully-connected multilayer perceptron network (MLP).Nonetheless, it has been known in the literature that MLP architectures, due to their huge complexity, do not usually admit efficient training and generalization for networks of more than a few layers.In this paper, we propose Group-Connected Multilayer Perceptron (GMLP) networks.The main idea behind GMLP is to learn and leverage expressive feature subsets, henceforth referred to as feature groups.A feature group is defined as a subset of features that provides a meaningful representation or high-level concept that would help the downstream taskfoot_0 .For instance, in the disease diagnosis example, the combination of a certain blood factor and age might be the indicator of a higher level clinical condition which would help the final classification task.Furthermore, GMLP leverages feature groups limiting network connections to local group-wise connections and builds a feature hierarchy via merging groups as the network grows in depth.GMLP can be seen as an architecture that learns expressive feature combinations and leverages them via group-wise operations.The main contributions of this paper are as follows: (i) proposing a method for end-to-end learning of expressive feature combinations, (ii) suggesting a network architecture to utilize feature groups and local connections to build deep representations, (iii) conducting extensive experiments demonstrating the effectiveness of GMLP as well as visualizations and ablation studies for better understanding of the suggested architecture.We evaluated the proposed method on five different real-world datasets in various application domains and demonstrated the effectiveness of GMLP compared to state-of-the-art methods in the literature.Furthermore, we conducted ablation studies and comparisons to study different architectural and training factors as well as visualizations on MNIST and synthesized data.To help to reproduce the results and encouraging future studies on group-connected architectures, we made the source code related to this paper available online 2 .Additional details and experimental results are provided as appendices to this paper."}
{"paper_id": 147, "abstract": "In the intricate realm of safety-critical systems, where the stakes are high and the consequences of missteps can be dire, mastering the art of control requires not just skill but also a willingness to embrace calculated risks. Imagine a world where we can harness the hidden dynamics of such systems\u2014like the delicate balance of deep brain stimulation\u2014to unveil their secrets. In this paper, we unveil a groundbreaking approach that melds the precision of probabilistic safety with the ingenuity of meta-active learning, allowing us to unravel system dynamics and discover optimal configurations with unprecedented efficiency.  At the heart of our method lies a unique fusion of meta-learning and chance-constrained optimization. Picture this: we begin by crafting an LSTM-based embedding that captures the essence of our active learning sample history. Then, we encode a sophisticated deep learning acquisition function into a mixed-integer linear program (MILP), transforming our insights into actionable strategies. With the MILP as our guiding compass, we navigate the delicate trade-off between anticipated information gain and the imperative of safe control, ultimately charting an optimal action trajectory.  Our results speak for themselves, setting a new benchmark in active learning for high-dimensional systems with latent dynamics. We achieved a remarkable 46% increase in information gain and a 20% acceleration in computation time. Moreover, our approach outshines existing methods in fine-tuning the parameters for deep brain stimulation in rats, enhancing their cognitive task performance while deftly sidestepping the perilous pitfalls of unwanted side effects, such as triggering seizures. In this quest for knowledge and safety, we forge a new path, one where the balance of risk and reward leads not only to understanding but to mastery.", "introduction": "Safe and efficient control of a novel systems with latent dynamics is an important objective in domains from healthcare to robotics.In healthcare, deep brain stimulation devices implanted in the brain can improve memory deficits in patients with Alzheimers (Posporelis et al., 2018) and responsive neurostimulators (RNS) can counter epileptiform activity to mitigate seizures.Yet, the surgeon's trial-and-error process of finding effective RNS parameters for each patient is time-consuming and risky, with poor device settings possibly damaging the brain.Researchers studying active learning and Bayesian optimization have sought to develop algorithms to efficiently and safely learn a systems' dynamics, e.g.learning a brain's dynamics for RNS configuration (Ashmaig et al., 2018;Sui et al., 2018).However, because these algorithms fail to scale up to higher-dimensional state-action spaces, researchers utilize only simple voltage and frequency controls rather than all 32 channels of the RNS waveform (Ashmaig et al., 2018).Similarly, tasks in robotics, e.g.learning the dynamics of novel robotic systems (e.g., an autopilot learning to fly a damaged aircraft), require active learning methods that succeed in higher-dimensional domains.In this paper, we develop a probabilistically-safe, meta-active learning approach to tackle these challenging tasks to efficiently learn system dynamics and optimal configurations.We draw inspiration from recent contributions in meta-learning (Finn et al., 2017;Nagabandi et al., 2019;Wang et al., 2016;Andrychowicz et al., 2016) that seek to leverage a distribution over training tasks to optimize the parameters of a neural network for efficient, online adaptation.Researchers have previously investigated meta-learning for active learning, e.g.learning a Bayesian prior over a Gaussian Process (Wang et al., 2018b) for learning an acquisition function.However, these approaches do not consider the important problem of safely and actively learning to control a system with altered dynamics, which is a requirement for safety-critical robotic applications.Furthermore, as we show in Section 5, on challenging control tasks for healthcare and robotics, the performance of prior active learning approaches (Kirsch et al., 2019;Hastie et al., 2017) leaves much to be desired.We seek to overcome these key limitations of prior work by harnessing the power of meta-learning for active learning in a chance-constrained optimization framework for safe, online adaptation by encoding a learned representation of sample history.Instead of hand-engineering an acquisition function for our specific domains, our approach employs a data-drive, meta-learning approach, which results in better performance than prior approaches, as shown in Section 5. Furthermore, our approach has the unique ability to impose analytical safety constraints over a sample trajectory.Contributions -We develop a probabilistically safe, meta-learning approach for active learning (\"meta-active learning\") that sets a new state-of-the-art.Our acquisition function (i.e., the function that predicts the expected information gain of a data point) is meta-learned offline, allowing the policy to benefit from past experience and provide a more robust measure of the value of an action.The key to our approach is a novel interweaving of our deep, meta-learned acquisition function as a Long-Short Term Memory Network (Gers et al., 1999) (LSTM) within a chance-constrained, mixed-integer linear program (MILP) (Schrijver, 1998).By encoding the LSTM's linear, piece-wise output layers into the MILP, we directly optimize an action trajectory that best ensures the safety of the system while also maximizing the information learned about the system.In this paper, we describe our novel architecture which uniquely combines the power of a learned acquisition function with chance-constrained optimization and evaluate its performance against state-of-the-art baselines in several relevant domains.To the best of our knowledge, this is the only architecture which meta-learns an acquisition function for optimization tasks and is capable of embedding this acquisition function in a chance-constrained linear program to guarantee a minimum level of safe operation.The contributions of this paper are as follows:1. Meta-active learning for autonomously synthesizing an acquisition function to efficiently infer altered or unknown system dynamics and optimize system parameters.2. Probabilistically-safe control combined with an active-learning framework through the integration of our deep learning-based acquisition function and integer linear programming.3. State-of-the art results for safe, active learning.We achieve a 46% increase in information gain in a high-dimensional environment of controlling a damaged aircraft, and we achieve a 58% increase in information gain in our deep brain stimulation against our baselines."}
{"paper_id": 148, "abstract": "In the ever-evolving realm of deep learning, a common practice is to wield a singular architecture across all input instances\u2014a method that, while straightforward, often falters in the face of diverse data landscapes. A fixed design can struggle to capture the rich tapestry of variations inherent in complex datasets. To bolster model capacity, many have turned to larger convolutional kernels or deeper network structures, yet these strategies often come at the steep price of increased computational demands.  Enter the Dynamic Graph Network, or DG-Net\u2014a novel approach that reimagines the very fabric of neural connectivity. Rather than relying on a one-size-fits-all architecture, DG-Net learns instance-aware connections, crafting unique forward paths tailored to each input. Picture a complete directed acyclic graph, where each node embodies a convolutional block and the edges delineate potential pathways. Through a learnable module we call the \"router,\" we generate edge weights, selectively activating those connections that surpass a specified threshold. This dynamic adjustment invigorates the neural network's structure, allowing it to adaptively aggregate features at each node, thereby enhancing its representational prowess.  To streamline training, we encapsulate the network's connectivity for each sample within an adjacency matrix. This matrix not only evolves during the forward pass to optimize feature aggregation but is also cached for gradient computations during backpropagation. We put our DG-Net to the test against a variety of static architectures, including MobileNetV2, ResNet, ResNeXt, and RegNet. Our extensive experiments on ImageNet classification and COCO object detection reveal the remarkable effectiveness and generalization capabilities of our approach, paving the way for a future where neural networks can truly adapt to the complexities of the data they encounter.", "introduction": "Deep neural networks have driven a shift from feature engineering to feature learning.The great progress largely comes from well-designed networks with increasing capacity of models (He et al., 2016a;Xie et al., 2017;Huang et al., 2017;Tan & Le, 2019).To achieve the superior performance, a useful practice is to add more layers (Szegedy et al., 2015) or expand the size of existing convolutions (kernel width, number of channels) (Huang et al., 2019;Tan & Le, 2019;Mahajan et al., 2018).Meantime, the computational cost significantly increases, hindering the deployment of these models in realistic scenarios.Instead of adding much more computational burden, we prefer adding sampledependent modules to networks, increasing the model capacity by accommodating the data variance.Several existing work attempt to augment the sample-dependent modules into network.For example, Squeeze-and-Excitation network (SENet) (Hu et al., 2018) learns to scale the activations in the channel dimension conditionally on the input.Conditionally Parameterized Convolution (CondConv) (Yang et al., 2019) uses over-parameterization weights and generates individual convolutional kernels for each sample.GaterNet (Chen et al., 2018) adopts a gate network to extract features and generate sparse binary masks for selecting filters in the backbone network based upon inputs.All these methods focus on the adjustment of the micro structure of neural networks, using a data-dependent module to influence the feature representation at the same level.Recall the deep neural network to mammalian brain mechanism in biology (Rauschecker, 1984), the neurons are linked by synapses and responsible for sensing different information, the synapses are activated to varying degrees when the neurons perceive external information.Such a phenomenon inspires us to design a data-dependent network structure so that different samples will activate different network paths.In this paper, we learn to optimize the connectivity of neural networks based upon inputs.Instead of using stacked-style or hand-designed manners, we allow more flexible selection for forwarding paths.Specifically, we reformulate the network into a directed acyclic graph, where nodes represent the convolution block while edges indicate connections.Different from randomly wired neural networks (Xie et al., 2019) that generate random graphs as connectivity using predefined generators, we rewire the graph as a complete graph so that all nodes establish connections with each other.Such a setting allows more possible connections and makes the task of finding the most suitable connectivity for each sample equivalent to finding the optimal sub-graph in the complete graph.In the graph, each node aggregates features from the preceding nodes, performs feature transformation (e.g.convolution, normalization, and non-linear operations), and distributes the transformed features to the succeeding nodes.The output of the last node in the topological order is employed as the representation through the graph.To adjust the contribution of different nodes to the feature representation, we further assign weights to the edges in the graph.The weights are generated dynamically for each input via an extra module (denoted as router) along with each node.During the inference, only crucial connections are maintained, which creates different paths for different instances.As the connectivity for each sample is generated through non-linear functions determined by routers, our method can enable the networks to have more representation power than the static network.We call our method as the Dynamic Graph Network (DG-Net).It doesn't increase the depth or width of the network, while only introduces an extra negligible cost to compute the edge weights and aggregate the features.To facilitate the training, we represent the network connection of each sample as a adjacent matrix and design a buffer mechanism to cache the matrices of a sample batch during training.With the buffer mechanism, we can conveniently aggregate the feature maps in the forward pass and compute the gradient in the backward pass by looking up the adjacent matrices.The main contributions of our work are as follows:\u2022 We first introduce the dynamic connectivity based upon inputs to exploit the model capacity of neural networks.Without bells and whistles, simply replacing static connectivity with dynamic one in many networks achieves solid improvement with only a slight increase of (\u223c 1%) parameters and (\u223c 2%) computational cost (see table 1).\u2022 DG-Net is easy and memory-conserving to train.The parameters of networks and routers can be optimized in a differentiable manner.We also design a buffer mechanism to conveniently access the network connectivity, in order to aggregate the feature maps in the forward pass and compute the gradient in the backward pass.\u2022 We show that DG-Net not only improves the performance for human-designed networks (e.g.MobielNetV2, ResNet, ResNeXt) but also boosts the performance for automatically searched architectures (e.g.RegNet).It demonstrates good generalization ability on ImageNet classification (see table 1) and COCO object detection (see table 2) tasks."}
{"paper_id": 149, "abstract": "In the realm of machine learning, where deep neural networks often tread perilously close to overfitting, the art of regularization emerges as a steadfast ally. Traditional techniques, however, have largely confined themselves to the i.i.d. assumption, drawing wisdom solely from the present sample while neglecting the intricate tapestry woven by the relationships among neighboring data points. In this exploration, we unveil a groundbreaking regularization method known as Patch-level Neighborhood Interpolation, or **Pani** for short.   Imagine a network that not only acknowledges the current input but also embraces the collective essence of its surroundings. Our approach constructs patch-level graphs across various layers of the network, enabling a rich, non-local representation through the linear interpolation of neighboring patch features. This innovative strategy serves as a robust and versatile regularization tool.  But we do not stop there. We have tailored our Pani framework to enhance two widely embraced regularization methods: Virtual Adversarial Training (VAT) and MixUp, along with its variants. The first, **Pani VAT**, introduces a novel paradigm for achieving non-local adversarial smoothness by leveraging patch-level interpolated perturbations. Meanwhile, **Pani MixUp** extends the original MixUp concept into a new dimension, yielding significant performance enhancements.  To substantiate our claims, we conducted a series of rigorous experiments, demonstrating the efficacy of our Patch-level Neighborhood Interpolation approach in both supervised and semi-supervised environments. In a world where data is both a treasure and a challenge, Pani stands poised to redefine the boundaries of regularization, weaving a stronger fabric of learning from the intricate connections that bind our samples together.", "introduction": "In the statistical learning theory, regularization techniques are typically leveraged to achieve the trade-off between empirical error minimization and the control of model complexity (Vapnik & Chervonenkis, 2015).In contrast to the classical convex empirical risk minimization where regularization can rule out trivial solutions, regularization plays a rather different role in deep learning due to its highly non-convex optimization nature (Zhang et al., 2016).Among all the explicit and implicit regularization, regularization with stochastic transformation, perturbations and randomness, such as adversarial training (Goodfellow et al., 2014), dropout and MixUp (Zhang et al., 2017), play a key role in the deep learning models due to their superiority in the performance (Berthelot et al., 2019b;Zhang et al., 2017;Miyato et al., 2018;Berthelot et al., 2019a).In this section, we firstly review two kinds of effective and prestigious regularization branches for deep neural networks, which can elegantly generalize from supervised learning to semi-supervised setting.Adversarial Training (Goodfellow et al., 2014;Madry et al., 2017) can provide an additional regularization beyond that provided by other generic regularization strategies, such as dropout, pretraining and model averaging.However, recent works (Zhang et al., 2019;Tsipras et al., 2018) demonstrated that this kind of training method holds a trade-off between the robustness and accuracy, limiting the efficacy of the adversarial regularization.Besides, Virtual Adversarial Training (VAT) (Miyato et al., 2018) can be regarded as a natural extension of adversarial training to semi-supervised setting through adversarially smoothing the posterior output distribution with the leverage of unlabeled data.This strategy has achieved great success in image classification (Miyato et al., 2018), text classification (Miyato et al., 2016) and node classification (Sun et al., 2019).Tangent-Normal Adversarial Regularization (TNAR) (Yu et al., 2019) extended VAT by taking the data manifold into consideration and applied VAT along the tangent space and the orthogonal normal space of the data manifold, outperforming previous semi-supervised approaches.MixUp (Zhang et al., 2017) augmented the training data by incorporating the prior knowledge that linear interpolation of input vectors should lead to linear interpolation of the associated targets, accomplishing consistent improvement of generalization on image, speech and tabular data.Mix-Match (Berthelot et al., 2019b) extended MixUp to semi-supervised tasks by guessing low-entropy labels for data-augmented unlabeled examples and mixing labeled and unlabeled data using MixUp.In contrast with VAT, MixMatch (Berthelot et al., 2019b) utilizes one specific form of consistency regularization, i.e., using the standard data augmentation for images, such as random horizontal flips, rather than computing adversarial perturbations to smooth the posterior distribution of the classifier.Nevertheless, the vast majority of regularization methods, including the aforementioned approaches, assume that the training samples are drawn independently and identically from an unknown data generating distribution.For instance, Support Vector Machine (SVM), Back-Propagation (BP) for Neural Networks, and many other common algorithms implicitly make this assumption as part of their derivation.However, this i.i.d.assumption is commonly violated in realistic scenarios where batches or sub-groups of training samples are likely to have internal correlations.In particular, Dundar et al. (2007) demonstrated that accounting for the correlations in real-world training data leads to statistically significant improvements in accuracy.Similarly, Peer-Regularized Networks (Peer-Net) (Svoboda et al., 2018) applied graph convolutions (Velickovic et al., 2017;Kipf & Welling, 2016) to harness information of peer samples, and verified its effectiveness on defending adversarial attacks.Motivated by these facts, we aim to design a general regularization strategy that can fully utilize the internal relationship between samples by explicitly constructing a graph within a minibatch in order to consistently improve the generalization of deep neural networks in both semi-and supervised settings.In this paper, we propose the Patch-level Neighborhood Interpolation (Pani) for deep neural networks, serving as a simple yet effective non-local regularization.We firstly construct a patch-level graph in each mini-batch during the stochastic gradient decent training process.Then we apply linear interpolation on the neighboring patch features and the resulting non-local representation additionally captures the relationship of neighboring patch features in different layers, serving as a general and effective regularization.Furthermore, to prove the generality and superiority of our Pani method, we explicitly customize our approach into two kinds of popular and general regularization strategies, i.e., Virtual Adversarial Regularization and MixUp, resulting in Pani VAT and Pani MixUp.For the Pani VAT, we reformulate the construction of adversarial perturbations, transforming from solely depending on the current sample to the linear interpolation of neighboring patch features.This non-local adversarial perturbations can leverage the information of neighboring correlation from all samples within a batch, providing more informative adversarial smoothness in semisupervised setting.Besides, in the Pani MixUp, we extend MixUp and its variant MixMatch from image to patch level by mixing fine-grained patch features and corresponding supervised signals.Finally, we conduct extensive experiments to demonstrate that both of the two derived regularization strategies can outperform other state-of-the-art approaches in both supervised and semi-supervised tasks.More importantly, these successful cases verify the generality and superiority of our Patchlevel Neighborhood Interpolation method.Our contributions can be summarized as follow:\u2022 We propose a general interpolation strategy either in input or feature space, i.e., Patch-level Neighborhood Interpolation, helping to improve the generalization of deep neural networks on both semi-and supervised scenarios.This strategy can serve as an effective graph-based representation method and has much potential to be leveraged in a wider range of tasks.\u2022 Based on our method, the customized approaches Pani VAT and Pani MixUP as well as Pani MixMatch can boost the generalization performance significantly, and thus provide a guidance to the deployment of our Pani strategy into more regularization methods."}
{"paper_id": 150, "abstract": "In the realm of relation extraction, the art of distant supervision has emerged as a powerful tool, enabling the creation of expansive training datasets by harmonizing knowledge bases with the vast ocean of unstructured text. Yet, a common assumption prevails among scholars: that this unstructured text is readily centralized. The reality, however, is far more complex. Text is often scattered across myriad platforms, hindered by the constraints of privacy and access. Thus, the challenge arises: how can we harness distant supervision within the framework of federated learning, a paradigm that liberates model training from the shackles of direct raw text access?  In this paper, we embark on a journey to explore this very challenge. We recognize that in federated settings, the specter of label noise looms larger than ever, as sentences linked to the same entity pair become dispersed across diverse platforms. To combat this, we introduce a novel federated denoising framework, meticulously designed to mitigate the chaos of label noise. At the heart of our approach lies a multiple instance learning-based denoising method, adept at sifting through the noise to identify reliable sentences through the power of cross-platform collaboration.  Our findings, drawn from rigorous experiments on the New York Times dataset and the miRNA gene regulation relation dataset, illuminate the efficacy of our proposed method. Join us as we unveil the potential of federated learning in the realm of distant supervision, paving the way for a new era of robust relation extraction.", "introduction": "Relation extraction (RE) aims to mine factual knowledge from free text by labeling relations between entity mentions, which is a crucial step in knowledge base (KB) construction.For example, given a sentence \"[Steve Jobs] e1 and Wozniak co-founded [Apple] e2 in 1967\", a relation extractor should identify that \"Steve Jobs\" and \"Apple\" are in a \"Founder\" relationship.Most existing supervised RE systems, such as Zeng et al. (2014); Zhang & Wang (2015); Wang et al. (2016); Zhou et al. (2016), rely on a large-scale manually annotated training dataset, which is extremely expensive and cannot cover all walks of life.To ease the reliance on annotated data, Mintz et al. (2009) proposed distant supervision to automatically generate training data by heuristically aligning a KB with unstructured text.The key assumption of distant supervision is that if two entities have a relation in the KB, then all sentences that mention these two entities will express this relation.Since then, there has been a rich literature devoted to this topic, such as Riedel et al. (2010); Hoffmann et al. (2011); Zeng et al. (2015); Lin et al. (2016); Ye & Ling (2019); Yuan et al. (2019).Though the progress is exciting, distant supervision approaches have so far been limited to the centralized learning paradigm, which assumes that a great deal of text is easily accessible.However, in practice, text may be distributed on different platforms and be massively convoluted with sensitive personal information, especially in the healthcare and financial fields (Yang et al., 2019;Zerka et al., 2020;Chamikara et al., 2020).Due to privacy restrictions, it is almost impossible or cost-prohibitive to centralize text from multiple platforms.Recently, federated learning (McMahan et al., 2016) provides a compelling solution for learning a model from decentralized and privacy-sensitive data.The main idea behind federated learning is that each platform trains a local model based on its own local data and a master server coordinates massive platforms to collaboratively train a global model by aggregating these local model updates.Unfortunately, directly applying federated learning to the decentralized distantly supervised data fails, because conventional federated learning requires the local data to come with labels without noise (Tuor et al., 2020), however, in distant supervision, automatic labeling inevitably accompanies with label noise (Riedel et al., 2010;Hoffmann et al., 2011;Zeng et al., 2015;Lin et al., 2016), which means not all sentences that mention an entity pair can represent the relation between them.Training on such noisy data will substantially hinder the performance of the RE model.S 2 : Steve Jobs resigned as chief executive from Apple in 2011.Platform 1Platform 2Founder Figure 1: An example of the sentences that contain the same entity pair distributed on two platforms.The triple (Steve Jobs, Founder, Apple) is a fact in the KB Moreover, even involving previous denoising methods, such as Zeng et al. (2015); Lin et al. (2016); Ye & Ling (2019), cannot handle label noise well in federated settings.This point can be illustrated by the example in Figure 1. S 1 and S 2 contain the same entity pair (\"Steve Jobs\", \"Apple\") but are distributed on two platforms.S 1 is true positive while S 2 is a false positive instance, which does not express the \"founder\" relation.In centralized training, there is no barrier between Platform 1 and Platform 2; therefore, simultaneously considering S 1 and S 2 can easily filter out noise via only selecting S 1 (Zeng et al., 2015) or placing a small weight on S 2 (Lin et al., 2016;Ye & Ling, 2019).However, raw data exchange between platforms is prohibited in federated settings.Due to the lack of comparison with S 1 , previous denoising methods would mistakenly regard S 2 as a true positive instance.As a result, S 2 is retained and then poisons the local model in platform 2, which would affect the global model in turn.To suppress label noise in federated settings, we propose a federated denoising framework in this paper.The core of this framework is a multiple instance learning (MIL) (Dietterich et al., 1997;Maron & Lozano-P\u00e9rez, 1998) based denoising algorithm, called Lazy MIL, which is only executed at the beginning of each communication round and then would rest until the next round.Since the sentences containing the same entity pair scatter around different platforms, Lazy MIL algorithm coordinates multiple platforms to jointly select reliable sentences.Once sentences have been selected, they would be used repeatedly to train local models until the end of this round.In summary, the contributions of this paper are:\u2022 Considering data decentralization and privacy protection, we investigate distant supervision under the federated learning paradigm, which decouples the model training from the need for direct access to the raw data.To our best knowledge, combining federated learning with distant supervision is still an unexplored territory, which is the main focus of this paper.\u2022 Since the automatic labeling in distant supervision inevitably accompanies with label noise, we present a multiple instance learning based denoising method, which can select reliable instances via cross-platform collaboration.\u2022 The proposed method yields promising results on two benchmarks datasets, and we perform various experiments to verify the effectiveness of the proposed method.The code will be released at http://anonymized."}
{"paper_id": 151, "abstract": "In this paper, we embark on a journey into the realm of grounded agent instruction learning, where we unveil a novel approach to compositional generalization\u2014a trait that lies at the very heart of human intelligence. Despite the remarkable capabilities of current neural network models, they often stumble when faced with the complexity of compositional tasks, especially in the multi-modal landscape where grounding is essential.   Our approach is built upon two foundational pillars. First, we harness the dynamic interplay between the agent and its environment, enabling us to dissect the output into its fundamental components. This interaction acts as a forge, crafting clarity from chaos. Second, we introduce entropy regularization, a guiding force that helps us identify and learn the corresponding input components for each output element.   The results of our experiments are nothing short of compelling. Our method not only eclipses existing baselines in a majority of tasks but does so with a remarkable leap\u2014an absolute average accuracy increase of over 25%. To further refine our understanding, we delve into the nuances of entropy regularization and other modifications through a rigorous ablation study.   We envision this work as a pivotal first step toward unraveling the complexities of grounded compositional generalization, a quest that promises to propel the field of artificial intelligence into new realms of understanding and capability.", "introduction": "Compositional generalization is a key skill for flexible and efficient learning.Humans leverage compositionality to create and recognize new combinations of familiar concepts (Chomsky, 1957;Minsky, 1986).Though there are many progresses for machine learning and deep learning in various areas recently (LeCun et al., 2015), current main learning algorithms are not able to perform compositional generalization, and require many samples to train models.Such efficient learning is even more important when machines interact with the environment for grounding, because interactions are usually slow.Machine learning has been mostly developed with an assumption that training and test distributions are identical.Compositional generalization, however, is a kind of out-of-distribution generalization (Bengio, 2017), where training and test distributions are different.During training, dataset does not contain the information of the difference, so it can only be given as prior knowledge.In compositional generalization, a sample is a combination of several components.Test distribution changes as test samples are new combinations of seen components in training.For example, if we can find \"large apple\" and \"small orange\" in some environments, then we can also find \"large orange\" among multiple objects in a new environment.The recombination is enabled when an output component depends only on the corresponding input components, and invariant of other components (please see Section 4.1 for more details).So there are two aspects to consider.What are the components in output, and how to find the corresponding input signals.We propose to use interactions between agent and the environment to define output components.This is analogues to model-free reinforcement learning (Sutton & Barto, 2018), where an agent does not have an environment model, but leans to act at each step during the interactions with the environment.Then we use entropy regularization (Li et al., 2019;Li & Eisner, 2019) to learn the minimal input components for outputs.We evaluate the approach with gSCAN dataset (Ruis et al., 2020), which is designed to study compositional generalization in grounded agent instruction learning.Please see Figure 1 for examples.The results show the proposed approach significantly outperforms baselines in most tasks, with more than 25% absolute average accuracy increase, and the high accuracy indicates that the proposed approach addresses the designed grounded compositional generalization problems in these tasks.We also look into the impact of entropy regularization and other changes with ablation study.We hope this work will be helpful in advancing grounded compositional generalization and artificial intelligence research.The contributions of this paper can be summarized as follows.\u2022 This is the first work to enable accurate compositional generalization in grounded instruction learning problem, serving for analyzing and understanding the mechanism.\u2022 The novelty of this paper is to find that the combination of environment interaction and entropy regularization helps the generalization."}
{"paper_id": 152, "abstract": "In the ever-evolving realm of classification, where the specter of noisy labels looms large, we unveil a groundbreaking approach: the Wasserstein Distributional Normalization (WDN) algorithm. Imagine, if you will, a world where data is divided into two distinct factions\u2014those samples cloaked in certainty and those shrouded in uncertainty, discerned through the lens of minimal loss. It is within this dichotomy that we delve into the intricate geometric interplay between these two realms, seeking to unearth hidden insights even from the uncertain.   To harness this potential, we weave a tapestry of geometric constraints, enveloping the uncertain samples within a Wasserstein ball, anchored by the steadfast certain samples. Our experiments reveal a compelling truth: the WDN algorithm not only surpasses the prowess of contemporary methods on challenging datasets like Clothing1M and CIFAR-10/100, but it also exhibits a remarkable adaptability, seamlessly integrating with existing classification techniques. This flexibility empowers practitioners to significantly elevate their accuracy, transforming the landscape of classification amidst the chaos of noisy labels.", "introduction": "The successful results of deep neural networks (DNNs) on supervised classification tasks heavily rely on accurate and high-quality label information.However, annotating large-scale datasets is extremely expensive and a time-consuming task.Because obtaining high-quality datasets is very difficult, in most conventional works, training data have been obtained alternatively using crowd-sourcing platforms Yu et al. (2018) to obtain large-scaled datasets, which leads inevitable noisy labels in the annotated samples.While there are numerous methods that can deal with noisy labeled data, recent methods actively adopt the small loss criterion, which enables to construct classification models that are not susceptible to noise corruption.In this learning scheme, a neural network is trained using easy samples first in the early stages of training.Harder samples are then gradually selected to train mature models as training proceeds.Jiang et al. (2018) suggested collaborative learning models, in which a mentor network delivers the data-driven curriculum loss to a student network.Han et al. (2018); Yu et al. (2019) proposed dual networks to generate gradient information jointly using easy samples and employed this information to allow the networks to teach each other.Wei et al. (2020) adopted a disagreement strategy, which determines the gradient information to update based on disagreement values between dual networks.Han et al. (2020) implemented accumulated gradients to escape optimization processes from over-parameterization and to obtain more generalized results.In this paper, we tackle to solve major issues raised from the aforementioned methods based on the small-loss criterion, as follows.In comprehensive experiments, the aforementioned methods gain empirical insight regarding network behavior under noisy labels.However, theoretical and quantitative explanation have not been closely investigated.In contrast, we give strong theoretical/empirical explanations to understand the network under noisy labels.In particular, we present an in-depth analysis of small loss criteria in a probabilistic sense.We exploit the stochastic properties of noisy labeled data and develop probabilistic descriptions of data under the small loss criteria, as follows.Let P be a probability measure for the pre-softmax logits of the training samples, l be an objective function for classification, and 1 {\u2022} be an indicator function.Then, our central object to deal with is a truncated measure defined aswhere X and Y , which are sampled from \u00b5|\u03b6 and \u03be|\u03b6, denote uncertain and certain samples defined in the pre-softmax feature spacefoot_0 (i.e., R d ), respectively.In equation 1, \u00b5 and \u03be denote the probability measures of uncertain and certain samples, respectively, and \u03b6 is a constant.Most previous works have focused on the usage of Y and the sampling strategy of \u03b6, but poor generalization capabilities based on the abundance of uncertain samples X has not been thoroughly investigated, even though these samples potentially contain important information.To understand the effect of noisy labels on the generalized bounds, we provide the concentration inequality of uncertain measure \u00b5, which renders the probabilistic relation between \u00b5 and \u03be and learnability of the network under noisy labels.While most conventional methods Han et al. (2018); Wei et al. (2020); Li et al. (2019a); Yu et al. (2019) require additional dual networks to guide misinformed noisy samples, the scalability is not guaranteed due to the existence of dual architectures, which have the same number of parameters as the base network.To alleviate this problem, we build a statistical machinery, which should be fully non-parametric, simple to implement, and computationally efficient to reduce the computational complexity of conventional approaches, while maintaining the concept of small-loss criterion.Based on the empirical observation of ill-behaved certain/uncertain samples, we propose the gradient flow in the Wasserstein space, which can be induced by simulating non-parametric stochastic differential equation (SDE) with respect to the Ornstein-Ulenbeck type to control the ill-behaved dynamics.The reason for selecting these dynamics will be thoroughly discussed in the following sections.Thus, key contributions of our work are as follows.\u2022 We theoretically verified that there exists a strong correlation between model confidence and statistical distance between X and Y .We empirically investigate that the classification accuracy worsens when the upper-bound of 2-Wasserstein distance W 2 (\u00b5, \u03be) \u2264 \u03b5 (i.e., distributional distance between certain and uncertain samples) drastically increase.Due to the empirical nature of upper-bound \u03b5, it can be used as an estimator to determine if a network suffers from over-parameterization.\u2022 Based on empirical observations, we develop a simple, non-parametric, and computationally efficient stochastic model to control the observed ill-behaved sample dynamics.As a primal object, we propose the stochastic dynamics of gradient flow (i.e.,, Ornstein-Ulenbeck process) to simulate simple/non-parametric stochastic differential equation.Thus, our method do not require any additional learning parameters.\u2022 We provide important theoretical results.First, the controllable upper-bound \u03b5 with the inverse exponential ratio is induced, which indicates that our method can efficiently control the diverging effect of Wasserstein distance.Second, the concentration inequality of transported uncertain measure is presented, which clearly renders the probabilistic relation between \u00b5 and \u03be."}
{"paper_id": 153, "abstract": "In the realm of reinforcement learning, the quest to accelerate the mastery of intricate tasks by drawing upon the wisdom of previously conquered challenges remains a formidable endeavor\u2014particularly when the threads of similarity between the source and target tasks are tenuous or shrouded in uncertainty. In this study, we unveil a novel approach we call the REPresentation-And-INstance Transfer algorithm (REPAINT), crafted specifically for the deep actor-critic framework of reinforcement learning.  At the heart of representation transfer lies a method akin to igniting a spark in a darkened room: we employ a kickstarted training technique that harnesses the power of a pre-trained teacher policy, enhanced by the introduction of an auxiliary cross-entropy loss to guide the way. In the realm of instance transfer, we forge a new path with our advantage-based experience replay\u2014a sampling strategy that meticulously curates transitions gathered under the tutelage of the teacher policy, retaining only those samples that boast high advantage estimates for the purpose of refining our policy.  Our exploration encompasses not only the challenge of tackling an unseen target task through the transfer of knowledge from previously mastered teacher tasks but also the intricate dance of learning a partially familiar task, composed of multiple sub-tasks, by leveraging the insights gleaned from a pre-learned teacher sub-task. Through rigorous benchmark experiments, REPAINT emerges as a beacon of efficiency, significantly slashing total training time while elevating asymptotic performance beyond that of training without prior knowledge or competing baselines. In this unfolding narrative of learning and adaptation, REPAINT stands as a testament to the power of strategic knowledge transfer in the ever-evolving landscape of reinforcement learning.", "introduction": "Most reinforcement learning methods train an agent from scratch, typically requiring a huge amount of time and computing resources.Accelerating the learning processes for complex tasks has been one of the most challenging problems in reinforcement learning (Kaelbling et al., 1996;Sutton & Barto, 2018).In the past few years, deep reinforcement learning has become more ubiquitous to solve sequential decision-making problems in many real-world applications, such as game playing (OpenAI et al., 2019;Silver et al., 2016), robotics (Kober et al., 2013;OpenAI et al., 2018), and autonomous driving (Sallab et al., 2017).The computational cost of learning grows as the task complexity increases in the real-world applications.Therefore, it is desirable for a learning algorithm to leverage knowledge acquired in one task to improve performance on other tasks.Transfer learning has achieved significant success in computer vision, natural language processing, and other knowledge engineering areas (Pan & Yang, 2009).In transfer learning, the teacher (source) and student (target) tasks are not necessarily drawn from the same distribution (Taylor et al., 2008a).The unseen student task may be a simple task which is similar to the previously trained tasks, or a complex task with traits borrowed from significantly different teacher tasks.Despite the prevalence of direct weight transfer, knowledge transfer from previously trained agents for reinforcement learning tasks has not been gaining much attention until recently (Barreto et al., 2019;Ma et al., 2018;Schmitt et al., 2018;Lazaric, 2012;Taylor & Stone, 2009).In this work, we propose a knowledge transfer algorithm for deep actor-critic reinforcement learning, i.e., REPresentation And INstance Transfer (REPAINT).The algorithm can be categorized as a representation-instance transfer approach.Specifically, in representation transfer, we adopt a kickstarted training method (Schmitt et al., 2018) using a previously trained teacher policy, where the teacher policy is used for computing the auxiliary loss during training.In instance transfer, we develop a new sampling algorithm for the replay buffer collected from the teacher policy, where we only keep the transitions that have advantage estimates greater than a threshold.The experimental results across several transfer learning tasks show that, regardless of the similarity between source and target tasks, by introducing knowledge transfer with REPAINT, the number of training iterations needed by the agent to achieve some reward target can be significantly reduced when compared to training from scratch and training with only representation transfer or instance transfer.Additionally, the agent's asymptotic performance is also improved by REPAINT in comparison with the baselines."}
{"paper_id": 154, "abstract": "In this paper, we unveil a groundbreaking approach to channel pruning, designed to tackle the dual challenges of compressing and accelerating Convolutional Neural Networks (CNNs). Traditional methods in channel pruning often overlook the intricate relationships that exist between channels and layers, opting instead to treat each channel as an isolated entity through gates or similar mechanisms. To bridge this critical gap, we introduce a hyper-structure network that meticulously crafts the architecture of the primary network. Much like existing hypernets, our hyper-structure network can be refined through standard backpropagation, allowing for a seamless integration into the training process.  Furthermore, we incorporate a regularization term that delineates the computational resources allocated to the compact network. While FLOPs (Floating Point Operations) are typically employed as a benchmark for computational resources, relying solely on FLOPs can inadvertently impose excessive penalties on the earlier layers of the network. To rectify this imbalance, we propose the introduction of learnable layer-wise scaling factors. These factors serve to harmonize the gradients from various components of our model and can be optimized through hyper-gradient descent.  Our extensive experimental evaluations on CIFAR-10 and ImageNet demonstrate that this innovative method stands shoulder to shoulder with the leading techniques in the field, offering a compelling alternative that enhances both performance and efficiency.", "introduction": "Convolutional Neural Networks (CNNs) have accomplished great success in many machine learning and computer vision tasks (Krizhevsky et al., 2012;Redmon et al., 2016;Ren et al., 2015;Simonyan & Zisserman, 2014a;Bojarski et al., 2016).To deal with real world applications, recently, the design of CNNs becomes more and more complicated in terms of width, depth, etc. (Krizhevsky et al., 2012;Simonyan & Zisserman, 2014b;He et al., 2016;Huang et al., 2017).Although these complex CNNs can attain better performance on benchmark tasks, their computational and storage costs increase dramatically.As a result, a typical application based on CNNs can easily exhaust an embedded or mobile device due to its enormous costs.Given such costs, the application can hardly be deployed on resource-limited platforms.To tackle these problems, many methods (Han et al., 2015b;a) have been devoted to compressing the original large CNNs into compact models.Among these methods, weight pruning and structural pruning are two popular directions.Unlike weight pruning or sparsification, structural pruning, especially channel pruning, is an effective way to truncate the computational cost of a model because it does not require any post-processing steps to achieve actual acceleration and compression.Many existing works (Liu et al., 2017;Ye et al., 2018;Huang & Wang, 2018;Kim et al., 2020;You et al., 2019) try to solve the problem of structure pruning by applying gates or similar concepts on channels of a layer.Although these ideas have achieved many successes in channel pruning, there are some potential problems.Usually, each gate has its own parameter, but parameters from different gates do not have dependence.As a result, they can hardly learn inter-channel or inter-layer relationships.Due to the same reason, the slimmed models from these methods could overlook the information between different channels and layers, potentially bringing sub-optimal model compression results.To address these challenges, we propose a novel channel pruning method inspired by hypernet (Ha et al., 2016).In hypernet, they propose to use a hyper network to generate the weights for another network, while the hypernet can be optimized through backpropagation.We extend a hypernet to a hyper-structure network to generate an architecture vector for a CNN instead of weights.Each architecture vector corresponds to a sub-network from the main (original) network.By doing so, the inter-channel and inter-layer relationships can be captured by our hyper-structure network.Besides the hyper-structure network, we also introduce a regularization term to control the computational budget of a sub-network.Recent model compression methods focus on pruning computational FLOPs instead of parameters.The problem of applying FLOPs regularization is that the gradients of the regularization will heavily penalize early layers which can be regarded as a bias towards latter layers.Such a bias will restrict the potential search space of sub-networks.To make our hyper-structure network explore more possible structures, we further introduce layer-wise scaling factors to balance the gradients from different losses for each layer.These factors can be optimized by hyper-gradient descent.Our contributions are summarized as follows:1) Inspired by hypernet, we propose to use a hyper-structure network for model compression to capture inter-channel and inter-layer relationships.Similar to hypernet, the proposed hyper-structure network can be optimized by regular backpropagation.2) Gradients from FLOPs regularization are biased toward latter layers, which truncate the potential search space of a sub-network.To balance the gradients from different terms, layerwise scaling factors are introduced for each layer.These scaling factors can be optimized through hyper-gradient descent with trivial additional costs.3) Extensive experiments on CIFAR-10 and ImageNet show that our method can outperform both conventional channel pruning methods and AutoML based pruning methods on ResNet and MobileNetV2."}
{"paper_id": 155, "abstract": "In the realm of data mining, where the intricate dance of relationships among individuals unfolds, networks\u2014those complex webs of connections\u2014serve as our guiding stars. Yet, as we delve into the depths of relational data, we encounter a formidable challenge: safeguarding the sensitive nature of this information while still reaping the benefits of its insights. In this paper, we embark on a quest to unveil innovative methods that strike a delicate balance between utility and privacy in structured data release.  Harnessing the formidable power of the differential privacy (DP) framework, we craft a robust approach that imposes stringent privacy constraints on deep graph generation models. Our focus sharpens on edge-DP, a guardian of individual link privacy, ensuring that no single thread of connection is left exposed. To achieve this, we introduce a strategic infusion of Gaussian noise into the gradients of our link reconstruction-based graph generation model, a calculated maneuver that fortifies privacy while enhancing the model\u2019s ability to learn structural intricacies through targeted graph comparisons.  Through rigorous experimentation on two real-world network datasets, we unveil our creation: the DPGGAN model. This innovative framework not only generates networks that maintain the integrity of their global structure but also stands as a bulwark against the breach of individual link privacy. Join us as we navigate this intricate landscape, where the convergence of privacy and utility leads to new horizons in data analysis.", "introduction": "Nowadays, open data of networks play a pivotal role in data mining and data analytics (Tang et al., 2008;Sen et al., 2008;Blum et al., 2013;Leskovec & Krevl, 2014).By releasing and sharing structured relational data with research facilities and enterprise partners, data companies harvest the enormous potential value from their data, which benefits decision-making on various aspects, including social, financial, environmental, through collectively improved ads, recommendation, retention, and so on (Yang et al., 2017;2018;Sigurbj\u00f6rnsson & Van Zwol, 2008;Kuhn, 2009).However, network data usually encode sensitive information not only about individuals but also their interactions, which makes direct release and exploitation rather unsafe.More importantly, even with careful anonymization, individual privacy is still at stake under collective attack models facilitated by the underlying network structure (Zhang et al., 2019;Cai et al., 2018).Can we find a way to securely release network data without drastic sanitization that essentially renders the released data useless?In dealing with such tension between the need to release utilizable data and the concern of data owners' privacy, quite a few models have been proposed recently, focusing on grid-based data like images, texts and gene sequences (Frigerio et al., 2019;Papernot et al., 2018;Triastcyn & Faltings, 2018;Narayanan & Shmatikov, 2008;Xie et al., 2018;Chen et al., 2018;Boob et al., 2018;Dy & Krause, 2018;Lecuyer et al., 2018;Zhang et al., 2018).However, none of the existing models can be directly applied to the network (graph) setting.While a secure generative model on grid-based data apparently aims to preserve high-level semantics (e.g., class distributions) and protect detailed training data (e.g., exact images or sentences), it remains obtuse what to be preserved and what to be protected for network data, due to its modeling of complex interactive objects.Motivating scenario.In Figure 1, a bank aims to encourage public studies on its customers' community structures.It does so by firstly anonymizing all customers and then sharing the network (i.e., (a) in Figure 1) to the public.However, an attacker interested in knowing the financial interactions (e.g., money transfer) between particular customers in the bank may happen to have access to another network of a similar set of customers (e.g., a malicious employee of another financial company).The similarity of simple graph properties like node degree distribution and triangle count between the two networks can then be used to identify specific customers with high accuracy in the released network (e.g., customer A as the only node with degree 5 and within 1 triangle, and customer B as the only node with degree 2 and within 1 triangle).Thus, the attacker confidently knows the A and B's identities and the fact that they have financial interactions in the bank, which seriously harms customers' privacy and poses potential crises.As the first contribution in this work, we define and formulate secure network release goals as preserving global network structure while protecting individual link privacy.Continue with the toy example, the solution we propose is to train a graph neural network model on the original network and release the generated networks (e.g., (b) in Figure 1).Towards the utility of generated networks, we require them to be similar to the original networks from a global perspective, which can be measured by various graph global properties (e.g., network (b) has very similar degree distribution and the same triangle count as (a)).In this way, we expect many downstream data-mining and analytical tasks on them to produce similar results as on the original networks.As for privacy protection, we require that the information in the generated networks cannot confidently reveal the existence or absence of any individual links in the original networks (e.g., the attacker may still identify customers A and B in network (b), but their link structure has changed).Subsequently, there are two unique challenges in learning such structure-preserved and privacyprotected graph generation models, which have not been explored by existing literature so far.Challenge 1: Rigorous protection of individual link privacy.The rich relational structures in graph data often allow attackers to recover private information through various ways of collective inference (Zhang et al., 2014;Narayanan & Shmatikov, 2009;Backstrom et al., 2007).Moreover, graph structure can always be converted to numerical features such as spectral embedding, after which most attacks on grid-based data like model inversion (Fredrikson et al., 2015) and membership inference (Shokri et al., 2017) can be directly applied for link identification.How can we design an effective mechanism with rigorous privacy protection on links in networks against various attacks?Challenge 2: Effective preservation of global network structure.To capture the global network structure, the model has to constantly compare the structures of the input graphs and currently generated graphs during training.However, unlike images and other grid-based data, graphs have flexible structures, and thus they lack efficient universal representations (Dong et al., 2019).How can we allow a network generation model to effectively learn from the structural difference between two graphs, without conducting very time-costly operations like isomorphism tests all the time?Present work.In this work, for the first time, we draw attention to the secure release of network data with deep generative models.Technically, towards the aforementioned two challenges, we develop Differentially Private Graph Generative Nets (DPGGAN), which imposes DP training over a link reconstruction based network generation model for rigorous individual link privacy protection, and further ensures structure-oriented graph comparison for effective global network structure preservation.In particular, we first formulate and enforce edge-DP via Gaussian gradient distortion by injecting designed noise into the sensitive modules during model training.Then we leverage graph convolutional networks (Kipf & Welling, 2017) through a variational generative adversarial network architecture (Gu et al., 2019;Larsen et al., 2016) to enable structure-oriented network comparison.To evaluate the effectiveness of DPGGAN, we conduct extensive experiments on two real-world network datasets.On one hand, we evaluate the utility of generated networks by computing a suite of commonly concerned graph properties to compare the global structure of generated networks with the original ones.On the other hand, we validate the privacy of individual links by evaluating links predicted from the generated networks on the original networks.Consistent experimental results show that DPGGAN is able to effectively generate networks that are similar to the original ones regarding global network structure, while at the same time useless towards individual link prediction.Differential Privacy (DP).Differential privacy is a statistical approach in addressing the paradox of learning nothing about an individual while learning useful information about a population (Dwork et al., 2006).Recent advances in deep learning have led to the rapid development of DP-oriented learning schemes.Among them, the Gaussian Mechanism (Dwork et al., 2014), defined as follows, provides a neat and compatible framework for DP analysis over machine learning models.Definition 1 (Gaussian Mechanism (Dwork et al., 2014)).For a deterministic function f with its 2 -norm sensitivity aswhere N (0, \u2206 2 f 2 \u03c3 2 ) is a random variable obeying the Gaussian distribution with mean 0 and standard deviation \u2206 2 f \u03c3.Following this framework, (Abadi et al., 2016) proposes a general training strategy called DPSGD, which looses the condition on the overall privacy loss than that in Definition 1 by tracking detailed information of the SGD process to achieve an adaptive Gaussian Mechanism.DP learning has also been widely adapted to generative models (Frigerio et al., 2019;Papernot et al., 2018;Triastcyn & Faltings, 2018;Narayanan & Shmatikov, 2008;Mohammed et al., 2011;Xie et al., 2018;Chen et al., 2018;Boob et al., 2018;Dy & Krause, 2018;Lecuyer et al., 2018;Zhang et al., 2018).For example, (Frigerio et al., 2019;Chen et al., 2018;Boob et al., 2018;Zhang et al., 2018) share the same spirit by enforcing DP on the discriminators, and thus inductively on the generators, in a generative adversarial network (GAN) scheme.However, none of them can be directly applied to graph data due to the lack of consideration of structure generation.For graphs' structural data, two types of privacy constraints can be applied, i.e., node-DP (Kasiviswanathan et al., 2013) and edge-DP (Blocki et al., 2012), which define two neighboring graphs to differ by at most one node or edge.In this work, we aim at the secure release of network data, and particularly, we focus on edge privacy because it is essential for the protection of object interactions unique for network data compared with other types of data.Several existing works have studied the protection of edge-DP.For example, (Sala et al., 2011) generates graphs based on the statistical representations extracted from the original graphs blurred by designed noise, whereas (Wang & Wu, 2013) enforces the parameters of dK-graph models to be private.However, based on shallow graph generation models, they do not flexibly capture global network structure that can support various unknown downstream analytical tasks (Zhang et al., 2019;Wasserman & Zhou, 2010).Graph Generation (GGen).GGen has been studied for decades and is widely used to synthesize network data used to develop various collective analysis and mining models (Evans & Lambiotte, 2009;Hallac et al., 2017).Earlier works mainly use probabilistic models to generate graphs with certain properties (Erd\u0151s & R\u00e9nyi, 1960;Watts & Strogatz, 1998;Barab\u00e1si & Albert, 1999;Newman, 2001), which are manually designed based on sheer observations and prior assumptions.Thanks to the surge of deep learning, many advanced GGen models have been developed recently, which leverage different powerful neural networks in a learn-to-generate manner (Kipf & Welling, 2016;Bojchevski et al., 2018;You et al., 2018b;Simonovsky & Komodakis, 2018;Li et al., 2018;You et al., 2018a;Jin et al., 2018;Grover et al., 2017;De Cao & Kipf, 2018;Zou & Lerman, 2018;Ma et al., 2018).For example, NetGAN (Bojchevski et al., 2018) converts graphs into biased random walks, learns the generation of walks with GAN, and assembles the generated walks into graphs; GraphRNN (You et al., 2018b) regards the generation of graphs as node-and-edge addition sequences, and models it with a heuristic breadth-first-search scheme and hierarchical RNN.These neural network based models can often generate graphs with much richer properties, and flexible structures learned from real-world graphs.To the best of our knowledge, no existing work on deep GGen has looked into the potential privacy threats laid during the learning and releasing of the powerful models.Such concerns are rather urgent in the network setting, where sensitive information can often be more easily compromised in a collective manner (Dai et al., 2018;Backstrom et al., 2007;Zhang et al., 2014) and privacy leakage can easily further propagate (Narayanan & Shmatikov, 2009;Z\u00fcgner et al., 2018)."}
{"paper_id": 156, "abstract": "In the vast realm of data analysis, we unveil a groundbreaking stochastic algorithm\u2014dubbed RSG+\u2014crafted for the intricate task of canonical correlation analysis (CCA). Our approach emerges from a profound understanding of the underlying optimization landscape, viewed through the lens of differential geometry. By delving into the Riemannian structure inherent to the problem, we uncover elegant strategies that enhance existing manifold stochastic gradient descent methods, which have long been employed in the pursuit of numerical optimization on curved spaces.  Traditionally, practitioners have faced the daunting challenge of algorithms that either demand a staggering $O(d^3)$ time complexity per iteration with a convergence rate of $O(\\frac{1}{\\sqrt{t}})$\u2014where $d$ represents the dimensionality\u2014or those that are limited to extracting a mere single component at a convergence rate of $O(\\frac{1}{t})$. In stark contrast, our innovative RSG+ algorithm operates with a remarkable efficiency of $O(d^2k)$ runtime complexity per iteration, deftly extracting the top $k$ canonical components while maintaining a convergence rate of $O(\\frac{1}{t})$.  Within this work, we not only present a rigorous theoretical analysis of our method but also share compelling empirical results that illustrate its performance. Furthermore, we explore the potential of our algorithm in the realm of fairness in machine learning, particularly in scenarios where the labels of protected attributes are absent or inaccessible. Join us as we navigate this new frontier, where mathematics and data converge to forge pathways toward more equitable models.", "introduction": "Canonical correlation analysis (CCA) is a popular method for evaluating correlations between two sets of variables.It is commonly used in unsupervised multi-view learning, where the multiple views of the data may correspond to image, text, audio and so on Rupnik & Shawe-Taylor (2010); Chaudhuri et al. (2009); Luo et al. (2015).Classical CCA formulations have also been extended to leverage advances in representation learning, for example, Andrew et al. (2013) showed how the CCA can be interfaced with deep neural networks enabling modern use cases.Many results over the last few years have used CCA or its variants for problems including measuring representational similarity in deep neural networks Morcos et al. (2018), speech recognition Couture et al. (2019), etc.The goal in CCA is to find linear combinations within two random variables X and Y which have maximum correlation with each other.Formally, the CCA problem is defined in the following way.Given a pair of random variables, a d x -variate random variable X and a d y -variate random variable Y, with unknown joint probability distribution, find the projection matrices U \u2208 R dx\u00d7k and V \u2208 R dy\u00d7k , with k \u2264 min{d x , d y }, such that the correlation is maximized:Here, X, Y are samples of X and Y respectively.The objective function in (1) is the expected crosscorrelation in the projected space and the constraints specify that different canonical components should be decorrelated.Let C X = E X [X T X] and C Y = E Y [Y T Y ] be the covariance matrices, and C XY = E (X,Y) [X T Y ] denote cross-covariance.Let us define the whitened covarianceand \u03a6 k (and \u03a8 k ) contains the top-k left (and right) singular vectors of T .It is known Golub & Zha (1992) that the optimum of ( 1) is achieved atIn practice, we may be given two views of N samples as X \u2208 R N \u00d7dx and Y \u2208 R N \u00d7dy .A natural approach to solving CCA is based on on the following sequence of steps.We first compute the empirical covariance and cross-covariance matrices, namely, C X = 1 /NX T X, C Y = 1 /NY T Y and C XY = 1 /NX T Y .We then calculate the empirical whitened cross-covariance matrix T , finally, compute U * , V * by applying a k-truncated SVD to T .Runtime and memory considerations.The above procedure is simple but is only feasible when the data matrices are small.But in most modern applications, not only are the datasets large but also the dimension d (let d = max{d x , d y }) of each sample can be quite high, especially if representations are being learned using deep neural network models.As a result, the computational footprint of the foregoing algorithm can be quite high.This has motivated the study of stochastic optimization routines for solving CCA.Observe that in contrast to the typical settings where stochastic optimization schemes are most effective, the CCA objective does not decompose over samples in the dataset.Many efficient strategies have been proposed in the literature: for example, Ge et al. (2016); Wang et al. (2016) present Empirical Risk Minimization (ERM) models which optimize the empirical objective.More recently, Gao et al. (2019); Bhatia et al. (2018); Arora et al. (2017) describe proposals that optimize the population objective.To summarize the approaches succinctly, if we are satisfied with identifying the top 1 component of CCA, effective schemes are available by utilizing either extensions of the Oja's rule Oja (1982) to generalized eigenvalue problem Bhatia et al. (2018) or the alternating SVRG algorithm Gao et al. (2019)).Otherwise, a stochastic approach must make use of an explicit whitening operation which involves a cost of d 3 for each iteration Arora et al. (2017).Observation.Most approaches either directly optimize (1) or instead a reparametrized or regularized form Ge et al. (2016); Allen-Zhu & Li (2016); Arora et al. (2017).Often, the search space for U and V corresponds to the entire R d\u00d7k (ignoring the constraints for the moment).But if the formulation could be cast in a form which involved approximately writing U and V as a product of several matrices with nicer properties, we may obtain specialized routines which are tailored to exploit those properties.Such a reformulation is not difficult to derive -where the matrices used to express U and V can be identified as objects that live in well studied geometric spaces.Then, utilizing the geometry of the space and borrowing relevant tools from differential geometry leads to an efficient approximate algorithm for top-k CCA which optimizes the population objective in a streaming fashion.Contributions.(a) First, we re-parameterize the top-k CCA problem as an optimization problem on specific matrix manifolds, and show that it is equivalent to the original formulation in equation 1.(b) Informed by the geometry of the manifold, we derive stochastic gradient descent algorithms for solving the re-parameterized problem with O(d 2 k) cost per iteration and provide convergence rate guarantees.(c) This analysis provides a direct mechanism to obtain an upper bound on the number of iterations needed to guarantee an error w.r.t. the population objective for the CCA problem.(d) The algorithm works in a streaming manner so it easily scales to large datasets and we do not need to assume access to the full dataset at the outset.(e) We present empirical evidence for both the standard CCA model and the DeepCCA setting Andrew et al. (2013), describing advantages and limitations."}
{"paper_id": 157, "abstract": "In the realm of deep generative models, flow-based architectures stand out as powerful tools, boasting tractable likelihoods and a host of appealing properties\u2014efficient density estimation and rapid sampling among them. Yet, despite their many strengths, current incarnations, such as normalizing flows, often come burdened with hefty memory and runtime demands, limiting their practical applications.   In this exploration, we delve into a novel approach that leverages the strengths of an effective autoencoder tailored to our specific dataset. Under a few reasonable assumptions, we unveil a pathway to map our data into a Reproducing Kernel Hilbert Space (RKHS). This clever maneuver opens the door to harnessing established techniques from the kernel methods domain, allowing us to align the RKHS distribution with a well-characterized template distribution through the use of kernel transfer operators.   The beauty of our approach lies in its simplicity: we achieve a direct and resource-efficient approximation without the need for cumbersome iterative optimization. Our empirical results speak volumes, demonstrating that this straightforward concept yields competitive performance on well-known datasets like CelebA, while also showing promise on a challenging public 3D brain imaging dataset, where sample sizes are notably limited. This work not only advances the capabilities of flow-based generative models but also paves the way for their broader application across various domains.", "introduction": "A flow-based generative model refers to a deep generative model composed using a set of invertible transformations.While GANs and VAEs remain the two dominant generative models in the community, flow based formulations have continually evolved and now offer competitive performance in applications including audio/speech synthesis Kim et al. (2019;2020), text to speech Miao et al. (2020), photo-realistic image generation Kingma & Dhariwal (2018), and learning cross-domain mappings Mahajan et al. (2020).An important property of such models is the explicit use of a tractable likelihood function, which enables leveraging maximum likelihood principles during training as well as efficient/exact density estimation and sampling.The formulation is invertible by design but this involves higher memory requirements.For example, permitting the bijective mapping to be expressive enough involves increases in the memory footprint Lee et al. (2020); Kim et al. (2019), an issue that is a focus of several recent results Jacobsen et al. ( 2018); Chen et al. (2016).Moreover, in these models we need to calculate the inverse and backpropagate through all invertible transformations during training.Calculating the inverse incurs a multiplicative increase in cost, usually as a function of the feature dimension, relative to the calculation of the likelihood, an issue addressed to some extent in Dinh et al. (2017); Kingma & Dhariwal (2018).At a high level, a flow-based generative model bijectively pushes the data density from a source to a target, i.e., from a known simple distribution to an unknown (may be intractable) data distribution.During training, we seek to learn this bijective mapping by maximizing the likelihood of the mapped training samples.In the generation step, we need the inverse of this mapping (given such an inverse exists) to map from a sample drawn from the known distribution back to the input (data) space.When the Jacobian of the transformation mapping can be efficiently computed or estimated (e.g., having a lower triangular form), directly optimizing the likelihood of the training samples is possible.However, in training flow-based generative either we must restrict the expressiveness at each layer or fall back on more numerically heavy solutions, see (Chen et al., 2018).Next, we discuss how several existing results may provide a simplification strategy."}
{"paper_id": 158, "abstract": "In the realm of deep learning, the triumph of models hinges significantly on the vast reservoirs of annotated data that fuel their growth. These models endeavor to unearth intricate features from this data\u2014elements imbued with the vital essence necessary for enhancing their prowess in targeted applications. Traditionally, the spotlight has shone on the direct optimization of target loss functions, allowing models to implicitly absorb representations from the data itself, all in the quest for greater accuracy. However, a curious gap remains: the underexplored potential of leveraging background and noise data to glean insights about the statistics of in-domain data, thereby enriching the feature representations of deep neural networks.  In this paper, we embark on an exploration of this uncharted territory, uncovering a compelling relationship between the estimation of unknown parameters within the probability density function (pdf) of input data and the accuracy of classification. Our findings reveal that refining the estimates of these unknown parameters\u2014by integrating both background and in-domain data\u2014yields superior features, subsequently enhancing classification accuracy.   Building upon this foundation, we introduce a novel yet straightforward approach: the Detection Booster Training (DBT) method. This technique applies a detection loss function to the early layers of a neural network, skillfully distinguishing in-domain data points from their noise-laden counterparts. The background and noise data, while derived from the same family of pdfs, possess distinct parameter sets\u2014such as mean and variance\u2014that enrich the learning process. Remarkably, our DBT method demonstrates its prowess even when faced with a scarcity of labeled in-domain training samples, outperforming conventional training methods.  Through rigorous experimentation across tasks such as face recognition, image classification, and speaker classification, we unveil the superior performance of our approach against formidable baselines, showcasing its efficacy across a diverse array of datasets and model architectures. In the grand tapestry of deep learning, our work seeks to illuminate a path forward, one where the synergy of data\u2014both in-domain and background\u2014cultivates deeper understanding and greater accuracy.", "introduction": "Modern pattern recognition systems achieve outstanding accuracies on a vast domain of challenging computer vision, natural language, and speech recognition benchmarks (Russakovsky et al. (2015); Lin et al. (2014); Everingham et al. (2015); Panayotov et al. (2015)).The success of deep learning approaches relies on the availability of a large amount of annotated data and on extracting useful features from them for different applications.Learning rich feature representations from the available data is a challenging problem in deep learning.A related line of work includes learning deep latent space embedding through deep generative models (Kingma & Welling (2014); Goodfellow et al. (2014); Berthelot et al. (2019) or using self-supervised learning methods (Noroozi & Favaro (2016); Gidaris et al. (2018); Zhang et al. (2016b)) or through transfer learning approaches (Yosinski et al. (2014); Oquab et al. (2014); Razavian et al. (2014)).In this paper, we propose to use a different approach to improve the feature representations of deep neural nets and eventually improve their accuracy by estimating the unknown parameters of the probability density function (pdf) of input data.Parameter estimation or Point estimation methods are well studied in the field of statistical inference (Lehmann & Casella (1998)).The insights from the theory of point estimation can help us to develop better deep model architectures for improving the model's performance.We make use of this theory to derive a correlation between the estimation of unknown parameters of pdf and classifier outputs.However, directly estimating the unknown pdf parameters for practical problems such as image classification is not feasible since it can sum up to millions of parameters.In order to overcome this bottleneck, we assume that the input data points are sampled from a family of pdfs instead of a single pdf and propose to use a detection based training approach to better estimate the unknowns using in-domain and background/noise data.One alternative is that we can use generative models for this task, however, they mimic the general distribution of training data conditioned on random latent vectors and hence cannot be directly applied for estimating the unknown parameters of a family of pdfs.Our proposed detection method involves a binary class discriminator that separates the target data points from noise or background data.The noise or background data is assumed to come from the same family of distribution of in-domain data but with different moments (Please refer to the appendix for more details about the family of distributions and its extension to a general structure).In image classification, this typically represents the background patches from input data that fall under the same distribution family.In speech domain, it can be random noise or the silence intervals in speech data.Collecting such background data to improve the feature representations is much simpler as compared to using labeled training data since it is time-consuming and expensive to collect labeled data.Since the background patches in images or noise in speech signals are used for binary classification in our method, we refer to such data as the noise of an auxiliary binary classification problem denoted by auxiliary binary classification (ABC)-noise dataset.An advantage of using ABC-noise data during training is that it can implicitly add robustness to deep neural networks against the background or noisy data.Since ABC-noise data can be collected in large quantities for free and using that data in our approach improves the classification benchmarks, we investigate whether this data can act as a substitute for labeled data.We conduct empirical analysis and show that using only a fraction of labeled training data together with ABC-noise data in our DBT method, indeed improves the accuracy as compared to normal training.To summarize, our contributions are threefold.First, we present a detailed theoretical analysis on the relation between the estimation of unknown parameters of pdf of data and classification outputs.Second, based on the theoretical analysis, we present a simple booster training method to improve classification accuracy which also doubles up as an augmented training method when only limited labeled data is available.Third, we consistently achieve improved performances over strong baselines on face recognition, image classification, and speaker recognition problems using our proposed method, showing its generalization across different domains and model architectures."}
{"paper_id": 159, "abstract": "In the ever-evolving realm of machine learning, the allure of sparsity has captured the imagination of researchers and practitioners alike. It promises not only to conserve precious computational resources but also to illuminate the inner workings of complex models and shield them from the perils of overfitting. In this exploration, we delve into the concept of sparsity through the lens of neural networks, where we unveil a novel perspective: layer sparsity. This fresh interpretation focuses on the individual layers of the network, harmonizing beautifully with the contemporary shift towards deeper architectures. To harness the power of layer sparsity, we introduce innovative regularization techniques and refitting strategies designed to seamlessly integrate with existing deep-learning workflows. The result? More compact, efficient, and accurate networks that push the boundaries of what we thought possible.", "introduction": "The number of layers and the number of nodes in each layer are arguably among the most fundamental parameters of neural networks.But specifying these parameters can be challenging: deep and wide networks, that is, networks with many layers and nodes, can describe data in astounding detail, but they are also prone to overfitting and require large memory, CPU, energy, and so forth.The resource requirements can be particularly problematic for real-time applications or applications on fitness trackers and other wearables, whose popularity has surged in recent years.A promising approach to meet these challenges is to fit networks sizes adaptively, that is, to allow for many layers and nodes in principle, but to ensure that the final network is \"simple\" in that it has a small number of connections, nodes, or layers (Changpinyo et al., 2017;Han et al., 2016;Kim et al., 2016;Liu et al., 2015;Wen et al., 2016).Popular ways to fit such simple and compact networks include successively augmenting small networks (Ash, 1989;Bello, 1992), pruning large networks (Simonyan & Zisserman, 2015), or explicit sparsity-inducing regularization of the weight matrices, which we focus on here.An example is the 1 -norm, which can reduce the number of connections.Another example is the 1 -norm grouped over the rows of the weight matrices, which can reduce the number of nodes.It has been shown that such regularizers can indeed produce networks that are both accurate and yet have a small number of nodes and connections either in the first layer (Feng & Simon, 2017) or overall (Alvarez & Salzmann, 2016;Liu et al., 2015;Scardapane et al., 2017).Such sparsity-inducing regularizers also have a long-standing tradition and thorough theoretical underpinning in statistics (Hastie et al., 2015).But while sparsity on the level of connections and nodes has been studied in some detail, sparsity on the level of layers is much less understood.This lack of understanding contrasts the current trend to deep network architectures, which is supported by state-of-the-art performances of deep networks (LeCun et al., 2015;Schmidhuber, 2015), recent approximation theory for ReLU activation networks (Liang & Srikant, 2016;Telgarsky, 2016;Yarotsky, 2017), and recent statistical theory (Golowich et al., 2017;Kohler & Langer, 2019;Taheri et al., 2020).Hence, a better understanding of sparsity on the level of layers seems to be in order.Therefore, we discuss in this paper sparsity with a special emphasis on the networks' layers.Our key observation is that for typical activation functions such as ReLU, a layer can be removed if all its parameter values are non-negative.We leverage this observation in the development of a new regularizer that specifically targets sparsity on the level of layers, and we show that this regularizer can lead to more compact and more accurate networks.1. We introduce a new notion of sparsity that we call layer sparsity.2. We introduce a corresponding regularizer that can reduce network sizes.3. We introduce an additional refitting step that can further improve prediction accuracies.In Section 2, we specify our framework, discuss different notions of sparsity, and introduce our refitting scheme.In Section 3, we establish a numerical proof of concept.In Section 4, we conclude with a discussion."}
{"paper_id": 160, "abstract": "In the realm of node classification, a formidable foe has emerged: over-smoothing, a peril that plagues graph convolutional networks (GCNs) like a creeping fog. This challenge arises from the intricate dance of message passing, where the chaotic whispers of a noisy graph topology allow information to traverse across inter-class edges, leading to a disheartening blend of features from disparate classes.   In this paper, we unveil an innovative solution: the VEM-GCN architecture. This creation harnesses the power of the variational EM algorithm, forging a path to simultaneously refine the graph topology and cultivate optimal node representations for classification. At its core, VEM-GCN employs a latent adjacency matrix, sculpted by the assortative-constrained stochastic block model (SBM), to bolster connections within classes while deftly curbing interactions between them, all in response to the noisy graph\u2019s challenges.  In the variational E-step, we embark on a quest to optimize the graph topology, employing a neural network that learns from the very essence of node embeddings to approximate the posterior probability distribution of our latent adjacency matrix. The M-step follows, where we draw forth node representations through a graph convolutional network, now fortified by the refined graph topology, primed for the critical task of classification.  Through rigorous testing across seven benchmark datasets, VEM-GCN has proven itself a champion, eclipsing existing strategies in the battle against over-smoothing and the optimization of graph topology in node classification. Join us as we delve into this innovative approach, one that promises to reshape our understanding of graph-based learning.", "introduction": "Complex graph-structured data are ubiquitous in the real world, ranging from social networks to chemical molecules.Inspired by the remarkable performance of convolutional neural networks (CNNs) in processing data with regular grid structures (e.g., images), a myriad of studies on GCNs have emerged to execute \"convolution\" in the graph domain (Niepert et al., 2016;Kipf & Welling, 2017;Gilmer et al., 2017;Hamilton et al., 2017;Monti et al., 2017;Gao et al., 2018).Many of these approaches follow a neighborhood aggregation mechanism (a.k.a., message passing scheme) that updates the representation of each node by iteratively aggregating the transformed messages sent from its neighboring nodes.Commencing with the pioneering works (Kipf & Welling, 2017;Gilmer et al., 2017), numerous strategies have been developed to improve the vanilla message passing scheme such as introducing self-attention mechanism (Veli\u010dkovi\u0107 et al., 2018;Zhang et al., 2020), incorporating local structural information (Zhang et al., 2020;Jin et al., 2019;Ye et al., 2020), and leveraging the link attributes (Gong & Cheng, 2019;Li et al., 2019;Jiang et al., 2019).Despite significant success in many fundamental tasks of graph-based machine learning, message passing-based GCNs almost all process the observed graph structure as ground truth and might suffer from the over-smoothing problem (Li et al., 2018), which would seriously affect the node classification performance.Given the observed noisy graph topology (i.e., excessive inter-class edges are linked while many intra-class edges are missing), when multiple message passing layers are stacked to enlarge the receptive field (the maximum hop of neighborhoods), features of neighboring nodes in different classes would be dominant in message passing.Thus, node representations would be corrupted by the harmful noise and affect the discrimination of graph nodes.The over-smoothing phenomenon in GCNs has already been studied from different aspects.Li et al. (2018) first interpreted over-smoothing from the perspective of Laplacian smoothing, while Xu et al. (2018) and Klicpera et al. (2019a) associated it with the limit distribution of random walk.Furthermore, Chen et al. (2020a) developed quantitative metrics to measure the over-smoothness from the topological view.They argued that the key factor leading to over-smoothing is the noise passing between nodes of different categories and the classification performance of GCNs is positively correlated with the proportion of intra-class node pairs in all edges.In this paper, we propose VEM-GCN, a novel architecture to address the over-smoothing problem with topology optimization for uncertain graphs.Considering that a \"clearer\" graph with more intra-class edges and fewer inter-class edges would improve the node classification performance of GCNs (Yang et al., 2019;Chen et al., 2020a), VEM-GCN approaches a latent adjacency matrix parameterized by the assortative-constrained stochastic block model (SBM) where nodes share the same label are linked and inter-class edges should be cut off.To jointly refine the latent graph structure and learn desirable node representations for classification, variational EM algorithm (Neal & Hinton, 1998) is adopted to optimize the evidence lower bound (ELBO) of the likelihood function.In the inference procedure (E-step), graph topology is optimized by approximating the posterior probability distribution of the latent adjacency matrix with a neural network learned from node embeddings.In the learning procedure (M-step), a conventional GCN is trained to maximize the log-likelihood of the observed node labels based on the learned latent graph structure.The E-step and M-step optimize the graph topology and improve the classification of unlabeled nodes in an alternating fashion.The proposed VEM-GCN architecture is flexible and general.In the E-step, the neural network can support arbitrary desirable node embeddings generated by algorithms such as node2vec (Grover & Leskovec, 2016), struc2vec (Ribeiro et al., 2017), and GCNs, or the raw node attributes.The GCN in the M-step can also be substituted with arbitrary graph models.Furthermore, recent strategies for relieving the over-smoothing issue, i.e., AdaEdge (Chen et al., 2020a) and DropEdge (Rong et al., 2020), are shown to be the specific cases of VEM-GCN under certain conditions.For empirical evaluation, we conduct extensive experiments on seven benchmarks for node classification, including four citation networks, two Amazon co-purchase graphs, and one Microsoft Academic graph.Experimental results demonstrate the effectiveness of the proposed VEM-GCN architecture in optimizing graph topology and mitigating the over-smoothing problem for GCNs."}
{"paper_id": 161, "abstract": "In the ever-evolving realm of neural networks, the quest for improved generalization through data augmentation continues to intrigue researchers. Among the many challenges that lie ahead, one stands out: how can we effectively augment graph data to elevate the prowess of Graph Neural Networks (GNNs)? While the traditional approach has centered on manipulating graph topologies\u2014adding or removing edges\u2014our exploration leads us down a different path. We present FLAG (Free Large-scale Adversarial Augmentation on Graphs), an innovative method that enhances the input node feature space itself, paving the way for superior performance.  FLAG operates through a straightforward yet powerful mechanism, iteratively introducing gradient-based adversarial perturbations to node features throughout the training process. This not only fortifies the model but also translates into significant improvements during testing. Remarkably, FLAG is designed with simplicity in mind; it can be implemented in just a few lines of code and demonstrates remarkable adaptability, seamlessly integrating with any GNN backbone across a diverse array of large-scale datasets, whether in transductive or inductive contexts.  What sets FLAG apart is its ability to deliver substantial performance enhancements without necessitating alterations to a model's architecture or training regimen. Across both node and graph classification tasks, FLAG consistently yields impressive results. In our experiments, we achieved state-of-the-art performance on challenging datasets such as ogbg-molpcba, ogbg-ppa, and ogbg-code. With FLAG, we not only push the boundaries of what is possible with GNNs but also chart a new course for future explorations in the realm of graph data augmentation.", "introduction": "Graph Neural Networks (GNNs) have emerged as powerful architectures for learning and analyzing graph representations.The Graph Convolutional Network (GCN) (Kipf & Welling, 2016) and its variants have been applied to a wide range of tasks, including visual recognition (Zhao et al., 2019;Shen et al., 2018), meta-learning (Garcia & Bruna, 2017), social analysis (Qiu et al., 2018;Li & Goldwasser, 2019), and recommender systems (Ying et al., 2018).However, the training of GNNs on large-scale datasets usually suffers from overfitting, and realistic graph datasets often involve a high volume of out-of-distribution test nodes (Hu et al., 2020), posing significant challenges for prediction problems.One promising solution to combat overfitting in deep neural networks is data augmentation (Krizhevsky et al., 2012), which is commonplace in computer vision tasks.Data augmentations apply label-preserving transformations to images, such as translations and reflections.As a result, data augmentation effectively enlarges the training set while incurring negligible computational overhead.However, it remains an open problem how to effectively generalize the notion of data augmentation to GNNs.Transformations on images rely heavily on image structures, and it is challenging to design low-cost transformations that preserve semantic meaning for non-visual tasks like natural language processing (Wei & Zou, 2019) and graph learning.Generally speaking, graph data for machine learning comes with graph structure (or edge features) and node features.In the limited cases where data augmentation can be done on graphs, it generally focuses exclusively on the graph structure by adding/removing edges (Rong et al., 2019).To date, there is no study on how to manipulate graphs in node feature space for enhanced performance.In the meantime, adversarial data augmentation, which happens in the input feature space, is known to boost neural network robustness and promote resistance to adversarially chosen inputs (Goodfellow et al., 2014;Madry et al., 2017).Despite the wide belief that adversarial training harms standard generalization and leads to worse accuracy (Tsipras et al., 2018;Balaji et al., 2019), recently a growing amount of attention has been paid to using adversarial perturbations to augment datasets and ultimately alleviate overfitting.For example, Volpi et al. (2018) showed adversarial data augmentation is a data-dependent regularization that could help generalize to out-of-distribution samples, and its effectiveness has been verified in domains including computer vision (Xie et al., 2020), language understanding (Zhu et al., 2019;Jiang et al., 2019), and visual question answering (Gan et al., 2020).Despite the rich literature about adversarial training of GNNs for security purposes (Z\u00fcgner et al., 2018;Dai et al., 2018;Bojchevski & G\u00fcnnemann, 2019;Zhang & Zitnik, 2020), it remains unclear how to effectively and efficiently improve GNN's clean accuracy using adversarial augmentation.Present work.We propose FLAG, Free Large-scale Adversarial Augmentation on Graphs, to tackle the overfitting problem.While existing literature focuses on modifying graph structures to augment datasets, FLAG works purely in the node feature space by adding gradient-based adversarial perturbations to the input node features with graph structures unchanged.FLAG leverages \"free\" methods (Shafahi et al., 2019) to conduct efficient adversarial training so that it is highly scalable on large-scale datasets.We verify the effectiveness of FLAG on the Open Graph Benchmark (OGB) (Hu et al., 2020), which is a collection of large-scale, realistic, and diverse graph datasets for both node and graph property prediction tasks.We conduct extensive experiments across OGB datasets by applying FLAG to prestigious GNN models, which are GCN, GraphSAGE, GAT, and GIN (Kipf & Welling, 2016;Hamilton et al., 2017;Veli\u010dkovi\u0107 et al., 2017;Xu et al., 2019) and show that FLAG brings consistent and significant improvements.For example, FLAG lifts the test accuracy of GAT on ogbn-products by an absolute value of 2.31%.DeeperGCN (Li et al., 2020) is another strong baseline that achieves top performance on several OGB benchmarks.FLAG enables DeeperGCN to generalize further and reach new state-of-the-art performance on ogbg-molpcba and ogbg-ppa.FLAG is simple (adding just a dozen lines of code), general (can be directly applied to any GNN model), versatile (works in both transductive and inductive settings), and efficient (able to bring salient improvement at tractable or even no extra cost).Our main contributions are summarized as follows:\u2022 We propose adversarial perturbations as a data augmentation in the input node feature space to efficiently boost GNN performance.The resulting FLAG framework is a scalable and flexible augmentation scheme for GNN, which is easy to implement and applicable to any GNN architecture for both node and graph classification tasks.\u2022 We advance the state-of-the-art on a number of large-scale OGB datasets, often by large margins.\u2022 We provide a detailed analysis and deep insights on the effects adversarial augmentation has on GNNs."}
{"paper_id": 162, "abstract": "In the realm of large-scale retrieval-based applications, the challenge of Feature Compatible Learning (FCL) emerges as a pressing concern. Imagine a world where updating an entire library of embedding vectors is akin to reshaping a mountain\u2014an expensive and laborious endeavor. When a new embedding model shines with promise, the desire to harness its potential without overhauling the entire library becomes paramount. While strides have been made in this innovative landscape, many existing methods for feature compatible learning are shackled by their reliance on outdated training data and classifiers\u2014resources often unavailable in the fast-paced world of industry.  In this work, we unveil a groundbreaking approach to feature compatible learning that liberates us from the constraints of inherited classifiers and old training data, coining it Non-Inherent Feature Compatible Learning. Our method requires only the features extracted from the backbone of the old model and fresh training data, boldly making no assumptions about the overlap between the two datasets. We propose a cohesive framework for FCL and extend its reach to accommodate scenarios where the old model operates as an enigmatic black box.  At the heart of our approach lies the development of a simple pseudo-classifier, which stands in for the old model, augmented by a random walk algorithm to enhance its capabilities. This innovative strategy ensures that the embedding features generated by the new model can seamlessly align with those of the old, all while maintaining peak performance. Our experiments, conducted on the renowned ImageNet ILSVRC 2012 and Places365 datasets, demonstrate the efficacy and robustness of our proposed method, paving the way for a new era of feature compatible learning.", "introduction": "In recent years, deep learning based methods achieved huge success in various of computer vision tasks, especially for visual searching since they could provide powerful feature representations.In a typical visual search system, the deployed deep learning model extracts the features of both gallery and query images as discriminate representations.During the retrieval stage, gallery images will be ranked based on their feature distances (e.g.Euclidean distance) to query images.In conventional approaches, the query and gallery features are generated by the same model.Once the deployed model of retrieval system is updated, the entire set of gallery features need to be 'backfilled' or 'reindexed' (Shen et al., 2020).As time goes by, the gallery becomes extremely large and 'backfilling' could be a painful process since millions even billions of images need to be re-processed by the new model, which is computationally expensive.There has to be a new mechanism that processes gallery images and the query image with two different models, while still maintaining the retrieval accuracy.In other words, the new deployed model extracted features should be 'compatible' to the existing ones without sacrificing accuracy.Such feature compatible learning problem is also named as 'Backward-Compatible Training' (Shen et al., 2020), or 'Asymmetric Metric Learning' (Budnik & Avrithis, 2020).Existing approaches for feature compatible learning assumed significant overlap between new and old training sets.In Shen et al. (2020), the training set for new embedding model is a superset of the old set.In Budnik & Avrithis (2020), the training set for large and small models is the same, which means obtaining new model in an incremental way is not possible.Besides, in Shen et al. (2020), the classifier for old model is also needed for computing the influence loss, which is a strong requirement in real applications.As an example, a model deployed in a recommendation system as a black-box API takes images as input and returns the processed features, but the parameters of the model are not accessible.In addition, its classifier and training details are not available, neither does the formula of the loss function.This kind of setting is quite common for various practical reasons in search, recommendation, content understanding and review applications.To address the limitation, we propose an approach for non-inherent feature compatible learning, which only exploits the old model backbone and new training data.Despite the lack of old training data or old classifier, the new model extracts compatible features without sacrificing accuracy.The proposed approach has three contributions including:\u2022 Study and formulate the non-inherent setting of the FCL problem for the first time\u2022 Establish a baseline with a data-incremental approach, where performance degradation is prevented by regularizing the training process of the new model\u2022 Extend the baseline with a random walk algorithm that further improves accuracyThe experiments conducted on several standard data sets validated the effectiveness of the proposed approach."}
{"paper_id": 163, "abstract": "In recent years, the realm of neural networks has witnessed remarkable strides in tackling the intricate challenges posed by large and complex graphs. Yet, the efficiency of these methods often hinges on the availability of extensive training and validation datasets, the procurement of which can be both costly and laborious due to the need for ground-truth labels. In response to this pressing issue, we introduce the Graph View-Consistent Learning Network (GVCLN), a novel architecture crafted specifically for semi-supervised learning in scenarios where labeled samples are scarce.  GVCLN harnesses the power of neighborhood aggregation, leveraging dual views to extract diverse representations from the same underlying data. Though these views may differ in perspective, they share a common subject of observation, necessitating a consistency in their representations. To achieve this, we propose a trio of loss functions: a supervised loss that utilizes the available labeled set, a view-consistent loss that aligns the representations across the two views, and a pseudo-label loss that capitalizes on high-confidence predictions to further refine our model.  Through this innovative approach, GVCLN adeptly generates view-consistent representations of the original features, paving the way for enhanced classification performance. Our findings reveal that preprocessing node features with a tailored filter prior to training significantly benefits subsequent classification tasks. We validate our methodology through rigorous experiments on three prominent citation network datasets\u2014Cora, Citeseer, and PubMed\u2014where GVCLN consistently achieves state-of-the-art results across various node classification challenges.", "introduction": "Convolutional neural networks (CNNs) (Krizhevsky et al., 2012) performed outstandingly in solving problems such as image classification (Rawat & Wang, 2017), semantic segmentation (Kampffmeyer et al., 2016) and machine translation (Cho et al., 2014) etc.This is because CNNs can effectively reuse the convolution kernel and use the given input to train optimal parameters.The original data mentioned in above problems all have a grid-like data structure, that is, Euclidean spatial data.In reality, there are also lots of non-Euclidean spatial data, such as social networks, telecommunication networks, biological networks, and brain connection structures, etc.These data are usually represented in the form of graphs, where every node in the graph represents a single individual.Graph problems can be roughly divided into there direction: link prediction (Zhang & Chen, 2018), graph classification (Zhang et al., 2018a) and node classification (Kipf & Welling, 2016).In this paper, we focus on semi-supervised node classification when the label rate is very low.Many new methods have been proposed to generalize the convolution operation to process graph structure data on arbitrary graphs for node classification.These methods can be divided into spatial convolution and spectral convolution methods (Zhang et al., 2018b).For spatial methods, they directly define graph convolution by designing certain operations on node's neighbors.For example, Duvenaud et al. (2015) propose a convolutional neural network that can directly operate on graph data, which can provide an end-to-end feature learning method; Atwood & Towsley (2016) propose a fusion convolutional neural network (DCNNS), which introduces the graph fusion method to incorporate the context information of the nodes in the graph node classification; The Graph Attention Network (GATs) (Veli\u010dkovi\u0107 et al., 2017) introduces the attention mechanism into the graph data processing to construct the attention layer for semi-supervised learning.The spectral method generally defines the graph convolution operation on spectral representation of graph.For example, Bruna et al. (2013) propose that graph convolution can be defined in the Fourier domain based on the eigenvalue decomposition of the graph Laplacian matrix; Defferrard et al. (2016) propose to use the Chebyshev expansion of the graph Laplacian to approximate the spectral domain filtering, which can avoid high computational complexity brought by eigenvalue decomposition; Kipf & Welling (2016) propose a simpler Graph Convolutional Networks (GCNs) for semi-supervised learning, which can achieve higher classification accuracy by using a simple two-layer networks.However, large training and validation sets are required in these methods to complish effect classification task, while obtaining true labels is time-consuming, laborious and costly.And they send original graph node features directly into networks for training, however, there are lots of redundant information in the original features of the nodes.In order to train an efficient model with only a few label nodes and even without validation, we put forward our own method: graph view-consistent learning network (GVCLN), which constructs a node classification network based on the consistency between two views.First, we independently train two-view encoders (can be different) to obtain two representations of every node.The function of the viewers is converting high-dimensional node features into low-dimensional embeddings (Zhu et al., 2020).The clustering hypothesis (Vandenberg & Matthias, 1977) show that examples in the same cluster are more likely to have the same label.According to this hypothesis, the decision boundary should try to pass through the place where the data is relatively sparse, so as to avoid dividing the data points in dense clusters on both sides of the decision boundary.Although the two views have different viewing angles, their observation objects are the same, so their observation results should be consistent.Therefore, the features encoded by the two viewers should make the decision boundary pass through the place where the date is sparse, that is, there should be consistency between the two views.Then, we design three loss functions, namely, supervised loss function, consistency loss function, and pseudo-label loss function.The supervised loss uses the known labeled set, while a view-consistent loss is applied to the two views to obtain the consistent representation and a pseudo-label loss is designed by using the common high-confidence predictions as pseudo label.GVCLN with these loss functions can obtain the view-consistent representation of the original feature.Our contributions are summarized as follows:\u2022 We propose a graph view-consistent learning framework for semi-supervised node classification, which fully demonstrates the theoretical structure of graph view-consistency.\u2022 We design GVCLN to successfully tackle label insufficiency in semi-supervised learning.\u2022 We demonstrate the high efficacy and efficiency of the proposed methods on various semisupervised node classification tasks."}
{"paper_id": 164, "abstract": "In the realm of graph analytics, where complexity weaves through connections like threads in a tapestry, graph neural networks (GNNs) have emerged as formidable champions. Their prowess in representation learning has garnered significant acclaim, yet the quest for the ultimate node embedding remains a challenge fraught with overlooked nuances. Many existing strategies focus on singular methods, adept at capturing specific features while neglecting the equally crucial elements that lie in the shadows.  In this paper, we unveil a groundbreaking approach: the Dual Graph Complementary Network (DGCN). This innovative framework operates through two distinct branches, both drawing from the same wellspring of structural and feature information. Yet, it is the interplay between these branches\u2014their complementary nature\u2014that breathes new life into the learning process. Our extensive experiments reveal a compelling truth: DGCN not only meets the challenge but surpasses the state-of-the-art, achieving remarkable performance across five widely recognized benchmark datasets. In this exploration, we invite you to witness the power of complementarity and the promise it holds for the future of graph representation learning.", "introduction": "Although many attempts have been made in literatures to find a better strategy to learn the target node representation, the feature extraction capabilities of most methods are still far from optimal, especially when only a small amount of data is labeled.However, in fact, compared with the expensive and laborious acquisition of labeled data, unlabeled data is much easier to obtain.Therefore, how to learn more useful representations with limited label information is the key direct of representation learning study.Methods of this issue, commonly referred to as semi-supervised learning, which essentially believe that the similar points have similar outputs.Thus, it can properly utilize the consistency of data to make full use of the rich information of unsupervised data.In the real world, it is common that we have data with specific topological structures which usually called graph data.The graph structure is usually expressed as the connection between nodes.By aggregating the features of neighborhood and performing appropriate linear transformation, graph neural networks (GNNs) can convert graph data into a low-dimensional, compact, and continuous feature space.Nevertheless, most of them only care about a single aggregation strategy, which is counter intuitive: for example, as far as social networks are concerned, the relationship between people is very complex, while, most of the traditional GNNs only consider the single connection between nodes and ignore other implicit information.In this paper, our work focuses on learning node representations by GNNs in a semi-supervised way.Despite there are already many graph-based semi-supervised learning methods (Kipf & Welling, 2016;Yang et al., 2016;Khan & Blumenstock, 2019), most of them can only find a single relationship between nodes.As a result, some information in unsupervised data is usually ignored.To overcome this problem, we develop a novel dual graph complementary network (DGCN) to extract information from both feature and topology spaces.An intuition of our method is to learn based on disagreement: network performance is largely related to the quality of the graph, which usually emphasizes the relevance of an attribute of instances.So, since we don't know what attributes are most important, we consider both of them in the model design.Compared with the traditional GNN-based methods, we perform two different aggregate strategies which emphasize different attributes in each branch, one from the perspective of node feature, and the other from the topological structure.Then, to further utilize implicit information, we employ two networks with different structures to extract embedding from input feature.By doing so, nodes' information can be propagated in different ways.Then, the supervised loss \u2113 sup and diversity constraint \u2113 div are used to guide the training.We use two different branches to extract common information in topology and feature spaces.By utilizing disagreements between the two branches, model can gain information that may be ignored by single branch.To prove the effectiveness of our method, we conducted experiments on five public benchmark datasets.The contributions of our work are summarized as follows:\u2022 We propose a novel dual graph complementary network (DGCN) to fuse complementary information, which utilizes different graphs to aggregate nodes that are similar in certain attributes in a complementary way.\u2022 By comparing with algorithms that use non-single graphs, it proves that our complementary architecture can extract richer information\u2022 Through extensive evaluation on multiple datasets, we demonstrate DGCN effectiveness over state-of-the-art baselines."}
{"paper_id": 165, "abstract": "In the realm of deep learning, where the shadows of small datasets loom large, the quest for robust models often leads us to the vast oceans of pretraining on expansive datasets. These pretraining methods, while powerful in their ability to ward off the specter of overfitting, can sometimes leave us stranded in a sea of uncertainty when it comes to finding the right model for our specific tasks. In this paper, we unveil a novel approach: a self-pretraining method that harnesses the hidden potential of patch information within the dataset itself, liberating us from the need to rely on external sources. Our experiments reveal a remarkable truth: this self-pretraining method not only enhances performance but does so with a grace that outshines traditional training from scratch, even when we are confined to our own dataset. Join us as we explore this innovative path, illuminating the way forward in the challenging landscape of deep learning.", "introduction": "Transfer learning has become the de facto approach of doing deep learning tasks on small datasets.Because of the data-hungry nature of deep learning methods, training from scratch using small datasets usually got overfitting.Although transfer learning using models pretrained on additional large datasets mitigates the problem of overfitting, it is hard to find an appropriate pretrained model like the ImageNet-classification pretrained model which used in detection and segmentation tasks when the appearance of input data or the goal of task in the target domain is special.Research on training with small datasets without using external information has emerged in these years.Barz et al.(Barz & Denzler, 2020) proposed the method of training from scratch on small datasets using the cosine loss, which got substantially better performance than using the cross entropy loss function on fine-grained classification tasks.Zhang et al.(Zhang et al., 2019) introduced a generative adversarial network into the process of training with limited datasets without using external data or prior knowledge.In contrast to doing data augmentation or training using special loss functions on small dataset tasks, we proposed the self-pretraining method which transfers patch information in the dataset itself to the model in a weakly supervised manner.Patches in images can represent image information in some extent.In (Kang et al., 2014), Kang et.al predicted the image quality score using the average quality score of image patches which are trained by image-level quality labels.Also, BagNet (Brendel & Bethge, 2019) indicated that small image patches which contain the class evidence can do well in the ImageNet classification challenge by aggregating the their score in the image without considering the spatial order.In a case of the fine-grained classification, (Wang et al., 2017) extracted features of image patches by training with external large datasets in a weakly supervised way.Inspired by (Gatys et al., 2016) which pointed out that convolution neural networks get local features in lower layers and global structure features in higher layers, our self-pretraining method pretrains lower layers to higher layers in the model using image pathces with the incremental size step by step."}
{"paper_id": 166, "abstract": "In the ever-evolving landscape of machine learning, the specter of test-time adversarial attacks looms large, challenging the very foundations of model robustness. These attacks often defy the conventional boundaries of small lp-norm perturbations, revealing the need for more nuanced defenses. Inspired by the intricacies of semantics-preserving attacks within the realms of vision and security, we delve into the realm of relational adversaries\u2014a diverse and formidable class of attackers adept at crafting adversarial examples that reside within the reflexive-transitive closure of logical relations.  In our exploration, we rigorously analyze the conditions under which robustness can be achieved, leading us to propose a novel learning framework we term normalize-and-predict. This framework not only offers provable guarantees of robustness but also harmonizes the strengths of adversarial training with a unified approach that reaps the benefits of both strategies. Armed with our theoretical insights, we apply this framework to two critical domains: malware detection and image classification.  The results are telling. While traditional models often fall prey to the cunning tactics of relational adversaries, our unified framework emerges as a beacon of resilience, significantly bolstering their defenses. In this endeavor, we not only illuminate the path toward enhanced robustness but also forge a new understanding of the intricate dance between attackers and defenders in the machine learning arena.", "introduction": "The robustness of machine learning (ML) systems has been challenged by test-time attacks using adversarial examples (Szegedy et al., 2013).These adversarial examples are intentionally manipulated inputs that preserve the essential characteristics of the original inputs, and thus are expected to have the same test outcome as the originals by human standard; yet they severely affect the performance of many ML models across different domains (Moosavi-Dezfooli et al., 2016;Eykholt et al., 2018;Qin et al., 2019).As models in high-stake domains such as system security are also undermined by attacks (Grosse et al., 2017;Rosenberg et al., 2018;Hu & Tan, 2018), robust ML in adversarial test environment becomes an imperative task for the ML community.Existing work on test-time attacks predominately considers \ufffd p -norm bounded adversarial manipulation (Goodfellow et al., 2014;Carlini & Wagner, 2017).However, in many security-critical settings, the adversarial examples need not respect the \ufffd p -norm constraint as long as they preserve the malicious semantics.In malware detection, for example, a malware author can implement the same function using different APIs, or bind a malware within benign softwares like video games or office tools.The modified malware preserves the malicious functionality despite the drastically different syntactic features.Hence, focusing on adversarial examples of small \ufffd p -norm in this setting will fail to address a sizable attack surface that attackers can exploit to evade detectors.In addition to security threats, another rising concern on ML models is the spurious correlations they could have learned in a biased data set.Ribeiro et al. (2016) show that a highly accurate wolf-vshusky-dog classifier indeed bases its prediction on the presence/absence of snow in the background.A reliable model, in contrast, should be robust to changes of this nature.Although dubbed as semantic perturbation or manipulation (Mohapatra et al., 2020;Bhattad et al., 2019), these changes do not alter the core of the semantics of input data, thus, we still consider them to be semantics-preserving pertaining to the classification task.Since such semantics-preserving changes often resulted in large \ufffd p -norms, they are likely to render the existing \ufffd p -norm based defenses ineffective.In this paper, we consider a general attack framework in which attackers create adversarial examples by transforming the original inputs via a set of rules in a semantics-preserving manner.Unlike the prior works (Rosenberg et al., 2018;Hu & Tan, 2018;Hosseini et al., 2017;Hosseini & Poovendran, 2018) which investigate specific adversarial settings, our paper extends the scope of attacks to general logical transformation: we unify the threat models into a powerful relational adversary, which can readily incorporate more complex input transformations.From the defense perspective, recent work has started to look beyond \ufffd p -norm constraints, including adversarial training (Grosse et al., 2017;Rosenberg et al., 2019;Lei et al., 2019), verificationloss regularization (Huang et al., 2019) and invariance-induced regularization (Yang et al., 2019).Adversarial training in principle can achieve high robust accuracy when the adversarial example in the training loop maximizes the loss.However, finding such adversarial examples is in general NPhard (Katz et al., 2017), and we show in Sec 4 that it is even PSPACE-hard for semantics-preserving attacks that are considered in this paper.Huang et al. (2019) and Yang et al. (2019) add regularizers that incorporate model robustness as part of the training objective.However, such regularization can not be strictly enforced in training, and neither can the model robustness.These limitations still cause vulnerability to semantics-preserving attacks.Normalize-and-Predict Learning Framework This paper attempts to overcome the limitations of prior work by introducing a learning framework that guarantees robustness by design.In particular, we target a relational adversary, whose admissible manipulation is specified by a logical relation.A logical relation is a set of input pairs, each of which consists of a source and target of an atomic, semantics-preserving transformation.We consider a strong adversary who can apply an arbitrary number of transformations.Our paper makes the following contribution towards the theoretical understanding of robust ML against relational adversaries:1. We formally describe admissible adversarial manipulation using logical relations, and characterize the necessary and sufficient conditions for robustness to relational adversaries.2. We propose normalize-and-predict (hereinafter abbreviated as N&P), a learning framework that first converts each data input to a well-defined and unique normal form and then trains and classifies over the normalized inputs.We show that our framework has guaranteed robustness, and characterize conditions to different levels of robustness-accuracy trade-off.3. We compare N&P to the popular adversarial training framework, which directly optimizes for accuracy under attacks.We show that N&P has the advantage in terms of explicit robustness guarantee and reduced training complexity, and in certain cases yields the same model accuracy as adversarial training.Motivated by the comparison, we propose a unified framework, which selectively normalizes over relations that tend to preserve the model accuracy and adversarially trains over the rest.Our unified approach gets the benefits from both frameworks.We then apply our theoretical findings to malware detection and image classification.For the former, first, we formulate two types of common program transformation -(1) addition of redundant libraries and API calls, and (2) substitution of equivalent API calls -as logical relations.Next, we instantiate our learning framework to these relations, and propose two generic relational adversarial attacks to determine the robustness of a model.Finally, we perform experiments over Sleipnir, a real-world WIN32 malware data set.Regarding image classification, we reused an attack method proposed by the prior work (Hosseini & Poovendran, 2018) -shifting of the hue in the HSV color space -that can be deemed as a specific instantiation of our attack framework.We then compare the accuracy and robustness of ResNet-32 (He et al., 2016), a common image classification model, trained with the unified framework against the standard adversarial training on CIFAR-10 ( Krizhevsky et al., 2009).The results we obtained in both tasks show that:1. Attacks using addition and substitution suffice to evade existing ML malware detectors.2. Our unified approach using input normalization and adversarial training achieves highest robust accuracy among all baselines in malware detection.The drop in accuracy on clean inputs is small and the computation cost is lower than pure adversarial training.3. When trained with the unified learning framework, ResNet-32 achieves similar clean accuracy but significantly higher robust accuracy than adversarial training alone.Finally, based on our theoretical and empirical results, we conclude that input normalization is vital to robust learning against relational adversaries.We believe techniques that can improve the quality of normalization are promising directions for future work."}
{"paper_id": 167, "abstract": "In this exploration, we delve into the often-overlooked yet pivotal influence of point sampling patterns within the realm of point cloud Generative Adversarial Networks (GANs). Through a series of rigorous experiments, we unveil a critical distinction: sampling-insensitive discriminators, such as PointNet-Max, tend to produce shape point clouds marred by clustering artifacts, while their sampling-oversensitive counterparts\u2014PointNet++, DGCNN, PointConv, and KPConv\u2014struggle to steer the generation of coherent shapes. To illuminate this complexity, we introduce the notion of a \"sampling spectrum,\" a framework that characterizes the varying sensitivities of discriminators to different sampling strategies.  Our investigation further reveals how diverse evaluation metrics balance the interplay between sampling patterns and geometric fidelity. In this pursuit, we propose a suite of perceptual metrics that collectively form a comprehensive sampling spectrum. Guided by this newfound understanding, we unveil PointNet-Mix, a middle-ground discriminator that is attuned to sampling nuances, which significantly enhances the performance of existing point cloud generators across all sampling-related metrics.  As we reflect on the landscape of recent research\u2014where the spotlight has predominantly shone on generator architectures\u2014we assert that the design of discriminators deserves equal, if not greater, scrutiny. Our findings not only offer valuable insights but also provide practical tools for the development of future discriminators in this evolving field. To foster further exploration, we will release our code, paving the way for continued innovation and discovery.", "introduction": "Point cloud, as the most common form of 3D sensor data, has been widely used in a variety of 3D vision applications due to its compact yet expressive nature and its amenability to geometric manipulations.It is natural to consider how to generate point cloud through deep learning approaches, which has been a popular research topic recently.The previous research efforts in the community have been mainly devoted to conditional generation of point clouds with 3D supervision.The condition could either be images (Fan et al., 2017;Groueix et al., 2018;Park et al., 2019) or partial point clouds (Li et al., 2019;Yang et al., 2018).Generating 3D point clouds with GANs in an unsupervised manner is an important but less explored problem.3D point cloud GAN learns to transform a random latent code into a 3D surface point cloud by playing an adversarial game.Its development is still in an early stage compared with 2D image GANs.While existing works such as (Achlioptas et al., 2018;Valsesia et al., 2018;Shu et al., 2019) have developed a variety of generators, they all use PointNet (Qi et al., 2017a) with max pooling (PointNet-Max) as their discriminator.PointNet, which is essentially a pointwise MLP followed by a global pooling operation, is too limited in capturing shape details for a successful GAN.However, advanced networks, e.g.PointNet++(Qi et al., 2017b), DGCNN (Wang et al., 2019), KPConv (Thomas et al., 2019), PointConv (Wu et al., 2019), which leverage relative positions between points and hierarchical feature extraction, may not help.From our empirical study, we find they all fail to be a functioning discriminator.Understanding their failure mechanism and improving discriminator design are hence important and urgent.To design a better discriminator, we first need to answer the following question: what should the discriminator examine for improving the generation quality?Or, even more fundamentally, what does it mean by the quality of generated point clouds?Since a shape point cloud are the points sampled from an object surface, its quality should be evaluated from two perspectives: the depicted surface geometry and the point sampling.Arguably, geometry plays a decisive role and should be the main focus of a discriminator.However, when the generated point clouds have a good shape, there is still a full spectrum on how much a discriminator cares about the sampling patterns.We introduce the concept of Sampling Spectrum to depict the sampling sensitivity of discriminators, as illustrated in Figure 1.A sampling-insensitive discriminator (e.g.PointNet-Max) may ignore the point density variations as long as it perceives a good overall shape.Such a discriminator could identify big geometric flaws as shown in Figure 1 (a), but turns a blind eye to highly non-uniform density distribution, e.g. point clusters in Figure 1  Resembling the sampling spectrum of discriminators, we also examine the existing point-cloud GAN evaluation metrics from the perspective of sampling sensitivity and propose several perceptual metrics forming a Sampling Spectrum of evaluation metrics.Understanding how the metrics weigh between sampling and geometry is a prerequisite for evaluating point cloud GANs.Many of the existing sampling-insensitive metrics only evaluate the geometry factor of the generated point clouds shapes, which are blind to the obvious point clustering artifacts and uneven point density.We propose novel sampling-sensitive metrics to further complete the spectrum of point-cloud GAN evaluation metrics.Guided by the proposed sampling spectrum of discriminators and evaluation metrics, experiments show that different discriminators in the spectrum could provide very different suggestions to improve a generator according to its sampling sensitivity.Sampling-insensitive discriminators, e.g.PointNet-Max, is unaware of point density variations and hence its generated point cloud inevitably suffer from clustering artifacts, while sampling-oversensitive discriminators, e.g.PointNet++, DGCNN, KPConv, PointConv, simply fail to function as the discriminators and can generate much degraded point cloud shapes.We design a diagnostic \"no generator\" experiment to factor out the impact from generators and reveal that the gradients of sampling-oversensitive discriminators prioritize adjusting sampling patterns over producing better shape geometry.Picking a middle-point on the sampling spectrum, we discover a simple yet effective sampling-aware discriminator, PointNet-Mix, and find that it can supervise both shape generation and point density uniformity.It improves all existing generators by a large margin on sampling-related metrics.Surprisingly, we find that even the most naive fully-connected generator, coupled with PointNet-Mix, simply beats all the start-of-the-art point cloud GANs.This discovery conveys an important message to the community: instead of focusing on the generator design, people should invest more time into discriminator and seek for more powerful sampling-aware discriminators."}
{"paper_id": 168, "abstract": "In the ever-evolving realm of deep learning, a plethora of adaptive algorithms, such as AMSGrad and Radam, have emerged, each claiming to enhance the journey of convergence. Yet, despite their ingenuity, these modifications have not succeeded in accelerating the convergence rate of adaptive algorithms, leaving the quest for a truly superior algorithm shrouded in mystery.   In this work, we embark on a new path, proposing a fresh perspective on the design of the proximal function within adaptive algorithms\u2014a concept we call marginal regret bound minimization. This innovative approach leads us to a groundbreaking class of adaptive algorithms that not only achieves a state of marginal optimality but also holds the promise of converging at a pace unmatched by any existing counterparts in the long run.   Through rigorous theoretical examination and compelling empirical evidence, we unveil the superiority of this new class of algorithms, demonstrating their potential to reshape the landscape of deep learning. Join us as we explore this uncharted territory, where the boundaries of adaptive algorithms are pushed beyond their previous limits.", "introduction": "Accelerating the convergence speed of optimization algorithms is one main concern of the machine learning community.After stochastic gradient descent (SGD) was introduced, quite a few variants of SGD have become popular, such as momentum (Polyak, 1964) and AdaGrad (Duchi et al., 2011).Instead of directly moving parameters in the negative direction of the gradient, AdaGrad proposed to scale the gradient by a matrix, which was the matrix in the proximal function of the composite mirror descent rule (Duchi et al., 2011).The diagonal version of AdaGrad designed this matrix to be the square root of the global average of the squared gradients.Duchi et al. (2011) proved that this algorithm could be faster than SGD when the gradients were sparse.However, AdaGrad's performance is known to deteriorate when the gradients are dense, especially in high dimensional problems such as deep learning (Reddi et al., 2018).To tackle this issue, many new algorithms were proposed to boost the performances of AdaGrad.Most of these algorithms focused on changing the design of the matrix in the proximal function.For example, RMSProp (Tieleman & Hinton, 2012) and Adam (Kingma & Ba, 2015) changed the global average design in AdaGrad to the exponential moving average.However, Reddi et al. (2018) proved that such a modification had convergence issues in the presence of high frequency noises and added a max operation to the matrix of Adam, leading to the AMSGrad algorithm.Other modifications, such as Padam (Chen & Gu, 2018), AdaShift (Zhou et al., 2019), NosAdam (Huang et al., 2019), and Radam (Liu et al., 2019), were based on various designs of this matrix as well.However, all aforementioned works did not improve the convergence rate of AdaGrad and simply supported their designs using experiments and synthetic examples.A theoretical foundation for the design of this matrix that improves the convergence and guides future adaptive algorithms is very much needed.In this work, we bring new insights to the design of the matrix in the proximal function.In particular, our major contributions in this paper are listed as follows\u2022 We propose a new motivation for designing the proximal function in adaptive algorithms.Specifically, we have found a marginally optimal design, which is the best matrix at each time step through minimizing the marginal increment of the regret bound.\u2022 Based on our proposal of marginal regret bound minimization, we create a new class of adaptive algorithms, named as AMX.We prove theoretically that AMX can converge with a regret bound of size \u00d5( \u221a \u03c4 ), where \u03c4 is smaller than T .Such a regret bound is potentially much smaller than those of common adaptive algorithms and can make AMX converge much faster than any existing adaptive algorithms, depending on \u03c4 .In the worst case, we show it is at least as fast as AMSGrad and AdaGrad under the same assumptions \u2022 We evaluate AMX's empirical performance on different tasks in deep learning.All experiments show our algorithm can converge fast and achieve good testing performances."}
{"paper_id": 169, "abstract": "In the realm of meta-learning, the arcane art of model-based reinforcement learning (MBRL) has proven itself a formidable ally, wielding the power of high sample efficiency. Yet, a shadow looms over its prowess. Traditionally, the optimization of policies within these meta-learning frameworks has relied heavily on rollouts that hinge entirely on the accuracy of a predictive model\u2014a model that, when flawed, can lead to a disheartening decline in performance in the chaotic unpredictability of real-world environments.  In this manuscript, we unveil a crucial insight: performance degradation can be mitigated through the innovative use of branched meta-rollouts. Armed with this theoretical breakthrough, we introduce a new paradigm\u2014Meta-Model-based Meta-Policy Optimization, or M3PO. This method harnesses the potential of branched meta-rollouts to refine policy optimization like a master craftsman honing their blade.  Through rigorous experimentation, we demonstrate that M3PO not only stands resilient against the pitfalls of its predecessors but also surpasses existing meta reinforcement learning methodologies in the demanding arena of continuous-control benchmarks. In this journey, we chart a course toward a more robust future for MBRL in meta-learning, revealing the untapped potential that lies within the branches of our approach.", "introduction": "Reinforcement learning (RL) methods have achieved remarkable success in many decision-making tasks, such as playing video games or controlling robots (e.g., Gu et al. (2017); Mnih et al. (2015)).In conventional RL methods, when multiple tasks are to be solved, a policy is independently learned for individual tasks.In general, each learning requires millions of training samples from the environment.This independent learning with a large number of samples prevents conventional RL methods from being applied to practical multi-task problems (e.g., robotic manipulation problems involving grasping or moving different types of objects (Yu et al., 2019)).Meta-learning methods (Schmidhuber et al., 1996;Thrun & Pratt, 1998) have recently gained much attention as a promising solution to this problem (Finn et al., 2017).They learn a structure shared in the tasks by using a large number of samples collected across the parts of the tasks.Once learned, these methods can adapt quickly to new (or the rest of the) tasks with a small number of samples given.Meta-RL methods have previously been introduced into both model-free and model-based settings.For model-free settings, there are two main types of approaches proposed so far, recurrent-based policy adaptation (Duan et al., 2017;Mishra et al., 2018;Rakelly et al., 2019;Wang et al., 2016) and gradient-based policy adaptation (Al-Shedivat et al., 2018;Finn & Levine, 2018;Finn et al., 2017;Gupta et al., 2018;Rothfuss et al., 2019;Stadie et al., 2018).In these approaches, policies adapt to a new task by leveraging the history of past trajectories.Following previous work (Clavera et al., 2018), we refer to these adaptive policies as meta-policies in our paper.In these modelfree meta-RL methods, in addition to learning control policies, the learning of policy adaptation is also required (Mendonca et al., 2019).Thus, these methods require more training samples than conventional RL methods.For model-based settings, there have been relatively few approaches proposed so far.Saemundsson et al. (2018) and Perez et al. ( 2020) use a predictive model (i.e., a transition model) conditioned by a latent variable for model predictive control.Nagabandi et al. (2019a;b) introduced both recurrentbased and gradient-based meta-learning methods into model-based RL.In these approaches, the predictive models adapt to a new task by leveraging the history of past trajectories.In analogy to the meta-policy, we refer to these adaptive predictive models as meta-models in our paper.Generally, these model-based meta-RL approaches are more sample efficient than the model-free approaches.However, in these approaches, the meta-policy (or the course of actions) is optimized via rollouts relying fully on the meta-model.Thus, its performance in a real environment tends to degrade when the meta-model is inaccurate.In this paper, we address this performance degradation problem in model-based meta-RL.After reviewing related work (Section 2) and preliminaries (Section 3), we present our work by first formulating model-based meta-RL (Section 4).Model-based (and model-free) meta-RL settings have typically been formulated as special cases of solving partially observable Markov decision processes (POMDPs) (e.g., Duan et al. (2017); Killian et al. (2017); Perez et al. (2020)).In these special cases, specific assumptions, such as intra-episode task invariance, are additionally introduced.However, there are model-based meta-RL settings where such assumptions do not hold (e.g., Nagabandi et al. (2019a;b)).To include these settings into our scope, we formulate model-based meta-RL settings as solving POMDPs without introducing such additional assumptions.Then, we conduct theoretical analysis on its performance guarantee (Section 5).We first analyse the performance guarantee in full meta-model-based rollouts, which most of the previous model-based meta-RL methods hold.We then introduce the notion of branched meta-rollouts.Branched meta-rollouts are Dyna-style rollouts (Sutton, 1991) in which we can adjust the reliance on the meta-model and real environment data.We show that the performance degradation due to the meta-model error in the branched meta-rollouts is smaller than that in the full meta-model-based rollouts.On the basis of this theoretical analysis, we propose a practical model-based meta-RL method called Meta-Model-based Meta-Policy Optimization (M3PO) where the meta-model is used in the branched rollout manner (Section 6).Finally, we experimentally demonstrate that M3PO outperforms existing methods in continuous-control benchmarks (Section 7).We make the following contributions in both theoretical and empirical frontiers.Theoretical frontier: 1.Our work is the first attempt to provide a theoretical relation between learning the metamodel and the real environment performance.In the aforementioned model-based meta-RL literature, it has not been clear how learning the meta-model relates to real environment performance.Our theoretical analysis provides relations between them (Theorems 1, 2 and 3).This result theoretically justifies meta-training a good transition model to improve overall performance in the real environment.2. Our analysis also reveals that the use of branched meta-rollouts can suppress performance degradation due to meta-model errors.3. We refine previous fundamental theories proposed by Janner et al. (2019) to consider important premises more properly (Theorems 4 and 5).This modification is important to strictly guarantee the performance especially when the model-rollout length is long.Empirical frontier: We propose and show the effectiveness of M3PO.Notably, we show that M3PO achieves better sample efficiency than existing meta-RL methods in complex tasks, such as controlling humanoids."}
{"paper_id": 170, "abstract": "In the realm of deep learning, where the training of neural networks often teeters on the brink of chaos, normalization techniques emerge as steadfast allies, wielding the power to stabilize and hasten the learning process. Yet, like many heroes in a tale of adventure, these methods have their limitations. They are crafted primarily for the predictable landscapes of independent and identically distributed (IID) data, leaving them ill-equipped to navigate the treacherous terrains of real-world out-of-distribution (OOD) scenarios.  In a departure from the conventional paths trodden by previous scholars, this paper unveils two innovative normalization techniques: SelfNorm and CrossNorm. These methods are designed not just to adapt but to thrive amidst the unpredictability of OOD environments. SelfNorm harnesses the arcane art of attention to recalibrate channel-wise statistics, adjusting means and variances with precision. Meanwhile, CrossNorm boldly ventures beyond the individual, facilitating an exchange of statistical knowledge between feature maps, a true collaboration of insights.  Together, SelfNorm and CrossNorm form a powerful duo, each illuminating different facets of statistical application, yet complementing one another in their quest for enhanced OOD generalization. Our extensive experiments, spanning diverse domains\u2014ranging from the visual to the linguistic\u2014across a variety of tasks, including classification and segmentation, and encompassing both supervised and semi-supervised settings, demonstrate their formidable effectiveness. In this narrative of discovery, we invite you to explore the transformative potential of these techniques as they reshape the landscape of deep learning.", "introduction": "Normalization methods, e.g., Batch Normalization (Ioffe & Szegedy, 2015), Layer Normalization (Ba et al., 2016), and Instance Normalization (Ulyanov et al., 2016), play a pivotal role in training deep neural networks.Most of them try to make training more stable and convergence faster, assuming that training and test data come from the same distribution.However, few studies investigate normalization in improving OOD generalization in real-world scenarios.For example, image corruptions (Hendrycks & Dietterich, 2019), e.g., snow and blur, can cause test data out of the clean training distribution.Moreover, training on synthetic data (Richter et al., 2016) to generalize to realistic data can significantly reduce the annotation burden.This work aims to encourage the interaction between normalization and OOD generalization.Specifically, we manipulate feature mean and variance to make models generalize better to out-of-distribution data.Our inspiration comes from the observation that channel-wise mean and variance of feature maps carry some style information.For instance, exchanging the RGB means and variances between two instances can transfer style between them, as shown in Figure 1 (a).For many tasks such as CIFAR classification (Krizhevsky et al., 2009), the style encoded by channel-wise mean and variance is usually less critical in recognizing the object than other information such as object shape.Therefore, we propose CrossNorm that swaps the channel-wise mean and variance of feature maps.CrossNorm can augment styles in training, making the model more robust to appearance changes.Furthermore, given one image in different styles, we can reduce their style discrepancy if adjusting their RGB means and variances properly, as illustrated in Figure 1 (b).Intuitively, the style recalibration can reduce appearance variance, which may be useful in bridging distribution gaps between training and unforeseen testing data.To this end, we propose SelfNorm to use attention (Hu et al., 2018) to adjust channel-wise mean and variance automatically.It is interesting to analyze the distinction and connection between CrossNorm and SelfNorm.At first glance, they take opposite actions (style augmentation v.s.style reduction).Even so, they use the same tool: channel-wise statistics and pursue the same goal: OOD robustness.Additionally, CrossNorm can increase the capacity of SelfNorm by style augmentation.SelfNorm, with the help from CrossNorm, can generalize better to OOD data.The style concept here refers to a family of weak cues associated with the semantic content of interest.For instance, the image style in object recognition can include many appearance-related factors such as color, contrast, and brightness.Style sometimes may help in decision-making, but the model should weigh more on more vital content cues to become robust.To reduce its bias rather than discard it, we use CrossNorm with probability in training.The insight beneath CrossNorm is that each instance, or feature map, has its unique style.Further, style cues are not equally important.For example, the yellow color seems more useful than other style cues in recognizing orange.In light of this, the intuition behind SelfNorm is that attention may help emphasize essential styles and suppress trivial ones.Assumption.Although we use the channel-wise mean and variance to modify styles, we do not assume that they are sufficient to represent all style cues.Better style representations are available with more complex statistics (Li et al., 2017) or even style transfer models (Ulyanov et al., 2017;Huang & Belongie, 2017).We choose the first and second-order statistics mainly because they are simple, efficient to compute, and can connect normalization to out-of-distribution generalization.In summary, the key contributions are:\u2022 We propose SelfNorm and CrossNorm, two simple yet effective normalization techniques to enhance out-of-distribution generalization.\u2022 SelfNorm and CrossNorm form a unity of opposites in using feature mean and variance for model robustness.\u2022 They are domain agnostic and can advance state-of-the-art robustness performance for different domains (vision or language), settings (fully or semi-supervised), and tasks (classification and segmentation)."}
{"paper_id": 171, "abstract": "In the ever-evolving landscape of multilingual representation learning, recent explorations have unveiled remarkable strides in performance across a multitude of cross-lingual tasks. Traditionally, researchers have opted for either an encoder-only Transformer, tailored for understanding tasks, or an encoder-decoder Transformer, designed specifically for generation tasks. Yet, this bifurcation overlooks the intricate relationship between these two realms and their respective frameworks.   Enter the Variable Encoder-Decoder (VECO) pre-training approach\u2014a groundbreaking method that seeks to harmonize these two dominant paradigms, merging both model architectures and pre-training tasks into a cohesive whole. VECO innovatively disassembles the conventional Transformer block into distinct sub-modules, each meticulously trained through both inner-sequence and cross-sequence masked language modeling. During inference, these sub-modules are deftly reorganized to cater to the nuances of understanding and generation tasks alike.  This ingenious workflow not only streamlines the training of essential parameters tailored for both task types but also fosters an environment where they mutually enhance one another through shared sub-modules. The results speak for themselves: VECO sets new benchmarks for cross-lingual understanding tasks within the XTREME framework, excelling in text classification, sequence labeling, question answering, and sentence retrieval. Moreover, in the realm of generation, VECO surpasses all existing cross-lingual models and state-of-the-art Transformer variants, achieving notable gains of 1 to 2 BLEU points on the WMT14 English-to-German and English-to-French translation datasets. This synthesis of understanding and generation is not merely an advancement; it is a leap into a new era of multilingual modeling.", "introduction": "Driven by the striking success of pre-trained language models (Devlin et al., 2019), recent crosslingual pre-training (Lample & Conneau, 2019;Liu et al., 2020b) has attracted an increasing of attention.It provides cross-lingual contextualized representations for the inputs of different languages, which significantly advances performance in both natural language understanding (NLU) and generation (NLG) tasks.There are two mainstream architectures in current cross-lingual pre-training literature: encoderonly and encoder-decoder.The former like XLM (Lample & Conneau, 2019) focuses on conducting masked language modeling (MLM) with a single Transformer (Vaswani et al., 2017) encoder.This paradigm is naturally compatible with various NLU tasks, but tends to suffer from limited gains on cross-lingual generation tasks (e.g., machine translation) due to the lack of effective decoder initialization.In contrast, the latter like mBART (Liu et al., 2020b) attempts to pre-train the encoder-decoder Transformer via denoising auto-encoding tasks to provide complete initialization for downstream generation tasks.However, when applied in NLU scenarios, it usually requires more computation and memory to match the performance of the encoder-only models.In light of the above pros and cons, this work presents Variable Encoder-deCOder (VECO) pretraining, which targets at providing pre-trained model initialization for both the encoder-only and encoder-decoder Transformer with the most streamlined parameters.We observe that Transformer encoder and decoder blocks have two common modules: SelfAttention and FFN (feed-forward network), with the main difference that the latter introduces an extra CrossAttention (attention across from the encoder to the decoder) module.Inspired by the lottery ticket hypothesis (Frankle & Carbin, 2018), we split the standard Transformer block into three independent modules During pre-training, we feed two masked segments x and \u0177 into different modules to perform inner-sentence mask language modeling (IS-MLM) and cross-sentence mask language modeling (CS-MLM).More specifically, the masked segment x can only attend to its context via self-attention to recover the original tokens x (IS-MLM), while masked segment \u0177 can attend to its preceding tokens via self-attention and the context x via cross-attention to predict the original tokens \u0233 (CS-MLM).For downstream NLU tasks, we throw out the cross-attention module and only fine-tune on the self-attention and FFN modules acted as an encoder.For NLG tasks, we keep all modules to initialize the corresponding encoder and decoders.{SelfAttention, CrossAttention, FFN} to be collaboratively trained via two specific MLM tasks.After that, we rebuild the desired complete architecture applicable for NLU or NLG with different specific combinations of these modules during fine-tuning.foot_0Specifically, to be equipped with the ability of language understanding during pre-training, SelfAttention and FFN are assembled into a standard Transformer encoder for conducting inner-sequence masked language modeling (IS-MLM).In terms of generation, SelfAttention, CrossAttention, and FFN act together as the decoder in the standard sequence-to-sequence model, and are trained by the elaborately designed cross-sequence masked language modeling (CS-MLM) task.When applied to downstream finetuning, both SelfAttention and FFN modules constitute the Transformer encoder for contextual modeling in NLU or NLG, or cooperate with additional CrossAttention to provide the effective initialization of Transformer decoder.With such kind of workflow, VECO can be applied to both NLU and NLG tasks with the most streamlined parameters, which significantly reduces computational overhead and memory costs.Moreover, IS-MLM is specifically designed for understanding of individual sequences, while both understanding and generation tasks can benefit from CS-MLM.With such parameter sharing, VECO enables SelfAttention and FFN modules to be jointly trained by the two MLMs, which boosts both NLU and NLG performance.We validate VECO on a variety of representative cross-lingual NLU and NLG benchmarks.For cross-lingual understanding tasks, we conduct experiments on the XTREME benchmark consisting of 9 cross-lingual tasks, including text classification, sequence labeling, question answering, and sentence retrieval.VECO ranks first at the XTREME leaderboardfoot_1 at the submission deadline and obtains new state-of-the-art results on most of the tasks.For cross-lingual generation tasks, we validate VECO on the widely used WMT14 English-German and English-French machine translation benchmarks.VECO obtains 44.4 and 31.5 BLEU scores, consistently outperforming existing crosslingual pre-training approaches and state-of-the-art Transformer variants by around 1\u223c2 BLEU."}
{"paper_id": 172, "abstract": "In the vast realm of Neural Architecture Search (NAS), the quest for the most optimal network architecture often feels akin to traversing an uncharted landscape, fraught with computational perils. Traditional approaches delve into the architecture-to-performance manifold, tirelessly training and evaluating a multitude of architectures, leading to an overwhelming expenditure of resources. However, a new dawn breaks with the introduction of predictor-based NAS methodologies, which seek to chart a more efficient course through this labyrinthine space.  At the heart of our exploration lies a pivotal realization: the ambition of modeling the entire performance distribution across the architecture spectrum may be an insurmountable challenge when constrained by limited samples. Instead, we propose a paradigm shift\u2014one that embraces the notion that the ultimate goal is not to capture every nuance of the space, but to unearth the singular best architecture. Our innovative framework introduces a strategy of progressive estimation through a series of weak predictors, each serving as a stepping stone toward the coveted pinnacle of performance.  Rather than relying on a solitary, robust predictor to encapsulate the entirety of the architecture landscape, we embark on a journey of incremental refinement. This approach hinges on the fundamental characteristic of our predictors: their propensity to increasingly favor the sampling of superior architectures. By judiciously selecting a handful of high-performing architectures, guided by our predictive model, we forge ahead to develop an improved weak predictor. This iterative process, reminiscent of a master craftsman honing their skills, gradually sharpens the sampling space, ultimately illuminating the path to optimal architectures.  Our experimental results stand as testament to the efficacy of this method, revealing a remarkable reduction in the number of samples required to identify top-performing architectures within the NAS-Bench-101 and NAS-Bench-201 frameworks. Moreover, our approach achieves state-of-the-art performance on ImageNet within the NASNet search space, heralding a new era in the pursuit of architectural excellence.", "introduction": "Neural Architecture Search (NAS) has become a central topic in recent years with great progress (Liu et al., 2018b;Luo et al., 2018;Wu et al., 2019;Howard et al., 2019;Ning et al., 2020;Wei et al., 2020;Luo et al., 2018;Wen et al., 2019;Chau et al., 2020;Luo et al., 2020).Methodologically, all existing NAS methods try to find the best network architecture by exploring the architecture-toperformance manifold, such as reinforced-learning-based (Zoph & Le, 2016), evolution-based (Real et al., 2019) or gradient-based Liu et al. (2018b) approaches.In order to cover the whole space, they often train and evaluate a large amount of architectures, thus causing tremendous computation cost.Recently, predictor-based NAS methods alleviate this problem with two key steps: one sampling step to sample some architecture-performance pairs, and another performance modeling step to fit the performance distribution by training a proxy accuracy predictor.An in-depth analysis of existing methods (Luo et al., 2018) founds that most of those methods (Ning et al., 2020;Wei et al., 2020;Luo et al., 2018;Wen et al., 2019;Chau et al., 2020;Luo et al., 2020) attempt to model the performance distribution over the whole architecture space.However, since the architecture space is often exponentially large and highly non-convex, modeling the whole space is very challenging especially given limited samples.Meanwhile, different types of predictors in these methods have to demand handcraft design of the architecture representations to improve the performance.In this paper, we envision that the ambitious goal of modeling the whole space may not be necessary if the final goal is to find the best architecture.Intuitively, we assume the whole space could be divided into different sub-spaces, some of which are relatively good while some are relatively bad.We tend to choose the good ones while neglecting the bad ones, which makes sure more samples will be used to model the good subspace precisely and then find the best architecture.From another perspective, instead of optimizing the predictor by sampling the whole space as well as existing methods, we propose to jointly optimize the sampling strategy and the predictor learning, which helps achieve better sample efficiency and prediction accuracy simultaneously.Based on the above motivation, we present a novel framework that estimates a series of weak predictors progressively.Rather than expecting a strong predictor to model the whole space, we instead seek a progressive evolving of weak predictors that can connect a path to the best architecture.In this way, it greatly simplifies the learning task of each predictor.To ensure moving the best architecture along the path, we increase the sampling probability of better architectures guided by the weak predictor at each iteration.Then, the consecutive weak predictor with better samples will be trained in the next iteration.We iterate until we arrive at an embedding subspace where the best architectures reside.The weak predictor achieved at the final iteration becomes the dedicated predictor focusing on such a fine subspace and the best performed architecture can be easily predicted.Compared to existing predictor-based NAS, our method has several merits.First, since only weak predictors are required to locate the good subspace, it yields better sample efficiency.On NAS-Benchmark-101 and NAS-Benchmark-201, it costs significantly fewer samples to find the top-performance architecture than existing predictorbased NAS methods.Second, it is much less sensitive to the architecture representation (e.g., different architecture embeddings) and the predictor formulation design (e.g., MLP, Gradient Boosting Regression Tree, Random Forest).Experiments show our superior robustness in all their combinations.Third, it is generalized to other search spaces.Given a limited sample budget, it achieves the state-of-the-art ImageNet performance on the NASNet search space."}
{"paper_id": 173, "abstract": "In a world brimming with visual complexity, humans possess an uncanny ability to pinpoint objects from mere image-level cues, even when no bounding boxes guide their gaze. Yet, the creation of reliable object detectors\u2014a task that typically demands costly instance-level annotations\u2014remains a formidable challenge. While strides have been made in harnessing weakly labeled samples, which offer only class labels, these efforts often stumble when it comes to precise localization. Here, we unveil a novel approach to enhance object detection from weakly annotated images of new categories, drawing upon the rich knowledge gleaned from fully labeled base categories. We introduce the concept of cross-supervised object detection, a paradigm previously explored but underutilized in the realm of intricate, realistic images like those found in the COCO dataset. Our unified framework ingeniously intertwines a detection head honed through instance-level annotations with a recognition head crafted from image-level insights, all fortified by a spatial correlation module that deftly connects detection and recognition. Through these innovative contributions, we empower the detection of novel objects within the chaos of complex multi-object scenes, pushing the boundaries of what weakly labeled data can achieve.", "introduction": "Deep architectures have achieved great success in many computer vision tasks including object recognition and the closely related problem of object detection.Modern detectors, such as the Faster RCNN (Ren et al., 2015), YOLO (Redmon et al., 2016), and RetinaNet (Lin et al., 2017), use the same network backbone as popular recognition models.However, even with the same backbone architectures, detection and recognition models require different types of supervision.A good detector relies heavily on precise bounding boxes and labels for each instance (we shall refer to these as instance-level annotations), whereas a recognition model needs only image-level labels.Needless to say, it is more time consuming and expensive to obtain high quality bounding box annotations than class labels.As a result, current detectors are limited to a small set of categories relative to their object recognition counterparts.To address this limitation, it is natural to ask, \"Is it possible to learn detectors with only class labels?\"This problem is commonly referred to as weakly supervised object detection (WSOD).Early WSOD work (Hoffman et al., 2014) showed fair performance by directly applying recognition networks to object detection.More recently, researchers have used multiple instance learning methods (Dietterich et al., 1997) to recast WSOD as a multi-label classification problem (Bilen & Vedaldi, 2016).However, these weakly supervised detectors perform poorly at localization.Most WSOD experiments have been conducted on the ILSVRC (Russakovsky et al., 2015) data set, in which images have only a single object, or on the PASCAL VOC (Everingham et al., 2010) data set, which has only 20 categories.The simplicity of these data sets limits the number and types of distractors in an image, making localization substantially easier.Learning from only class labels, it is challenging to detect objects at different scales in an image that contains many distractors.In particular, as shown in our experiments, weakly supervised object detectors do not work well in complex multi-object scenes, such as the COCO dataset (Lin et al., 2014).To address this challenge, we focus on a form of learning in which the localization of classes with only object labels (weakly labeled classes) can benefit from other classes that have ground truth bounding boxes (fully labeled classes).We refer to this interesting learning paradigm as crosssupervised object detection (CSOD).While several works (Hoffman et al., 2014;Tang et al., 2016;Yang et al., 2019a;Redmon & Farhadi, 2017) have explored this problem before, they still have the same limitation as the WSOD work we mentioned above.Those cross-supervised object detectors work under simplified scenarios (e.g., ILSVRC data set) where images contain single objects and are object-centered.They struggle to learn under more complex and realistic scenarios, where there are multiple objects from potentially very different classes, and objects could be small and appear anywhere in the images.In this work, we show that by doing multi-task learning on both weaklysupervised base classes and fully-supervised novel classes, our model is able to learn a good detector under the CSOD setting.More formally, we define CSOD as follows.At training time, we are given 1) images contain objects from both base and novel classes, 2) both class labels and ground truth bounding boxes for base objects, and 3) only class labels for novel objects.Our goal is to detect novel objects.In CSOD, base classes and novel classes are disjoint.Thus, it can be seen as performing fullysupervised detection on the base classes and weakly supervised detection on the novel classes.It has similarities to both transfer learning and semi-supervised learning, since it transfer knowledge from base class to novel class and have more information about some instances than other instances.However, CSOD represents a distinct and novel paradigm for learning.The current weakly-supervised method has several drawbacks to learn from a multi objects image.As shown in Fig. 1, a weakly supervised object detector tends to detect only the most discriminating part of novel objects instead of the whole object.Notice how only the head of the person, and not the whole body, is detected.Another issue is that the localizer for one object (e.g., the horse) may be confused by the occurrence of another object, such as the person on the horse.This example illustrates the gap between detection and recognition: without ground truth bounding boxes, the detector acts like a standard recognition model -focusing on discriminating rather than detecting.In this paper, we explore two major mechanisms for improving on this.Our first mechanism is unifying detection and recognition.Using the same network backbone architecture, recognition and detection can be seen as image-level classification and region-level classification respectively, suggesting a strong relation between them.In particular, it suggests a shared training framework in which the same backbone is used with different heads for detection and recognition.Thus, we combine a detection head learned from ground truth bounding boxes, and a recognition head learned in a weakly supervised fashion from class labels.Unlike a traditional recognition head, our recognition head produces a class score for multiple proposals and is capable of detecting objects.The second mechanism is learning a spatial correlation module to reduce the gap between detection and recognition.It takes several high-confidence bounding boxes produced by the recognition head as input, and learns to regress ground truth bounding boxes.By combining these mechanisms together, our model outperforms all previous models when all novel objects are weakly labeled.In summary, our contributions are three-fold.First, we define a new task-cross-supervised object detection, which enables us to leverage knowledge from fully labeled base categories to help learn a robust detector from novel object class labels only.Second, we propose a unified framework in which two heads are learned from class labels and detection labels respectively, along with a spatial correlation module bridging the gap between recognition and detection.Third, we significantly outperform existing methods (Zhang et al. (2018a); Tang et al. (2017;2018)) on PASCAL VOC and COCO, suggesting that CSOD could be a promising approach for expanding object detection to a much larger number of categories."}
{"paper_id": 174, "abstract": "In the ever-evolving realm of Natural Language Processing, the Transformer model has risen to prominence, achieving unparalleled success across numerous tasks. Yet, lurking within its architecture lies a formidable challenge: the Feed Forward Network (FFN) embedded within each Transformer block, which proves to be a computational behemoth. In our exploration, we unveil a groundbreaking framework that metamorphoses Recurrent Neural Networks (RNNs) and their variants into self-attention-style models, drawing inspiration from the elegant principles of the Banach Fixed-point Theorem.  At the heart of this endeavor is StarSaber, a novel model crafted from the intricate equations derived from RNNs through the lens of the Fixed-point Theorem. By approximating these equations with a Multi-layer Perceptron, we offer a fresh perspective on the stacking of layers, breathing new life into the architecture. StarSaber not only eclipses the performance of the traditional Transformer but also surpasses its enhanced counterpart, ReZero, across three diverse datasets\u2014all while boasting remarkable computational efficiency, thanks to a streamlined FFN layer.  StarSaber is constructed upon two pivotal innovations. First, it employs a dual-matrix approach to encode positional information, with one matrix governing the positions preceding a given point in the sequence and another addressing those that follow. Second, we introduce direct pathways that connect the input layer to subsequent layers, fostering a more fluid flow of information. Rigorous ablation studies validate the efficacy of these components, illuminating their contributions to the model's success. Moreover, we demonstrate that this transformative approach extends to other RNN variants, including those equipped with gating mechanisms, allowing them to outshine both traditional and advanced Transformers alike. In this intricate dance of innovation, StarSaber emerges not only as a testament to the power of reimagining existing architectures but also as a beacon of efficiency in the vast landscape of machine learning.", "introduction": "Recurrent Neural Network, known as RNN, has been widely applied to various tasks in the last decade, such as Neural Machine Translation (Kalchbrenner & Blunsom, 2013;Sutskever et al., 2014), Text Classification (Zhou et al., 2016), Name Entity Recognition (Zhang & Yang, 2018;Chiu & Nichols, 2016), Machine Reading Comprehension (Hermann et al., 2015;Kadlec et al., 2016) and Natural Language Inference (Chen et al., 2017;Wang et al., 2017).Models applied to these tasks are not the vanilla RNNs but two of their famous variants, Gated Recurrent Unit (Cho et al., 2014), known as GRU, and Long Short Term Memory (Hochreiter & Schmidhuber, 1997), known as LSTM, in which gates play an important role.RNNs are hard to be computed parallelly.They are not bidirectional either, meaning that a word cannot utilize the information of words coming after it.A general way to alleviate this problem is to reverse the input sequence and combine results given by two different RNN encoders with operations like concatenation and addition.However, Transformer (Vaswani et al., 2017) has provided a better solution.It is based on purely attention mechanism, which has been widely used in Neural Machine Translation since Bahdanau et al. (2014).Models based on self-attention mechanism are mostly Transformer and its variants, such as Transformer-XL (Dai et al., 2019), Universal Transformer (Dehghani et al., 2019) and Star-Transformer (Guo et al., 2019).Compared with recurrent units such as GRU and LSTM, self-attention-style models can be computed parallelly, which means they suit better large-scale training.But each of these Transformers has an FFN layer with a very high vector dimension, which still is the bottleneck to improve the computation efficency.In this paper, we present a new framework based on Banach Fixed-point Theorem to transform the vanilla RNN and its variants with self-attention mechanism.StarSaber, one of such transformed models, outperforms both the vanilla Transformer and ReZero (Bachlechner et al., 2020) in our experiments with less parameters and thus less computational power.To start with, we need a different view of attention.Attention is a way to build a relation graph between words, and the vanilla RNN is nothing but a model with a relation graph as a chain.This graph is in fact represented with an adjacent matrix, which is computed by mapping each pair of positions to a positive real number and normalizing the numbers related to each position, which are just those in the same row of the adjacent matrix, so that they sum up to one.The vanilla RNN updates hidden states through a chain, that is, the hidden state for each position only depends on that in the previous position.However, if we have this relation graph, the hidden state for each position depends on hidden states for all other positions in a sequence.This is where we obtain equations.In our opinion, a bidirectional RNN is defined by some equations and Banach Fixed-point Theorem inspires us to iterate according to them.When we fix the number of iterations and specify distinct weights for each of them, a self-attention-style model is then constructed.In Transformer, Position Embedding(PE) as a way to capture word order information in language by adding a matrix to the input, is indispensable.But in StarSaber, position encoding is done in the aggregation step after the construction of a relation graph.For each position, we sum up linear transformations of hidden states in all positions with the corresponding weights in the relation matrix in order to get an attention vector.In the calculation of such a vector, we specify different linear transformation weights for the \"future\" and the \"past\".Then the hidden vector for a position is computed with the corresponding attention vector and an input vector, which turns into a direct path from the input layer to each hidden layer.And we directly drop the FFN layer in Transformer achieving still competive and even better results with much less parameters on three datasets provided by CLUE (Xu et al., 2020): the AFQMC dataset of Sentence Similarity, the TNEWS dataset of Text Classification and the CMNLI dataset of Natural Language Inference.More importantly, our derivation of StarSaber shows a universal way to transform different RNNs, such as LSTM and GRU discussed in the following content, providing possibilities other than Transformers for self-attention models."}
{"paper_id": 175, "abstract": "In the realm of deep reinforcement learning, where the quest for mastery often collides with the limitations of reward-driven feature learning from images, we embark on a bold new path. We propose a radical separation of representation learning from the intricate dance of policy learning. At the heart of our approach lies a novel unsupervised learning task, which we have dubbed Augmented Temporal Contrast (ATC). This innovative task trains a convolutional encoder to forge connections between pairs of observations that are merely moments apart, all while navigating the complexities of image augmentations and employing a contrastive loss.  Through rigorous online RL experiments, we unveil a striking revelation: when the encoder is trained solely through ATC, it either matches or surpasses the performance of traditional end-to-end RL methods across a multitude of environments. Our exploration doesn\u2019t stop there. We pit several leading unsupervised learning algorithms against one another, pre-training encoders on expert demonstrations and then deploying them\u2014weights frozen\u2014within RL agents. The results are clear: agents utilizing ATC-trained encoders emerge victorious, outshining all competitors.  But our journey of discovery continues. We venture into the realm of multi-task encoders, trained on diverse datasets drawn from various environments, demonstrating remarkable generalization to a spectrum of downstream RL tasks. In our quest for understanding, we meticulously ablate the components of ATC and unveil a groundbreaking data augmentation technique that allows for the replay of (compressed) latent images from pre-trained encoders, enhancing RL\u2019s adaptability when faced with the need for augmentation.  Our experiments traverse a visually rich landscape of RL benchmarks, including the likes of DeepMind Control, DeepMind Lab, and Atari. The full breadth of our findings and methodologies is encapsulated in our comprehensive code, available for all to explore at \\url{hidden url}.", "introduction": "Ever since the first fully-learned approach succeeded at playing Atari games from screen images (Mnih et al., 2015), standard practice in deep reinforcement learning (RL) has been to learn visual features and a control policy jointly, end-to-end.Several such deep RL algorithms have matured (Hessel et al., 2018;Schulman et al., 2017;Mnih et al., 2016;Haarnoja et al., 2018) and have been successfully applied to domains ranging from real-world (Levine et al., 2016;Kalashnikov et al., 2018) and simulated robotics (Lee et al., 2019;Laskin et al., 2020a;Hafner et al., 2020) to sophisticated video games (Berner et al., 2019;Jaderberg et al., 2019), and even high-fidelity driving simulators (Dosovitskiy et al., 2017).While the simplicity of end-to-end methods is appealing, relying on the reward function to learn visual features can be severely limiting.For example, it leaves features difficult to acquire under sparse rewards, and it can narrow their utility to a single task.Although our intent is broader than to focus on either sparse-reward or multi-task settings, they arise naturally in our studies.We investigate how to learn visual representations which are agnostic to rewards, without degrading the control policy.A number of recent works have significantly improved RL performance by introducing auxiliary losses, which are unsupervised tasks that provide feature-learning signal to the convolution neural network (CNN) encoder, additionally to the RL loss (Jaderberg et al., 2017;van den Oord et al., 2018;Laskin et al., 2020b;Guo et al., 2020;Schwarzer et al., 2020).Meanwhile, in the field of computer vision, recent efforts in unsupervised and self-supervised learning (Chen et al., 2020;Grill et al., 2020;He et al., 2019) have demonstrated that powerful feature extractors can be learned without labels, as evidenced by their usefulness for downstream tasks such as ImageNet classification.Together, these advances suggest that visual features for RL could possibly be learned entirely without rewards, which would grant greater flexibility to improve overall learning performance.To our knowledge, however, no single unsupervised learning (UL) task has been shown adequate for this purpose in general vision-based environments.In this paper, we demonstrate the first decoupling of representation learning from reinforcement learning that performs as well as or better than end-to-end RL.We update the encoder weights using only UL and train a control policy independently, on the (compressed) latent images.This capability stands in contrast to previous state-of-the-art methods, which have trained the UL and RL objectives jointly, or Laskin et al. (2020b), which observed diminished performance with decoupled encoders.Our main enabling contribution is a new unsupervised task tailored to reinforcement learning, which we call Augmented Temporal Contrast (ATC).ATC requires a model to associate observations from nearby time steps within the same trajectory (Anand et al., 2019).Observations are encoded via a convolutional neural network (shared with the RL agent) into a small latent space, where the InfoNCE loss is applied (van den Oord et al., 2018).Within each randomly sampled training batch, the positive observation, o t+k , for every anchor, o t , serves as negative for all other anchors.For regularization, observations undergo stochastic data augmentation (Laskin et al., 2020b) prior to encoding, namely random shift (Kostrikov et al., 2020), and a momentum encoder (He et al., 2020;Laskin et al., 2020b) is used to process the positives.A learned predictor layer further processes the anchor code (Grill et al., 2020;Chen et al., 2020) prior to contrasting.In summary, our algorithm is a novel combination of elements that enables generic learning of the structure of observations and transitions in MDPs without requiring rewards or actions as input.We include extensive experimental studies establishing the effectiveness of our algorithm in a visually diverse range of common RL environments: DeepMind Control Suite (DMControl; Tassa et al. 2018), DeepMind Lab (DMLab;Beattie et al. 2016), andAtari (Bellemare et al., 2013).Our experiments span discrete and continuous control, 2D and 3D visuals, and both on-policy and off policy RL algorithms.Complete code for all of our experiments is available at hiddenurl.Our empirical contributions are summarized as follows:Online RL with UL: We find that the convolutional encoder trained solely with the unsupervised ATC objective can fully replace the end-to-end RL encoder without degrading policy performance.ATC achieves nearly equal or greater performance in all DMControl and DMLab environments tested and in 5 of the 8 Atari games tested.In the other 3 Atari games, using ATC as an auxiliary loss or for weight initialization still brings improvements over end-to-end RL.Encoder Pre-Training Benchmarks: We pre-train the convolutional encoder to convergence on expert demonstrations, and evaluate it by training an RL agent using the encoder with weights frozen.We find that ATC matches or outperforms all prior UL algorithms as tested across all domains, demonstrating that ATC is a state-of-the-art UL algorithm for RL.Multi-Task Encoders: An encoder is trained on demonstrations from multiple environments, and is evaluated, with weights frozen, in separate downstream RL agents.A single encoder trained on four DMControl environments generalizes successfully, performing equal or better than end-to-end RL in four held-out environments.Similar attempts to generalize across eight diverse Atari games result in mixed performance, confirming some limited feature sharing among games.Ablations and Encoder Analysis: Components of ATC are ablated, showing their individual effects.Additionally, data augmentation is shown to be necessary in DMControl during RL even when using a frozen encoder.We introduce a new augmentation, subpixel random shift, which matches performance while augmenting the latent images, unlocking computation and memory benefits."}
{"paper_id": 176, "abstract": "In the realm of graph matching (GM), the traditional approach has long been a rigid, deterministic optimization problem, bound by the constraints of an affinity matrix and a pre-defined graph topology. While the quest for more effective node-level affinities and representations has seen numerous attempts, these methods remain shackled to an initial graph structure\u2014often derived through heuristic means like Delaunay triangulation or $k$-nearest neighbors. This static foundation is rarely adjusted during the learning process, leaving it ill-equipped to adapt to the nuanced patterns specific to each problem.  We contend that relying solely on established graph representations falls short for the GM task, as a GM solver may inherently favor a latent topology that diverges from the preordained one. To address this limitation, we propose a novel approach: the learning of a latent graph topology that supplants the fixed input structure. To achieve this, we introduce two innovative procedures for latent graph generation\u2014one deterministic and the other generative in nature. Notably, our generative approach prioritizes consistency across graphs, positioning it as a **co-generative** model.  The results of our methods speak volumes, outperforming previous state-of-the-art techniques across multiple benchmarks, thereby lending robust support to our hypothesis. In this way, we pave a new path in the landscape of graph matching, one that embraces the dynamic nature of graph structures and their potential for adaptation.", "introduction": "Being a long standing NP-hard problem (Loiola et al., 2007), graph matching (GM) has received persistent attention from the machine learning and optimization communities for many years.Concretely, for two graphs with n nodes for each, graph matching seeks to solvefoot_0 : max where the affinity matrix M \u2208 R n 2 \u00d7n 2 + encodes node (diagonal elements) and edge (off-diagonal) affinities/similarities and z is the column-wise vectorization form of the permutation matrix Z. H is a selection matrix ensuring each row and column of Z summing to 1. 1 is a column vector filled with 1. Eq. ( 1) is the so-called quadratic assignment problem (QAP) (Cho et al., 2010).Maximizing Eq. (1) amounts to maximizing the sum of the similarity induced by matching vector Z.While Eq. (1) does not encode the topology of graphs, Zhou & Torre (2016) further propose to factorize M to explicitly incorporate topology matrix, where a connectivity matrix A \u2208 {0, 1} n\u00d7n is used to indicate the topology of a single graph (A ij = 1 if there exists an edge between nodes i and j; A ij = 0 otherwise).To ease the computation, Eq. ( 1) is typically relaxed by letting z \u2208 [0, 1] n 2 and keeping other parts of Eq. ( 1) intact.Traditional solvers to such relaxed problem generally fall into the categories of iterative update (Cho et al., 2010;Jiang et al., 2017) or numerical continuation (Zhou & Torre, 2016;Yu et al., 2018), where the solvers are developed under two key assumptions: 1) Affinity M is pre-computed with some non-negative metrics, e.g.Gaussian kernel, L 2 -distance or Manhattan distance; 2) Graph topology is pre-defined as input either in dense (Schellewald & Schn\u00f6rr, 2005) or sparse (Zhou & Torre, 2016) fashion.There have been several successful attempts towards adjusting the first assumption by leveraging the power of deep networks to learn more effective graph representation for GM (Wang et al., 2019a;Yu et al., 2020;Fey et al., 2020).However, to our best knowledge, there is little previous work questioning and addressing the problem regarding the second assumption in the context of learning-based graph matching 2 .For example, existing standard pipeline of keypoint matching in computer vision will construct initial topology by Delaunay triangulation or k-nearest neighbors.Then this topology will be freezed throughout the subsequent learning and matching procedures.In this sense, the construction of graph topology is peeled from matching task as a pre-processing stage.More examples can be found beyond the vision communities such as in social network alignment (Zhang & Tong, 2016;Heimann et al., 2018;Xiong & Yan, 2020) assuming fixed network structure for individual node matching in two networks.We argue that freezing graph topology for matching can hinder the capacity of graph matching solvers.For a pre-defined graph topology, the linked nodes sometimes result in less meaningful interaction, especially under the message-passing mechanism in graph neural networks (Kipf & Welling, 2017).We give a schematic demonstration in Fig. 1.Though some earlier attempts (Cho & Lee, 2012;Cho et al., 2013) seek to adjust the graph topology under traditional non-deep learning setting, such procedures cannot be readily integrated into end-to-end deep learning frameworks due to undifferentiable nature.Building upon the hypothesis that there exists some latent topology better than heuristically created one for GM, our aim is to learn it (or its distribution) for GM.Indeed, jointly solving matching and graph topology learning can be intimidating due to the combinatorial nature, which calls for more advanced approaches.In this paper, we propose an end-to-end framework to jointly learn the latent graph topology and perform GM, termed as deep latent graph matching (DLGM).We leverage the power of graph generative model to automatically produce graph topology from given features and their geometric relations, under specific locality prior.Different from generative learning on singleton graphs (Kipf & Welling, 2016;Bojchevski et al., 2018), our graph generative learning is performed in a pairwise fashion, leading to a novel matching-guided generative paradigm.The source code will be made publicly available.Contributions: 1) We explore a new direction for more flexible GM by actively learning latent topology, in contrast to previous works using fixed topology as input; 2) Under this setting, we propose a deterministic optimization approach to learn graph topology for matching; 3) We further present a generative way to produce latent topology under a probabilistic interpretation by Expectation-Maximization.This framework can also adapt to other problems where graph topology is the latent structure to infer; 4) Our method achieves state-ofthe-art performance on public benchmarks."}
{"paper_id": 177, "abstract": "In the realm of linguistic exploration, cross-lingual word embeddings (CLWE) have emerged as a powerful tool, illuminating the path for numerous cross-lingual tasks. Yet, a shadow lingers over most existing methods, including those that harness the magic of contextual embeddings\u2014they remain resolutely sense agnostic. In this endeavor, we unveil a groundbreaking framework designed to align contextual embeddings at the nuanced sense level, drawing upon the rich cross-lingual signals embedded within bilingual dictionaries.  Our journey begins with the introduction of a novel sense-aware cross-entropy loss, meticulously crafted to model word senses with precision. By employing this innovative loss function, we enhance monolingual ELMo and BERT models, revealing a remarkable surge in performance across word sense disambiguation tasks. But our quest does not end there. We extend our framework with a sense alignment objective that builds upon our sense-aware cross-entropy loss, paving the way for cross-lingual model pretraining. This approach is applied to several language pairs\u2014English to German, Spanish, Japanese, and Chinese\u2014each a testament to our commitment to linguistic diversity.  The results speak for themselves: our cross-lingual models outshine the best baseline performances, achieving average improvements of 0.52%, 2.09%, and 1.29% on zero-shot cross-lingual named entity recognition, sentiment classification, and the XNLI tasks, respectively. As we stand at the precipice of this linguistic breakthrough, we are excited to share our code, inviting others to join us on this journey of discovery and innovation.", "introduction": "Cross-lingual word embeddings (CLWE) provide a shared representation space for knowledge transfer between languages, yielding state-of-the-art performance in many cross-lingual natural language processing (NLP) tasks.Most of the previous works have focused on aligning static embeddings.To utilize the richer information captured by the pre-trained language model, more recent approaches attempt to extend previous methods to align contextual representations.Aligning the dynamic and complex contextual spaces poses significant challenges, so most of the existing approaches only perform coarse-grained alignment.Schuster et al. (2019) compute the average of contextual embeddings for each word as an anchor, and then learn to align the static anchors using a bilingual dictionary.In another work, Aldarmaki & Diab (2019) use parallel sentences in their approach, where they compute sentence representations by taking the average of contextual word embeddings, and then they learn a projection matrix to align sentence representations.They find that the learned projection matrix also works well for word-level NLP tasks.Besides, unsupervised multilingual language models (Devlin et al., 2018;Artetxe & Schwenk, 2019;Conneau et al., 2019;Liu et al., 2020) pretrained on multilingual corpora have also demonstrated strong cross-lingual transfer performance.Cao et al. (2020) and Wang et al. (2020) show that unsupervised multilingual language model can be further aligned with parallel sentences.Though contextual word embeddings are intended to provide different representations of the same word in distinct contexts, Schuster et al. (2019) find that the contextual embeddings of different senses of one word are much closer compared with that of different words.This contributes to the anisomorphic embedding distribution of different languages and causes problems for cross-lingual alignment.For example, it will be difficult to align the English word bank and its Japanese translations \u9280\u884c and \u5cb8 that correspond to its two different senses, since the contextual embeddings of different senses of bank are close to each other while those of \u9280\u884c and \u5cb8 are far.Recently, Zhang et al. (2019) propose two solutions to handle multi-sense words: 1) remove multi-sense words and then align anchors in the same way as Schuster et al. (2019); 2) generate cluster level average anchor for contextual embeddings of multi-sense words and then learn a projection matrix in an unsupervised way with MUSE (Conneau et al., 2017).They do not make good use of the bilingual dictionaries, which are usually easy to obtain, even in low-resource scenarios.Moreover, their projection-based approach still cannot handle the anisomorphic embedding distribution problem.In this work, we propose a novel sense-aware cross entropy loss to model multiple word senses explicitly, and then leverage a sense level translation task on top of it for cross-lingual model pretraining.The proposed sense level translation task enables our models to provide more isomorphic and better aligned cross-lingual embeddings.We only use the cross-lingual signal from bilingual dictionaries for supervision.Our pretrained models demonstrate consistent performance improvements on zero-shot cross-lingual NER, sentiment classification and XNLI tasks.Though pretrained on less data, our model achieves the state-of-the-art result on zero-shot cross-lingual German NER task.To the best of our knowledge, we are the first to perform sense-level contextual embedding alignment with only bilingual dictionaries."}
{"paper_id": 178, "abstract": "In the realm of unsupervised and self-supervised learning, the quest for effective whole-graph representation is akin to a hero's journey\u2014one fraught with challenges yet rich with potential. This paper embarks on such a journey, delving into the intricacies of representation learning, a crucial endeavor for tasks ranging from drug discovery to the exploration of novel materials.   While current methodologies have adeptly navigated the local structures of various graph instances, they often stumble when faced with the grand tapestry of global semantic structures that weave through entire datasets. To address this, we introduce a groundbreaking framework: Local-instance and Global-semantic Learning, or GraphLoG. This innovative approach not only preserves the vital local instance-level structures but also harnesses a nonparametric strategy to forge hierarchical prototypes from the data itself. These prototypes serve as beacons, illuminating the semantic clusters hidden within the latent space, and possess the remarkable ability to adapt dynamically to the diverse distributions of features.  Our method undergoes rigorous evaluation, embarking on a pre-training odyssey across vast landscapes of unlabeled graphs, followed by a fine-tuning phase on downstream tasks that test its mettle. The results of our extensive experiments on both chemical and biological benchmark datasets stand as testament to the efficacy of GraphLoG, heralding a new chapter in the saga of whole-graph representation learning.", "introduction": "Learning informative representations of whole graphs is a fundamental problem in a variety of domains and tasks, such as molecule properties prediction in drug and material discovery (Gilmer et al., 2017;Wu et al., 2018), protein function forecast in biological networks (Alvarez & Yan, 2012;Jiang et al., 2017), and predicting the properties of circuits in circuit design (Zhang et al., 2019).Recently, Graph Neural Networks (GNNs) have attracted a surge of interest and showed the effectiveness in learning graph representations.These methods are usually trained in a supervised fashion, which requires a large number of labeled data.Nevertheless, in many scientific domains, labeled data are very limited and expensive to obtain.Therefore, it is becoming increasingly important to learn the representations of graphs in an unsupervised or self-supervised fashion.Self-supervised learning has recently achieved profound success for both natural language processing, e.g.GPT (Radford et al., 2018) and BERT (Devlin et al., 2019), and image understanding, e.g.MoCo (He et al., 2019) and SimCLR (Chen et al., 2020).However, how to effectively learn the representations of graphs in a self-supervised way is still an open problem.Intuitively, a desirable graph representation should be able to preserve the local-instance structure, so that similar graphs are embedded close to each other and dissimilar ones stay far apart.In addition, the representations of a set of graphs should also reflect the global-semantic structure of the data, so that the graphs with similar semantic properties are compactly embedded, which benefits various downstream tasks, e.g.graph classification or regression.Such structure can be sufficiently captured by semantic clusters (Caron et al., 2018;Ji et al., 2019), especially in a hierarchical fashion (Li et al., 2020).There are some recent works that learn graph representation in a self-supervised manner, such as local-global mutual information maximization (Velickovic et al., 2019;Sun et al., 2019), structuralsimilarity/context prediction (Navarin et al., 2018;Hu et al., 2019;You et al., 2020) and contrastive multi-view learning (Hassani & Ahmadi, 2020).However, all these methods are capable of modeling only the local structure between different graph instances but fail to discover the global-semantic structure.To address this shortcoming, we are seeking for an approach that is sufficient to model both the local and global structure of a given set of graphs.To attain this goal, we propose a Local-instance and Global-semantic Learning (GraphLoG) framework for self-supervised graph representation learning.In specific, for preserving the local similarity between various graph instances, we first align the embeddings of correlated graphs/subgraphsfoot_0 by maximizing their mutual information.In this locally smooth embedding space, we further represent the distribution of different graph embeddings with hierarchical prototypesfoot_1 whose number is adaptively determined by the data in a nonparametric fashion.During training, these prototypes guide each graph to map to the semantically-similar feature cluster, and, simultaneously, the prototypes are maintained by online-updated graph embeddings.In this process, the global-semantic structure of the data is gradually discovered and refined.The whole model is pre-trained with a large number of unlabeled graphs, and then fine-tuned and evaluated on some downstream tasks.We summarize our contributions as follows:\u2022 We contribute a unified framework called Local-instance and Global-semantic Learning (GraphLoG) for self-supervised graph representation learning, which is able to model the structure of a set of graphs both locally and globally.\u2022 We novelly propose to infer the global-semantic structure underlying the unlabeled graphs by learning hierarchical prototypes via a nonparametric strategy.\u2022 We empirically verify our framework's superior performance on different GNN architectures through pre-training on a large-scale unlabeled dataset and fine-tuning on benchmark tasks in both the chemistry and biology domains."}
{"paper_id": 179, "abstract": "In the realm of Graph Neural Networks (GNNs), the art of crafting effective node and graph representations often hinges on the delicate dance of aggregating neighboring attributes. Yet, therein lies a conundrum: the intricate tapestry of graph structure and node attributes can sometimes weave threads of semantic inconsistency, each strand beckoning for its own unique purpose across different tasks. In this exploration, we embark on a quest to unravel this complexity through a method we call Structure-Attribute Disentanglement, or GraphSAD for short.  Our approach is akin to a skilled artisan separating the vibrant hues of a painting, as we strive to disentangle the dual essence of graph structure and node attributes into two distinct sets of representations. This separation can be executed either in the primordial realm of input or within the refined confines of the embedding space. To measure the success of our endeavor, we introduce a novel metric that quantifies the degree of this disentanglement.  Through rigorous experimentation across a multitude of datasets, our findings reveal that GraphSAD not only successfully disentangles the semantics of graph structure and node attributes but also excels in the realms of node and graph classification tasks, achieving performance that stands as a testament to the power of clarity in representation.", "introduction": "Representing nodes or entire graphs with informative low-dimensional feature vectors plays a crucial role in many real-world applications and domains, e.g.user analysis in social networks (Tan et al., 2011;Yan et al., 2013), relational inference in knowledge graphs (Bordes et al., 2013;Trouillon et al., 2016;Sun et al., 2019), molecular property prediction in drug/material discovery (Gilmer et al., 2017;Wu et al., 2018) and circuit response prediction in circuit design (Zhang et al., 2019).Recently, Graph Neural Networks (GNNs) (Kipf & Welling, 2017;Velickovic et al., 2018;Xu et al., 2019) have shown their superiority in many different tasks.In general, the essential idea of these methods is to learn effective node representations (or graph representations with an additional graph pooling) through aggregating the attributes of each node and its neighbors in an iterative and nonlinear way.For an attributed graph, GNNs commonly encode the information of its graph structure and node attributes into a single representation.This might be problematic, since the semantic space of graph structure and node attributes might not be well aligned, and these two types of information could be useful for different tasks.For example, predicting the health condition of a user mainly depends on his/her profile information, and the social network does not provide too much meaningful information; in another case, the prediction of a user's social class mainly relies on his/her social network structure.Therefore, a more reasonable solution is to disentangle these two types of information into two distinct sets of representations, and the importance of which can be further determined by downstream tasks.Such disentangled representation has been proved to be beneficial to model's generalization ability and interpretability (Chen et al., 2016;Higgins et al., 2017;Alemi et al., 2017).Recently, DisenGNN (Ma et al., 2019) studied disentangled node representation learning by grouping the neighbors of each node to different channels, and each channel corresponds to a different latent factor.In other words, DisenGNN focuses on disentangling the various latent factors of graph structure.By contrast, our work intends to disentangle the representations of graph structure and node attributes, which is orthogonal to their work and also more general.In this paper, we aim to learn node/graph representations with Structure-Attribute Disentanglement (GraphSAD).As a naive trial, we first attempt to conduct disentanglement in the input space, named as Input-SAD, which separates a graph into a structure and an attribute component and then encodes these two components respectively.However, since graph structure and node attributes are not completely independent, it is better to suppress the dependency of these two factors in the embedding space, instead of directly separating the input graph.Inspired by this fact, we propose to distill a graph's structure and attribute information into the distinct channels of embedding vectors, named as Embed-SAD.Concretely, for each node embedding, half of its elements capture the graph structure through edge reconstruction, and the other half extracts the attribute information by minimizing the mutual information with the structure counterpart and, at the same time, preserving semantic discriminability.In addition, we devise a metric to quantitatively evaluate graph representation's structure-attribute disentanglement, denoted as SAD-Metric, which measures the sensitivity of a model when varying either the graph structure or node attributes of an input graph.We summarize our contributions as follows:\u2022 We study structure-attribute disentangled node/graph representation learning through separating graph structure and node attributes in either the input or the embedding space.\u2022 We design a quantitative metric to measure the extent of structure-attribute disentanglement, which is novel on its graph-specific data processing scheme.\u2022 Through combining the proposed disentangling techniques with various GNNs, we empirically verify our method's superior performance on both the node and graph classification benchmark datasets.Also, we analyze the disentangled graph representations via the proposed metric and qualitative visualization."}
{"paper_id": 180, "abstract": "In this work, we unveil a groundbreaking framework for evolutionary learning that fosters the emergence of unbiased state representations, all without the need for supervision. Traditional evolutionary methods, particularly self-play, often stumble into undesirable local optima when navigating the complex terrain of multi-agent reinforcement learning, especially in non-cooperative environments where information asymmetry reigns. To counter this, we introduce a novel twist on self-play, drawing inspiration from the principles of mechanism design\u2014what some might call \"reverse game theory.\" This approach is designed to coax agents into cooperation by eliciting truthful signals from their interactions. At the heart of our strategy lies the innovative use of imaginary rewards, facilitated through the peer prediction method, which serves as a mechanism to assess the validity of the information exchanged in a decentralized setting. Our numerical experiments across diverse scenarios\u2014including predator-prey dynamics, traffic junction management, and the intricate battles of StarCraft\u2014demonstrate that our framework not only meets but exceeds the performance benchmarks of existing state-of-the-art methodologies.", "introduction": "Evolving culture prevents deep neural networks from falling into bad local optima (Bengio, 2012).For example, self-play (Samuel, 1967;Tesauro, 1995) has not only demonstrated the ability to abstract high-dimensional state spaces as typified by AlphaGo (Silver et al., 2017), but also improved exploration coverage in partially observable environments and communication (Sukhbaatar et al., 2016;Singh et al., 2019) to exchange their internal representations, such as explored observation and hidden state in RNNs.Evolutionary learning is expected to be a general framework for creating superhuman AIs as such learning can generate a high-level abstract representation without any bias in supervision.However, when applying evolutionary learning to a partially observable environment with noncooperative agents, improper bias is injected into the state representation.This bias originates from the environment.A partially observable environment with non-cooperative agents induces actions that disable an agent from honestly sharing the correct internal state resulting in the agent taking actions such as concealing information and deceiving other agents at equilibrium (Singh et al., 2019).The problem arises because the agent cannot fully observe the state of the environment, and thus, it does not have sufficient knowledge to verify the information provided by other agents.Furthermore, neural networks are vulnerable to adversarial examples (Szegedy et al., 2014) and are likely to induce erroneous behavior with small perturbations.Many discriminative models for information accuracy are available; these include GANs (Goodfellow et al., 2014;Radford et al., 2016) and curriculum learning (Lowe et al., 2020).However, these models assume that accurate samples can be obtained by supervision.Because of this assumption, is it impossible to apply these models to a partially observable environment, where the distribution is not stable.We generalize self-play to non-cooperative partially observable environments via mechanism design (Myerson, 1983;Miller et al., 2005), which is also known as reverse game theory.The key idea is to add imaginary rewards by using the peer prediction method (Miller et al., 2005), that is, a mechanism for evaluating the validity of information exchanged between agents in a decentralized environment, which is calculated based on social influence on the signals.We formulate the non-cooperative partially observable environment as an extention of the pertially observable stochastic games (POSG) (Hansen et al., 2004); introduce truthfulness (Vickrey, 1961), which is an indicator of the validity of state representation.We show that the imaginary reward enables us to reflect the bias of state representation on the gradient without oracles.As the first contribution, we propose truthful self-play (TSP) and analytically demonstrate convergence to the global optimum (Section 4).We propose the imaginary reward on the basis of the peer prediction method (Miller et al., 2005) and apply it to self-play.The mechanism affects the gradient of the local optima, but not the global optima.The trick is to use the actions taken by the agents as feedback to verify the received signal from the every other agent, instead of the true state, input, and intent, which the agents cannot fully observe.TSP only requires a modification of the baseline function for self-play; it drastically improves the convergence to the global optimum in Comm-POSG.As the second contribution, based on the results of numerical experiments, we report that the TSP achieved state-of-the-art performance for various multi-agent tasks made of up to 20 agents (Section 5).Using predator prey (Barrett et al., 2011), traffic junction (Sukhbaatar et al., 2016;Singh et al., 2019), andStarCraft Synnaeve et al. (2016) environments, which are typically used in Comm-POSG research, we compared the performances of TSP with the current neural nets, including the state-ofthe-art method, with LSTM, CommNet (Sukhbaatar et al., 2016), and IC3Net (Singh et al., 2019).We report that the model with IC3Net optimized by TSP has the best performance.This work is the first attempt to apply mechanism design to evolutionary learning.TSP is a general optimization algorithm whose convergence is theoretically guaranteed for arbitrary policies and environments.Furthermore, since no supervision is required, TSP has a wide range of applications to not only game AIs (Silver et al., 2017), but also the robots (Jaderberg et al., 2018), chatbots (Gupta et al., 2019;Chevalier et al., 2019), and autonomous cars (Tang, 2019) that are employed in multiagent tasks.Notation: Vectors are columns.Let n := {1, . . ., n}.R is a set of real numbers.i is the imaginary unit.Re u and Im u are a real and an imaginary part of complex number u, respectively.n-tuple are written as boldface of the original variables a := a 1 , . . ., a n , and a -i is a (n -1)-tuple obtained by removing the i-th entry from a. Let 1 := (1, . . ., 1) T .Matrices are shown in uppercase letters L := ( ij ).E is the unit matrix.The set of probability distributions based on the support X is described as P(X )."}
{"paper_id": 181, "abstract": "In a world where neural networks rise to prominence, their allure captivates many, yet a profound mathematical grasp of their capabilities and constraints remains elusive. In this exploration, we embark on a journey into the realm of neural network prediction accuracies through the lens of statistics. Our quest reveals robust statistical guarantees for deep learning, particularly as we navigate the intricate landscape of various sparsity-inducing regularizations. The bounds we uncover exhibit a gentle interplay with the widths and depths of networks, thereby reinforcing the prevailing movement toward architectures that are both wide and deep. The methodologies we employ in our analysis diverge from the conventional paths of deep learning, offering insights that may intrigue both the seasoned practitioner and the curious newcomer alike.", "introduction": "Sparsity reduces network complexities and, consequently, lowers the demands on memory and computation, reduces overfitting, and improves interpretability (Changpinyo et al., 2017;Han et al., 2016;Kim et al., 2016;Liu et al., 2015;Wen et al., 2016).Three common notions of sparsity are connection sparsity, which means that there is only a small number of nonzero connections between nodes, node sparsity, which means that there is only a small number of active nodes (Alvarez & Salzmann, 2016;Changpinyo et al., 2017;Feng & Simon, 2017;Kim et al., 2016;Lee et al., 2008;Liu et al., 2015;Nie et al., 2015;Scardapane et al., 2017;Wen et al., 2016), and layer sparsity, which means that there is only a small number of active layers (Hebiri & Lederer, 2020).Approaches to achieving sparsity include augmenting small networks (Ash, 1989;Bello, 1992), pruning large networks (Simonyan & Zisserman, 2015;Han et al., 2016), constraint estimation (Ledent et al., 2019;Neyshabur et al., 2015;Schmidt-Hieber, 2020), and statistical regularization (Taheri et al., 2020).The many empirical observations of the benefits of sparsity have sparked interest in mathematical support in the form of statistical theories.But such theories are still scarce and, in any case, have severe limitations.For example, statistical guarantees for deep learning with connection-sparse regularization have been established in Taheri et al. (2020), but they do not cover node sparsity, which, in view of the removal of entire nodes, has become especially popular.Moreover, their estimator involves an additional parameter, their theory is limited to a single output node, and their results have a suboptimal dependence on the input vectors.Statistical guarantees for constraint estimation over connection-and node-sparse networks follow from combining results in Neyshabur et al. (2015) and Bartlett & Mendelson (2002).But for computational and practical reasons, regularized estimation is typically preferred over constraint estimation in deep learning as well as in machine learning at large (Hastie et al., 2015).Moreover, their theory is limited to a single output node and ReLU activation, scales exponentially in the number of layers, and requires bounded loss functions.Statistical prediction guarantees for constraint estimation over connection-sparse networks have been derived in Schmidt-Hieber (2020), but their theory is limited to a single output node and ReLU activation and assumes bounded weights.In short, the existing statistical theory for deep learning with connection and node sparsity is still deficient.The goal of this paper is to provide an improved theory for sparse deep learning.We focus on regression-type settings with layered, feedforward neural networks.The estimators under consideration consist of a standard least-squares estimator with additional regularizers that induce connection or node sparsity.We then derive our guarantees by using techniques from high-dimensional statistics (Dalalyan et al., 2017) and empirical process theory (van de Geer, 2000).In the case of subgaussian noise, we find the rates l log [mnp] 3 n and mlp(log [mnp] 3 n for the connection-sparse and node-sparse estimators, respectively, where l is the number of hidden layers, m the number of output nodes, n the number of samples, p the total number of parameters, and p the maximal width of the network.The rates suggest that sparsity-inducing approaches can provide accurate prediction even in very wide (with connection sparsity) and very deep (with either type of sparsity) networks while, at the same time, ensuring low network complexities.These findings underpin the current trend toward sparse but wide and especially deep networks from a statistical perspective.Outline of the paper Section 2 recapitulates the notions of connection and node sparsity and introduces the corresponding deep learning framework and estimators.Section 3 confirms the empirically-observed accuracies of connection-and node-sparse estimation in theory.Section 4 summarizes the key features and limitations of our work.The Appendix contains all proofs."}
{"paper_id": 182, "abstract": "In the realm of generative adversarial networks (GANs), the journey began with a logistic MiniMax cost formulation, a noble endeavor that often faltered under the weight of saturation. This led to the advent of the Non-Saturating GAN (NS-GAN), a clever reformulation that sought to alleviate the saturation woes. However, in its quest for improvement, NS-GAN inadvertently flipped the script on sample weighting, shifting the focus from the higher-scoring samples to the lower-scoring ones during parameter updates. This subtle shift, while well-intentioned, opened the door to an insidious issue: mode dropping.  In our exploration, we unveil both theoretical insights and empirical evidence that illuminate this vulnerability within NS-GAN. To counteract the pitfalls of saturation while preserving the integrity of sample weighting, we introduce MM-nsat. This innovative approach maintains the original MM-GAN sample weighting but cleverly rescales the minibatch gradient, aligning its magnitude with that of the NS-GAN gradient. The result? A transformation in training dynamics that breathes new life into the model.  Our experiments on MNIST and CIFAR-10 reveal that MM-nsat not only excels in mode coverage and stability but also achieves a favorable Fr\u00e9chet Inception Distance (FID). While the results are indeed promising, our primary contribution lies in unraveling the intricate relationship between NS-GAN's sample weighting and the dual threats of mode dropping and training collapse. In doing so, we hope to forge a clearer path through the complex landscape of GAN training, illuminating the way for future advancements in this captivating field.", "introduction": "Generative adversarial networks have come a long way since their introduction (Goodfellow et al., 2014) and are currently state of the art for some tasks, such as generating images.A combination of deep learning developments, GAN specific advances and vast improvements in data sets and computational resources have enabled GANs to generate high resolution images that require some effort to distinguish from real photos (Zhang et al., 2018;Brock et al., 2018;Karras et al., 2018).(3) Husz\u00e1r (2015) and Arjovsky & Bottou (2017) have suggested NS-GAN's divergence as an explanation for the ubiquitous mode dropping and mode collapsing problems with GANs (Metz et al., 2016;Salimans et al., 2016;Srivastava et al., 2017).While MM-GAN seems promising in terms of its Jensen-Shannon divergence, the formulation has largely been ignored because the saturating cost causes training to break down.A variety of other GAN formulations have been introduced, such as WGAN-GP (Arjovsky et al., 2017;Gulrajani et al., 2017), LS-GAN (Mao et al., 2016) and Hinge-GAN (Miyato et al., 2018).Lucic et al. (2018) finds that different cost formulations tend to get similar results given sufficient parameter tuning, including various forms of regularization.Despite the questionable form of NS-GAN in terms of divergences, it is widely used and can produce very impressive results, such as in the improved StyleGAN (Karras et al., 2019)."}
{"paper_id": 183, "abstract": "In this study, we unveil the Lexical Unit Analysis (LUA), a robust framework crafted for the intricate art of sequence segmentation. When presented with a natural language sentence, LUA embarks on a meticulous journey, evaluating all potential segmentation candidates with a scoring system that leaves no stone unturned. Through the power of dynamic programming (DP), it deftly extracts the highest-scoring segmentation, ensuring that the outcome is not only optimal but also inherently valid.  LUA boasts a suite of compelling attributes: it guarantees that the segmentations it predicts are valid and promotes a training and inference process that is globally optimal. Remarkably, we\u2019ve honed its practical time complexity to a linear scale, rendering it exceptionally efficient.   Our rigorous experimentation spans five distinct tasks\u2014synthesizing syntactic chunking, named entity recognition (NER), slot filling, Chinese word segmentation, and Chinese part-of-speech (POS) tagging\u2014across a diverse array of 15 datasets. The results are nothing short of impressive, with our models achieving state-of-the-art performance on 13 of these datasets. Notably, we have observed a significant enhancement in the F1 score for identifying long-length segments, marking a substantial leap forward in the field.", "introduction": "Sequence segmentation is essentially the process of partitioning a sequence of fine-grained lexical units into a sequence of coarse-grained ones.In some scenarios, each composed unit is assigned a categorical label.For example, Chinese word segmentation splits a character sequence into a word sequence (Xue, 2003).Syntactic chunking segments a word sequence into a sequence of labeled groups of words (i.e., constituents) (Sang & Buchholz, 2000).There are currently two mainstream approaches to sequence segmentation.The most common is to regard it as a sequence labeling problem by using IOB tagging scheme (Mesnil et al., 2014;Ma & Hovy, 2016;Liu et al., 2019b;Chen et al., 2019a;Luo et al., 2020).A representative work is Bidirectional LSTM-CRF (Huang et al., 2015), which adopts LSTM (Hochreiter & Schmidhuber, 1997) to read an input sentence and CRF (Lafferty et al., 2001) to decode the label sequence.This type of method is very effective, providing tons of state-of-the-art performances.However, it is vulnerable to producing invalid labels, for instance, \"O, I-tag, I-tag\".This problem is very severe in low resource settings (Peng et al., 2017).In experiments (see section 4.6), we also find that it performs poorly in recognizing long-length segments.Recently, there is a growing interest in span-based models (Zhai et al., 2017;Li et al., 2019;Yu et al., 2020).They treat a span rather than a token as the basic unit for labeling.Li et al. (2019) cast named entity recognition (NER) to a machine reading comprehension (MRC) task, where entities are extracted as retrieving answer spans.Yu et al. (2020) rank all the spans in terms of the scores predicted by a bi-affine model (Dozat & Manning, 2016).In NER, span-based models have significantly outperformed their sequence labeling based counterparts.While these methods circumvent the use of IOB tagging scheme, they still rely on post-processing rules to guarantee the extracted span set to be valid.Moreover, since span-based models are locally normalized at span level, they potentially suffer from the label bias problem (Lafferty et al., 2001).This paper seeks to provide a new framework which infers the segmentation of a unit sequence by directly selecting from all valid segmentation candidates, instead of manipulating tokens or spans.To this end, we propose Lexical Unit Analysis (LUA) in this paper.LUA assigns a score to every valid segmentation candidate and leverages dynamic programming (DP) (Bellman, 1966) to search for the maximum scoring one.The score of a segmentation is computed by using the scores of its all segments.Besides, we adopt neural networks to score every segment of the input sentence.The purpose of using DP is to solve the intractability of extracting the maximum scoring segmentation candidate by brute-force search.The time complexity of LUA is quadratic time, yet it can be optimized to linear time in practice by performing parallel matrix computations.For training criterion, we incur a hinge loss between the ground truth and the predictions.We also extend LUA to unlabeled segmentation and capturing label correlations.Figure 1 illustrates the comparison between previous methods and the proposed LUA.Prior models at token level and span level are vulnerable to generating invalid predictions, and hence rely on heuristic rules to fix them.For example, in the middle part of Figure 1, the spans of two inferred named entities, [Word Cup] MISC and [Cup] MISC , conflicts, which is mitigated by comparing the predicted scores.LUA scores all possible segmentation candidates and uses DP to extract the maximum scoring one.In this way, our models guarantee the predictions to be valid.Moreover, the globality of DP addresses the label bias problem.Extensive experiments are conducted on syntactic chunking, NER, slot filling, Chinese word segmentation, and Chinese part-of-speech (POS) tagging across 15 tasks.We have obtained new stateof-the-art results on 13 of them and performed competitively on the others.In particular, we observe that LUA is expert at identifying long-length segments."}
{"paper_id": 184, "abstract": "In the realm of computer vision, classification stands as a cornerstone, underpinning essential tasks like image recognition, object detection, and scene segmentation. The prevailing method\u2014multinomial logistic regression with a solitary final layer of dense connections\u2014has become the standard for classification in convolutional neural networks (CNNs). Yet, while these classifiers adeptly map inputs to a set of output categories, they often fall short in providing a rich, nuanced understanding of those categories. For instance, when a CNN accurately identifies an image of a chimpanzee, it merely labels the creature without revealing its deeper connections to the Primate, Mammal, and Chordate families, or its very existence as a living being.  To address this limitation, we introduce a novel architecture featuring multilayer dense connectivity within the CNN framework, allowing the model to predict not only the category but also its conceptual superclasses in a hierarchical manner. Through rigorous experimentation, we demonstrate that our proposed dense connections, when paired with widely-used convolutional feature layers, can effectively learn to predict these conceptual classes. Remarkably, this approach incurs only a minimal increase in network size, all while preserving the accuracy of categorical classification. In doing so, we pave the way for a more comprehensive understanding of the relationships that define the categories we seek to classify.", "introduction": "Classification is a core concept for numerous computer vision tasks.Given the convolutional features, different architectures classify either the image itself (He et al., 2015;Szegedy et al., 2016), the region/bounding boxes for object detection (He et al., 2017;Liu et al., 2015), or, at the granular level, pixels for scene segmentation (Chen et al., 2018).Although early image recognition works employed multilayer classification layers (Krizhevsky et al., 2012;Simonyan & Zisserman, 2015), the more recent models have all been using single layer dense connection (He et al., 2016;Szegedy et al., 2016) or convolutions (Lin et al., 2017).The vision community has invented a multitude of techniques to enhance the capacity of feature computation layers (Xie et al., 2017;Huang et al., 2017;Hu et al., 2018;Dai et al., 2017;Chollet, 2016;Tan & Le, 2019).But, the classification layer has mostly retained the form of a multinomial/softmax logistic regression performing a mapping from a set of inputs (images) to a set of categories/labels.As such, the final output of these networks do not furnish a comprehensive depiction about the input entity.In particular, when an existing CNN correctly identifies an image of an English Setter, it is not laid out in the output that it is an instance of a dog, or more extensively, a hunting dog, a domestic animal and a living thing.It is rational to assume that convolutional layers construct some internal representation of the conceptual superclasses, e.g., dog, animal etc., during training.We argue that, by appropriately harnessing such representation, one can retrieve a much broader description of the input image from a CNN than it is supplied by a single layer output.Extensive information about most categories are freely available in repositories such as Word-Net (Fellbaum, 1998).WordNet provides the hierarchical organization of category classes (e.g., English Setter) and their conceptual superclasses (e.g., Hunting dog, Domestic animal, Living thing).However, a surprisingly limited number of CNNs utilize the concept hierarchy.The primary goal of almost all existing studies is to improve the category-wise classification performance by exploiting the conceptual relations, often via a separate tool.Deng et al. (2014) and Ding et al. (2015) apply graphical models to capture the interdependence among concept labels to improve category classification accuracy.Other works either do not clarify the semantic meaning of the ancestor concepts (Yan et al., 2015) or impose a level of complexity in the additional tool (RNN) that is perhaps unnecessary (Hu et al., 2016).We have not found an existing (deep learning) model that attempts to predict both the finer categories and the chain of ancestor concepts for an input image by a single network.The classical hedging method (Deng et al., 2012) computes either the finer labels or one of its superclasses exclusively, but not both simultaneously.Figure 1: The goal of the proposed algorithm.In contrast to the existing methods, our proposed CNN architecture predicts the chain of superclass concepts as well as the finer category.In this paper, we introduce a CNN to classify the category and the concept superclasses simultaneously.As illustrated in Figure 1, in order to classify any category class (e.g., English Setter), our model is constrained to also predict the ancestor superclasses (e.g., Hunting dog, Domestic animal, Living thing) in the same order as defined in a given ontology.We propose a configuration of multilayer dense connections to predict the category & concept superclasses as well as model their interrelations based on the ontology.We also propose a simple method to prune and rearrange the label hierarchy for efficient connectivity.Capturing the hierarchical relationship within the CNN architecture itself enables us to train the model end-to-end (as opposed to attaching a separate tool) by applying existing optimization strategies for training deep networksfoot_0 .We experimentally demonstrate that one can train the proposed architecture using standard optimization protocols to predict the concept classes with two popular CNN backbones: ResNet and InceptionV4, while maintaining their category-wise accuracy.The proposed multilayer connection is shown to further refine the learned representations of these backbone CNNs to yield better concept and category classification than 1) multinomial logistic regression, and, 2) other existing works (that apply separate mechanisms) on standard datasets and challenging images.Predicting coarser superclasses in addition to finer level categories improves interpretability of the classifier performance.Even if an eagle is misclassified as a parrot, the capability of inferring that it is a bird, and not an artifact (e.g., drone), may be beneficial in some applications (e.g., surveillance).More importantly, an object detector can enhance its capability on unseen categories by adopting the proposed classification scheme (as demonstrated in Section 4.4).For example, a movie/TV violence recognition/detection tool can recognize an equipment as a 'weapon' concept class even if that particular weapon category was not in the training set.In visual question answering (VQA), encoding concept classes would expand the scope of query terms by allowing broader description ('how many vehicles are present' in addition to 'how many buses', 'how many trucks' etc.; see Cao et al. (2018); Wang et al. (2016)).In Appendix G, we point out how our architecture can be extended to object detectors to compute the concept classes.In addition, we allude to the potential applications of our model to capture label structures different from concept graph, e.g., spatial or compositional dependence."}
{"paper_id": 185, "abstract": "For years, the realm of deep reinforcement learning (DRL) has embraced the intriguing concept of multi-step returns for refining value functions. This approach, akin to wielding a diverse arsenal of magical tools, allows us to harness the strengths of various backup lengths, each offering unique benefits in terms of bias, variance in value estimates, speed of convergence, and the agent's exploration behavior. Traditional methods, such as TD-lambda, skillfully blend these advantages by employing a target value that represents an exponential average of multiple step returns. However, this synthesis often results in a loss of the rich diversity that each step return target provides.   To overcome this limitation, we introduce Mixture Bootstrapped DQN (MB-DQN), a novel architecture built upon the foundation of bootstrapped DQN. By employing different backup lengths across multiple bootstrapped heads, MB-DQN fosters a vibrant heterogeneity in target values, a feature absent in methods that rely solely on a singular target. This innovation allows us to retain the myriad advantages that different backup lengths bestow.   In this paper, we embark on our journey by exploring the motivational insights gleaned from a simple maze environment, illustrating the potential of our approach. To substantiate the efficacy of MB-DQN, we delve into rigorous experiments within the challenging landscapes of the Atari 2600 benchmark environments, showcasing its performance enhancements over various baseline methods. Additionally, we conduct a series of ablation studies, meticulously examining the impacts of different design configurations of MB-DQN, further illuminating its strengths and capabilities.", "introduction": "In recent value-based deep reinforcement learning (DRL), a value function is usually utilized to evaluate state values, which stand for estimates of the expected long-term cumulative rewards that might be collected by an agent.In order to perform such an evaluation, a deep neural network (DNN) is employed by a number of contemporary value-based DRL methods (Mnih et al., 2015;Wang et al., 2016;Hasselt et al., 2016;Osband et al., 2016;Hessel et al., 2018) as the value function approximator, in which the network parameters are iteratively updated based on the agent's experience of interactions with an environment.For many of these methods (Mnih et al., 2015;Wang et al., 2016;Hasselt et al., 2016;Osband et al., 2016;Hessel et al., 2018), the update procedure is carried out by one-step temporal-difference (TD) learning (Sutton & Barto, 1998) (or simply \"one-step TD\"), which calculates the error between an estimated state value and a target differing by one timestep.One-step TD has been demonstrated effective in backing up immediate reward signals collected by an agent.Nevertheless, the long temporal horizon that the reward signals from farther states have to propagate through might lead to an extended learning period of the value function approximator.Learning from multi-step returns (Sutton & Barto, 1998) is a way of propagating rewards newly observed by the agent faster to earlier visited states, and has been adopted in several previous works.Asynchronous advantage actor-critic (A3C) (Mnih et al., 2016) employs multi-step returns as targets to update the value functions of its asynchronous threads.Rainbow deep Q-network (Rainbow DQN) (Hessel et al., 2018) also utilizes multi-step returns during the backup procedure.The authors in (Barth-Maron et al., 2018) also modify the target value function of deep deterministic dolicy gradient (DDPG) (Lillicrap et al., 2016) to estimate TD errors using multi-step returns.Updating value functions with different backup lengths provides advantages in different aspects, including bias and variance of value estimates, convergence speed, and exploration behavior of the agent.Backing up reward signals through multi-step returns shifts the bias-variance tradeoff (Hessel et al., 2018).Therefore, backing up with different step return lengths (or simply 'backup length' hereafter (Asis et al., 2018)) might lead to different target values in the Bellman equation, resulting in different exploration behaviors of the agent as well as different achievable performance of it.The authors in (Amiranashvili et al., 2018) have demonstrated that the performance of the agent varies with different backup lengths, and showed that both very short and very long backup lengths could cause performance drops.These insights suggest that identifying the best backup length for an environment is not straightforward.In addition, although learning based on multi-step returns enhances the immediate sensitivity to future rewards, it is at the expense of greater variance which may cause the value function approximator to require more data samples to converge to the true expectation.Moreover, relying on a single target value with any specific backup length constrains the exploration behaviors of the agent, and might limit the achievable performance of it.Based on the above observations, there have been several research works proposed to unify different target values with different backup lengths to leverages their respective advantages.The traditional TD (\u03bb) (Sutton & Barto, 1998) uses a target value equivalent to an exponential average of all n-step returns (where n is a natural number), providing a faster empirical convergence by interpolating between low-variance TD returns and low-bias Monte Carlo returns.DQN (\u03bb) (Daley & Amato, 2019) further proposes an efficient implementation of TD (\u03bb) for DRL by modifying the replay buffer memory such that \u03bb-returns can be pre-computed.Although these methods benefit from combining multiple distinct backup lengths, they still rely on a single target value during the update procedure.Integrating step returns into a single target value, nevertheless, may sacrifice the diversity of the advantages provided by different step return targets.As a result, in this paper, we propose Mixture Bootstrapped DQN (abbreviated as \"MB-DQN\") to address the above issues.MB-DQN is built on top of bootstrapped DQN (Osband et al., 2016), which contains multiple bootstrapped heads with randomly initialized weights to learn a set of value functions.MB-DQN leverages the advantages of different step return targets by assigning a distinct backup length to each bootstrapped head.Each bootstrapped head maintains its own target value derived from the assigned backup length during the update procedure.Since the backup lengths of the bootstrapped heads are distinct from each other, MB-DQN provides heterogeneity in the target values as well as diversified exploration behaviors of the agent that is unavailable in approaches relying only on a single target value.To validate the proposed concept, in our experiments, we first provide motivational insights on the influence of different configurations of backup lengths in a simple maze environment.We then evaluate the proposed MB-DQN on the Atari 2600 (Bellemare et al., 2015) benchmark environments, and demonstrate its performance improvement over a number of baseline methods.We further provide a set of ablation studies to analyze the impacts of different design configurations of MB-DQN.In summary, the primary contributions of this paper include: (1) introducing an approach for maintaining the advantages from different backup lengths, (2) providing heterogeneity in the target values by utilizing multiple bootstrapped heads, and (3) enabling diversified exploration behaviors of the agent.The remainder of this paper is organized as the following.Section 2 provides the background material related to this work.Section 3 walks through the proposed MB-DQN methodology.Section 4 reports the experimental results, and presents a set of the ablation analyses.Section 5 concludes this paper."}
{"paper_id": 186, "abstract": "In the ever-evolving realm of unsupervised representation learning, we find ourselves on the cusp of a revolution, propelled by the remarkable strides of contrastive learning. This innovative approach treats each image\u2014and its various augmentations\u2014as distinct classes, yet it often overlooks the deeper semantic connections that bind images together. In response to this challenge, we introduce a novel data augmentation technique known as Center-wise Local Image Mixture (CLIM), designed to broaden the neighborhood space of an image in a meaningful way.  CLIM operates on the principle of fostering both local similarity and global cohesion, deftly pulling together images that share a kinship. It does so by meticulously searching for local samples that resonate with the essence of the target image, selecting only those that lie closest to their respective cluster centers\u2014an approach we term center-wise local selection. This method ensures that similar representations gravitate toward their clusters without disrupting the delicate fabric of local similarity.  Moreover, we employ image mixture as a form of smoothing regularization, a safeguard against the pitfalls of overconfidence in our selected samples. To further enhance our technique, we incorporate multi-resolution augmentation, imbuing our representations with a remarkable scale invariance. The synergy of these two augmentations culminates in superior feature representation across a variety of unsupervised benchmarks.  Our results speak volumes: we achieve a striking 75.5% top-1 accuracy with linear evaluation on the ResNet-50 architecture, and an impressive 59.3% top-1 accuracy when fine-tuned with a mere 1% of labeled data. Not only do we consistently surpass the performance of supervised pretraining on numerous downstream transfer tasks, but we also pave the way for a new understanding of how images can be interconnected in the vast expanse of unsupervised learning.", "introduction": "Learning general representations that can be transferable to different downstream tasks is a key challenge in computer vision.This is usually achieved by fully supervised learning paradigm, e.g., making use of ImageNet labels for pretraining over the past several years.Recently, self-supervised learning has attracted more attention due to its free of human labels.In self-supervised learning, the network aims at exploring the intrinsic distributions of images via a series of predefined pretext tasks (Doersch et al., 2015;Gidaris et al., 2018;Noroozi & Favaro, 2016;Pathak et al., 2016).Among them, instance discrimination (Wu et al., 2018) based methods have achieved remarkable progress (Chen et al., 2020a;He et al., 2020;Grill et al., 2020;Caron et al., 2020).The core idea of instance discrimination is to push away different images, and encourage the representation of different transformations (augmentations) of the same image to be similar.Following this paradigm, self-supervised models are able to generate features that are comparable or even better than those produced by supervised pretraining when evaluated on some downstream tasks, e.g.,COCO detection and segmentation (Chen et al., 2020c;b).In contrastive learning, the positive pairs are simply constrained within different transformations of the same image, e.g., cropping, color distortion, Gaussian blur, rotation, etc.. Recent advances have demonstrated that better data augmentations (Chen et al., 2020a) really help to improve the representation robustness.However, contrasting two images that are de facto similar in semantic space is not applicable for general representations.It is intuitive to pull semantically similar images for better transferability.DeepCluster (Caron et al., 2018) and Local Aggregation (Zhuang et al., 2019) relax the extreme instance discrimination task via discriminating groups of images instead of an individual image.However, due to the lack of labels, it is inevitable that the positive pairs contain noisy samples, which limits the performance.In this paper, we target at expanding instance discrimination by exploring local similarities among images.Towards this goal, one need to solve two issues: i) how to select similar images as positive pairs of an image, and ii) how to incorporate these positive pairs, which inevitably contain noisy assignments, into contrastive learning.We propose a new kind of data augmentation, named Centerwise Local Image Mixture, to tackle the above two issues in a robust and efficient way.CLIM consists of two core elements, i.e., a center-wise positive sample selection, as well as a data mixing operation.For positive sample selection, the motivation is that a good representation should be endowed with high intra-class similarity, and we find that although MoCo (He et al., 2020) does not explicitly model invariance to similar images, the intra-class similarity becomes higher as the training process goes.Based on this observation, we explicitly enforce semantically similar images towards the center of clusters, and generate representation with higher intra-class similarity, which we find is beneficial for few shot learning.This is achieved by searching nearest neighbors of an image, and only retaining similar samples that are closer to the corresponding cluster center, which we denote as center-wise local sample selection.As a result, an image is pulled towards the center while do not break the local similarity.Once similar samples are selected, a direct way is to treat these similar samples as multiple positives for contrastive learning.However, since feature representation in high dimensional space is complex, the returned positive samples inevitably contain noisy assignments, which should not be overconfident.Instead, we rely on data mixing as augmented samples, which can be treated as a smoothing regularization in unsupervised learning.In particular, we apply Cutmix (Yun et al., 2019), a widely used data augmentation in supervised learning, where patches are cut and pasted among the positive pairs to generate new samples.Benefit from the center-wise sample selection, the Cutmix augmentation is only constrained within the local neighborhood of an image, and can be treated as an expansion of current neighborhood space.In this way, similar samples are pulled together in a smoother and robust way, which we find is beneficial for general representation.Furthermore, we propose multi-resolution augmentation, which aims at contrasting the same image (patch) at different resolutions explicitly, to enable the representation to be scale invariant.We argue that although previous operations such as crop and resize introduce multi-resolution implicitly, they do not compare the same patch at different resolutions directly.As comparisons, multi-resolution incorporates scale invariance into contrastive learning, and significantly boosts the performance even based on a strong baseline.The multi-resolution strategy is simple but effective, and can be combined with current data augmentations for further improving performance.We evaluate the feature representation on several self-supervised learning benchmarks.In particular, on ImageNet linear evaluation protocol, we achieve 75.5% top-1 accuracy with a standard ResNet-50.In few shot setting, when finetuned with only 1% labels, we achieve 59.3% top-1 accuracy, surpassing previous works by a large margin.We also validate its transferring ability on several downstream tasks, and consistently outperform the fully supervised counterparts."}
{"paper_id": 187, "abstract": "In the realm of model compression, we unveil a groundbreaking approach known as Automatic Bit Sharing (ABS), a technique designed to navigate the intricate landscape of optimal compression configurations\u2014where pruning ratios and bitwidths converge. Unlike traditional methods that treat model pruning and quantization as separate entities, our approach seeks to harmonize these processes, optimizing them in tandem for greater efficiency.  To tackle the vast and complex design space that emerges from this joint optimization, we introduce an innovative super-bit model. This single-path methodology elegantly encodes all potential compression configurations, sidestepping the cumbersome need for multiple paths that each represent a different option. At the heart of our strategy lies a novel decomposition of quantization, which encompasses all candidate bitwidths within the search space. We initiate our exploration from a minimal bitwidth and progressively consider higher bitwidths, adding re-assignment offsets in a recursive manner, much like a master craftsman refining their work step by step.  Further enhancing our approach, we incorporate learnable binary gates that deftly encode the choice of bitwidth, including the ability to prune filters down to a 0-bit representation. By jointly training these binary gates alongside the network parameters, we empower the model to autonomously determine the optimal compression configuration for each layer.  The benefits of our ABS framework are twofold: first, it deftly sidesteps the combinatorial explosion of the design space, resulting in a streamlined number of trainable parameters and reduced search costs. Second, it alleviates the challenges associated with fitting an extremely low-bit quantizer to the data, significantly diminishing the optimization hurdles posed by non-differentiable quantization.  Our empirical evaluations on CIFAR-100 and ImageNet demonstrate that ABS not only achieves remarkable reductions in computational costs but also maintains impressive performance, heralding a new era in the efficient design of compressed models.", "introduction": "Deep neural networks (DNNs) have achieved great success in many challenging computer vision tasks, including image classification (Krizhevsky et al., 2012;He et al., 2016) and object detection (Lin et al., 2017a;b).However, a deep model usually has a large number of parameters and consumes huge amounts of computational resources, which remains great obstacles for many applications, especially on resource-limited devices with limited memory and computational resources, such as smartphones.To reduce the number of parameters and computational overhead, many methods (He et al., 2019;Zhou et al., 2016) have been proposed to conduct model compression by removing the redundancy while maintaining the performance.In the last decades, we have witnessed a lot of model compression methods, such as network pruning (He et al., 2017;2019) and quantization (Zhou et al., 2016;Hubara et al., 2016).Specifically, network pruning reduces the model size and computational costs by removing redundant modules while network quantization maps the full-precision values to low-precision ones.It has been shown that sequentially perform network pruning and quantization is able to get a compressed network with small model size and lower computational overhead (Han et al., 2016).However, performing pruning and quantization in a separate step may lead to sub-optimal results.For example, the best quantization strategy for the uncompressed network is not necessarily the optimal one after network pruning.Therefore, we need to consider performing pruning and quantization simultaneously.Recently, many attempts have been made to automatically determine the compression configurations of each layer (i.e., pruning ratios, and/or bitwidths), either based on reinforcement learning (RL) (Wang et al., 2019), evolutionary search (ES) (Wang et al., 2020), Bayesian optimization (BO) (Tung & Mori, 2018) or differentiable methods (Wu et al., 2018;Dong & Yang, 2019).In particular, previous differentiable methods formulate model compression as a differentiable searching problem to explore the search space using gradient-based optimization.As shown in Figure 1(a), each candi- \u2026  32   4   2  + Input \u01b8  Output \u01b8  =  2   2 + \u22ef +  32   32  4  32  2 4-bit (a) Multi-path scheme (Wu et al., 2018)  4 \u0302 =  2 +  4   4 + \u22ef +  16   16 +  32   32 Input Super-bit  Output \u0302  2  4  32  4 =  2 +  4  8 =  4 +  8 \u2026  32 =  16 +  32  2    4   4  +  4    8   8  \u2026 +    32   32  +  32  16  16  32(b) Single-path scheme (Ours)Figure 1: Multi-path v.s.single-path compression scheme.(a) Multi-path search scheme (Wu et al., 2018): represents each candidate configuration as a separate path and formulates the compression problem as a path selection problem, which gives rise to huge numbers of trainable parameters and high computational overhead when the search space becomes combinatorially large.Here, z k is the k-bit quantized version of z and \u03b1 q k is the architecture parameters corresponding to the path of k-bit quantization.(b) Single-path search scheme (Ours): represents each candidate configuration as a subset of a \"super-bit\" and formulates the compression problem as a subset selection problem, which greatly reduces the computational costs and optimization difficulty from the discontinuity of quantization.Here, the super-bit denotes the highest bitwidth in the search space, g q k is a binary gate that controls the decision of bitwidth, and k is the re-assignment offset (quantized residual error).date operation is maintained as a separate path, which leads to a huge number of trainable parameters and high computational overhead when the search space becomes combinatorially large.Moreover, due to the non-differentiable quantizer and pruning process, the optimization of heavily compressed candidate networks can be more challenging than that in the conventional search problem.In this paper, we propose a simple yet effective model compression method named Automatic Bit Sharing (ABS) to reduce the search cost and ease the optimization for the compressed candidates.Inspired by recent single-path neural architecture search (NAS) methods (Stamoulis et al., 2019;Guo et al., 2020), the proposed ABS introduces a novel single-path super-bit to encode all effective bitwidths in the search space instead of formulating each candidate operation as a separate path, as shown in Figure 1(b).Specifically, we build upon the observation that the quantized values of a high bitwidth can share the ones of low bitwidths under some conditions.Therefore, we are able to decompose the quantized representation into the sum of the lowest bit quantization and a series of re-assignment offsets.We then introduce learnable binary gates to encode the choice of bitwidth, including filter-wise 0-bit for pruning.By jointly training the binary gates and network parameters, the compression ratio of each layer can be automatically determined.The proposed scheme has several advantages.First, we only need to solve the search problem as finding which subset of the super-bit to use for each layer's weights and activations rather than selecting from different paths.Second, we enforce the candidate bitwidths to share the common quantized values.Hence, we are able to optimize them jointly instead of separately, which greatly reduces the optimization difficulty from the discontinuity of discretization.Our main contributions are summarized as follows:\u2022 We devise a novel super-bit scheme that encapsulates multiple compression configurations in a unified single-path framework.Relying on the super-bit scheme, we further introduce learnable binary gates to determine the optimal bitwidths (including filter-wise 0-bit for pruning).The proposed ABS casts the search problem as subset selection problem, hence significantly reducing the search cost.\u2022 We formulate the quantized representation as a gated combination of the lowest bitwidth quantization and a series of re-assignment offsets, in which we explicitly share the quantized values between different bitwidths.In this way, we enable the candidate operations to learn jointly rather than separately, hence greatly easing the optimization, especially in the non-differentiable quantization scenario.\u2022 We evaluate our ABS on CIFAR-100 and ImageNet over various network architectures.Extensive experiments show that the proposed method achieves the state-of-the-art performance.For example, on ImageNet, our ABS compressed MobileNetV2 achieves 28.5\u00d7 Bit-Operation (BOP) reduction with only 0.2% performance drop on the Top-1 accuracy."}
{"paper_id": 188, "abstract": "In the intricate realm of neural networks, one of the most pressing challenges lies in determining the ideal number of hidden neurons while ensuring that we can establish tight statistical risk bounds. In this exploration, we unveil a novel lens through which to view the bias-variance tradeoff inherent in these networks. Rather than fixating solely on the neuron count, we demonstrate theoretically that employing \\(L_1\\) regularization serves as a powerful tool to manage generalization error and effectively sparsify the input dimensions.  Specifically, by applying \\(L_1\\) regularization to the output layer, we can achieve a statistical risk that approaches the coveted minimax optimality. Furthermore, when we impose \\(L_1\\) regularization on the input layer, we derive a risk bound that intriguingly remains independent of the input data's dimensionality. Our analysis weaves together dimension-based and norm-based complexity frameworks, creating a robust method for bounding generalization error.  A striking revelation from our findings is that an abundance of neurons does not inherently lead to increased generalization errors\u2014provided that appropriate regularization is in place. In this dance of complexity and control, we find a path forward, one that balances the delicate interplay between capacity and performance in the ever-evolving landscape of neural networks.", "introduction": "Neural networks have been successfully applied in modeling nonlinear regression functions in various domains of applications.A critical evaluation metric for a predictive learning model is to measure its statistical risk bound.For example, the L 1 or L 2 risks of typical parametric models such as linear regressions are at the order of (d/n) 1/2 for small d (Seber & Lee, 2012), where d and n denote respectively the input dimension and number of observations.Obtaining the risk bound for a nonparametric regression model such as neural networks is highly nontrivial.It involves an approximation error (or bias) term as well as a generalization error (or variance) term.The standard analysis of generalization error bounds may not be sufficient to describe the overall predictive performance of a model class unless the data is assumed to be generated from it.For the model class of two-layer feedforward networks and a rather general data-generating process, Barron (1993;1994) proved an approximation error bound of O(r -1/2 ) where r denotes the number of neurons.The author further developed a statistical risk error bound of O((d/n) 1/4 ), which is the tightest statistical risk bound for the class of two-layer neural networks up to the authors' knowledge (for d < n).This risk bound is based on an optimal bias-variance tradeoff involving an deliberate choice of r.Note that the risk is at a convergence rate much slower than the classical parametric rate.We will tackle the same problem from a different perspective, and obtain a much tighter risk bound.A practical challenge closely related to statistical risks is to select the most appropriate neural network architecture for a particular data domain (Ding et al., 2018).For two-layer neural networks, this is equivalent to selecting the number of hidden neurons r.While a small r tends to underfit, researchers have observed that the network is not overfitting even for moderately large r.Nevertheless, recent research has also shown that an overly large r (e.g., when r > n) does cause overfitting with high probability (Zhang et al., 2016).It can be shown under some non-degeneracy conditions that a two-layer neural network with more than n hidden neurons can perfectly fit n arbitrary data, even in the presence of noise, which inevitably leads to overfitting.A theoretical choice of r suggested by the asymptotic analysis in (Barron, 1994) is at the order of (n/d) 1/2 , and a practical choice of r is often from cross-validation with an appropriate splitting ratio (Ding et al., 2018).An alternative perspective that we advocate is to learn from a single neural network with sufficiently many neurons and an appropriate L 1 regularization on the neuron coefficients, instead of performing a selection from multiple candidate neural models.A potential benefit of this approach is easier hardware implementation and computation since we do not need to implement multiple models separately.Perhaps more importantly, this perspective of training enables much tighter risk bounds, as we will demonstrate.In this work, we focus on the model class of two-layer feedforward neural networks.Our main contributions are summarized below.First, we prove that L 1 regularization on the coefficients of the output layer can produce a risk bound O((d/n) 1/2 ) (up to a logarithmic factor) under the L 1 training loss, which approaches the minimax optimal rate.Such a rate has not been established under the L 2 training loss so far.The result indicates a potential benefit of using L 1 regularization for training a neural network, instead of selecting from a number of neurons.Additionally, a key ingredient of our result is a unique amalgamation of dimension-based and norm-based risk analysis, which may be interesting on its own right.The technique leads to an interesting observation that an excessively large r can reduce approximation error while not increasing generalization error under L 1 regularizations.This implies that an explicit regularization can eliminate overfitting even when the specified number of neurons is enormous.Moreover, we prove that the L 1 regularization on the input layer can induce sparsity by producing a risk bound that does not involve d, where d may be much larger compared with the true number of significant variables.Related work on neural network analysis.Despite the practical success of neural networks, a systematic understanding of their theoretical limit remains an ongoing challenge and has motivated research from various perspectives.Cybenko (1989) showed that any continuous function could be approximated arbitrarily well by a two-layer perceptron with sigmoid activation functions.Barron (1993;1994) established an approximation error bound of using two-layer neural networks to fit arbitrary smooth functions and their statistical risk bounds.A dimension-free Rademacher complexity for deep ReLU neural networks was recently developed (Golowich et al., 2017;Barron & Klusowski, 2019).Based on a contraction lemma, a series of norm-based complexities and their corresponding generalization errors are developed (Neyshabur et al., 2015, and the references therein).Another perspective is to assume that the data are generated by a neural network and convert its parameter estimation into a tensor decomposition problem through the score function of the known or estimated input distribution (Anandkumar et al., 2014;Janzamin et al., 2015;Ge et al., 2017;Mondelli & Montanari, 2018).Also, tight error bounds have been established recently by assuming that neural networks of parsimonious structures generate the data.In this direction, Schmidt-Hieber (2017) proved that specific deep neural networks with few non-zero network parameters can achieve minimax rates of convergence.Bauer & Kohler (2019) developed an error bound that is free from the input dimension, by assuming a generalized hierarchical interaction model.Related work on L 1 regularization.The use of L 1 regularization has been widely studied in linear regression problems (Hastie et al., 2009, Chapter 3).The use of L 1 regularization for training neural networks has been recently advocated in deep learning practice.A prominent use of L 1 regularization was to empirically sparsify weight coefficients and thus compress a network that requires intensive memory usage (Cheng et al., 2017).The extension of L 1 regularization to group-L 1 regularization (Yuan & Lin, 2006) has also been extensively used in learning various neural networks (Han et al., 2015;Zhao et al., 2015;Wen et al., 2016;Scardapane et al., 2017).Despite the above practice, the efficacy of L 1 regularization in neural networks deserves more theoretical study.In the context of two-layer neural networks, we will show that the L 1 regularizations in the output and input layers play two different roles: the former for reducing generalization error caused by excessive neurons while the latter for sparsifying input signals in the presence of substantial redundancy.Unlike previous theoretical work, we consider the L 1 loss, which ranks among the most popular loss functions in, e.g., learning from ordinal data (Pedregosa et al., 2017) or imaging data (Zhao et al., 2016), and for which the statistical risk has not been studied previously.In practice, the use of L 1 loss for training has been implemented in prevalent computational frameworks such as Tensorflow (Google, 2016), Pytorch (Ketkar, 2017), and Keras (Gulli & Pal, 2017)."}
{"paper_id": 189, "abstract": "In the realm of machine learning, the quest for true robustness in classifiers often leads us into treacherous waters. This paper embarks on an exploration of adversarial accuracy and the intricacies of adversarial training, revealing the fundamental flaws in our conventional understanding. We assert that the standard measure of adversarial accuracy is woefully inadequate, failing to capture the essence of a classifier's resilience. It presents a perilous tradeoff with standard accuracy, a dilemma that persists even when we set aside the complexities of generalization.  To navigate these turbulent waters, we introduce a novel concept: genuine adversarial accuracy. This new measure stands apart, allowing us to evaluate the robustness of classifiers without sacrificing performance on unperturbed data or yielding to the whims of adversarial perturbations. Moreover, it does not favor models that rely on invariance-based adversarial examples\u2014those peculiar samples that retain their predicted classes despite shifts in perceptual classifications.  Our findings culminate in a striking revelation: a single nearest neighbor (1-NN) classifier emerges as the paragon of robustness when judged by genuine adversarial accuracy, provided we are working with unique class labels and a norm-based distance metric. This insight leads us to ponder the implications of our distance metrics, suggesting that their inadequacies might be a significant contributor to the tradeoff we observe between test accuracy and $l_p$ norm-based adversarial robustness. In this unfolding narrative, we invite you to reconsider the very foundations of classifier evaluation and embrace a deeper understanding of what it means to be truly robust in the face of adversarial challenges.", "introduction": "Even though deep learning models have shown promising performances in image classification tasks (Krizhevsky et al., 2012), most deep learning classifiers are vulnerable to adversarial attackers.By applying a carefully crafted, but imperceptible perturbation to input images, so-called adversarial examples can be constructed that cause the classifier to misclassify the perturbed inputs (Szegedy et al., 2013).These vulnerabilities have been shown to be exploitable even when printed adversarial images were read through a camera (Kurakin et al., 2016).Adversarial examples for a specific classifier can be transferable to other models (Goodfellow et al., 2014).The transferability of adversarial examples (Papernot et al., 2017) enables attackers to exploit vulnerabilities even with limited access to the target classifier.Problem setting.In a nonempty clean input set X \u2282 R d , let every sample x exclusively belong to one of the classes Y, and their classes will be denoted as c x .A classifier f assigns a class label from Y for each sample x \u2208 R d .Assume f is parameterized by \u03b8 and L(\u03b8, x, y) is the cross entropy loss of the classifier provided the input x and the label y \u2208 Y.Note that this exclusive class assumption is introduced to simplify the analysis.Otherwise, the definition of adversarial examples (Biggio et al., 2013) may not match with our intuition as explained in Section 1.1."}
{"paper_id": 190, "abstract": "In the realm of physical reasoning, the art of forward prediction emerges as a vital skill\u2014one that allows us to envision the unfolding of events based on an initial state of the world. In our exploration, we delve into the capabilities of cutting-edge forward-prediction models as they tackle the intricate challenges presented by the PHYRE benchmark (Bakhtin et al., 2019). By weaving together models that interpret the world through object-based or pixel-based lenses, we enhance the capabilities of straightforward physical-reasoning agents.  What we discover is both illuminating and complex: while these forward-prediction models significantly bolster performance, particularly in tasks laden with multiple objects, their effectiveness hinges on the nature of the test tasks\u2014often mere variations of those encountered during training. The leap to generalize across entirely new task templates proves to be a formidable challenge. Intriguingly, we find that a higher pixel accuracy in our forward predictors does not automatically translate to superior physical-reasoning outcomes. Yet, despite these hurdles, our most refined models achieve a remarkable milestone, establishing a new benchmark for excellence within the PHYRE framework.", "introduction": "When presented with a picture of a Rube Goldberg machine, we can predict how the machine works.We do so by using our intuitive understanding of concepts such as force, mass, energy, collisions, etc., to imagine how the machine state would evolve once released.This ability allows us to solve real world physical-reasoning tasks, such as how to hit a billiards cue such that the ball ends up in the pocket, or how to balance the weight of two children on a see-saw.In contrast, physical-reasoning abilities of machine-learning models have largely been limited to closed domains such as predicting dynamics of multi-body gravitational systems (Battaglia et al., 2016), stability of block towers (Lerer et al., 2016), or physical plausibility of observed dynamics (Riochet et al., 2018).In this work, we explore the use of imaginative, forward-prediction approaches to solve complex physical-reasoning puzzles.We study modern object-based (Battaglia et al., 2016;Sanchez-Gonzalez et al., 2020;Watters et al., 2017) and pixel-based (Finn et al., 2016;Ye et al., 2019;Hafner et al., 2020) forward-prediction models in simple search-based agents on the PHYRE benchmark (Bakhtin et al., 2019).PHYRE tasks involve placing one or two balls in a 2D world, such that the world reaches a state with a particular property (e.g., two balls are touching) after being played forward.PHYRE tasks are very challenging because small changes in the action (or the world) can have a very large effect on the efficacy of an action; see Figure 1 for an example.Moreover, PHYRE tests models' ability to generalize to completely new physical environments at test time, a significantly harder task than prior work that mostly varies number or properties of objects in the same environment.As a result, physical-reasoning agents may struggle even when their forward-prediction model works well.Nevertheless, our best agents substantially outperform the prior state-of-the-art on PHYRE.Specifically, we find that forward-prediction models can improve the performance of physical-reasoning agents when the models are trained on tasks that are very similar to the tasks that need to be solved at test time.However, we find forward-prediction based agents struggle to generalize to truly unseen tasks, presumably, because small deviations in forward predictions tend to compound over time.We also observe that better forward prediction does not always lead to better physical-reasoning performance on PHYRE (c.f.Buesing et al. (2018) for similar observations in RL).In particular, we find that object-based forward-prediction models make more accurate forward predictions but pixel-based models are more helpful in physical reasoning.This observation may be the result of two key advantages of models using pixel-based state representations.First, it is easier to determine whether a task is solved in a pixel-based representation than in an object-based one, in fully observable 2D environments like PHYRE.Second, pixel-based models facilitate end-to-end training of the forward-prediction model and the task-solution model in a way that object-based models do not in the absence of a differentiable renderer (Liu et al., 2019;Loper & Black, 2014)."}
{"paper_id": 191, "abstract": "In this exploration, we delve into the intricate realm of finite-sample prediction risk associated with the high-dimensional least squares estimator. As we traverse the landscape of statistics, we unveil a central limit theorem that governs the behavior of prediction risk in the context where both the sample size and the number of features soar toward infinity. Along this journey, we also present the finite-sample distribution and construct confidence intervals for the prediction risk, illuminating the nuances of our findings. Our theoretical revelations reveal a fascinating paradox: the sample-wise non-monotonicity of prediction risk, shedding light on the counterintuitive \"more data hurt\" phenomenon. In this interplay of numbers and uncertainty, we uncover truths that challenge conventional wisdom, inviting further inquiry into the depths of high-dimensional estimation.", "introduction": "More data hurt refers to the phenomenon that training on more data can hurt the prediction performance of the learned model, especially for some deep learning tasks.Loog et al. (2019) shows that various standard learners can lead to sample-wise non-monotonicity.Nakkiran et al. (2019) experimentally confirms the sample-wise non-monotonicity of the test accuracy on deep neural networks.This challenges the conventional understanding in large sample properties: if an estimator is consistent, more data makes the estimator more stable and improves its finite-sample performance.Nakkiran (2019) considers adding one single data point to a linear regression task and analyzes its marginal effect to the test risk.Derezi\u0144ski et al. (2019) gives an exact non-asymptotic risk of the high-dimensional least squares estimator and observes the sample-wise non-monotonicity on mean square error.For adversarially robust models, Min et al. (2020) proves that more data may increase the gap between the generalization error of adversarially-trained models and standard models.Chen et al. (2020) shows that more training data causes the generalization error to increase in the strong adversary regime.In this work, we derive the finite-sample distribution of the prediction risk under linear models and prove the \"more data hurt\" phenomenon from an asymptotic point of view.Intuitively, the \"more data hurt\" stems from the \"double descent\" risk curve: as the model complexity increases, the prediction risk of the learned model first decreases and then increases, and then decreases again.The double descent phenomenon can be precisely quantified for certain simple models (Hastie et al. (2019); Mei & Montanari (2019); Ba et al. (2019); Belkin et al. (2019); Bartlett et al. (2020); Xing et al. (2019)).Among these works, Hastie et al. (2019) and Mei & Montanari (2019) use the tools from random matrix theory and explicitly prove the double descent curve of the asymptotic risk of linear regression and random features regression in high dimensional setup.Ba et al. (2019) gives the asymptotic risk of two-layer neural networks when either the first or the second layer is trained using a gradient flow.The second decline of the prediction risk in the double descent curve is highly related to the more data hurt phenomenon.In the over-parameterized regime when the model complexity is fixed while the sample size increases, the degree of over-parameterization decreases and becomes close to the interpolation boundary (for example p/n = 1 in Hastie et al. (2019)), in which a high prediction risk is achieved.However, the existing asymptotic results, which focus on the first-order limit of the prediction risk, cannot fully describe the more data hurt phenomenon.In fact, the \"double descent\" curve is a function of the limiting ratio lim p/n, which may not be able to characterize the empirical prediction risk in finite sample situations.There will be a non-negligible discrepancy between the empirical prediction risk and its limit, especially when the sample size or dimension is small.Finegrained second-order results are thus needed to fully characterize such discrepancy and further, a confidence band for the prediction risk can be constructed to evaluate its finite sample performance.We take Figure 1 as an example to illustrate this.According to the first-order limit, given a fixed dimension p = 100, the prediction risks at sample size n = 90 and n = 98 are about 10.20 and 49.02.More data hurt seems true.However, the 95% confidence interval of the prediction risks with sample size 98 is [4.91, 142.12], which contains the risk for n = 90.Then more data hurt is not statistically significant.Hence, in this work, we characterize the second-order fluctuations of the prediction risk and make attempts to fill this gap.We employ the linear regression task in Hastie et al. (2019) and Nakkiran (2019), and introduce new tools from the random matrix theory, e.g. the central limit theorems for linear spectral statistics in Bai & Silverstein (2004); Bai et al. (2007), to derive the central limit theorem of the prediction risk.Consider a linear regression task with n data points and p features, the setup of the more data hurt is similar to that in the classical asymptotic analysis in Van der Vaart (2000).According to the classical asymptotic analysis with p fixed and n \u2192 \u221e, the least square estimator is unbiased and \u221a n-consistent to the ground truth.This implies that the more data will not hurt and even improve the prediction performance.However, the story is very different in the over-parameterized regime.The prediction risk doesn't decrease monotonously with n when p > n.More data does hurt in the over-parameterized case.In the following, we will justify this phenomenon by developing the second-order asymptotic results as both n and p tend to infinity.We assume p/n \u2192 c, and denote 0 < n 1 < n 2 < +\u221e, c 1 = p/n 1 and c 2 = p/n 2 .Then the direct comparison of the prediction risk between sample sizes n 1 and n 2 can be decomposed into three parts: (i) the gap between the finitesample risk under n = n 1 and the asymptotic risk with c = c 1 ; (ii) the gap between the finite-sample risk under n = n 2 and the asymptotic risk with c = c 2 ; (iii) the comparison between two asymptotic risks under c = c 1 and c = c 2 .Theorem 1 and 2 of Hastie et al. (2019) give answers to the task (iii).For (i) and (ii), we develop in this paper the convergence rate and the limiting distribution of the prediction risk as n, p \u2192 +\u221e, p/n \u2192 c.Furthermore, the confidence interval of the finite-sample risk can be obtained as well.Figure 1: Sample-wise double descent.We take p = 100 and 1 \u2264 n \u2264 200.Left: The conditional density of the prediction risk when sample size varies from 1 to 200.According to the conditional distribution of the prediction risk, we can observe the sample-wise double descent phenomenon.Right: The 95%-confidence band (point-wise) of the prediction risk.In the over-parameterized regime 1 \u2264 n < 100, there exists some pairs (n 1 , n 2 ), 1 \u2264 n 1 < n 2 < 100 such that the upper boundary of the confidence interval at n 1 is smaller than the lower boundary of the confidence interval at n 2 .This confirms the more data hurt phenomenon.The main goal of this paper is to study the second order asymptotic behavior of two different types of conditional prediction risk in the linear regression model.One is R X,\u03b2 ( \u03b2, \u03b2) given both the training data and regression coefficient while the other is R X ( \u03b2, \u03b2) given the training data only.We summarize our main results as follows: (1) The regression coefficient is set to be either random or nonrandom to cover more cases.Different convergence rates and limiting distributions of both prediction risk are derived under various scenarios.(2) In particular, the finite-sample distribution of the conditional prediction risk given both the training data and regression coefficient is derived and the sample-wise double descent is characterized in Theorem 4.2 and Theorem 4.5 (see Figure 1).Under certain assumptions, the more data hurt phenomenon can be confirmed by comparing the confidence intervals built via the central limit theorems.(3) Our results incorporate non-Gaussian observations.For Gaussian data, the limiting mean and variance in the central limit theorems have simpler forms, see Section 4.2 and 4.3 for more details.The rest of this paper is organized as follows.Section 3 introduces the model settings and two different prediction risk.Section 4 presents the main results on CLTs for the two types of risk with discussion.Section 5 conducts simulation experiments to verify the main results.All the technical proofs and lemmas are relegated to the appendix in the supplementary file.The double descent curve describes how generalization ability changes as model capacity increases.It subsumes the classical bias-variance trade-off, a U-shape curve, and further show that the test error exhibits a second drop when the model capacity exceeds the interpolation threshold (Belkin et al. (2018);Geiger et al. (2019); Spigler et al. (2019); Advani & Saxe (2017)).The double descent phenomenon has been quantified for certain models, including two-layer neural networks via non-asymptotic bounds or asymptotic risk (Belkin et al. (2019);Muthukumar et al. (2020); Hastie et al. (2019); Mei & Montanari (2019); Ba et al. (2019)).As our results are based on the linear regression, we mainly focus on the literature of linear models.Muthukumar et al. (2020) and Bartlett et al. (2020) derive the generalization bounds for over-parametrized linear models and show the benefits of the interpolation.Hastie et al. (2019) gives the first-order limit of the generalization error for linear regressions as n, p \u2192 +\u221e.Derezi\u0144ski et al. (2019) provides an exact non-asymptotic expression for double descent of the high-dimensional least square estimator.Wu & Xu (2020) extends the first-order limit of the prediction error of the generalized weighted ridge estimator to more general case with anisotropic features and signals.Montanari et al. (2019), Deng et al. (2019) and Kini & Thrampoulidis (2020) investigate the sharp asymptotic of binary classification tasks with the max-margin solution and the maximum likelihood solution.Emami et al. (2020) and Gerbelot et al. (2020a) consider the double descent in generalized linear models.Furthermore, the double descent phenomenon is also observed on linear tasks with various problems and assumptions, e.g.LeJeune et al. (2020); Gerbelot et al. (2020b); Javanmard et al. (2020); Dar & Baraniuk (2020); Xu & Hsu (2019); Dar et al. (2020).Xing et al. (2019) sharply quantifies the benefit of interpolation in the nearest neighbors algorithm.Mei & Montanari (2019) derives the limiting risk on the random features model and shows that minimum generalization error is achieved by highly overparametrized interpolators.Ba et al. (2019) gives the limiting risk of the regression problem under two-layer neural networks.However, the existing asymptotic results focus on the first-order limit of the prediction risk and do not indicate the convergence rate.There are very few second-order results in the literature, Shen & Bellec (2020) establishes the asymptotic normality for the derivatives of 2-layers neural network, but not the exact limiting distribution of the risk.In this work, we are the first to develop results on second-order fluctuations of the prediction risk in linear regressions and provide its corresponding confidence intervals.The more data hurt phenomenon is further justified from the asymptotic point of view.Random Matrix Theory The primary tool for analyzing the second-order fluctuations of prediction risk comes from random matrix theory.In particular, Bai & Silverstein (2004) refines the central limit theorem for linear spectral statistics of large dimensional sample covariance matrix with general population and the population is not necessary to be Gaussian.Similar central limit theorems are also developed for other random matrix ensembles, see Sinai & Soshnikov (1998); Bai & Yao (2005); Zheng (2012).Other than the central limit theorem for linear spectral statistics, Bai et al. (2007) and Pan & Zhou (2008) study the asymptotic fluctuation of eigenvectors of sample covariance matrices.Bai & Yao (2008) considers the fluctuation of quadratic forms.All these technical tools and results are adopted and fully utilized in this paper, especially those related to Stieltjes transform, which are closely connected to the prediction risk studied in this paper."}
{"paper_id": 192, "abstract": "In the realm of electronic system design, circuit routing has long stood as a formidable adversary, particularly in the intricate worlds of very large-scale integration (VLSI) and printed circuit boards (PCBs). The true challenge lies in the daunting task of interconnecting a multitude of electronic components while adhering to a labyrinth of design rules and constraints\u2014a quest that leads us into an expansive search space, one that has been proven to be NP-complete.  Traditionally, early solutions relied heavily on rigidly defined heuristics, often yielding suboptimal results and lacking the adaptability required for evolving design needs. Although recent advances have introduced a few learning-based approaches, these methods frequently come burdened with complexities that hinder their scalability to larger applications.   In response to this challenge, we unveil a novel algorithm for circuit routing, aptly named Ranking Cost (RC). This innovative approach seamlessly merges search-based techniques, such as the A* algorithm, with learning-driven strategies like Evolution Strategies, crafting an efficient and trainable router that thrives under careful parameterization. Unlike conventional two-stage routing methods, which separate global routing from detailed routing, our algorithm embraces a streamlined one-stage process that directly optimizes the overarching objective function. This design choice bestows remarkable flexibility, allowing for swift adaptation to new routing rules and constraints.  At the heart of our method lies a groundbreaking set of variables known as cost maps, which empower the A* router to discern optimal paths that align with the global objective. Additionally, we introduce a ranking parameter that not only establishes a hierarchy among options but also enhances the overall performance of our approach. Our algorithm is trained in an end-to-end fashion, eschewing the need for artificial data or human demonstrations.  In rigorous experiments, we pit our method against the classic A* algorithm and a standard reinforcement learning framework. The results speak volumes: our approach consistently outshines these baselines, achieving superior connectivity rates and enhanced scalability. An ablation study further underscores the efficacy of our trained cost maps, revealing their capacity to encapsulate global information and steer routing outcomes toward the elusive global optimum.", "introduction": "As described in Moore's Law (Schaller, 1997), the number of transistors in a dense integrated circuit (IC) increases exponentially over time and the complexity of chips and printed circuit boards (PCBs) becomes higher and higher.Such high complexity makes the IC design a time-consuming and errorprone work.Thus more capable automatic design systems, such as electronic design automation (EDA) tools, are needed to improve the performance.In the flow of IC designs, we need to find proper paths to place wires which connect electronic components on ICs, and these wires need to achieve expected connectivity under certain constraints.One of the most important constraints is that wires on the same layout should not intersect.In addition, to reduce the signal propagation delay, the wire-length should be minimized.This is a critical and challenging stage in the IC design flow (Hu & Sapatnekar, 2001), known as circuit routing, which has been studied by lots of researchers (Kramer, 1984;Zhang & Chu, 2012;He & Bao, 2020).Circuit routing involves a large number of nets (a net is a set of vertices with the same electrical property) to be routed, which is computationally expensive and makes manual design extremely time-consuming (Kong et al., 2009;Coombs & Holden, 2001).Even under the simplest setting, where only two pairs of pins need to be routed, it is an NP-complete problem (Kramer, 1984).Although lots of circuit routing algorithms have been proposed (Zhang, 2016), there still remain three major challenges: (1) Early solutions (Hu & Sapatnekar, 2001) are typically designed with hard-coded heuristics , which suffer from problems of non-optimal solutions (Zhang, 2016) and lack of flexibility over new design needs.Therefore, a more powerful routing method that does not depend on domain knowledge is highly desired.(2) To reduce the difficulty of complex routing problems, traditional routing algorithms often adopt a two-stage procedure -first global routing and then detailed routing (Chen & Chang, 2009;Kahng et al., 2011).The problem is that these two stages do not always coordinate well (Zhang & Chu, 2012;Shi & Davoodi, 2017).Sometimes a low-congested global routing result may lead to downstream detailed router un-routable.Hence, an end-to-end algorithm is preferred which can optimize the final global objective (e.g., the total wire-length) directly.(3) Although a few learning-based methods have been proposed (Liao et al., 2020;He & Bao, 2020) recently, their methods are hard to extend to large-scale applications.In real settings, there are lots of components and nets on a single chip, which shows greater demand for the scalability of routing algorithms.To relieve the problems mentioned above, we propose a new algorithm, denoted as Ranking Cost (RC), for circuit routing.In this paper, we innovatively combine search-based methods (i.e., A* algorithm) and learning-based methods (i.e., Evolution Strategies (Salimans et al., 2017)) to form an trainable router with proper parametrization.Our method is flexible for integrating new constraints and rules so long as they can be merged into the global objective function.Moreover, our method is an one-stage algorithm, which optimizes the global objective function directly.In our method, we introduce a new set of variables called cost maps, which can help the A* routers to find out proper paths to achieve the global objective.We also train a ranking parameter, which can produce the ranking order and further improve the performance of our method.In the experiments, we compare our method with the commonly used A* method and a canonical reinforcement learning approach, and results show that our method outperforms these baselines with higher connectivity rates.Our ablation study also shows that trained cost maps can capture the global information and guide the routing solution to approach global optimal.Experiments also show that our method is scalable to larger applications."}
{"paper_id": 193, "abstract": "In the realm of unsupervised learning, data clustering stands as a venerable technique, akin to a seasoned mage wielding the arcane arts of pattern recognition. Yet, even amidst the recent innovations fueled by deep neural networks, a formidable challenge persists: the elusive task of discerning the number of clusters within a dataset, all while navigating the treacherous waters of unlabeled data. Historically, practitioners have relied on classical methods rooted in statistical analysis, a laborious endeavor that requires the discerning eye of a data scientist to unearth the probable cluster count hidden within the data's intricate tapestry.  In this manuscript, we unveil a novel approach\u2014a beacon of clarity in the fog of uncertainty. Our method empowers the unsupervised prediction of cluster numbers, drawing forth insights from the raw data itself, unencumbered by the need for labels. We rigorously evaluate our technique across a diverse array of randomly generated datasets, employing the robust capabilities of the scikit-learn package, alongside a suite of computer vision datasets. The results speak volumes: our method not only meets the challenge head-on but does so with remarkable efficacy, illuminating the path to understanding the inherent structure of datasets without the guiding hand of supervision.", "introduction": "Clustering is an important task in machine learning, and it has a wide range of applications (Lung et al. (2004); Aminzadeh & Chatterjee (1984); Gan et al. (2007)).Clustering often consists of two steps: the feature extraction step and the clustering step.There have been numerous works on clustering (Xu & Tian (2015)), and among the proposed algorithms, K-Means (Bock (2007)) is renowned for its simplicity and performance.Despite its popularity, K-Means has several shortcomings discussed in (Ortega et al. (2009); Shibao & Keyun (2007)).In particular, with an increase in the dimensionality of the input data, K-Means' performance decreases (Prabhu & Anbazhagan (2011)).This phenomenon is called the curse of dimensionality (Bellman (2015)).Dimensionality reduction and feature transformation methods have been used to minimize this effect.These methods map the original data into a new feature space, in which the new data-points are easier to be separated and clustered (Min et al. (2018)).Some examples of existing data transformation methods are: PCA (Wold et al. (1987)), kernel methods (Hofmann et al. (2008)) and spectral methods (Ng et al. (2002)).Although these methods are effective, a highly complex latent structure of data can still challenge them ( (Saul et al., 2006;Min et al., 2018)).Due to the recent enhancements in deep neural networks ( Liu et al. (2017)) and because of their inherent property of non-linear transformations, these architectures have the potential to replace classical dimensionality reduction methods.In the research field of deep clustering, popularized by the seminal paper \"Unsupervised Deep Embedding for Clustering Analysis\" (Xie et al. (2016)), deep neural networks are adopted as the feature extractor and are combined with a clustering algorithm to perform the clustering task.A unique loss function is defined which updates the model.Deep clustering methods typically take k, the number of clusters, as a hyper-parameter.In real-world scenarios, where datasets are not labeled, assigning a wrong value to this parameter can reduce the overall accuracy of the model.Meta-learning, a framework that allows a model to use information from its past tasks to learn a new task quickly or with little data, has been adopted by a handful of papers (Ferrari & de Castro (2012);Ferrari & De Castro (2015); Garg & Kalai (2018); Kim et al. (2019); Jiang & Verma (2019)) to improve the performance of clustering tasks.Closest to our work is the approach proposed by Garg & Kalai (2018) that tries to predict the number of clusters in K-Means clustering using meta-information.To solve the same issue, we propose Meta-k, a gradient-based method for finding the optimal number of clusters and an attempt to have a self-supervised approach for clustering.Our work is based on the observation that a network can take input points and learn parameters to predict the best numberDecoder k of clusters, k.The predicted k, along with the points in the dataset, are the inputs to the clustering algorithm K-Means.For the clusters created, the silhouette score (Rousseeuw (1987)) is calculated.Using this metric value as the reward signal, we can compute the policy gradient to update the controller.As a result, in the next iteration, the controller gives higher probabilities to the k that causes better (closer to 1) silhouette scores to be calculated.To be able to perform optimized clustering on both low-and high-dimensional spaces, we augment our model with a feature extraction module, a deep auto-encoder.In this regard, our work is related to the idea of learning to learn or meta-learning, a general framework to use the information that is learned in one task for improving a future task.Figure 1 shows the diagram of our model for lowand high-dimensional data.We evaluate our method in multiple scenarios on different clustering tasks using synthetic and computer vision datasets, and we show that our approach can predict the number of clusters in most settings with an insignificant error.\u2022 A novel self-supervised approach for predicting the number of clusters using policy gradient methods.\u2022 Extensive evaluation on synthetic scikit-learn ( Pedregosa et al. (2011)) datasets and wellknown vision datasets MNIST ( LeCun et al. (2010)) and Fashion-MNIST ( Xiao et al. (2017)).\u2022 Our results show that our approach is able to predict the number of clusters in most scenarios identical or very close to the real number of data clusters.\u2022 We plan to release the source code of this work upon its acceptance."}
{"paper_id": 194, "abstract": "In the realm of computational puzzles, the Propositional Satisfiability Problem (SAT) stands as a formidable adversary, a challenge woven into the very fabric of mathematical inquiry. It beckons the seeker to discover an assignment of values to a collection of variables, all while adhering to a tapestry of intricate constraints. As the landscape of problem-solving evolves, a new champion has emerged: neural symbolic methods, which promise to illuminate the path through the labyrinth of the Constraint Satisfaction Problem (CSP).  Recent advancements have favored sequential model-based approaches, employing neural embeddings that intertwine reinforcement learning with the elegant structures of neural graph networks and graph recurrent neural networks. Yet, we propose a bold departure\u2014a one-shot model, inspired by the illustrious Transformer architecture, specifically tailored for the factor graph framework to tackle the CSP conundrum.  At the heart of our innovation lies a heterogeneous attention mechanism, intricately designed around meta-paths. This mechanism facilitates self-attention among literals and orchestrates cross-attention through the connections of a bipartite graph, bridging literals to clauses and back again. By harnessing the power of parallelism, our model not only accelerates the solving process but also achieves remarkable precision, proving its mettle on factor graphs of arbitrary size. With this approach, we stand on the brink of a new era in CSP resolution, where speed and accuracy harmonize in a dance of computational elegance.", "introduction": "The Constraint Satisfaction Problems (CSP) is of central importance in several aspects of computer science, including theoretical computer science, complexity theory, algorithmics, cryptography, and artificial intelligence.CSP aims at finding a consistent assignment of values to variables such that all constraints, which are typically defined over a finite domain, are satisfied.In particular, there is an assortment of problems arising from artificial intelligence and circuit design that can be reduced to CSP subtypes, including map coloring, vertex cover, independent set, dominating set, and clique detection.Solving a CSP on a finite domain is often an NP-complete problem with respect to the domain size.The conventional CSP-solvers rely on handcrafted heuristics that guide the search for satisfying assignments.These algorithms are focused on solving CSP via backtracking or local search.Hence, the resulted model is bounded by the greedy strategy, which is generally sub-optimal.With the advent of Graph Neural Networks (Scarselli et al. (2009)), the geometric deep learning (Bronstein et al. (2017)) for Non-Euclidean data has become one of the most emerging fields of machine learning.In particular, it brought deep learning solutions to one of the most dominant combinatorial optimization problems, the Constraint Satisfaction Problem (CSP) (Khalil et al. (2017)).Works including NeuroSAT (Selsam et al. (2018)) and Circuit-SAT (Amizadeh et al. (2018)) commenced the study of neural methods targeted at CSP.Later works, such as Yolcu & P\u00f3czos (2019) and You et al. (2019), attempted to solve CSP through different deep learning approaches.However, most pioneering works, such as neural approaches utilizing RNN or Reinforcement Learning, are still restricted to sequential algorithms, while clauses are parallelizable even though they are strongly correlated through shared variables.In this work, we propose a hybrid model of the Transformer architecture (Vaswani et al. (2017)) and the Graph Neural Network for solving combinatorial problems, especially CSP.Our main contributions in this work are: (a) We derived meta-paths adopted from Sun et al. (2011) to formulate the message passing mechanism between homogeneous nodes (i.e., variable to variable, or clause to clause), which enable us to perform self-attention and let message pass through either variables sharing the same clauses, or clauses that include the same variables.We apply the cross-attention mechanism to optimize message exchanges between heterogeneous nodes(i.e., clause to variable, or variable to clause).(b) With the combination of homogeneous attention and heterogeneous atten-tion mechanisms on bipartite graph structure, we then combine Transformer with Neuro-Symbolic methods to resolve combinatorial optimization on graphs.(c) We proposed Heterogeneous Graph Transformer (HGT), a general framework for graphs with heterogeneous nodes.In this work, we trained the HGT framework to approximate the solutions of CSP (but not limited to CSP).Our model is able to achieve competitive accuracy, parallelism, and generality on CSP problems with arbitrary sizes"}
{"paper_id": 195, "abstract": "In the realm of point cloud data processing, where the intricacies of the real world collide with the precision of technology, a plethora of point-based methodologies have emerged, each claiming to push the boundaries of performance. Our exploration delves into the essence of this evolution, revealing two pivotal discoveries that illuminate the path forward.   First, we unveil the profound impact of auxiliary elements\u2014evaluation schemes, data augmentation techniques, and loss functions\u2014that operate independently of model architecture. These factors wield such influence that they often obscure the true capabilities of the underlying architectures. When we meticulously control for these variables, we find that PointNet++, despite its age, stands shoulder to shoulder with the latest innovations, proving that wisdom can indeed reside in the old.  Second, we introduce a surprisingly effective contender: SimpleView, a straightforward projection-based method that defies expectations. This unassuming approach not only matches but occasionally surpasses the performance of the most sophisticated, cutting-edge models on the ModelNet40 benchmark, all while boasting a footprint that is a mere fraction of PointNet++. Moreover, SimpleView excels on ScanObjectNN, a benchmark drawn from the complexities of the real world, showcasing its remarkable ability to generalize across datasets.  In this journey through the landscape of point cloud processing, we uncover not just advancements but the fundamental truths that will guide future exploration in this dynamic field.", "introduction": "Processing 3D point cloud data accurately is crucial in many applications including autonomous driving (Navarro-Serment et al., 2010) and robotics (Rusu et al., 2009).In these settings, sensors like LIDAR produce unordered sets of points that correspond to object surfaces.Correctly classifying objects from this data is important for 3D scene understanding (Uy et al., 2019).While classical approaches for this problem have relied on hand-crafted features (Arras et al., 2007), recent efforts have focused on the design of deep neural networks (DNNs) to learn features directly from raw point cloud data (Qi et al., 2017a).Deep learning-based methods have proven effective in aggregating information across a set of 3D points to accurately classify objects.The most widely adopted benchmark for comparing methods for point cloud classification has been ModelNet40 (Wu et al., 2015b).The accuracy on ModelNet40 has steadily improved over the last few years from 89.2% by PointNet (Qi et al., 2017a) to 93.6% by RSCNN (Liu et al., 2019c) (Fig. 1).This progress is commonly perceived to be a result of better designs of network architectures.However, after performing a careful analysis of recent works we find two surprising results.First, we find that auxiliary factors including differing evaluation schemes, data augmentation strategies, and loss functions affect performance to such a degree that it can be difficult to disentangle improvements due to the network architecture.Second, we find that a very simple projection-based architecture works surprisingly well, outperforming state-of-the-art point-based architectures.In deep learning, as results improve on a benchmark, attention is generally focused on the novel architectures used to achieve those results.However, there are many factors beyond architecture design that influence performance including data augmentation and evaluation procedure.We refer to these additional factors as a method's protocol.A protocol defines all details orthogonal to the network architecture that can be controlled to compare differing architectures.Note that it is possible for some specific form of loss or data augmentation to be tied to a specific architecture and inapplicable to other architectures.In these cases, it would be inappropriate to treat them as part of the protocol.However, for all the methods we consider in this paper, their losses and augmentation schemes are fully compatible with each other and can be considered independently.We do experiments to study the effect of protocol and discover that it accounts for a large difference in performance, so large as to obscure the contribution of a novel architecture.For example, the performance of the PointNet++ architecture (Qi et al., 2017b) jumps from 90.0\u00b10.3 to 93.3\u00b10.3, when switching from its original protocol to RSCNN's protocol (Liu et al., 2019c).We further find that the protocols that lead to the strongest performance rely on feedback from the test set, which differs from conventional evaluation setups.We re-evaluate prior architectures using the best augmentation and loss functions, while not using any feedback from the test set.We find that by taking protocol into account, the PointNet++ architecture performs competitively with more recent ones in various settings.In addition to the surprising importance of protocol, in reviewing past approaches, another surprising discovery is that a very simple projection based baseline works very well.One needs to simply project the points to depth maps along the orthogonal views, pass them through a light-weight CNN and fuse the features.We refer to this baseline as SimpleView.Compared to previous projection-based method (Roveri et al., 2018;Sarkar et al., 2018) for pointcloud classification, SimpleView is very simple.Prior methods have developed special modules for view selection, rendering, and feature merging, as well as use larger CNN backbones that are pretrained on ImageNet (refer to Sec. 2 for more details).In contrast, SimpleView has no such special operations, and only requires simple point projections, a much smaller CNN backbone, and no ImageNet pretraining.The discovery of SimpleView is surprising because recent state-of-the-art results have all been achieved by point-based architectures of increasing sophistication.In recent literature, it is often assumed that point-based methods are the superior choice for point-cloud processing as they \"do not introduce explicit information loss\" (Guo et al., 2020).Prior work has stated that \"convolution operation of these methods lacks the ability to capture nonlocally geometric features\" (Yan et al., 2020), that a projection-base method \"often demands a huge number of views for decent performance\" (Liu et al., 2019c), and that projection-based methods often \"fine-tune a pre-trained image-based architecture for accurate recognition\" (Liu et al., 2019c).It is thus surprising that a projection-based method could achieve state-of-the-art results with a simple architecture, only a few views, and no pretraining.On ModelNet40, SimpleView performs on par or better than more sophisticated state-of-the-art networks across various protocols, which includes the ones used by prior methods (Table .3) as well as our protocol (Table. 5).At the same time, SimpleView outperforms state-of-the-art architectures on ScanObjectNN (Uy et al., 2019), a real-world dataset where point clouds are noisy (background points, occlusions, holes in objects) and are not axis-aligned.SimpleView also demonstrates better cross-dataset generalization than prior works.Furthermore, SimpleView uses less parameters than state-of-the-art networks (Table.5).Note that we are not proposing a new architecture or method, but simply evaluating a simple and strong projection-based baseline for point-cloud classification that is largely ignored in the literature.We do not claim any novelty in the design of SimpleView because all of its components have appeared in the literature.Our contribution is showing that such a simple baseline works surprisingly well, which is a result absent in existing literature.It is worth noting that one might think that projection-based methods are not directly comparable with point-based methods because projection-based methods may have the full mesh as input, as opposed to just a point cloud.While this is true for existing results in the literature, it is not the case with SimpleView, whose input is the exact same point cloud given to a point-based method.In other words, SimpleView is directly comparable to a point-based method because they solve the exact same task.In summary, our contributions are threefold: \u2022 We show that training and evaluation factors independent of network architecture have a large impact on point-cloud classification performance.With these factors controlled for, PointNet++ performs as well as more recent architectures.\u2022 We demonstrate how SimpleView, a very simple projection based baseline performs surprisingly well on point-cloud classification.It performs on par with or better than prior networks on Mod-elNet40 while using fewer parameters.It also outperforms state-of-the-art methods on real-world point-cloud classification and achieves better cross-dataset generalization."}
{"paper_id": 196, "abstract": "In the ever-evolving landscape of machine learning, Graph Neural Networks (GNNs) have emerged as powerful tools for understanding the intricate web of relationships within graphs. At the heart of their prowess lies a remarkable trait: the ability to generate node representations that are permutation-equivariant. This property, while advantageous for certain applications, presents a conundrum. It inadvertently hinders GNNs from grasping the nuances of proximity\u2014those vital connections that reflect the closeness between nodes based on their paths through the graph.   While some GNN variants have attempted to address this shortcoming by focusing on proximity preservation, they often sacrifice the coveted permutation-equivariance in the process. Thus, the quest remains: how can we forge a path that allows GNNs to embrace both proximity-awareness and permutation-equivariance?  In this paper, we unveil Stochastic Message Passing (SMP), a groundbreaking approach that elegantly intertwines these two essential properties. By introducing stochastic node representations designed to uphold node proximities, we offer a solution that, while seemingly straightforward, is grounded in robust theoretical foundations. Our findings demonstrate that with the right parameterization, SMP can indeed harmonize the preservation of node proximities with the maintenance of permutation-equivariance.  Through extensive experiments, we showcase the effectiveness and efficiency of SMP across various tasks, including node classification and link prediction. Join us as we navigate this complex terrain, revealing the potential of GNNs to not only understand the structure of graphs but also to appreciate the subtle dance of proximity that lies within.", "introduction": "Graph neural networks (GNNs), as generalizations of neural networks in analyzing graphs, have attracted considerable research attention.GNNs have been widely applied to various applications such as social recommendation (Ma et al., 2019), physical simulation (Kipf et al., 2018), and protein interaction prediction (Zitnik & Leskovec, 2017).One key property of most existing GNNs is permutation-equivariance, i.e., if we randomly permutate the IDs of nodes while maintaining the graph structure, the representations of nodes in GNNs are permutated accordingly.Mathematically, permutation-equivariance reflects one basic symmetric group of graph structures.Although it is a desirable property for tasks such as node or graph classification (Keriven & Peyr\u00e9, 2019;Maron et al., 2019b), permutation-equivariance also prevents GNNs from being proximity-aware, i.e., permutation-equivariant GNNs cannot preserve walk-based proximities between nodes such as the shortest distance or high-order proximities (see Theorem 1).Pairwise proximities between nodes are crucial for graph analytical tasks such as link prediction (Hu et al., 2020;You et al., 2019).To enable a proximity-aware GNN, Position-aware GNN (P-GNN) (You et al., 2019) foot_0 proposes a sophisticated GNN architecture and shows better performance for proximity-aware tasks.But P-GNN needs to explicitly calculate the shortest distance between nodes and its computational complexity is unaffordable for large graphs.Moreover, P-GNN completely ignores the permutation-equivariance property.Therefore, it cannot produce satisfactory results when permutation-equivariance is helpful.In real-world scenarios, both proximity-awareness and permutation-equivariance are indispensable properties for GNNs.Firstly, different tasks may require different properties.For example, recommendation applications usually require the model to be proximity-aware (Konstas et al., 2009) while permutation-equivariance is a basic assumption in centrality measurements (Borgatti, 2005).Even for the same task, different datasets may have different requirements on these two properties.Taking link prediction as an example, we observe that permutation-equivariant GNNs such as GCN (Kipf & Welling, 2017) or GAT (Velickovic et al., 2018) show better results than P-GNN in coauthor graphs, but the opposite in biological graphs (please see Section 5.2 for details).Unfortunately, in the current GNN frameworks, these two properties are contradicting, as we show in Theorem 1. Whether there exists a general GNN to be proximity-aware while maintaining permutation-equivariance remains an open problem.In this paper, we propose Stochastic Message Passing (SMP), a general and simple GNN to preserve both proximity-awareness and permutation-equivariance properties.Specifically, we augment the existing GNNs with stochastic node representations learned to preserve proximities.Though seemingly simple, we prove that our proposed SMP can enable GNNs to preserve walk-based proximities in theory (see Theorem 2 and Theorem 3).Meanwhile, SMP is equivalent to a permutationequivariant GNN with certain parametrization and thus is at least as powerful as those GNNs in permutation-equivariant tasks (see Remark 1).Therefore, SMP is general and flexible in handling both proximity-aware and permutation-equivariant tasks, which is also demonstrated by our extensive experimental results.Besides, owing to the simple structure, SMP is computationally efficient, with a running time roughly the same as those of the most simple GNNs such as SGC (Wu et al., 2019) and is at least an order of magnitude faster than P-GNN on large graphs.Ablation studies further show that a linear instantiation of SMP is expressive enough as adding extra non-linearities does not lift the performance of SMP on the majority of datasets.Our contributions are as follows.\u2022 We propose SMP, a simple and general GNN to handle both proximity-aware and permutationequivariant graph analytical tasks.\u2022 We prove that SMP has theoretical guarantees in preserving walk-based proximities and is at least as powerful as the existing GNNs in permutation-equivariant tasks.\u2022 Extensive experimental results demonstrate the effectiveness and efficiency of SMP.We show that a linear instantiation of SMP is expressive enough on the majority of datasets."}
{"paper_id": 197, "abstract": "In the realm of program synthesis, where the arcane art of automatically crafting code from specifications unfolds, a compelling method emerges: the use of examples that illustrate the desired dance of input and output. Many contemporary strategies have dazzled us with their prowess, thriving on a diet of randomly generated input-output pairs. Yet, lurking beneath this veneer of success lies a troubling truth\u2014these approaches often falter when faced with data distributions that stray from the familiar randomness.   In our exploration, we unveil that this limitation extends beyond mere outliers, ensnaring even the most celebrated methods in its grasp. The tools wielded to combat this challenge, though valiant, prove inadequate against the tides of variance.   Thus, we forge a new path, introducing an adversarial framework designed to deftly steer the bias of synthetic data distributions. In our trials, this innovative approach not only rises to the occasion but also eclipses the performance of its predecessors, offering a beacon of hope in the quest for robust program synthesis.", "introduction": "Program synthesis has long been a key goal of AI research.In particular, researchers have become increasingly interested in the task of programming by example (PBE), where the goal is to generate a program consistent with a given set of input-output (I/O) pairs.Recent studies have achieved impressive results, capable of solving PBE problems that humans would find difficult (e.g., Sharma et al. (2017); Zohar & Wolf (2018); Ellis et al. (2019)).However these studies have a concerning weakness: since large, naturally occurring datasets of program synthesis problems do not exist, these studies train and test their models on synthetic datasets of randomly generated programs and I/O pairs.The justification for using these synthetic datasets is that if a model can correctly predict programs for arbitrary PBE problems, then it has likely learned the semantics of the programming language and can generalize to problems outside the synthetic data distribution (Devlin et al., 2017).While this justification is plausible, a model might also perform well because it has learned specific aspects of the synthetic data distribution, and recent studies have found this to be the case for several state-of-the-art models (Shin et al., 2019;Clymo et al., 2019).These studies find that current PBE models often perform poorly on distributions different from that of the training data, and they propose methods to mitigate this issue by generating synthetic data with more varied distributions.The idea behind these methods is that a model trained on more varied synthetic data should generalize to a wider variety of distributions, hopefully including those of real-world PBE problems.Nevertheless, we find that these methods are often insufficient.Previous studies differ on what constitutes a \"varied distribution\" of synthetic data, creating definitions based on problem-specific heuristics.While generating training data based on these heuristics does help models generalize to certain distributions, we find that models trained using these methods still fail to generalize to many other distributions, including those resembling distributions of real-world problems.Moreover, different methods fail to generalize to different distributions, raising the question of how one should construct test sets to evaluate these methods.While previous studies have arbitrarily picked test sets that they believe present a reasonable challenge for state-of-the-art methods, this approach may lead to overly optimistic evaluations.A study may report that a method performed well because the researchers failed to find those distributions on which the method performs poorly.In this paper, we propose an adversarial method to generate a training set.Our adversarial approach builds a training set iteratively, finding data distributions on which a given model performs poorly and adding data drawn from those distributions to the training set on each iteration.We test this method by using it to generate training data for the PCCoder model from Zohar & Wolf (2018), and we show that models trained using our method generalize to a variety of distributions better than previously proposed methods.Moreover, we propose using a variation of our adversarial approach to generate test sets to evaluate PBE methods.We create test sets for different versions of PCCoder using this approach and show that these test sets reveal weaknesses in models that are not obvious when using other test sets.This paper makes the following key contributions:1. We propose a new, adversarial method to generate desirable distributions on which to train models for PBE.2. We show that models trained using our method generalize to a variety of datasets better than models trained using previously proposed methods.3. We show that our adversarial approach may also be used to generate test sets that are less likely to overestimate the performance of a model."}
{"paper_id": 198, "abstract": "In the ever-evolving realm of machine learning, particularly when it comes to the intricate world of tabular data, a common belief has emerged: that beyond the basic scaling of numeric features, further engineering is often deemed unnecessary in the context of deep neural networks. Yet, this paper dares to challenge that notion, unveiling the untapped potential of advanced encodings for numeric streams in deep learning. Through a comprehensive exploration of the Automunge open-source Python library\u2014a veritable treasure trove for crafting tabular data pipelines\u2014we delve into a myriad of transformation techniques that can be applied to individual columns, forming intricate \u201cfamily tree\u201d structures that branch out into generations of derivations.  Among the arsenal of transformation options at our disposal are normalization, binning, noise injection, and derivatives, each offering unique pathways to enhance the representation of numeric features. By aggregating these methods into cohesive family tree sets, we present a compelling case for their application in presenting numeric features to machine learning models, allowing for a rich tapestry of configurations that vary in informational depth and nuance, particularly when dealing with numeric sets whose interpretations remain elusive.  Our experiments reveal a groundbreaking approach to data augmentation, leveraging noise injection to bolster model performance in scenarios plagued by limited training data. In doing so, we illuminate a path forward, demonstrating that the careful crafting of numeric transformations can significantly elevate the capabilities of deep learning models, paving the way for advancements in applications that have long been underserved.", "introduction": "Of the various modalities of machine learning application (e.g.images, language, audio, etc.) tabular data, aka structured data, as may comprise tables of feature set columns and collected sample rows, in my experience does not command as much attention from the research community, for which I speculate may be partly attributed to the general non-uniformity across manifestations precluding the conventions of most other modalities for representative benchmarks and availability of pre-trained architectures as could be adapted with fine-tuning to practical applications.That is not to say that tabular data lacks points of uniformity across data sets, for at its core the various feature sets can at a high level be grouped into just two primary types: numeric and categoric.It was the focus of a recent paper by this author (Author, 2020) to explore methods of preparing categoric sets for machine learning as are available in the Automunge open source python library platform for tabular data pipelines.This paper will give similar treatment for methods to prepare numeric feature sets for machine learning.Of course it would be an oversimplification to characterize \"numeric feature sets\" as a sufficient descriptor alone to represent the wide amount of diversity as may be found between different such instances.Numeric could be referring to integers, floats, or combinations thereof.The set of entries could be bounded, the potential range of entries could be bounded on the left, right, or both sides, the distribution of values could be thin or fat tailed, single or multi-modal.The order of samples could be independent or sequential.In some cases the values could themselves be an encoded representation of a categoric feature.Beyond the potential diversity found within our numeric features, another source of diversity could be considered based on relationships between multiple feature sets.For example one feature could be independent of the others, could contain full or partial redundancy with one or more other variables by correlation, or in the case of sequential data there could even be causal relationships between variables across time steps.The primary focus of transformations to be discussed in this paper will not take into account variable interdependencies, and will instead operate under the assumption that the training operation of a downstream learning algorithm may be more suitable for the efficient interpretation of such interdependencies, as the convention for Automunge is that data transformations (and in some cases sets of transformations) are to be directed for application to a distinct feature set as input.In many cases the basis for these transformations will be properties derived from the received feature in a designated \"train\" set (as may be passed to the automunge(.)function) for subsequent application on a consistent basis to a designated \"test\" set (as may be passed to the postmunge(.)function)."}
{"paper_id": 199, "abstract": "In the realm of inverse reinforcement learning (IRL), a challenge looms\u2014a challenge as intricate as the tangled threads of fate itself. The aim is clear: to unearth the hidden reward functions that guide expert demonstrations. Yet, like a riddle wrapped in shadows, the IRL problem reveals its flaws. A single policy may shine brightly as optimal for a multitude of reward functions, while those expert demonstrations can dance gracefully atop a spectrum of policies, each one seemingly valid.  In this work, we embark on a journey to reshape the IRL conundrum into a well-defined quest: the stochastic inverse reinforcement learning (SIRL) problem. Here, we seek not just a single reward function, but a tapestry of possibilities\u2014a probability distribution over these elusive rewards. To navigate this complex landscape, we employ the Monte Carlo expectation-maximization (MCEM) method, a beacon of clarity that illuminates our path. This method serves as our first step toward solving the SIRL problem, offering a solution that is not only succinct and robust but also transferable across various learning tasks.  Through our innovative approach, we gain a panoramic view of the IRL problem, revealing its intrinsic properties in a way that was previously obscured. The results speak for themselves, showcasing substantial performance improvements in the object world. In this unfolding narrative of learning, we find not just answers, but a myriad of alternative solutions to the IRL dilemma, each one a testament to the complexity and beauty of the journey we undertake.", "introduction": "The IRL problem addresses an inverse problem that a set of expert demonstrations determines a reward function over a Markov decision process (MDP) if the model dynamics are known Russell (1998); Ng et al. (2000).The recovered reward function provides a succinct, robust, and transferable definition of the learning task and completely determines the optimal policy.However, the IRL problem is ill-posed that the policy may be optimal for many reward functions and expert demonstrations may be optimal for many policies.For example, all policies are optimal for a constant reward function.In a real-world scenario, experts always act sub-optimally or inconsistently, which is another challenge.To overcome these limitations, two classes of probabilistic approaches for the IRL problem are proposed, i.e., Bayesian inverse reinforcement learning (BIRL) Ramachandran & Amir (2007) based on Bayesians' maximum a posteriori (MAP) estimation and maximum entropy IRL (MaxEnt) Ziebart et al. (2008); Ziebart (2010) based on frequentists' maximum likelihood (MLE) estimation.BIRL solves for the distribution of reward functions without an assumption that experts behave optimally, and encode the external a priori information in a choice of a prior distribution.However, BIRL also suffers from the practical limitation that a large number of algorithmic iterations is required for the procedure of Markov chain Monte Carlo (MCMC) in a sampling of posterior over reward functions.Advanced techniques, for example Kernel technique Michini & How (2012) and gradient method Choi & Kim (2011), are proposed to improve the efficiency and tractability of this situation.MaxEnt employs the principle of maximum entropy to resolve the ambiguity in choosing demonstrations over a policy.This class of methods, inheriting the merits from previous non-probabilistic IRL approaches including Ng et al. (2000); Abbeel & Ng (2004); Ratliff et al. (2006); Abbeel et al. (2008); Syed & Schapire (2008); Ho et al. (2016), imposes regular structures of reward functions in a combination of hand-selected features.Formally, the reward function is a linear or nonlinear combination of the feature basis functions which consists of a set of real-valued functions {\u03c6 i (s, a)} i=1 hand-selected by experts.The goal of this approach is to find the best-fitting weights of feature basis functions through the MLE approach.Wulfmeier et al. (2015) and Levine et al. (2011) use deep neural networks and Gaussian processes to fit the parameters based on demonstrations respectively but still suffer from the problem that the true reward shaped by the changing environment dynamics.Influenced by the work of Finn et al. (2016a;b), Fu et al. (2017) propose a framework called adversarial IRL (AIRL) to recover robust reward functions in a changing dynamics based on adversarial learning and achieves superior results.Compared with AIRL, another adversarial method called generative adversarial imitation learning (GAIL) Ho & Ermon (2016) seeks to directly recover the expert's policy rather than reward functions.Many follow-up methods enhance and extend GAIL for multipurpose in various application scenarios Li et al. (2017); Hausman et al. (2017); Wang et al. (2017).However, GAIL is in a lack of an explanation of expert's behavior and a portable representation for the knowledge transfer which are the merits of the class of the MaxEnt approach, because the MaxEnt approach is equipped with the \"transferable\" regular structures over reward functions.In this paper, under the framework of the MaxEnt approach, we propose a generalized perspective of studying the IRL problem called stochastic inverse reinforcement learning (SIRL).It is formulated as an expectation optimization problem aiming to recover a probability distribution over the reward function from expert demonstrations.The solution of SIRL is succinct and robust for the learning task in the meaning that it can generate more than one weight over feature basis functions which compose alternative solutions to the IRL problem.Benefits of the class of the MaxEnt method, the solution to our generalized problem SIRL is also transferable.Since of the intractable integration in our formulation, we employ the Monte Carlo expectation-maximization (MCEM) approach Wei & Tanner (1990) to give the first solution to the SIRL problem in a model-based environment.In general, the solutions to the IRL problem are not always best-fitting in the previous approaches because a highly nonlinear inverse problem with the limited information is very likely to get trapped in a secondary maximum in the recovery.Taking advantage of the Monte Carlo mechanism of a global exhaustive search, our MCEM approach avoids the secondary maximum and theoretically convergent demonstrated by pieces of literature Caffo et al. (2005);Chan & Ledolter (1995).Our approach is also quickly convergent because of the preset simple geometric configuration over weight space in which we approximate it with a Gaussian Mixture Model (GMM).Hence, our approach works well in a real-world scenario with a small and variability set of expert demonstrations.In particular, the contributions of this paper are threefold:1. We generalize the IRL problem to a well-posed expectation optimization problem SIRL.2. We provide the first theoretically existing solution to SIRL by the MCEM approach.3. We show the effectiveness of our approach by comparing the performance of the proposed method to those of the previous algorithms on the objectworld."}
{"paper_id": 200, "abstract": "In the grand tapestry of human cognition, we weave together the threads of knowledge through the art of compositionality, a skill that allows us to learn with both flexibility and efficiency. Yet, as we delve into the realm of machine learning, we find ourselves confronted with a stark reality: our current algorithms fall short of this remarkable ability. Despite numerous attempts to tackle specific instances of compositionality, a systematic framework for understanding it remains elusive.   In this paper, we embark on a quest, harnessing the power of group theory to illuminate the mysteries of compositional representations. We establish rigorous mathematical proofs that answer two pivotal questions: (1) What properties must a collection of components possess to be expressed in a compositional manner? (2) What characteristics define the mappings between compositional representations and their entangled counterparts?   To bring our findings to life, we present illustrative examples that clarify these conditions and demonstrate their practical application. Among our revelations, we offer a fresh perspective on the role of attention mechanisms, shedding light on how they bolster compositionality. Through this work, we aspire to deepen our understanding of compositionality and pave the way for advancements in artificial intelligence that bring us closer to the heights of human-like reasoning.", "introduction": "Humans recognize the world and create imaginations in a supple way by leveraging systematic compositionality to achieve compositional generalization, the algebraic capacity to understand and produce large amount of novel combinations from known components (Chomsky, 1957;Montague, 1970).This is a key element of human intelligence (Minsky, 1986;Lake et al., 2017), and we hope to equip machines with such ability.Conventional machine learning has been mainly developed with an assumption that training and test distributions are identical.Compositional generalization, however, is a type of out-of-distribution generalization (Bengio, 2017) which has different training and test distributions.In compositional generalization, a sample is a combination of several components.For example, an image object may have two factor components of color and rotation.In language, a sentence is composed of the lexical meanings and the grammatical structure.The generalization is enabled by recombining seen components for an unseen combination during inference.One approach for compositional generalization is to learn compositional representationsfoot_0 , or disentangled representation (Bengio, 2013), which contain several component representations.Each of them depends only on the corresponding underlying factor, and does not change when other factors change.Please see Section 3 for details.Multiple methods have been proposed to learn compositional representations.However, little discussion has been made for some fundamental questions.What kind of factor combinations can be expressed in compositional representation?Though there are some common factor components such as colors and size, what property enable them?When a set of components satisfy the conditions, what kind of mappings are available between the entangled and compositional representations?Can we use the conditions to explain compositionality in conventional models such as attention?In this paper, we mathematically prove two propositions (Proposition 1.1 and Proposition 1.2) for necessary and sufficient conditions regarding compositional representations.We construct groups for changes on representations, and relate compositional representation with group direct product, and compositional mapping with group action equivalence (Higgins et al., 2018).Then, we use theorems and propositions in group theory to prove the conditions.Proposition 1.1 (Compositional representation).A set of components can be expressed compositionally if and only if the subgroup product equals to the original group, each component subgroup is normal subgroup of the original group, and the group elements intersect only at identity element.Proposition 1.2 (Compositional mapping).Given compositional representation, a mapping is compositional if and only if each component has equivalent action in compositional and entangled representations, and for each element of the entangled representation, the orbits intersect only at the element.Please see Proposition 4.2 and Proposition 4.10 for symbolic statements.We also provide examples to better understand the conditions and how to use them (Section 5).For representations, we see that whether the components can be expressed with compositional representation does not depend only on each component itself, but also on their combination, and the possible values to take.We use the condition for compositional mapping to explain some existing neural network models and tasks, e.g., attention mechanism, spacial transformer and grammar tree nodes.We hope, with these examples, the conditions will be used for validating different compositional representations and mappings, and guiding designs of tasks and algorithms with compositionality.Our contributions can be summarized as follows.\u2022 We propose and prove necessary and sufficient conditions for compositional representation and compositional mapping.\u2022 We provide examples to understand and use the conditions, such as new explanation of attention models."}
{"paper_id": 201, "abstract": "In the ever-evolving landscape of natural language processing, the shadows of adversarial attacks loom large, casting doubt upon the robustness of our systems. Recent explorations into black-box adversarial attacks have sparked significant interest, revealing the vulnerabilities that lie beneath the surface of seemingly secure models. Traditionally, these attacks operate under the assumption that an adversary can glean output labels from a target model based on strategically chosen inputs.   In this paper, we embark on a novel journey, drawing inspiration from the concept of adversarial transferability. We introduce a groundbreaking approach to black-box NLP adversarial attacks, wherein an attacker can select a closely related domain to craft adversarial examples that seamlessly transfer to the target domain, ultimately undermining the target model's performance.   Guided by the principles of domain adaptation theory, we also unveil a formidable defensive strategy, aptly named Learn2Weight. This innovative method trains a model to predict the necessary weight adjustments for the target system, fortifying it against the onslaught of similar-domain adversarial examples.   Through rigorous empirical evaluation on the Amazon multi-domain sentiment classification dataset, we demonstrate that the Learn2Weight model stands resilient against adversarial attacks, outperforming traditional black-box defense mechanisms such as adversarial training and defense distillation. This work not only advances our understanding of machine learning safety but also strengthens the defenses of our NLP systems against the ever-present threat of adversarial manipulation.", "introduction": "As machine learning models are applied to more and more real-world tasks, addressing machine learning safety is becoming an increasingly pressing issue.Deep learning algorithms have been shown to be vulnerable to adversarial examples (Szegedy et al., 2013;Goodfellow et al., 2014;Papernot et al., 2016a).In particular, prior black-box adversarial attacks assume that the adversary is not aware of the target model architecture, parameters or training data, but is capable of querying the target model with supplied inputs and obtaining the output predictions.The phenomenon that adversarial examples generated from one model may also be adversarial to another model is known as adversarial transferability (Szegedy et al., 2013).Motivated by adversarial transferability, we conjecture another black-box attack pipeline where the adversary does not even need to have access to the target model nor query labels from crafted inputs.Instead, as long as the adversary knows the task of the target, he can choose a similar domain, to build a substitute model and then attack the target model with adversarial examples that are generated from the attack domain.The similar-domain adversarial attack may be more practical than prior blackbox attacks as label querying from target model is not needed.This attack can be illustrated with the following example (Figure 1b) in medical insurance fraud (Finlayson et al., 2019).Insurance companies may use hypothetical opioid risk models to classify the likelihood (high/low) of a patient to abuse the opioids to be prescribed, based on the patient's medical history as text input.Physicians can run the original patient history through the attack pipeline to generate an adversarial patient history, where the original is more likely to be rejected (\"High\" risk) and the adversarial is more likely to be accepted (\"Low\" risk).Perturbations in patient history could be, for example, a slight perturbation from \"alcohol abuse\" to \"alcohol dependence\", and it may successfully fool the insurance company's model.Based on domain adaption theory (Ben-David et al., 2010), we conjecture that it is the domain-variant features that cause the success of the similar-domain attack.The adversarial examples with domainvariant features are likely to reside in the low-density regions (far away from decision boundary) of the empirical distribution of the target training data which could fool the target model (Zhang et al., 2019b).Literature indicates that worsened generalizability is a tradeoff faced by existing defenses such as adversarial training (Raghunathan et al., 2019) and domain generalization techniques (Wang et al., 2019).In trying to increase robustness against adversarial inputs, a model faces a tradeoff of weakened accuracy towards clean inputs.Given that an adversarial training loss function is composed of a loss against clean inputs and loss against adversarial inputs, improper optimization where the latter is highly-optimized and the former weakly-optimized does not improve general performance in the real-world.To curb this issue, methods have been proposed (Zhang et al., 2019b;Lamb et al., 2019;Schmidt et al., 2018), such as factoring in under-represented data points in training set.To defend against this similar-domain adversarial attack, we propose a weight transfer network approach, Learn2Weight, so that the target model's decision boundary can adapt to the examples from low-density regions.Experiments confirm the effectiveness of our approach against the similardomain attack over other baseline defense methods.Moreover, our approach is able to improve robustness accuracy without losing the target model's standard generalization accuracy.Our contribution can be summarized as follows:\u2022 We are among the first to propose the similar-domain adversarial attack.This attack pipeline relaxes the previous black-box attack assumption that the adversary has access to the target model and can query the model with crafted examples.\u2022 We propose a defensive strategy for this attack based on domain adaptation theory.Experiment results show the effectiveness of our approach over existing defense methods, against the similar-domain attack."}
{"paper_id": 202, "abstract": "In the realm of programming, where lines of code weave intricate tapestries of logic, understanding the essence of source code transcends mere identifiers. It is a dance of tokens, bound by the grammar that governs their relationships. To truly grasp this complexity, we turn to the powerful tools of graph representation\u2014namely, the Abstract Syntax Tree (AST) and Control Flow Graph (CFG). These structures illuminate the hidden connections between tokens, revealing patterns that elude the naked eye.  Yet, despite their potential, many existing approaches falter, relying on late fusion techniques that fail to fully harness the synergy between textual and graphical representations. Enter our innovative solution: the GN-Transformer. This method forges a new path, merging insights gleaned from both graph and text modalities within the robust framework of Graph Networks (GN), enhanced by an attention mechanism that sharpens focus on relevant features.  At the heart of our approach lies the Syntax-Code Graph (SCG), a meticulously constructed graph that serves as the foundation for our embeddings. Through rigorous experimentation, we dissect the architecture of the SCG, conduct an ablation study on our model design, and explore the impact of hyperparameters. Our findings reveal a critical insight: the true strength of our method arises not from the minutiae of model specifics, but from the elegance of its fusion strategy.  The results speak for themselves. Our GN-Transformer achieves state-of-the-art performance across two prominent code summarization datasets, excelling in three distinct metrics. In this journey through the labyrinth of code understanding, we have unveiled a method that not only enhances comprehension but also sets a new standard for the field.", "introduction": "Code summarization is the task of generating a readable summary that describes the functionality of a snippet.Such task requires a high-level comprehension of a source code snippet thus it is an effective task to evaluate whether a Deep Learning Model is able to capture complex relations and structures inside code.Programming languages are context-free formal language, an unambiguous representation, Abstract Syntax Tree (AST), could be derived from a source code snippet.A parse tree based representation of code is precise and without noise.An AST accurately describes the structure of a snippet and relationships between tokens which provides valuable supplementary information for code understanding.Using graph representations of source code has been the focus of multiple methods that perform code summarization.For example, Alon et al. (2019a) encoded AST paths between tokens and aggregated them by an attention mechanism.Huo et al. (2020)  extract AST features, however the cross-modal interaction (Veli\u010dkovi\u0107, 2019) is very limited since the AST and code features are independently extracted by separate models then simply concatenated or summed.In this paper we propose a novel architecture GN-Transformer shown in Figure 2 to fuse Graph information with an equivalent sequence representation.In summary:\u2022 We extend Graph Networks (GN) (Battaglia et al., 2018) to a novel GN-Transformer architecture that is a sequence of GN encoder blocks followed by a vanilla Transformer decoder.\u2022 We propose a novel method for early fusion of the AST representation and that of a code snippet sequence called Syntax-Code Graph (SCG) \u2022 We evaluate our approach on the task of code summarization and outperform the previous state of the art in two datasets and across three metrics.We denote '+' as a residual connection followed by a normalization layer.In 'Node embeddings of graph batch', each black bar represents the nodes embedding of a graph in the input batch.Blue dots represent token nodes, grey dots denote padding.Nodes embedding in the grey box are fetched as input to the decoder and AST nodes embedding (red dots) are discarded.We evaluated our model on Java and Python datasets used by Ahmad et al. (2020).We compared our results to those of Ahmad et al. (2020).Two qualitative results are presented in Figure 1.We make available our code, trained models and pre-processed datasets in our supplementary package, and we will open-source it after the review process concludes."}
{"paper_id": 203, "abstract": "In the realm of extreme multi-label learning (XML), the quest to bestow objects with relevant labels from an expansive and often unwieldy set is fraught with challenges. Traditional approaches have tended to treat labels as if they were all equal, leading to a scenario where the models shine brightly on the more common head labels, while the tail labels languish in obscurity. Yet, in many practical applications, the ability to predict those elusive tail labels is of paramount importance.   In this study, we unveil both theoretical insights and experimental findings that highlight the shortcomings of prevailing XML methods when it comes to tail labels. Our investigation reveals a striking correlation: the norm of label classifier weights mirrors a long-tailed distribution akin to that of label frequency, resulting in a detrimental suppression of tail labels. Armed with this revelation, we introduce two innovative modules: (1) **\\algoa**, which re-ranks predictions by optimizing a population-aware loss, ensuring that tail labels rise to prominence; (2) **\\algob**, which employs a decoupled learning scheme to enhance the representation of tail labels, forging a more equitable classification boundary.  Through rigorous experimentation on widely recognized XML benchmarks, encompassing hundreds of thousands of labels, we demonstrate that our proposed methods significantly elevate the performance of leading XML models, achieving an impressive average gain of 6% in PSP@1. The journey to balance the scales of classification has only just begun, and our findings pave the way for a more inclusive approach to multi-label learning.", "introduction": "Extreme multi-label learning (XML) aims to annotate objects with relevant labels from an extremely large candidate label set.Recently, XML has demonstrated its broad applications.For example, in webpage categorization Partalas et al. (2015), millions of labels (categories) are collected in Wikipedia and one wishes to annotate new webpages with relevant labels from a huge candidate set; in recommender systems McAuley et al. (2015), one hopes to make informative personalized recommendations from millions of items.Because of the high dimensionality of label space, classic multi-label learning algorithms, such as Zhang & Zhou (2007); Tsoumakas & Vlahavas (2007), become infeasible.To this end, a number of computational efficient XML approaches are proposed Weston et al. (2011); Agrawal et al. (2013); Bi & Kwok (2013); Yu et al. (2014); Bhatia et al. (2015); E.-H.Yen et al. (2016); Yeh et al. (2017); Yen et al. (2017); Tagami (2017).In XML, one important statistical characteristic is that labels follow a long-tailed distribution as illustrated in Figure 4 (left).Most labels occur only a few times in the dataset.Infrequently occurring labels (referred to as tail label) possess limited training samples and are harder to predict than frequently occurring ones (referred to as head label).Many existing XML approaches treat labels with equal importance, such as Prabhu & Varma (2014); Babbar & Sch\u00f6lkopf (2017); Khandagale et al. (2019), while Wei & Li (2018) demonstrates that most predictions of well-established methods are heads labels.However, in many real-world applications, it is still desirable to predict more tail labels which are more rewarding and informative, such as recommender systems Jain et al. (2016); Babbar & Sch\u00f6lkopf (2019); Wei & Li (2018); Wei et al. (2019).To improve the performance for tail labels, existing solutions typically involve optimizing loss functions that are suitable for tail labels Jain et al. (2016); Babbar & Sch\u00f6lkopf (2019), leveraging the sparsity of tail labels in the annotated label matrix Xu et al. (2016), and transferring knowledge from data-rich head labels to data-scarce tail labels K. Dahiya (2019).These methods typically achieve better performance on tail labels than standard XML methods which treat labels equally, while they Figure 1: Left: Label frequency follows a long-tailed distribution.Middle: Norm of classifier weights of Bonsai models Khandagale et al. (2019).Right: Norm of classifier weights of Bonsai models when decoupled tail label augmentation is applied.usually involve high computational costs.Moreover, previous studies do not explicitly explain the underlying cause of the inferior performance of many standard XML methods for tail labels.In this work, we disclose theoretical and experimental evidence for the inferior performance of previous XML methods on tail labels.Our finding is that the norm of label classifier weights follows a long-tailed distribution similar to the label frequency as shown in Figure 4 (middle), and the prediction score of tail labels thereby is underrated.To alleviate this problem, we propose to rectify the classifier's outputs and training data distribution such that the prediction of tail labels is enhanced.We present two general modules suitable for any well-established XML methods:(1) RANKNET learns to re-rank the predictions by optimizing a population-aware loss function, which predicts tail labels with high rank; (2) TAUG augments tail labels via a decoupled learning scheme, which reduces the skewness of training data and yields more balanced classification boundary.We conduct experiments to verify the effectiveness of the aforementioned instantiations.From our extensive studies across four benchmark datasets, we make the following intriguing contributions:\u2022 We show that from both theoretical and experimental perspectives, the norm of label classifier weights follow a long-tailed distribution, i.e., the norms of head label classifier weights are considerably larger than that of tail label classifiers, which is a key cause of the inferior performance of many XML methods on tail labels.\u2022 We propose two general modules: RANKNET for prediction score re-ranking by optimizing a new population-aware loss, and TAUG for decoupled tail label augmentation.Both methods can be paired with any XML model without changing the model.\u2022 Experiments verify that our proposed modules achieve significant improvements (6% w.r.t.PSP@1 on average) for well-established XML methods on benchmark datasets.\u2022 We provide an ablation study to highlight the effectiveness of each individual factor."}
{"paper_id": 204, "abstract": "In this paper, we unveil Apollo, a groundbreaking quasi-Newton method tailored for the intricate realm of nonconvex stochastic optimization. Apollo distinguishes itself by intricately weaving the curvature of the loss function into its fabric, utilizing a diagonal matrix to approximate the Hessian. This algorithm operates with remarkable efficiency, relying solely on first-order gradients while dynamically updating the Hessian approximation in a manner that adheres to the weak secant condition.  To navigate the challenges posed by nonconvexity, we ingeniously substitute the Hessian with its absolute value. This clever transformation, facilitated by our diagonal approximation, ensures that our optimization algorithm maintains a linear complexity in both time and memory.   Through rigorous experimentation across three distinct tasks in vision and language, we demonstrate that Apollo not only accelerates convergence but also enhances generalization performance, outpacing traditional stochastic optimization methods such as SGD and various Adam variants. The results speak for themselves, showcasing Apollo as a formidable contender in the optimization landscape.", "introduction": "Nonconvex stochastic optimization is of core practical importance in many fields of machine learning, in particular for training deep neural networks (DNNs).First-order gradient-based optimization algorithms, conceptually attractive due to their linear efficiency on both the time and memory complexity, have led to tremendous progress and impressive successes.A number of advanced first-order algorithms have emerged over the years to pursue fast and stable convergence, among which stochastic gradient descent (SGD) (Robbins & Monro, 1951;LeCun et al., 1998), equipped with momentum (Rumelhart et al., 1985;Qian, 1999;Bottou & Bousquet, 2008), has stood out for its simplicity and effectiveness across a wide range of applications (Hinton & Salakhutdinov, 2006;Hinton et al., 2012;Graves, 2013).However, one disadvantage of SGD is that the gradients in different directions are scaled uniformly, resulting in limited convergence speed and sensitive choice of the learning rate, and thus has spawned a lot of recent interest in accelerating SGD from the algorithmic and practical perspectives.Recently, many adaptive first-order optimization methods have been proposed to achieve rapid training progress with element-wise scaled learning rates, and we can only mention a few here due to space limits.In their pioneering work, Duchi et al. (2011) proposed AdaGrad, which scales the gradient by the square root of the accumulative square gradients from the first iteration.While AdaGrad works well for sparse settings, its performance significantly degrades for dense settings, primarily due to the monotonic increase of the accumulation.Subsequently, several methods have been proposed with the intuition to limit the accumulation to a small window of past iterations, and in particular exponentially reduce the weight of earlier iterations.Notable works incorporating this method are RMSProp (Tieleman & Hinton, 2012), AdaDelta (Zeiler, 2012), and Adam (Kingma & Ba, 2015), among which Adam has become the default optimization algorithm across many deep learning applications because of its fast convergence speed and relatively consistent selections of hyper-parameters (Ruder, 2016;Zhang et al., 2020).However, it has been observed that these adaptive optimization methods may converge to bad/suspicious local optima, resulting in worse generalization ability than their non-adaptive counterparts (Wilson et al., 2017), or fail to converge due to unstable and extreme learning rates (Luo et al., 2019).Quasi-Newton methods have been widely used in solving convex optimization problems, due to their efficient computation and fast convergence rate (Broyden, 1967;Dennis & Mor\u00e9, 1977).However, the stochastic, high-dimensional and nonconvex nature of many machine learning tasks, such as training deep neural networks, has rendered many classical quasi-Newton methods ineffective and/or inefficient (Keskar & Berahas, 2016;Wang et al., 2017;Yao et al., 2020).Indeed, in many natural language processing (NLP) and computer vision (CV) tasks (He et al., 2016;Ma & Hovy, 2016;Luo et al., 2019), SGD (with momentum) is chosen as the optimizer, benefiting from its stable and efficient training and outstanding generalization.In this work, we develop APOLLO, a quasi-Newton method for nonconvex stochastic optimization to simultaneously tackle the aforementioned challenges of stochastic variance, nonconvexity and inefficiency.Algorithmically, APOLLO dynamically incorporates the curvature of the objective function with diagonally approximated Hessian.It only requires first-order gradients and updates the approximation of the Hessian diagonally so that it satisfies a parameter-wise version of the weak secant condition (Wolfe, 1959).To handle nonconvexity, we replace the Hessian with its rectified absolute value, the computation of which is also efficient under our diagonal approximation, yielding an efficient optimization algorithm with linear complexity for both time and memory ( \u00a73).Experimentally, through three tasks on CV and NLP with popular deep neural networks, including ResNets (He et al., 2016), LSTMs (Hochreiter & Schmidhuber, 1997) and Transformers (Vaswani et al., 2017), we demonstrate that APOLLO significantly outperforms SGD and variants of Adam, in terms of both convergence speed and generalization performance ( \u00a74)."}
{"paper_id": 205, "abstract": "In a world where many challenges lack singular solutions, the need for machine learning models that can propose a spectrum of plausible answers becomes paramount. In this paper, we introduce a groundbreaking framework known as **modal uncertainty estimation** (MUE), designed to unravel the intricate one-to-many relationships between inputs and outputs while delivering robust uncertainty assessments.  Inspired by the often-overlooked multi-modal posterior collapse issue seen in contemporary conditional generative models, MUE employs a collection of discrete latent variables. Each of these variables embodies a distinct hypothesis, capturing a unique facet of the input-output relationship, thereby facilitating the generation of diverse mappings. Thanks to the discrete nature of these latent representations, MUE adeptly estimates the conditional probability distribution of outputs for any given input, enhancing its effectiveness.  Moreover, MUE shines in its training efficiency, as it learns the discrete latent space and its associated uncertainty estimations in tandem. Through rigorous theoretical development and extensive validation across both synthetic and real-world tasks, we demonstrate that MUE not only surpasses the current state-of-the-art in accuracy of uncertainty estimation but also proves to be highly informative for practical applications. In a realm where complexity reigns, MUE stands as a beacon of innovation, ready to illuminate the path forward.", "introduction": "Making predictions in the real world has to face with various uncertainties.One of the arguably most common uncertainties is due to partial or corrupted observations, as such it is often insufficient for making a unique and deterministic prediction.For example, when inspecting where a single CT scan of a patient contains lesion, without more information it is possible for radiologists to reach different conclusions, as a result of the different hypotheses they have about the image.In such an ambiguous scenario, the question is thus, given the observable, which one(s) out of the many possibilities would be more reasonable than others?Mathematically, this is a one-to-many mapping problem and can be formulated as follows.Suppose the observed information is x \u2208 X in the input space, we are asked to estimate the conditional distribution p(y|x) for y \u2208 Y in the prediction space, based on the training sample pairs (x, y).There are immediate challenges that prevent p(y|x) being estimated directly in practical situations.First of all, both X and Y, e.g.as spaces of images, can be embedded in very high dimensional spaces with very complex structures.Secondly, only the unorganized pairs (x, y), not the one-tomany mappings x \u2192 {y i } i , are explicitly available.Fortunately, recent advances in conditional generative models based on Variational Auto-Encoder (VAE) framework from Kingma & Welling (2014) shed light on how to tackle our problem.By modelling through latent variables c = c(x), one aims to explain the underlying mechanism of how y is assigned to x.And hopefully, variation of c will result in variation in the output \u0177(x, c), which will approximate the true one-to-many mappings distributionally.Many current conditional generative models, including cVAE in Sohn et al. (2015), BiCycleGAN in Zhu et al. (2017b), Probabilistic U-Net in Kohl et al. (2018), etc., are developed upon the VAE framework, with Gaussian distribution with diagonal covariance as the de facto parametrization of the latent variables.However, in the following we will show that such a parametrization put a dilemma between model training and actual inference, as a form of what is known as the posterior collapse problem in the VAE literature Alemi et al. (2018); Razavi et al. (2018).This issue is particularly easy to understand in our setting, where we assume there are multiple y's for a given x.et al. (2015), as long as both prior and variational posterior are parameterized by Gaussians.Now suppose for a particular x, there there are two modes y 1 , y 2 for the corresponding predictions.Since the minimization is performed on the entire training set, p(c|x) is forced to approximate a posterior mixture p(c|x, y (\u2022) ) of two Gaussians from mode y 1 and y 2 .In the situation when the minimization is successful, meaning the KL divergence is small, the mixture of the variational posteriors must be close to a Gaussian, i.e.posterior collapsed as in Fig. 1(b), and hence the multi-modal information is lost.Putting it in contrapositive, if multi-modal information is to be conveyed by the variational posterior, then the minimization will not be successful, meaning higher KL divergence.This may partly explain why it can be a delicate matter to train a conditional VAE.The situation is schematically illustrated in Figure 1 in one dimension.Note that the case in Figure 1(a) is usually more preferable, however the density values of the prior used during testing cannot reflect the uncertainty level of the outputs.We quantitative demonstrate this in Section 4 and Fig. 2.One direction to solve the above problem is to modify the strength of KL-divergence or the variational lower bound, while keeping the Gaussian parametrization, and has been explored in the literature extensively, as in Higgins et al. (2017); Alemi et al. (2018); Rezende & Viola (2018).However, besides the need of extensive parameter tuning for these approaches, they are not tailored for the multi-modal posterior collapse problem we described above, thus do not solve the inaccurate uncertainty estimation problem.Mixture or compositions of Gaussian priors have also been proposed in Nalisnick et al. (2016); Tomczak & Welling (2018), but the number of Gaussians in the mixture is usually fixed apriori.Hence making it a conditional generative model further complicates the matter, since the number in the mixture should depend on the input.We therefore adopt another direction, which is to use a latent distribution parameterization other than Gaussians, and one that can naturally exhibit multiple modes.The simplest choice would be to constrain the latent space to be a finite set, as proposed in van den Oord et al. (2017), so that we can learn the conditional distribution as a categorical distribution.We argue that the approach of discrete latent space may be beneficial particularly in our setting.First, different from unconditional or weak conditional generative modelling tasks where diversity is the main consideration, making accurate predictions based on partial information often leads to a significantly restricted output space.Second, there is no longer noise injection during training, so that the decoder can utilize the information from the latent variable more effectively.This makes it less prone to ignore the latent variable completely, in contrast to many conditional generation methods using noise inputs.Third, the density value learned on the latent space is more interpretable, since the learned prior can approximate the variational posterior better.In our case, the latent variables can now represent latent mode hypotheses for making the corresponding most likely predictions.We call our approach modal uncertainty estimation (MUE).The main contributions of this work are: (1) We solve the MUE problem by using c-VAE and justify the use of a discrete latent space from the perspective of multi-modal posterior collapse problem.(2) Our uncertainty estimation improves significantly over the existing state-of-art.(3)In contrast to models using noise inputs that require sampling at the testing stage, our model can directly produce results ordered by their latent mode hypothesis probabilities, and is thus more informative and convenient for practical use.The rest of paper is organized as follows.In Section 2 we sample some works that related to ours and stress the key differences between them.In Section 3 we layout our general framework and model details.We conducted a series of experiments on both synthetic and real datasets described in Section 4. The paper is concluded in Section 5."}
{"paper_id": 206, "abstract": "In the realm of visual perception, humans possess an innate ability to discern objects based on their shapes, a skill that seems almost magical in its precision. Yet, in stark contrast, convolutional neural networks (CNNs) often find themselves ensnared by the allure of texture, leading them down a perilous path where they become vulnerable to the deceptive nature of adversarial examples. This disparity in focus may very well be the Achilles' heel of CNNs, a critical flaw that we seek to mend.  In this exploration, we embark on a quest to weave the essence of shape bias into the fabric of CNNs, enhancing their resilience against the onslaught of adversarial attacks. We unveil two innovative algorithms, inspired by the steadfast nature of edges, which remain steadfast even in the face of subtle, imperceptible perturbations. Our first approach involves adversarially training a classifier on images augmented with an edge map as an additional channel. During the trial of inference, we recalibrate this edge map and merge it seamlessly with the original image, fortifying our defenses.  The second algorithm ventures into the realm of conditional GANs, where we train a model to transform edge maps extracted from both pristine and perturbed images into their clean counterparts. The inference process then relies on the generated image that corresponds to the input's edge map, creating a robust shield against adversarial threats.  Through a rigorous series of experiments spanning over ten diverse datasets, we demonstrate the formidable efficacy of our proposed algorithms against FGSM and \u2113\u221e PGD-40 attacks. Yet, our findings extend beyond mere defense; we reveal that edge information can enhance various adversarial training methods, prove even more potent when paired with background subtraction, and serve as a bulwark against poisoning attacks. Moreover, our approach imbues CNNs with a heightened resilience against natural image corruptions\u2014such as motion blur, impulse noise, and JPEG compression\u2014far surpassing those trained solely on RGB images.  Ultimately, our study sheds light on a pivotal truth: CNNs have yet to fully embrace the intricate structures and operations within images that are vital for true robustness. As we delve deeper into this uncharted territory, we invite you to explore our code, available at: https://github.com/[masked].", "introduction": "Deep neural networks (LeCun et al., 2015) remain the state of the art across many areas and are employed in a wide range of applications.They also provide the leading model of biological neural networks, especially in visual processing (Kriegeskorte, 2015).Despite the unprecedented success, however, they can be easily fooled by adding carefully-crafted imperceptible noise to normal inputs (Szegedy et al., 2014;Goodfellow et al., 2015).This poses serious threats in using them in safety-and security-critical domains.Intensive efforts are ongoing to remedy this problem.Our primary goal here is to learn robust models for visual recognition inspired by two observations.First, object shape remains largely invariant to imperceptible adversarial perturbations (Fig. 1).Shape is a sign of an object and plays a vital role in recognition (Biederman, 1987).We rely heavily on edges and object boundaries, whereas CNNs emphasize more on texture (Geirhos et al., 2018).Second, unlike CNNs, we recognize objects one at a time through attention and background subtraction (e.g., Itti & Koch (2001)).These may explain why adversarial examples are perplexing.The convolution operation in CNNs is biased towards capturing texture since the number of pixels constituting texture far exceeds the number of pixels that fall on the object boundary.This in turn provides a big opportunity for adversarial image manipulation.Some attempts have been made to emphasize more on edges, for example by utilizing normalization layers (e.g., contrast and divisive normalization (Krizhevsky et al., 2012)).Such attempts, however, have not been fully investigated for adversarial defense.Overall, how shape and texture should be reconciled in CNNs continues to be an open question.Here we propose two solutions that can be easily implemented and integrated in existing defenses.We also investigate possible adaptive attacks against them.Extensive experiments across ten datasets, over which shape and texture have different relative importance, demonstrate the effectiveness of our solutions against strong attacks.Our first method performs adversarial training on edge-augmented inputs.The second method uses a conditional GAN (Isola et al., 2017) to translate edge maps to clean images, essentially finding a perturbation-invariant transformation.Figure 1: Adversarial attacks against ResNet152 over the giant panda image using FGSM (Goodfellow et al., 2015), PGD-40 (Madry et al., 2017) (\u03b1=8/255), DeepFool (Moosavi-Dezfooli et al., 2016) and Carlini-Wagner (Carlini & Wagner, 2017) attacks.The second columns in panels show the difference (L2) between the original image (not shown) and the adversarial one (values shifted by 128 and clamped).The edge map (using Canny edge detector) remains almost intact at small perturbations.Notice that edges are better preserved for the PGD-40.See Appx.A for a more detailed version of this figure, and also the same using the Sobel method.There is no need for adversarial training (and hence less computation) in this method.Further, and perhaps less surprising, we find that incorporating edges also makes CNNs more robust to natural images corruptions and backdoor attacks.The versatility and effectiveness of these approaches, without significant parameter tuning, is very promising.Ultimately, our study shows that shape is the key to build robust models and opens a new direction for future research in adversarial robustness."}
{"paper_id": 207, "abstract": "In the ever-evolving landscape of strategic interactions, the art of opponent modeling emerges as a crucial skill, one that allows players to deftly exploit the weaknesses of less adept adversaries. Yet, a formidable challenge looms: how can one swiftly adapt to opponents who wield a myriad of strategic styles? While many prior attempts have sought to construct explicit models to forecast an opponent's tactics, these approaches often demand vast amounts of data for training and struggle to accommodate the unpredictable nature of new adversaries.  Enter our innovative Learning to Exploit (L2E) framework, a beacon of hope in the realm of implicit opponent modeling. L2E is designed to hone its ability to exploit a range of opponents through just a handful of interactions during training, equipping it with the agility to swiftly adjust to unfamiliar styles during testing. At the heart of this framework lies our groundbreaking Opponent Strategy Generation (OSG) algorithm, which autonomously crafts effective opponents for training purposes. Through adversarial training against the formidable challengers generated by OSG, L2E systematically dismantles the vulnerabilities inherent in its own strategies.  Moreover, the robustness of L2E is significantly bolstered by its exposure to a diverse array of opponents, all meticulously produced by OSG through a diversity-regularized policy optimization process. We rigorously tested the L2E framework across two poker games and one grid soccer game, recognized benchmarks in the field of opponent modeling. The results are compelling: L2E demonstrates a remarkable capacity to adapt swiftly to the diverse strategies of unknown opponents, marking a significant advancement in the quest for mastery in strategic gameplay.", "introduction": "One core research topic in modern artificial intelligence is creating agents that can interact effectively with their opponents in different scenarios.To achieve this goal, the agents should have the ability to reason about their opponents' behaviors, goals, and beliefs.Opponent modeling, which constructs the opponents' models to reason about them, has been extensively studied in past decades (Albrecht & Stone, 2018).In general, an opponent model is a function that takes some interaction history as its input and predicts some property of interest of the opponent.Specifically, the interaction history may contain the past actions that the opponent took in various situations, and the properties of interest could be the actions that the opponent may take in the future, the style of the opponent (e.g., \"defensive\", \"aggressive\"), or its current goals.The resulting opponent model can inform the agent's decision-making by incorporating the model's predictions in its planning procedure to optimize its interactions with the opponent.Opponent modeling has already been used in many practical applications, such as dialogue systems (Grosz & Sidner, 1986), intelligent tutor systems (McCalla et al., 2000), and security systems (Jarvis et al., 2005).Many opponent modeling algorithms vary greatly in their underlying assumptions and methodology.For example, policy reconstruction based methods (Powers & Shoham, 2005;Banerjee & Sen, 2007) explicitly fit an opponent model to reflect the opponent's observed behaviors.Type reasoning based methods (Dekel et al., 2004;Nachbar, 2005) reuse pre-learned models of several known opponents by finding the one which most resembles the behavior of the current opponent.Classification based methods (Huynh et al., 2006;Sukthankar & Sycara, 2007) build models that predict the play style of the opponent, and employ the counter-strategy, which is effective against that particular style.Some recent works combine opponent modeling with deep learning methods or reinforcement learning methods and propose many related algorithms (He et al., 2016;Foerster et al., 2018;Wen et al., 2018).Although these algorithms have achieved some success, they also have some obvious disadvantages.First, constructing accurate opponent models requires a lot of data, which is problematic since the agent does not have the time or opportunity to collect enough data about its opponent in most applications.Second, most of these algorithms perform well only when the opponents during testing are similar to the ones used for training, and it is difficult for them to adapt to opponents with new styles quickly.More related works on opponent modeling are in Appendix A.1.To overcome these shortcomings, we propose a novel Learning to Exploit (L2E) framework in this work for implicit opponent modeling, which has two desirable advantages.First, L2E does not build an explicit model for the opponent, so it does not require a large amount of interactive data and eliminates the modeling errors simultaneously.Second, L2E can quickly adapt to new opponents with unknown styles, with only a few interactions with them.The key idea underlying L2E to train a base policy against various styles of opponents by using only a few interactions between them during training, such that it acquires the ability to exploit different opponents quickly.After training, the base policy can quickly adapt to new opponents using only a few interactions during testing.In effect, our L2E framework optimizes for a base policy that is easy and fast to adapt.It can be seen as a particular case of learning to learn, i.e., meta-learning (Finn et al., 2017).The meta-learning algorithm (c.f ., Appendix A.2 for details), such as MAML (Finn et al., 2017), is initially designed for single-agent environments.It requires manual design of training tasks, and the final performance largely depends on the user-specified training task distribution.The L2E framework is designed explicitly for the multi-agent competitive environments, which generates effective training tasks (opponents) automatically (c.f ., Appendix A.3 for details).Some recent works have also initially used meta-learning for opponent modeling.Unlike these works, which either use meta-learning to predict the opponent's behaviors (Rabinowitz et al., 2018) or to handle the non-stationarity problem in multi-agent reinforcement learning (Al-Shedivat et al., 2018), we focus on how to improve the agent's ability to adapt to unknown opponents quickly.In our L2E framework, the base policy is explicitly trained such that a few interactions with a new opponent will produce an opponent-specific policy to effectively exploit this opponent, i.e., the base policy has strong adaptability that is broadly adaptive to many opponents.In specific, if a deep neural network models the base policy, then the opponent-specific policy can be obtained by fine-tuning the parameters of the base policy's network using the new interactive data with the opponent.A critical step in L2E is how to generate effective opponents to train the base policy.The ideal training opponents should satisfy the following two desiderata.1) The opponents need to be challenging enough (i.e., hard to exploit).By learning to exploit these challenging opponents, the base policy eliminates its weakness and learns a more robust strategy.2) The opponents need to have enough diversity.The more diverse the opponents during training, the stronger the base policy's generalization ability is, and the more adaptable the base policy to the new opponents.To this end, we propose a novel opponent strategy generation (OSG) algorithm, which can produce challenging and diverse opponents automatically.We use the idea of adversarial training to generate challenging opponents.Some previous works have also been proposed to obtain more robust policies through adversarial training and showed that it improves the generalization (Pinto et al., 2017;Pattanaik et al., 2018).From the perspective of the base policy, giving an opponent, the base policy first adjusts itself to obtain an adapted policy, the base policy is then optimized to maximize the rewards that the adapted policy gets when facing the opponent.The challenging opponents are then adversarially generated by minimizing the base policy's adaptability by automatically generating difficult to exploit opponents.These hard-to-exploit opponents are trained such that even if the base policy adapts to them, the adapted base policy cannot take advantage of them.Besides, our OSG algorithm can further produce diverse training opponents with a novel diversity-regularized policy optimization procedure.In specific, we use the Maximum Mean Discrepancy (MMD) metric (Gretton et al., 2007) to evaluate the differences between policies.The MMD metric is then incorporated as a regularization term into the policy optimization process to obtain a diverse set of opponent policies.By training with these challenging and diverse training opponents, the robustness and generalization ability of our L2E framework can be significantly improved.To summarize, the main contributions of this work are listed bellow in four-fold:\u2022 We propose a novel learning to exploit (L2E) framework to exploit sub-optimal opponents without building explicit models for it.L2E can quickly adapt to a new opponent with unknown style using only a few interactions.\u2022 We propose to use an adversarial training procedure to generate challenging opponents automatically.These hard to exploit opponents help L2E eliminate its weakness and improve its robustness effectively.The opponent strategy generation part first generates hard-to-exploit opponents for the current base policy (Hard-OSG, see Section 2.2.1), then generates diverse opponent policies to improve the generalization ability of the base policy (Diverse-OSG, see Section 2.2.2).The resulting base policy can fast adapt to completely new opponents with a few interactions.\u2022 We further propose a diversity-regularized policy optimization procedure to generate diverse opponents automatically.The generalization ability of L2E is improved significantly by training with these diverse opponents.\u2022 We conduct detailed experiments to evaluate the L2E framework in three different environments.The experimental results demonstrate that the base policy trained with L2E quickly exploits a wide range of opponents compared to other algorithms."}
{"paper_id": 208, "abstract": "In the ever-evolving realm of artificial intelligence, we unveil Deep Adaptive Semantic Logic (DASL), a groundbreaking framework designed to harness the power of user-provided formal knowledge, thereby enhancing the learning capabilities of deep neural networks. Imagine a system where the wisdom of logic intertwines seamlessly with the raw potential of data\u2014this is the essence of DASL.   Our exploration reveals a robust formal semantics that establishes DASL\u2019s knowledge representation as a true embodiment of first-order logic, ensuring that even when drawing from the vast well of infinite domains, our finite samples converge to accurate truth values. DASL transcends previous neuro-symbolic approaches by deftly sidestepping the pitfalls of vanishing gradients, enabling a deeper logical architecture, and fostering intricate interactions between the realms of knowledge and learning.  To illustrate the prowess of DASL, we present a simple yet revealing toy problem. Here, we infuse structure into an image classification challenge, resulting in a staggering reduction of data requirements by a factor of 1000, thanks to our strategic incorporation of knowledge. Furthermore, we apply DASL to a visual relationship detection task, where the infusion of commonsense knowledge yields an impressive 10.7% boost in performance, even in the face of data scarcity. In a world where every bit of information counts, DASL stands as a beacon of innovation, bridging the gap between logic and learning in profound new ways.", "introduction": "Early work on Artificial Intelligence focused on Knowledge Representation and Reasoning (KRR) through the application of techniques from mathematical logic [Genesereth & Nilsson (1987)].The compositionality of KRR techniques provides expressive power for capturing expert knowledge in the form of rules or assertions (declarative knowledge), but they are brittle and unable to generalize or scale.Recent work has focused on Deep Learning (DL), in which the parameters of complex functions are estimated from data [LeCun et al. (2015)].DL techniques learn to recognize patterns not easily captured by rules and generalize well from data, but they often require large amounts of data for learning and in most cases do not reason at all [Yang et al. (2017); Garcez et al. (2012); Marcus (2018); Weiss et al. (2016)].In this paper we present [Deep Adaptive Semantic Logic (DASL)], a framework that attempts to take advantage of the complementary strengths of KRR and DL by fitting a model simultaneously to data and declarative knowledge.DASL enables robust abstract reasoning and application of domain knowledge to reduce data requirements and control model generalization.DASL represents declarative knowledge as assertions in first order logic.The relations and functions that make up the vocabulary of the domain are implemented by neural networks that can have arbitrary structure.The logical connectives in the assertions compose these networks into a single deep network that is trained to maximize their truth.Figure 1 provides an example network that implements a simple rule set through composition of network components performing image classification.Logical quantifiers \"for all\" and \"there exists\" generate subsamples of the data on which the network is trained.DASL treats labels like assertions about data, removing any distinction between knowledge and data.This provides a mechanism by which supervised, semi-supervised, unsupervised, and distantly supervised learning can take place simultaneously in a single network under a single training regime.The field of neuro-symbolic computing [Garcez et al. (2019)] focuses on combining logical and neural network techniques in general, and the approach of [Serafini & Garcez (2016)] may be the closest of any prior work to DASL.To generate differentiable functions to support backpropagation, these approaches replace pure Boolean values of 0 and 1 for True and False with continuous values from [0, 1] and select fuzzy logic operators for implementing the Boolean connectives.These operators generally employ maximum or minimum functions, removing all gradient information at the limits, Figure 1: DASL integrates user-provided expert knowledge with training data to learn DNNs.It achieves this by compiling a DNN from knowledge, expressed in first order logic, and domain-specific neural components.This DNN is trained using backpropagation, fitting both the data and knowledge.Here DASL applies commonsense knowledge to the visual relationship detection task.\u2227 and \u2192 refer to 'and' and 'implies' connectives respectively.or else they use a product, which drives derivatives toward 0 so that there is very little gradient for learning (see subsection A.7). DASL circumvents these issues by using a logit representation of truth values, for which the range is all real numbers.Approaches to knowledge representation, both in classical AI and in neuro-symbolic computing, often restrict the language to fragments of first order logic (FOL) in order to reduce computational complexity.We demonstrate that DASL captures full FOL with arbitrary nested quantifiers, function symbols, and equality by providing a single formal semantics that unifies DASL models with classical Tarski-style model theory [Chang & Keisler (1973)].We show that DASL is sound and complete for full FOL.FOL requires infinite models in general, but we show that iterated finite sampling converges to correct truth values in the limit.In this paper we show an application of DASL to learning from small amounts of data for two computer vision problems.The first problem is an illustrative toy problem based on the MNIST handwritten digit classification problem.The second is a well-known challenge problem of detecting visual relationships in images.In both cases, we demonstrate that the addition of declarative knowledge improves the performance of a vanilla DL model.This paper makes the following contributions:1.The novel framework DASL, which compiles a network from declarative knowledge and bespoke domain-specific reusable component networks, enabling gradient-based learning of model components;2. Grounding of the proposed framework in model theory, formally proving its soundness and completeness for full first order logic;3. A logit representation of truth values that avoids vanishing gradients and allows deep logical structures for neural-symbolic systems;4. Syntactic extensions that allow (i) restricted quantification over predicates and functions without violating first order logic constraints, and (ii) novel hybrid network architectures;5. Evaluation on two computer vision problems with limited training data, demonstrating that knowledge reduces data requirements for learning deep models e.g.factor of 1000 for the MNIST toy problem and 10.7% improvement in accuracy for visual relationship detection in conditions of data scarcity.Neuro-Symbolic Computing: Early efforts to augment DNNs with logic focused on propositional logic, which supports only logical connectives between (atomic) propositions [Garcez et al. (2012;2019)].For example, KBANN [Towell & Shavlik (1994)] maps a set of propositions into a graph, constructs a neural network, and then trains it.DASL follows this basic idea but fully supports full first order logic (FOL) as well as arithmetic expressions.Similar to several prior efforts [Hu et al. (2016); Rockt\u00e4schel et al. (2015); Li & Srikumar (2019)], DASL replaces Booleans with real-valued pseudo-probabilities to make the logical operations differentiable.This circumstance has motivated the invention of a collection of ad hoc aggregation operators for representing logical connectives [Detyniecki (2001)].These include the t-norm, used by Logic Tensor Networks (LTNs) [Serafini & Garcez (2016)] and the above works.Instead, DASL uses a logit representation for truth values, which avoids vanishing gradients more comprehensively than the logarithmic representations of [Giannini et al. (2019); Rockt\u00e4schel et al. (2015)], enabling learning in deeper logical structures.DASL also differs in supporting multiple entity types, arithmetic, and non-traditional operations such as softmax that enable richer interaction between the NN and knowledge (Section 4).Prior neuro-symbolic work has demonstrated the ability to represent predicate relations as neural networks, generally for formulas without any nested quantifiers.Full first-order logic allows arbitrary nesting of quantifiers and connectives, and allows function symbols and equality under \"open world\" semantics.DASL represents the first time that soundness and completeness have been established for a FOL system applied to neural networks, demonstrating that we can accommodate full first-order logic.There is also a body of work on tensor representations of logic over finite domains that is focused on efficient evaluation of formulas but is not concerned with learning [Sato (2017); Nguyen et al. (2018)].Compositional DL: DASL is related to works that execute a task by composing trainable neural modules by parsing a query (in natural language) [Andreas et al. (2016); Yi et al. (2018a); Mao et al. (2019); Yi et al. (2018b)].For example, [Yi et al. (2018b)] focuses on visual question answering and employs a differentiable tree-structured logic representation, similar to DASL.However, it only does so to learn to translate questions, whereas DASL learns the semantics of the application domain and can also integrate useful domain knowledge.Structured Learning: Other work exploits underlying structure in the data or the label space to learn DNNs using techniques such conditional random fields, graph neural networks, attention models, etc., including [Belanger et al. (2017); Kim et al. (2017);Battaglia et al. (2018); Peng et al. (2018); Zheng et al. (2015)].These methods impose structure by either adapting the DNN architecture Battaglia et al. (2018) or the loss function [Zheng et al. (2015)].DASL instead imposes soft constraints by compiling DNNs based on rules that can be stated in a flexible manner using FOL.Semantic Reasoning: By the semantics of a logical language we mean an interpretation of its symbols (which do not include logical connectives and quantifiers); a model in the sense of model theory [Weiss & D'Mello (1997)].In common with several methods [Xie et al. (2019)], DASL grounds its entities in vector spaces (embeddings) and its predicates and functions in trainable modules.DASL builds on prior works on semantic representation techniques Pennington et al. (2014); Mikolov et al. (2013); Deerwester et al. (1990) by enabling logical statements to modify the entity embeddings so as to mirror semantic similarity in the application.Another research direction [Rockt\u00e4schel & Riedel (2017);Cohen et al. (2017);de Jong & Sha (2019)] is to structure DNNs and embedding spaces around proof trees to reduce the brittleness of theorem provers [Siekmann & Wrightson (1983)].An alternative to representing predicates by networks is to restrict oneself to propositional knowledge [Xu et al. (2018)] which can be applied to the outputs of a neural network.Propositional logic cannot represent the rules in Figure 1, for example, but can be useful for placing restrictions on fixed outputs.Bayesian Belief Networks: Substitution of pseudo-probabilities for Booleans fails to capture uncertainty the way fully Bayesian methods do [Jaynes (2003)].Bayesian Belief networks [Pearl (2009)] accurately represent probabilities but lack expressivity and face computability challenges.Bayes nets are most comfortably confined to propositional logic.Efforts to extend them to first-order logic include Markov Logic Networks [Richardson & Domingos (2006)], which use an undirected network to represent a distribution over a set of models, i.e., groundings or worlds that can interpret a theory.The lifted inference approach [Kimmig et al. (2004)] reasons over populations of entities to render the grounded theory computationally tractable.These methods generally do not support the concept of (continuous) soft semantics through the use of semantic embedding spaces, as DASL does, although DeepProbLog Manhaeve et al. (2018) uses semantic embeddings and neural predicate representations within Bayesian Prolog rather than full FOL."}
{"paper_id": 209, "abstract": "In the vast realm of reinforcement learning, a question looms: how can an agent prepare to tackle unforeseen challenges when those very challenges remain shrouded in mystery? Enter the realm of unsupervised skill discovery\u2014a captivating class of algorithms that forge a set of policies, all while navigating the landscape without the guiding light of a reward function. These algorithms share a kinship with representation learning techniques, such as contrastive learning, which thrive in the supervised domain; both are akin to pretraining methods that seek to maximize an approximation to a mutual information objective.  While previous explorations have illuminated the potential of these skill discovery methods to expedite performance in downstream RL tasks, they have left a critical gap in understanding the nature of optimality in this context. Are these skill-learning algorithms truly the best of their kind? What does optimality even mean in their intricate dance with uncertainty?  In our work, we unveil a crucial insight: unsupervised skill discovery algorithms, driven by mutual information maximization, do not necessarily yield skills that are optimal for every conceivable reward function. Yet, amidst this revelation lies a silver lining. We demonstrate that the distribution of skills they produce can serve as an optimal starting point, effectively minimizing regret against adversarially chosen reward functions, provided we employ a specific adaptation procedure. Furthermore, our analysis offers a geometric lens through which to view these skill-learning methodologies, enriching our understanding of their underlying mechanics and potential. In this way, we not only chart a course through the complexities of skill discovery but also illuminate the path toward more robust and adaptable reinforcement learning agents.", "introduction": "The high sample complexity of reinforcement learning (RL) algorithms has prompted a large body of prior work to study pretraining of RL agents.During the pretraining stage, the agent collects unsupervised experience from the environment that is not labeled with any rewards.Prior methods have used this pretraining stage to learn representations of the environment that might assist the learning of downstream tasks.For example, some methods learn representations of the observations (Laskin et al., 2020;Schwarzer et al., 2021) or representations of the dynamics model (Ebert et al., 2018;Ha & Schmidhuber, 2018;Sekar et al., 2020).In this work, we focus on methods that learn a set of potentially-useful policies, often known as skills (Salge et al., 2014;Mohamed & Rezende, 2015;Gregor et al., 2016;Achiam et al., 2018;Eysenbach et al., 2018).That is, the learned representation corresponds to a reparametrization of policies.The aim of this unsupervised pretraining is to learn skills that, when a reward function is given, can quickly be combined or composed to maximize this reward.Prior work has demonstrated that this general approach does accelerate learning downstream RL (Gregor et al., 2016;Eysenbach et al., 2018;Achiam et al., 2018).However, prior work offers little analysis about when and where such methods are provably effective.Even simple questions, such as what it means for a set of skills to be optimal, remain unanswered.Algorithms for unsupervised skill learning are conceptually related to the representation learning methods used to improve supervised learning (Gutmann & Hyv\u00e4rinen, 2010;Belghazi et al., 2018;Wu et al., 2018;Oord et al., 2018;Hjelm et al., 2018;He et al., 2020).Both typically maximize a lower bound on mutual information, and the learned representations are often combined linearly to solve downstream tasks (Hjelm et al., 2018;Oord et al., 2018).However, whereas prior work in supervised learning has provided thorough analysis of when and where these representation learning methods produce useful features (Kraskov et al., 2004;Song & Ermon, 2019;McAllester & Stratos, 2020), there has been comparatively little analysis into when unsupervised skill learning methods produce skills that are useful for solving downstream RL tasks.In this paper, we analyze when and where existing skill learning methods based on mutual information maximization are (or are not) optimal for preparing to solve unknown, downstream tasks.On the one hand, we show that the skills learned by these methods are not complete, in that they cannot be used to represent the solution to every RL problem.This result implies that using the learned skills for hierarchical RL may result in suboptimal performance, and suggests new opportunities for better skill learning algorithms.One the other hand, we show that existing methods acquire a policy initialization that is optimal for learning downstream tasks, if that adaptation is performed using an idealized adaptation procedure.To the best of our knowledge, this is the first result showing that unsupervised skill learning methods are optimal in any sense.Our analysis also illuminates a number of properties of these methods.For example, we show that every skill is optimal for some reward function, and we provide a nontrivial upper bound on the number of unique skills learned.This result implies that these methods cannot learn an infinite number of unique skills, and instead will learn duplicate copies of some skills.The key to our analysis is to view RL algorithms and skill learning algorithms as geometric operations (see Fig. 1).Points correspond to distributions over states, and the set of all possible distributions is a convex polytope that lies on a probability simplex.We show that all reward-maximizing policies lie at vertices of this polytope and that maximizing mutual information corresponds to solving a facility assignment problem on the simplex.The main contribution of this paper is a proof that skill learning algorithms based on mutual information are optimal for minimizing regret against unknown reward functions, assuming that adaptation is performed using a certain procedure.Our proof of optimality relies on certain problem assumptions, leaving the door open for future skill learning algorithms to perform better under different problem assumptions.This contribution provides a rigorous notion of what it means for an unsupervised RL algorithm to be optimal, and also answers additional questions about unsupervised skill learning algorithms, such as whether the skills correspond to reward-maximizing policies and how many unique skills will be learned."}
{"paper_id": 210, "abstract": "In this exploration of dynamic motion within the realm of physical systems, we embark on a quest to uncover the hidden parameters that govern their behavior, all from the mere flicker of a video\u2014one where the intricate details of rendering configurations remain shrouded in mystery. The challenges posed by traditional methods, which often demand vast troves of training data or falter when confronted with unfamiliar rendering scenarios, beckon for a more innovative solution.  Thus, we introduce a pioneering approach that weaves together the threads of domain randomization and the powerful insights gleaned from differentiable rendering gradients. At the heart of our endeavor lies the Rendering-Invariant State-Prediction (RISP) network\u2014a bold creation designed to transmute the subtle differences in images into meaningful variations in state, unbound by the constraints of lighting, shadows, or the whims of material reflectance.  To bring this vision to fruition, we craft a novel loss function that captures the nuances of rendering variances, leveraging the gradients provided by differentiable rendering. In tandem, we unveil an efficient second-order method for calculating these gradients, ensuring that our approach can be seamlessly integrated into the cutting-edge frameworks of modern deep learning.  Our method is rigorously tested across a spectrum of environments\u2014both rigid and deformable\u2014through four distinct tasks: state estimation, system identification, imitation learning, and visuomotor control. In a further testament to its prowess, we apply our technique to a real-world scenario, deftly inferring the state and action sequences of a quadrotor, all through the lens of its motion captured on video.   When pitted against existing methodologies, our approach stands tall, boasting markedly lower reconstruction errors and a remarkable ability to generalize across previously unseen rendering configurations. In this endeavor, we not only push the boundaries of what is possible in motion analysis but also illuminate a path forward for future explorations in the intricate dance of physical systems.", "introduction": "Reconstructing dynamic information about a physical system directly from a video has received considerable attention in the robotics, machine learning, computer vision, and graphics communities.This problem is fundamentally challenging because of its deep coupling among physics, geometry, and perception of a system.Traditional solutions like motion capture systems (Vicon; OptiTrack; Qualisys) can provide high-quality results but require prohibitively expensive external hardware platforms.More recent development in differentiable simulation and rendering provides an inexpensive and attractive alternative to the motion capture systems and has shown promising proof-of-concept results (Murthy et al., 2020).However, existing methods in this direction typically assume the videos come from a known renderer.Such an assumption limits their usefulness in inferring dynamic information from an unknown rendering domain, which is common in real-world applications due to the discrepancy between rendering and real-world videos.Existing techniques for aligning different rendering domains, e.g., CycleGAN (Zhu et al., 2017), may help alleviate this issue, but they typically require access to the target domain with massive data, which is not always available.To our best knowledge, inferring dynamic parameters of a physical system directly from videos under unknown rendering conditions remains far from being solved, and our work aims to fill this gap.Figure 1: A gallery of our four environments (left to right) across three rendering domains (top to bottom).For each environment, we train a RISP with images under varying lighting, background, and materials generated from a differentiable render (top).Each environment then aims to find proper system and control parameters to simulate and render the physical system (middle) so that it matches the dynamic motion of a reference video (bottom) with unknown rendering configurations.We deliberately let three rows use renderers with vastly different rendering configurations.We propose a novel approach combining three ideas to address this challenge: domain randomization, state estimation, and rendering gradients.Domain randomization is a classic technique for transferring knowledge between domains by generating massive samples whose variances can cover the discrepancy between domains.We upgrade it with two key innovations: First, we notice that image differences are sensitive to changes in rendering configurations, which shadows the renderinginvariant, dynamics-related parameters that we genuinely aim to infer.This observation motivates us to propose a rendering-invariant state predictor (RISP) that extracts state information of a physical system from videos.Our second innovation is to leverage rendering gradients from a differentiable renderer.Essentially, requiring the output of RISP to be agnostic to rendering configurations equals enforcing its gradients for rendering parameters to be zero.We propose a new loss function using rendering gradients and show an efficient method for integrating it into deep learning frameworks.Putting all these ideas together, we develop a powerful pipeline that effectively infers parameters of a physical system directly from video input under random rendering configurations.We demonstrate the efficacy of our approach on a variety of challenging tasks evaluated in four environments (Sec.4 and Fig. 1) as well as in a real-world application (Fig. 4).The experimental results show that our approach outperforms the state-of-the-art techniques by a large margin in most of these tasks due to the inclusion of rendering gradients in the training process.In summary, our work makes the following contributions:\u2022 We investigate and identify the bottleneck in inferring state, system, and control parameters of physical systems from videos under various rendering configurations (Sec.3.1); \u2022 We propose a novel solution combining domain randomization, state estimation, and rendering gradients to achieve generalizability across rendering domains (Sec.3.2); \u2022 We demonstrate the efficacy of our approach on several challenging tasks in both simulation and real-world environments (Sec.4)."}
{"paper_id": 211, "abstract": "In the realm of generative models, diffusion probabilistic models (DPMs) stand as a formidable force, wielding the power to create with remarkable finesse. Yet, despite their prowess, the path to inference is fraught with challenges, often demanding the expenditure of countless timesteps\u2014a laborious endeavor indeed. Central to this struggle is the intricate task of estimating the variance at each step of the reverse process, a puzzle that has long eluded straightforward solutions.  In this work, we unveil a revelation: both the optimal reverse variance and the corresponding KL divergence can be expressed analytically in relation to the score function of a DPM. This breakthrough paves the way for our novel framework, which we call \\textit{Analytic-DPM}. Remarkably, this approach requires no training, instead harnessing the power of the Monte Carlo method alongside a pretrained score-based model to derive these analytic forms for variance and KL divergence.  But we do not stop there. Recognizing the potential biases introduced by the score-based model, we delve deeper, deriving both lower and upper bounds for the optimal variance. By clipping our estimates, we refine our results, ensuring they reflect the true potential of the model.   Empirical evidence speaks volumes: our \\textit{Analytic-DPM} not only enhances the log-likelihood across various DPMs but also yields high-quality samples, all while achieving an impressive speedup of 20 to 80 times. In a world where time is of the essence, this advancement could change the landscape of generative modeling as we know it.", "introduction": "A diffusion process gradually adds noise to a data distribution over a series of timesteps.By learning to reverse it, diffusion probabilistic models (DPMs) (Sohl-Dickstein et al., 2015;Ho et al., 2020;Song et al., 2020b) define a data generative process.Recently, it is shown that DPMs are able to produce high-quality samples (Ho et al., 2020;Nichol & Dhariwal, 2021;Song et al., 2020b;Dhariwal & Nichol, 2021), which are comparable or even superior to the current state-of-the-art GAN models (Goodfellow et al., 2014;Brock et al., 2018;Wu et al., 2019;Karras et al., 2020b).Despite their success, the inference of DPMs (e.g., sampling and density evaluation) often requires to iterate over thousands of timesteps, which is two or three orders of magnitude slower (Song et al., 2020a) than other generative models such as GANs.A key problem in the inference is to estimate the variance in each timestep of the reverse process.Most of the prior works use a handcrafted value for all timesteps, which usually run a long chain to obtain a reasonable sample and density value (Nichol & Dhariwal, 2021).Nichol & Dhariwal (2021) attempt to improve the efficiency of sampling by learning a variance network in the reverse process.However, it still needs a relatively long trajectory to get a reasonable log-likelihood (see Appendix E in Nichol & Dhariwal (2021)).In this work, we present a surprising result that both the optimal reverse variance and the corresponding optimal KL divergence of a DPM have analytic forms w.r.t.its score function (i.e., the gradient of a log density).Building upon it, we propose Analytic-DPM, a training-free inference framework to improve the efficiency of a pretrained DPM while achieving comparable or even superior performance.Analytic-DPM estimates the analytic forms of the variance and KL divergence using the Monte Carlo method and the score-based model in the pretrained DPM.The corresponding trajectory is calculated via a dynamic programming algorithm (Watson et al., 2021).Further, to correct the potential bias caused by the score-based model, we derive both lower and upper bounds of the optimal variance and clip its estimate for a better result.Finally, we reveal an interesting relationship between the score function and the data covariance matrix.Analytic-DPM is applicable to a variety of DPMs (Ho et al., 2020;Song et al., 2020a;Nichol & Dhariwal, 2021) in a plug-and-play manner.Empirically, Analytic-DPM consistently improves the log-likelihood of these DPMs and meanwhile enjoys a 20\u00d7 to 40\u00d7 speed up.Besides, Analytic-DPM also consistently improves the sample quality of DDIMs (Song et al., 2020a) and requires up to 50 timesteps (which is a 20\u00d7 to 80\u00d7 speed up compared to the full timesteps) to achieve a comparable FID to the corresponding baseline."}
{"paper_id": 212, "abstract": "In this paper, we unveil a groundbreaking architecture known as CycleMLP, a robust and adaptable backbone designed for the realms of visual recognition and dense predictions. Unlike its contemporaries\u2014such as MLP-Mixer, ResMLP, and gMLP\u2014which tether their designs to specific image sizes, rendering them ill-suited for tasks like object detection and segmentation, CycleMLP stands apart with two significant advantages. First, it gracefully accommodates a variety of image sizes, allowing for greater flexibility in application. Second, it boasts a linear computational complexity relative to image size, thanks to its innovative use of local windows. This contrasts sharply with earlier MLP models that suffered from the burdensome $O(N^2)$ computations due to their fully connected spatial structures.  We present a family of models that not only eclipses existing MLP frameworks but also challenges the very best in Transformer-based architectures, such as Swin Transformer, all while utilizing fewer parameters and requiring fewer FLOPs. CycleMLP expands the horizons of MLP-like models, establishing them as a versatile backbone for a diverse array of dense prediction tasks. The results speak for themselves: CycleMLP achieves competitive performance across object detection, instance segmentation, and semantic segmentation. Notably, CycleMLP-Tiny surpasses Swin-Tiny by an impressive 1.3% mIoU on the ADE20K dataset, all while maintaining a lower computational footprint. Furthermore, CycleMLP demonstrates remarkable zero-shot robustness on the ImageNet-C dataset, solidifying its status as a formidable player in the landscape of visual recognition.", "introduction": "Vision models in computer vision have been long dominated by convolutional neural networks (CNNs) (Krizhevsky et al., 2012;He et al., 2016).Recently, inspired by the successes in Natural Language Processing (NLP) field, Transformers (Vaswani et al., 2017) are adopted into the computer vision community.Built with self-attention layers, multi-layer perceptrons (MLPs), and skip connections, Transformers make numerous breakthroughs on visual tasks (Dosovitskiy et al., 2020;Liu et al., 2021b).More recently, (Tolstikhin et al., 2021;Liu et al., 2021a) have validated that building models solely on MLPs and skip connections without the self-attention layers can achieve surprisingly promising results on ImageNet (Deng et al., 2009)  Despite promising results on visual recognition tasks, these MLP-like models can not be used in dense prediction tasks (e.g., object detection and semantic segmentation) due to the three challenges: (1) Current models are composed of blocks with non-hierarchical architectures, which make the model infeasible to provide pyramid and high-resolution feature representations.(2) Current models cannot deal with flexible input scales due to the Spatial FC as shown in Figure 1b.The spatial FC is configured by an image-size related weightfoot_0 .Thus, this denotes the output position.For simplicity, we omit batch dimension and set the feature's width to 1 here for example.Several more general cases can be found in Figure 7 (Appendix G).Best viewed in color.structure typically requires the input image with a fixed scale during both the training and inference procedure.It contradicts the requirements of dense prediction tasks, which usually adopt a multi-scale training strategy (Carion et al., 2020) and different input resolutions in training and inference stages (Lin et al., 2014;Cordts et al., 2016).(3) The computational and memory costs of the current MLP models are quadratic to input image sizes for dense prediction tasks (e.g., COCO benchmark (Lin et al., 2014)).To address the first challenge, we construct a hierarchical architecture to generate pyramid features.For the second and third issues, we propose a novel variant of fully connected layer, named as Cycle Fully-Connected Layer (Cycle FC), as illustrated in Figure 1c.The Cycle FC is capable of dealing with various image scales and has linear computational complexity to image size.Our Cycle FC is inspired by Channel FC layer illustrated in Figure 1a, which is designed for channel information communication (Lin et al., 2013;Szegedy et al., 2015;He et al., 2016;Howard et al., 2017).The main merit of Channel FC lies in that it can deal with flexible image sizes since it is configured by image-size agnostic weight of C in and C out .However, the Channel FC is infeasible to aggregate spatial context information due to its limited receptive field.Our Cycle FC is designed to enjoy Channel FC's merit of taking input with arbitrary resolution and linear computational complexity while enlarging its receptive field for context aggregation.Specifically, Cycle FC samples points in a cyclical style along the channel dimension (Figure 1c).In this way, Cycle FC has the same complexity (both the number of parameters and FLOPs) as channel FC while increasing the receptive field simultaneously.To this end, we adopt Cycle FC to replace the Spatial FC for spatial context aggregation (i.e., token mixing) and build a family of MLP-like models for both recognition and dense prediction tasks.The contributions of this paper are as follows: (1) We propose a new MLP-like operator, Cycle FC, which is computational friendly to cope with flexible input resolutions.(2) We take the first attempt to build a family of hierarchical MLP-like architectures (CycleMLP) based on Cycle FC operator for dense prediction tasks.(3) Extensive experiments on various tasks (e.g., ImageNet classification, COCO object instance detection, and segmentation, and ADE20K semantic segmentation) demonstrate that CycleMLP outperforms existing MLP-like models and is comparable to and sometimes better than CNNs and Transformers on dense predictions.All models are trained on ImageNet-1K (Deng et al., 2009) without extra data.CycleMLP surpasses existing MLP-like models such as MLP-Mixer (Tolstikhin et al., 2021), ResMLP (Touvron et al., 2021a), gMLP (Liu et al., 2021a), S 2 -MLP (Yu et al., 2021) and ViP (Hou et al., 2021).Related Work.Convolution Neural Networks (CNNs) has dominated the visual backbones for several years (Krizhevsky et al., 2012;Simonyan & Zisserman, 2014;He et al., 2016).(Dosovitskiy et al., 2020) introduced the first pure Transformer-based (Vaswani et al., 2017) model into computer vision and achieved promising performance, especially pre-trained on the large scale JFT dataset.Recently, some works (Tolstikhin et al., 2021;Touvron et al., 2021a;Liu et al., 2021a) removed the attention in Transformer and proposed pure MLP-based models.Please see Appendix A for a comprehensive review of the literature on the visual backbones."}
{"paper_id": 213, "abstract": "In a world where automatic speech recognition systems have opened up a realm of thrilling applications, they also cast a shadow, creating avenues for insidious eavesdropping. To counter this growing concern, we present a groundbreaking method designed to cloak an individual's voice from these prying systems, all while preserving the natural flow of conversation among those present. Traditional adversarial attacks falter in the realm of real-time streaming; by the time an attack is initiated, the very essence of the signal has transformed. Thus, we unveil the concept of predictive adversarial attacks\u2014an innovative strategy that anticipates and crafts the most effective attack vector for the future, achieving remarkable real-time performance. Under the constraints of immediacy, our approach disrupts the established speech recognition system, DeepSpeech, outperforming online projected gradient descent by a staggering 3.9 times in terms of word error rate and an impressive 6.6 times in character error rate. Furthermore, we validate the practicality of our method in dynamic environments, navigating the complexities of realistic scene geometries with ease.", "introduction": "Automatic speech recognition models are embedded in nearly all smart devices.Although these models have many exciting applications, the concern for the potential of these devices to eavesdrop is significant.It is becoming increasingly important to develop methods that give users the autonomy to safeguard their speech from voice processing software.Fortunately, over the last decade, there has been work demonstrating that neural networks models are easily fooled.For example, they remain vulnerable to small additive perturbations (Carlini & Wagner, 2018), ambient noise (Xu et al., 2020), and unusual examples (Nguyen et al., 2015).Predominant methods such as gradient-based methods and their variants have remained the standard approach to generating challenging examples for deep neural networks (Madry et al., 2019).However, to achieve this, these methods require the full input upfront, and thus users can not practically use them as they continuously speak.Therefore, the community has increasingly been focusing on researching general, robust methods of breaking neural networks that can be used in real-time.We define robust to mean an obstruction that can not be easily removed, real-time to mean an obstruction that is generated continuously as speech We introduce \"Neural Voice Camouflage,\" an approach that disrupts automatic speech recognition systems in real time.To operate on live speech, our approach must predict corruptions into the future so that they may be played in real-time.The method works for the majority of the English language.Green/red indicates correct/incorrect transcription respectively.is spoken, and general to mean applicable to the majority of vocabulary in a language.Existing prior work has successfully tackled at least one of these three requirements, but none all three.While some work is real-time (Chen et al., 2020;Sch\u00f6nherr et al., 2018), these disruptions can be filtered out as they are constrained to specific frequency ranges.Universal attacks (Lu et al., 2021) can be similarly subtracted.Gong et al. (2019) achieved both real-time and robust obstructions, but are limited to a predefined set of ten words.Streaming audio is a particularly demanding domain to disrupt because the calculation needs to be performed in real-time.By the time a sound is computed, time will have passed and the streaming signal will have changed, making standard generative methods obsolete.The sampling rate of audio is at least 16 kHz, meaning the corruption for a given input must be estimated and played over a speaker within milliseconds, which is currently infeasible.Additionally, when attacks are played over-the-air, the attack needs to be loud enough to disrupt any rogue microphone that could be far away.The attack sound needs to carry the same distance as the voice.We introduce predictive attacks, which are able to disrupt any word that automatic speech recognition models are trained to transcribe.Our approach achieves real-time performance by forecasting an attack on the future of the signal, conditioned on two seconds of input speech.Our attack is optimized to have a volume similar to normal background noise, allowing people in a room to converse naturally and without monitoring from an automatic speech recognition system.Forecasting with deep neural networks has already been successfully used in other domains to achieve real-time performance, for instance in packet loss concealment (Pascual et al., 2021).In this paper, we demonstrate how and why this approach lends itself particularly well to developing general, robust and real-time attacks for automatic speech recognition models.Our experiments show that predictive attacks are able to largely disrupt the established DeepSpeech (Amodei et al., 2016) recognition system which was trained on the LibriSpeech dataset (Panayotov et al., 2015).On the standard, large-scale dataset LibriSpeech, our approach causes at least a three fold increase in word error rate over baselines, and at least a six fold increase in character error rate.Our method is practical and straightforward to implement on commodity hardware.We additionally demonstrate the method works inside real-world rooms with natural ambient noise and complex scene geometries.We call our method Neural Voice Camouflage."}
{"paper_id": 214, "abstract": "In the realm of modern language models, the ability to conjure high-quality short texts is a well-established triumph. Yet, as these models stretch their narrative wings to craft longer compositions, they often lose their way, wandering into the territory of incoherence and disjointedness. This conundrum stems from the limitations of the next-token-only language modeling objective, which lacks the foresight necessary for sustained narrative flow.   To combat these challenges, we unveil Time Control (TC), a groundbreaking language model that embraces a latent stochastic process to weave its tales. TC operates by learning a sophisticated representation that aligns the intricate dynamics of textual evolution within a document to the rhythms of a stochastic process. This innovative approach allows the model to first craft an implicit document plan through the stochastic process, laying the groundwork for a coherent narrative. Subsequently, TC generates text that harmonizes with this latent blueprint, ensuring a seamless flow of ideas.  When pitted against domain-specific techniques and the fine-tuning of GPT-2 across a diverse array of textual landscapes, TC emerges victorious, enhancing performance in text infilling and discourse coherence. In the realm of long-form generation, TC excels, demonstrating remarkable improvements in structural integrity\u2014boasting up to a 40% increase in ordering consistency and a 17% boost in text length fidelity. Notably, human evaluators express a clear preference for TC\u2019s outputs, favoring them by a striking 28.6% over traditional baselines. In this new age of narrative crafting, TC stands as a beacon of coherence and creativity, guiding the way toward a more structured and engaging storytelling experience.", "introduction": "Large language models (LLM) such as GPT-2 have been extremely successful in text generation (Radford et al., 2019;Brown et al., 2020).However, LLMs are known to generate incoherent long texts.One reason is that they are unable to plan ahead or represent long-range text dynamics (Kiddon et al., 2016;Fan et al., 2019;Hua & Wang, 2020;Duboue & McKeown, 2001;Stent et al., 2004;Tamkin et al., 2020).As a result, they oftentimes produce wandering content with poor discourse structure and low relevance (Hua & Wang, 2020;Zhao et al., 2017;Xu et al., 2020); the text reads as if the model has no anchored goal when generating.These problems with coherence are further exacerbated when forcing autoregressive models to generate longer texts as the model struggles to extrapolate beyond its expected text end point.These problems suggest that LLMs currently fail to properly capture how documents evolve from beginning to end.Doing so is critical for succeeding in goal-oriented tasks such as story, dialog or recipe generation.Prior work has explored the use of planning-based methods for generating globally coherent text (Kiddon et al., 2016;Fan et al., 2019;Hua & Wang, 2020;Duboue & McKeown, 2001;Stent et al., 2004).However, these methods rely on manually defining text dynamics for specific domains.Other work has attempted to use sentence representations for modeling text, such as with variational autoencoders (Bowman et al., 2016) or contrastive learning (Gao et al., 2021;Devlin et al., 2019).Their shortcoming in text generation settings is that the latent representations are static: they capture semantic similarity between sentence neighbors, but don't capture how sentence embeddings evolve over a document.Methods including van den Oord et al. (2019) have tried to remedy this by learning a model of local latent dynamics.However, it is difficult to use learned local dynamics for generating accurate goal-conditioned trajectories, especially long-horizon ones.We explore an alternative that explicitly assumes a simple, fixed dynamics model with goal-conditioned generation.In this work, we propose Time Control as a way to learn a latent space with known, goal-conditioned dynamics.We begin by assuming that meandering text generated without a goal can be represented as Brownian motion in latent space; this motion enforces the embeddings of neighboring sentences to be similar to each other, whereas those of distant sentences to be dissimilar.Goal-directed behavior can be incorporated into this model by conditioning on a fixed start and end point.In this case, the Brownian motion becomes a Brownian bridge and the resulting latent trajectories abide by simple, closed-form dynamics.In Time Control, we derive a novel contrastive objective for learning a latent space with Brownian bridge dynamics.We can then use this latent space to generate text that retains local coherence and has improved global coherence.To perform text generation, Time Control first plans a latent trajectory via the Brownian bridge process pinned at a start and end point.It then conditionally generates sentences using this latent plan.In our work, we decode latent plans by fine-tuning GPT2 to generate text conditioned on Time Control's latent trajectory.Trajectories from Time Control act as abstract semantic positions in a document that guide generation of fine-tuned language models.In summary, our work's contributions are the following:\u2022 We derive Time Control, a language model which explicitly models latent structure with Brownian bridge dynamics learned using a novel contrastive objective.\u2022 Across a range of text domains, we show that Time Control generates more or equally coherent text on tasks including text infilling and forced long text generation, compared to task-specific methods.\u2022 We validate that our latent representations capture text dynamics competitively by evaluating discourse coherence with human experiments.\u2022 We ablate our method to understand the importance of the contrastive objective, enforcing Brownian bridge dynamics, and explicitly modeling latent dynamics."}
{"paper_id": 215, "abstract": "In the realm of deep learning, one of the paramount quests is the mastery of disentangled representation learning\u2014a pursuit vital for crafting models that are not only explainable but also generalizable across diverse tasks. At the heart of the most advanced unsupervised representation disentanglement techniques, particularly those grounded in Variational Autoencoders (VAEs), lies the endeavor to minimize the total correlation among the latent variables of a joint distribution. Yet, it has become evident that achieving this noble goal necessitates the introduction of additional inductive biases, for the path is fraught with challenges.  In this paper, we embark on a journey inspired by the elegant structure of the n-th dihedral group, utilizing the principles of Group Theory to forge a new theoretical framework aimed at unsupervised representation disentanglement. We establish a mathematical connection between data transformations and their corresponding representations, illuminating the shadows that linger in the space of representation learning.  We introduce a model that builds upon existing VAE architectures, designed specifically to confront the unsupervised learning challenges posed by our framework. Within this framework, we delineate three sufficient conditions pertaining to the model, group structure, and data itself, which must be met to achieve disentangled representations in accordance with our group-based definition. By addressing these conditions, we provide a fresh perspective on the inductive biases that have eluded traditional VAE models.  To validate our theoretical constructs, we conducted an extensive empirical investigation, training 1,800 models across the leading VAE methodologies on five distinct datasets. The results speak volumes: our Groupified VAEs consistently outperform their original VAE counterparts, delivering superior mean performance while exhibiting reduced variance. Thus, we stand at the precipice of a new understanding in representation learning, one that promises to reshape the landscape of deep learning with clarity and precision.", "introduction": "Learning independent and semantic representations of which individual dimension has interpretable meaning, usually referred to as disentangled representations learning, is critical for artificial intelligence research (Bengio et al., 2013).Such disentangled representations are useful for many tasks: domain adaptation (Li et al., 2019;Zou et al., 2020), zero-shot learning (Lake et al., 2017), and adversarial attacks (Alemi et al., 2016), etc. Intuitively, a disentangled representation should reflect the factors of variations behind the observed data of the world, and one latent unit is only sensitive to changes of an individual factor.Due to the facts that obtaining the ground-truth labels requires significant human effort and humans can learn those factors unsupervisedly, unsupervised representation disentanglement draws much attention from researchers recently.A lot of methods are proposed base on some intuitions.Most of the state-of-the-art methods (Higgins et al., 2017;Burgess et al., 2018;Kim & Mnih, 2018;Chen et al., 2018;Kumar et al., 2017) are based on Variational Autoencoder (VAE) (Kingma & Welling, 2013).These methods are fully unsupervised and can be applied to a variety of complex datasets (Lee et al., 2020).However, these methods suffer from the unidentifiability problem (Locatello et al., 2019b) due to a lack of theoretical guarantee.Another stream of works (Chen et al., 2016;Lin et al., 2020;Khrulkov et al., 2021;Lee et al., 2020) leverage generative adversarial network (GAN) (Goodfellow et al., 2014) to achieve disentanglement but are not interpretable.In general, a well-defined theoretical guarantee is needed for those methods.The research of symmetry in physics demonstrates that infinitesimal transformations that conform to some symmetry groups on physical objects can reflect their nature (Anderson, 1972;Noether, 2017), Anneal-VAE (Burgess et al., 2018), \u03b2-TCVAE (Chen et al., 2018), and FactorVAE (Kim & Mnih, 2018).More recent works (Srivastava et al., 2020;Shao et al., 2020;Kim et al., 2019;Lezama, 2018;Rezende & Viola, 2018) also do not consider the group-based definition.Therefore, how group-based definition will facilitate these methods is still an open question.Besides, all these works suffer from the unidentifiability problem (Locatello et al., 2019b), which is a challenging problem in this literature.From group-based definition, our framework points out that, the unidentifiability problem could be solved once the data constraint is satisfied.However, in this work, we can only get a necessary condition for data constraint, and we still can not solve this challenging problem.As pointed out in Quessard et al. (2020), it is not straightforward to reconcile the probabilistic inference methods with the group-based definition framework.Caselles-Dupr\u00e9 et al. (2019), Quessard et al. (2020), Painter et al. (2020) leverage the interaction with the environment (assuming it is available) as supervision instead of minimizing the total correlation as the VAE-based methods do.Consequently, the effectiveness of these methods is limited to the datasets with the environment available.Our framework learns a representation conforming to the group-based definition without relying on the environment.Pfau et al. (2020) propose a non-parametric method to unsupervisedly learn linear disentangled planes in data manifold under a metric.However, as pointed out by the authors, the method does not generalize to held-out data and performs poorly when trying to disentangle directly from pixels.To summarize, the existing probabilistic inference methods lack theoretical support, while the application scope of existing methods based on the group-based mathematical definition Higgins et al. (2018) is very limited.To the best of our knowledge, our work is the first to reconcile the probabilistic generative methods with the inherently deterministic group-based definition framework of Higgins et al. (2018)."}
{"paper_id": 216, "abstract": "In the realm of deep neural networks (DNNs), the specter of adversarial transferability looms large, allowing cunning attackers to craft deceptive examples from one model to ensnare another, raising urgent security alarms in real-world applications. In this study, we embark on a quest to reimagine adversarial transferability through the lens of data distribution, unveiling new pathways to enhance this trait via score-matching optimization. Our exploration reveals a fascinating truth: certain samples, when infused with minuscule Gaussian noise, possess an uncanny ability to confound a variety of target models. These adversarial examples, originating from diverse source models, exhibit an exceptional level of transferability. We propose a bold hypothesis: these samples reside in the low-density regions of the true data distribution, areas where models falter in their training. To amplify the success rate of our adversarial assaults, we align our attacks with directions that effectively diminish the density of the ground truth. Enter the Intrinsic Adversarial Attack (IAA)\u2014a novel approach that smooths activation functions and mitigates the influence of the later layers in a conventional model, thereby enhancing the synergy between our adversarial strategies and the gradients of the joint data distribution. Through rigorous experimentation, we unleash a series of transferable attacks against a multitude of DNNs, demonstrating that our IAA not only elevates the transferability of crafted attacks across the board but also surpasses the capabilities of existing state-of-the-art methodologies.", "introduction": "Deep neural networks (DNNs) are widely used in various safety-critical fields, but they are vulnerable to adversarial examples (Szegedy et al., 2013).Adversarial attacks are imperceptible to humans but catastrophic for the DNNs and can be transferred between different models (Goodfellow et al., 2015;Liu et al., 2017).Adversarial transferability enables attackers to generate adversarial examples from the source model to attack unknown target models, which has raised security concerns about the deployment of DNNs in practice.Understanding the essence of adversarial transferability is a fundamental problem in deep learning.On the one hand, some works show that the characteristics of the source model, such as model architecture (Wu et al., 2019), model capacity (Tram\u00e8r et al., 2017), and test accuracy (Wu & Zhu, 2020), influence adversarial examples' transferability.On the other hand, some works think that the data-relevant information may be the key factor for adversarial transferability.Ilyas et al. (2019) explain that adversarial perturbations are non-robust features and not meaningless bugs, but it is hard to specifically define non-robust features.We want to further study transferability quantitatively from the data distribution perspective.It has been empirically observed that DNNs are relatively robust to random noise (Fawzi et al., 2016).However, in this work we find an intriguing phenomenon: some samples are sensitive to Gaussian noise, in the sense that injecting small Gaussian noise into these samples can fool different models trained on the same dataset.Furthermore, their adversarial counterparts generated by different source models have much stronger transferability against different target models than other samples.We hypothesize that these samples are in the low-density regions of the ground truth distribution both source and target models are trained on, and models are not well trained in these regions.Thus predictions of these samples are easy to be perturbed and even not robust to small random noises.We denote this kind of data as Low-Density Data (LDD), while others as High-Density Data (HDD).As shown in Fig. 1 (Left), the attack success rate against different target models of LDD with different strengths of Gaussian noise is much higher than that of HDD.Furthermore, in Fig. 1 (Right), the adversarial counterparts of LDD have much stronger transferability than the adversarial counterparts of HDD (see Appendix B for details).This phenomenon reveals that the location of data plays a vital role in adversarial transferability and the adversarial examples of samples in the low-density region are strongly transferable.The most efficient direction towards the low-density region is -\u2207 x log p D (x, y), where p D (x, y) is the ground truth density of natural data.We name this direction Intrinsic Attack because it doesn't depend on the models and only depends on the ground truth distribution.Thus, we propose to match the adversarial attack with intrinsic attack for generating strong transferable adversarial examples.We explore the potential of a classifier p \u03b8,\u039b (y|x) with parameters \u03b8 and structure hyper-parameters \u039b (see Sec. 3.1) to generate more transferable adversarial examples by aligning adversarial attack with intrinsic attack -\u2207 x log p D (x, y).The adversarial attack of p \u03b8,\u039b (y|x) is usually generated by PGD/FGSM method, and is determined by -\u2207 x log p \u03b8,\u039b (y|x).We match the Alignment between the Adversarial attack and Intrinsic attack (AAI), E p D (x,y) \u2207x log p \u03b8,\u039b (y|x) \u2207x log p \u03b8,\u039b (y|x) 2 \u2022 \u2207 x log p D (x, y) , by modifying the structure parameters \u039b for a pre-trained network.In order to maximize AAI, we should make p \u03b8,\u039b (y|x) smoother.Otherwise, \u2207 x log p \u03b8,\u039b (y|x) will oscillate frequently and hard to match \u2207 x log p D (x, y).For the commonly used ReLU network, we can smooth it by replacing ReLU activation with Softplus (Nair & Hinton, 2010) with little change of the model's output.Maennel et al. (2020) show that the early layers of a network learn the local statistics of the data distribution better than the later layers, which motivates us to decrease the impact of later layers when generating adversarial examples to utilize the data distribution-relevant information.We can closely match the adversarial attack with the intrinsic attack -\u2207 x log p D (x, y) and improve the adversarial transferability by optimizing structure hyper-parameters \u039b to maximize AAI as the objective function.We name our method as Intrinsic Adversarial Attack (IAA).There are some interesting observations in our IAA experiments.Firstly, we find that the test accuracy of the source model may not be important.As shown in Fig 2, the accuracy of the pre-trained model with Sof tplus \u03b2=15 is around 60%, but the adversarial transferability of this model is much stronger than the model with Sof tplus \u03b2=45 .Secondly, although the existing methods (Madry et al., 2018;Wu et al., 2019) can significantly decrease the top-1 accuracy of the target models, the top-5 accuracy is still high.IAA can both decrease the top-1 accuracy and top-5 accuracy.Furthermore, the existing methods (Xie et al., 2019;Wu et al., 2019) can just slightly improve the one-step attack under different strengths, while our IAA surpasses the existing methods by a large margin.These phenomena verify our hypothesis that IAA pulls examples to the low-density region, which causes prediction difficulty to the target models.Our main contributions are summarized below:\u2022 We propose an effective metric, AAI, to evaluate the alignment of the model's adversarial attack with intrinsic attack -\u2207 x log p D (x, y).Furthermore, we show that AAI is also an effective metric for adversarial transferability.\u2022 We propose the Intrinsic Adversarial Attack (IAA) by maximizing AAI to generate more transferable adversarial examples.\u2022 We conduct comprehensive transfer attack experiments from different source models against nine naturally trained models and three ensemble secured models, showing that IAA can significantly improve the state-of-the-art transferability (both targeted and untargeted attack) of adversarial examples (even improve 20% under some settings)."}
{"paper_id": 217, "abstract": "In the vast realm of game theory, potential games stand as towering pillars, revered for their ability to harmonize the interests of multiple agents through a shared potential function. This concept, so elegantly simple, begs the question: can it be woven into the intricate fabric of Markov games? What threads connect the worlds of multi-agent coordination with and without the influence of state? To unravel these mysteries, we delve into a compelling class of Markov Potential Games (MPGs), a sophisticated evolution that seeks to encapsulate the complexities of state-dependent multi-agent interactions.  Yet, as we embark on this exploration, we find that the insights drawn from traditional potential games often falter in the face of MPGs, where the dynamics of state-games can devolve into zero-sum confrontations. Conversely, we discover that while some Markov games may exhibit potential game characteristics across every state-game, they do not necessarily qualify as MPGs. Still, within this nuanced landscape, MPGs retain essential attributes, including the existence of deterministic Nash policies.  At the heart of our investigation lies a pivotal technical achievement: we demonstrate the convergence of independent policy gradients, along with their stochastic counterparts, to Nash policies\u2014an endeavor that unfolds at a polynomial pace relative to the approximation error. This breakthrough is made possible through the adaptation of recent arguments surrounding gradient dominance, originally conceived for single-agent Markov decision processes, now applied to the intricate dance of multi-agent learning. Thus, we illuminate a path forward, bridging the realms of potential games and Markov dynamics, and revealing the profound intricacies of cooperative strategy in uncertain environments.", "introduction": "Multi-agent reinforcement learning (MARL) has been the fundamental driver of numerous recent advances in Artificial Intelligence (AI) and Machine Learning (ML) ranging from super-human performance in competitive game-playing (Silver et al., 2016;2018;Brown & Sandholm, 2019;Jaderberg et al., 2019) and multi-tasking (Mnih et al., 2015;OpenAI, 2018;Vinyals et al., 2019) to robotics, autonomous-driving and cyber-physical systems (Busoniu et al., 2008;Zhang et al., 2019).However, despite the popularity of MARL algorithms to analyze these systems in practice, the theory that underpins their empirical success lags behind (Dafoe et al., 2020).Many state-of-the-art theoretical results concern single-agent RL systems, typically modelled as single-agent Markov Decision Processes (MDPs) (Bertsekas, 2000;Panait & Luke, 2005;Sutton & Barto, 2018).The main challenge when transitioning from single to multi-agent RL settings is the computation of Nash policies.For n \u2265 2 agents, a Nash policy is defined to be a profile of policies (\u03c0 * 1 , ..., \u03c0 * n ) so that by fixing the policies of all agents but i, \u03c0 * i is optimal for the resulting single-agent MDP and this is true for all 1 \u2264 i \u2264 nfoot_0 (Definition 1).In multi-agent settings, Nash policies may not be unique in principle and, unlike singe-agent MDPs, agents' rewards may differ dramatically between them.A common approach to compute Nash policies in MDPs is the use of policy gradient methods.The significant progress in the analysis of such methods (see Agarwal et al. (2020) and references therein) has mainly concerned the single-agent case or the case of pure common interests (identical agents) (Wang & Sandholm, 2002;Panait & Luke, 2005): the convergence properties of policy gradient in general MARL settings remain poorly understood.Recent steps towards a theory for multi-agent settings involve Daskalakis et al. (2020) who show convergence of independent policy gradient to the optimal policy for two-agent zero-sum stochastic games, Wei et al. (2021) who improve the result of Daskalakis et al. (2020) using optimistic policy gradient and Zhao et al. (2021) who study extensions of Natural Policy Gradient using function approximation.It is worth noting that the positive results of Daskalakis et al. (2020); Wei et al. (2021) and Zhao et al. (2021) depend on the fact that two-agent stochastic zero-sum games satisfy the min-max equals max-min property (Shapley, 1953).If we move away from the extremes of single-agent or purely competitive settings (two-agents, zero-sum), a lot of these regularities, and in particular the value-uniqueness property, cease to hold.However, building a theory to analyze problems of cooperation between two or more agents constitutes a primary open challenge for the fields of AI and ML (Dafoe et al., 2020;Dafoe et al., 2021).Based on the above, our work is motivated by the following natural question:Can we get (provable) convergence guarantees for multi-agent RL settings in which agents have aligned incentives, i.e., in which coordination is desirable?Model and Informal Statement of Results.To make progress in this direction, we study a class of n-agent MDPs that naturally generalize normal form potential games (Monderer & Shapley, 1996), the archetypal model of interactions between multiple agents with aligned, yet not necessarily identical interests, called Markov Potential Games (MPGs).In words, a multi-agent MDP is an MPG as long as there exists a (state-dependent) real-valued potential function \u03a6 so that if an agent i changes their policy (and the rest of the agents keep their policy unchanged), the difference in agent i's value/utility, V i , is captured by the difference in the value of \u03a6 (Definition 2).Our first task is to understand the structural properties of MPGs and their Nash policies.Rather surprisingly, many insights from normal-form potential games do not carry over as MPGs involve settings with purely competitive (zero-sum) interactions at some states.Moreover, Markov games in which every state-interaction is a potential game are not necessarily MPGs.These findings suggest that MPGs form a class of MDPs with rich structure which challenges our intuition on the nature of cooperation in state-based interactions.On the other hand, MPGs trivially include MDPs of pure common interests (MDPs in which agents have identical rewards) and showcase intuitively expected properties such as the existence of deterministic Nash policies.Our structural results are as follows.Theorem 1.1 (Structural Properties of MPGs).The following facts are true for MPGs with n-agents.(a) There always exists a deterministic Nash policy profile (see Theorem 3.1).(b) We can construct MDPs for which each state is a (normal-form) potential game but which are not MPGs.This can be true regardless of whether the whole MDP is competitive or cooperative in nature (see Examples 1 and 2, respectively).On the opposite side, we can construct MDPs that are MPGs, but which include states that are purely competitive (i.e., zero-sum games), see Figure 3. (c) We provide sufficient conditions so that an MDP is an MPG.These include cases where each state is a (normal-form) potential game and the transition probabilities are not affected by agents actions or the reward functions satisfy certain regularity conditions between different states (see conditions C1 and C2 in Proposition 3.2).We then turn to our motivating question above and, in our main contribution, we answer it in the affirmative.We show that if every agent i independently runs (with simultaneous updates) projected gradient ascent (PGA) on their policy (using their value V i ), then, after O(1/ 2 ) iterations, the system will reach an -approximate Nash policy.Here, independence means that (PGA) requires only local information to determine the updates, i.e., each agent's own rewards, actions, and view of the environment.Such protocols are naturally motivated in distributed settings where all information about type of interaction and other agents' actions is encoded in the agent's environment.For the finite samples analogue, we show that the system will reach an -approximate Nash policy after O(1/ 6 ) iterations.Our main convergence results are summarized in the following (informal) Theorem.Theorem 1.2 (Convergence of Policy Gradient (Informal)).Consider an MPG with n agents and let > 0. (a) Exact Gradients: If each agent i runs independent policy gradient using direct parameterization on their policy and the updates are simultaneous, then the learning dynamics reach an -Nash policy after O(1/ 2 ) iterations.(b) Finite Samples: If each agent i runs stochastic policy gradient using greedy parameterization (see equation 3) on their policy and the updates are simultaneous, then the learning dynamics reach an -Nash policy after O(1/ 6 ) iterations.The formal statements for cases (a) and (b) are provided in Theorems 4.2 and 4.4, respectively.The technical details are presented in Section 4. The main step in the proof of Theorem 1.2 establishes that Projected Gradient Ascent (PGA) on the potential function generates the same dynamics as each agent running independent PGA on their value function.This follows from a straightforward derivation of an agent-wise version of the single-agent gradient domination property which can be used to show that every (approximate) stationary point (Definition 4) of the potential function is an (approximate) Nash policy (Lemma 4.1).If agents do not have access to exact gradients, the key is to get an unbiased sample for the gradient of the value functions and prove that it has bounded variance (in terms of the parameters of the MPG).This is established by requiring agents to perform stochastic PGA with \u03b1-greedy exploration (equation 3).The main idea is that this parameterization stays away from the boundary of the simplex throughout its trajectory (Daskalakis et al., 2020).Other Related Works.Our paper contributes to the growing literature on cooperative AI and ML (Carroll et al., 2019;Dafoe et al., 2020).The results on convergence of MARL algorithms are scarce and largely restricted to purely competitive (Daskalakis et al., 2020;Wei et al., 2021;Zhao et al., 2021) or purely cooperative (Wang & Sandholm, 2002;Bard et al., 2020) settings.As Daskalakis et al. (2020) argue, the current frontier concerns the extension to settings that are not zero-sum, involve more than two agents and/or are cooperative in nature, albeit likely for weaker solution concepts.Our current paper proceeds precisely in this direction, and in fact, it does so without reverting to a weaker solution concept.Concerning the setup, our paper contributes to the rapidly growing literature on MPGs and variations thereof which is showcased by the works of Marden (2012);Valcarcel Macua et al. (2018); Mguni (2021); Mguni et al. (2021) and the partially concurrent work of Zhang et al. (2021).Marden (2012) study Markov games that are potential at every state and which satisfy a strong additional state-transitivity property.Under the same assumption, Mguni (2021) derive an analogous result to our Theorem 3.1 on the existence of deterministic Nash policies.Valcarcel Macua et al. ( 2018) provide an analytical way to find a closed-loop Nash policy in a class of Markov games that is more closely related to our current MPG setting (cf.Proposition 3.2).Zhang et al. (2021) study the same class of MPGs, and present additional practical applications.By introducing the notion of averaged MDPs, they derive an alternative, model-based policy evaluation method, which, interestingly, establishes the same sample-complexity (of O( -6 )) as our model-free estimation.As pointed out by Zhang et al. (2021), it will be instructive to further explore this connection.Concerning the technical parts, our methods are related to Daskalakis et al. (2020); Agarwal et al. (2020); Kakade & Langford (2002) and to Davis & Drusvyatskiy (2018); Bubeck (2015); Nemirovski et al. (2009)."}
{"paper_id": 218, "abstract": "In this article, we embark on an exploration of the spectral behavior of kernel matrices derived from random features, specifically of the form   \\[ {\\bf K} = \\mathbb{E}_{{\\bf w}} \\left[\\sigma\\left({\\bf w}^{\\sf T}{\\bf x}_i\\right)\\sigma\\left({\\bf w}^{\\sf T}{\\bf x}_j\\right)\\right]_{i,j=1}^n. \\]  Here, \\(\\sigma(\\cdot)\\) is a nonlinear function, the data points \\({\\bf x}_1, \\ldots, {\\bf x}_n\\) lie within \\(\\mathbb{R}^p\\), and the random projection vector \\({\\bf w}\\) is characterized by independent and identically distributed (i.i.d.) entries. In a high-dimensional landscape where both the number of data points \\(n\\) and their dimensionality \\(p\\) swell to significant proportions, we unveil a remarkable truth: under a Gaussian mixture model for our data, the eigenspectrum of the kernel matrix \\({\\bf K}\\) becomes agnostic to the specific distribution of the i.i.d. entries of \\({\\bf w}\\) (provided they are zero-mean and unit-variance). Instead, it hinges solely on the function \\(\\sigma(\\cdot)\\) through its generalized Gaussian moments, specifically \\(\\mathbb{E}_{z\\sim \\mathcal N(0,1)}[\\sigma'(z)]\\) and \\(\\mathbb{E}_{z\\sim \\mathcal N(0,1)}[\\sigma''(z)]\\).  From this foundation, we introduce a groundbreaking approach we term Ternary Random Features (TRFs). This innovative technique (i) asymptotically aligns the limiting kernel with the original \\({\\bf K}\\) in a spectral context, and (ii) offers a computational and storage efficiency that is markedly superior. By deftly tuning both the function \\(\\sigma\\) and the random vector \\({\\bf w}\\) to take values in \\(\\{-1,0,1\\}\\) in a manner that is responsive to the data, we eliminate the need for multiplication in our computations. This leads to a dramatic reduction in storage requirements\u2014specifically, a factor of \\(b\\) fewer bits compared to traditional random features, where \\(b\\) represents the bit count needed to store full precision values.  Moreover, our experiments with real-world data reveal that these substantial computational and storage efficiencies are not merely theoretical; they coincide with enhanced performance relative to leading methods in the realm of random features. Thus, the Ternary Random Features stand as a beacon of innovation, promising both elegance and efficacy in the complex landscape of high-dimensional data analysis.", "introduction": "Kernel methods are among the most powerful machine learning approaches with a wide range of successful applications (Sch\u00f6lkopf & Smola, 2018) which, however, suffer from scalability issues in large-scale problems, due to their high space and time complexities (with respect to the number of data n).To address this key limitation, a myriad of random features based kernel approximation techniques have been proposed (Rahimi & Recht, 2008;Liu et al., 2021a): random features methods randomly project the data to obtain low-dimensional nonlinear representations that approximate the original kernel features.This allows practitioners to apply them, with a large saving in both time and space, to various kernel-based downstream tasks such as kernel spectral clustering (Von Luxburg, 2007), kernel principal component analysis (Sch\u00f6lkopf et al., 1997), kernel canonical correlation analysis (Lai & Fyfe, 2000), kernel ridge regression (Vovk, 2013), to name a few.A wide variety of these kernels can be written, for data points x i , x j \u2208 R p , in the form \u03ba(x i , x j ) = E w \u03c3 w T x i \u03c3 w T x j(1) with w \u2208 R p having i.i.d.entries, which can be \"well approximated\" by a sample mean 1 m m t=1 \u03c3 w T t x i \u03c3 w T t x j over m independent random features for m sufficiently large.For instance, taking \u03c3(x) = [cos(x), sin(x)] and w with i.i.d.standard Gaussian entries, one obtains the popular Random Fourier Features (RFFs) that approximate the Gaussian kernel (and the As shall be seen subsequently, (random) neural networks are, to a large extent, connected to kernel matrices of the form (1).More specifically, the classification or regression performance at the output of random neural networks are functionals of random matrices that fall into the wide class of kernel random matrices.Perhaps more surprisingly, this connection still exists for deep neural networks which are (i) randomly initialized and (ii) trained with gradient descent, as testified by the recent works on neural tangent kernels (Jacot et al., 2018), by considering the \"infinitely many neurons\" limit, that is, the limit where the network widths of all layers go to infinity simultaneously.This close connection between neural networks and kernels has triggered a renewed interest for the theoretical investigation of deep neural networks from various perspectives, including optimization (Du et al., 2019;Chizat et al., 2019), generalization (Allen- Zhu et al., 2018;Arora et al., 2019;Bietti & Mairal, 2019), and learning dynamics (Lee et al., 2019;Advani et al., 2020;Liao & Couillet, 2018a).These works shed new light on the theoretical understanding of deep neural network models and specifically demonstrate the significance of studying networks with random weights and their associated kernels to assess the mechanisms underlying more elaborate deep networks.In this article, we consider the random feature kernel of the type (1), which can also be seen as the limiting kernel of a single-hidden-layer neural network with a random first layer.By assuming a high-dimensional Gaussian Mixture Model (GMM) for the data {x i } n i=1 with x i \u2208 R p , we show that the centered kernel matrixfoot_0 K P{\u03ba(x i , x j )} n i,j=1 P, P I n -is asymptotically (as n, p \u2192 \u221e with p/n \u2192 c \u2208 (0, \u221e)) equivalent, in a spectral sense, to another random kernel matrix K which depends on the GMM data statistics and the generalized Gaussian moments E[\u03c3 (z)], E[\u03c3 (z)] of the activation function \u03c3(\u2022), but is independent of the specific law of the i.i.d.entries of the random vector w, as long as they are normalized to have zero mean and unit variance.As such, one can design novel random features schemes with limiting kernels asymptotically equivalent to the original K.For instance, define \u03ba ter (x i , x j ) E w ter \u03c3 ter x T i w ter \u03c3 ter x T j w ter(3) with w ter \u2208 R p having i.i.d.entries taking value w ter i = 0 (with probability ) and valueeach with probability 1/2 -/2, where \u2208 [0, 1) represents the level of sparsity of w, andfor some thresholds s -< s + chosen to match the generalized Gaussian moments E[\u03c3 (z)], E[\u03c3 (z)] of any \u03c3 function (e.g., ReLU, cos, sin) widely used in random features or neural network contexts.The proposed Ternary Random Features (TRFs, with limiting kernel matrices defined in (3) asymptotically \"matching\" any random features kernel matrices in a spectral sense) have the computational advantage of being sparse and not requiring multiplications but only additions, as well as the storage advantage of being composed of only a finite set of words, e.g., {-1, 0, 1} for = 0.Given the urgent need for environmentally-friendly, but still efficient, neural networks such as binary neural networks (Hubara et al., 2016;Lin et al., 2015;Zhu et al., 2016;Qin et al., 2020;Hubara et al., 2016), pruned neural networks (Liu et al., 2015;Han et al., 2015a;b), weights-quantized neural networks (Gupta et al., 2015;Gong et al., 2014), we hope that our analysis will open a new door to a random matrix-improved framework of computationally efficient methods for machine learning and neural network models more generally.10 5 10 6 10 7 10 8 0.6 0.7 0.8 Memory (bits) Accuracy LP-RFF (8 bit) LP-RFF (1 bit) Nystr\u00f6m (32 bits) Nystr\u00f6m (16 bits) Proposed TRF Figure 1: Test accuracy of logistic regression on quantized random features for different number of features m \u2208 {102 , 10 3 , 5.10 3 , 10 4 , 5.10 4 }, with LP-RFF (8-bit and 1-bit, in black) (Zhang et al., 2019), Nystr\u00f6m approximation (32 bits in red, 16 bits in green) (Williams & Seeger, 2001), versus the proposed TRF approach (in blue), on the two-class Cov-Type dataset from UCI ML repo, with n = 418 000 training samples, n test = 116 000 test samples, and data dimension p = 54."}
{"paper_id": 219, "abstract": "In the realm of self-supervised contrastive learning, a persistent challenge looms: models often succumb to the temptation of collapsing into trivial solutions, yielding nothing more than constant outputs. Recent advancements in objective design have sought to address this issue, yet they frequently demand a square complexity that can be burdensome, either in terms of the number of instances ($\\mathcal{O}(N^{2})$) or the dimensions of features ($\\mathcal{O}(d)^2$).   To counteract this collapse, we unveil two innovative strategies that operate by decorrelating different dimensions within the instance embedding stacking matrix: \\textbf{I}nstance-wise Contrastive Learning (ICL) and \\textbf{F}eature-wise Contrastive Learning (FCL). These methods can be synergistically combined into a novel framework we call Zero-CL, where the term \"Zero\" signifies that negative samples hold \\textbf{zero} relevance. This unique approach allows Zero-CL to entirely eliminate negative pairs\u2014essentially, operating with \\textbf{zero} negative samples.  Zero-CL brings forth three significant advantages over its predecessors: First, it operates free of negatives within a symmetric architecture. Second, through a whitening transformation, it ensures that the correlation among various features is reduced to zero, effectively mitigating information redundancy. Third, Zero-CL retains much of the original information post-transformation, enhancing accuracy compared to other whitening techniques.   Our extensive experiments on CIFAR-10/100 and ImageNet demonstrate that Zero-CL not only meets but often exceeds the performance of state-of-the-art symmetric contrastive learning methods, paving a new path in the landscape of contrastive learning.", "introduction": "One of the current main bottlenecks in deep network training is the dependence on heavy annotated training data, and this motivates the recent surge of interests in unsupervised (Donahue & Simonyan, 2019) and self-supervised (Chen & He, 2021;Chen et al., 2020) methods.Specifically, in self-supervised representation learning (SSL), a network is pre-trained without any form of manual annotation, thus providing a means to extract information from unlabeled data sources (e.g., text corpora, videos, images from the Internet, etc.).In self-supervision, label-based information is replaced by a prediction problem using a certain context or using a pretext task.Pretext task in SSL can mainly be divided into three categories: 1) Generative based approaches (Donahue & Simonyan, 2019) learn to generate or otherwise model pixels in the input space.However, pixel-level generation is computationally expensive and may not be necessary for representation learning.2) Contextual based methods (Vincent et al., 2008;Pathak et al., 2016;Ye et al., 2019) design pretext tasks (denoising auto-encoders (Vincent et al., 2008), context auto encoders (Zhang et al., 2016;2017), etc).3) Contrastive based methods (Chen et al., 2020;Grill et al., 2020;Caron et al., 2020;Asano et al., 2019) take augmented views of the same image as positive pair and others as negative pairs.Generally, one positive sample corresponds to lots of negative samples.In recent works, contrastive based methods have shown great promise, achieving state-of-the-art results in image classification (Chen et al., 2020), video classification (Han et al., 2020) and other downstream tasks (Chen & He, 2021).However, trivial constant solutions (different samples get the same representation) is easily happening without the proper design of architecture and objective function.The well-known solutions to avoid this problem can be summarized into two parts: asymmetric model architecture and proper objective function.1) Model architecture: MoCo (He et al., 2020), BYOL (Grill et al., 2020) update encoders separately and stopping gradient operation is adopted to avoid such problem.Then, BYOL and SimSiam (Chen & He, 2021) introduce a predictor module to avoid collapse, which is composed by MLP (Goodfellow et al., 2016).Current mainstream interpretation is using the predictor to construct an asymmetric structure, which is useful to alleviate trivial solutions.2) Objective function: SimCLR uses symmetric framework in contrastive learning.They prevent trivial solutions by using negative pairs and InfoNCE, where InfoNCE can be divided into an alignment term and a uniformity term (Arora et al., 2019).The uniformity term pulls different samples to a hyper-sphere uniformly, forcing obtaining different representations and avoiding trivial solutions.Recently, Barlow Twins (Zbontar et al., 2021) designs a new objective function from the information redundancy perspective, which also has two terms (an invariance term and a redundancy reduction term).The invariance term maximizes the correlation of the same feature across different views, and the redundancy term reduces information redundancy.However, for the symmetric framework, both SimCLR and Barlow Twins require square order complexity in objective functions, and the main complexity comes from the uniformity and redundancy term.In this paper, we propose two methods, named Zero-ICL and Zero-FCL, where Zero-ICL discards the uniformity term and only requires O(N ) complexity by instance-wise whitening.Correspondingly, Zero-FCL discards the redundancy term by feature-wise whitening and only requires O(d) complexity.Our contributions are:1) We propose a new contrastive learning framework to prevent trivial solutions, Zero-CL, which includes two parts, i.e., Zero-ICL (instance-wise) and Zero-FCL (feature-wise), either of which can work independently and only requires linear order complexity (objective function).2) To our best knowledge, Zero-ICL is the first attempt of instance-wise whitening, which is conceptually comprehensible for preventing collapses in contrastive learning.Note that most previous methods (including other domains beyond vision) e.g.(Eldar & Oppenheim, 2003;Kessy et al., 2018) only use whitening transformation to reduce the information redundancy on feature-wise.3) We give empirical analysis on the relationship between previous methods and our Zero-CL, where previous negative sample consuming methods (Zbontar et al., 2021;Chen et al., 2020) can be regarded as our method with Lagrangian transformation.Then, we theoretically introduce ZCA-based whitening from the maximal correlation (Kessy et al., 2018) perspective.Experimental results on standard image benchmarks (CIFAR-10/100 and ImageNet-100/1k) show our method achieves new state-of-the-art results for symmetric contrastive learning compared with (Chen et al., 2020;Zbontar et al., 2021), especially for small hidden dimension and batch size."}
{"paper_id": 220, "abstract": "In the realm of disentanglement, we envision a tapestry of image variations, each thread representing a distinct generative factor, woven together yet separate. The challenge has long been to uncover these threads and craft a representation that honors their individuality. Traditional methods have sought to impose additional regularization terms to guide the creation of realistic images, yet this often leads to a precarious balance, where the clarity of disentanglement is sacrificed at the altar of generation quality. However, we have observed a glimmer of hope: generative models, when pretrained without such constraints, can yield images that exhibit semantically rich variations, each direction in the latent space revealing a new facet of meaning.  Building on this insight, we propose a new path forward\u2014one that sidesteps the age-old trade-off by (i) harnessing the power of high-quality pretrained generative models, and (ii) honing in on the traversal directions that serve as the true generative factors for disentangled representation learning. Thus, we introduce Disentanglement via Contrast (DisCo), a framework designed to model variations in alignment with our target disentangled representations. Through a process of contrasting these variations, DisCo enables the simultaneous discovery of disentangled directions and the learning of coherent representations.  Our approach not only sets a new benchmark for disentangled representation learning but also excels in identifying distinct directions, leveraging pretrained models such as GANs, VAEs, and Flows. For those eager to explore this frontier, our source code awaits at https://github.com/xrenaa/DisCo.", "introduction": "Disentangled representation learning aims to identify and decompose the underlying explanatory factors hidden in the observed data, which is believed by many to be the only way to understand the world for AI fundamentally (Bengio & LeCun, 2007).To achieve the goal, as shown in Figure 1 (a), we need an encoder and a generator.The encoder to extract representations from images with each dimension corresponds to one factor individually.The generator (decoder) decodes the changing of each factor into different kinds of image variations.With supervision, we can constrain each dimension of the representation only sensitive to one kind of image variation caused by changing one factor respectively.However, this kind of exhaustive supervision is often not available in real-world data.The typical unsupervised methods are based on a generative model to build the above encoder and generator framework, e.g., VAE (Kingma & Welling, 2014) provides encoder and generator, and GAN (Goodfellow et al., 2014;Miyato et al., 2018;Karras et al., 2019) provides generator.During the training process of the encoder and generator, to achieve disentangled representation, the typical methods rely on an additional disentanglement regularization term, e.g., the total correlation for VAE-based methods (Higgins et al., 2017;Burgess et al., 2018;Kumar et al., 2017;Kim & Mnih, 2018;Chen et al., 2018) or mutual information for InfoGAN-based methods (Chen et al., 2016;Lin et al., 2020).However, the extra terms usually result in a trade-off between disentanglement and generation quality (Burgess et al., 2018;Khrulkov et al., 2021).Furthermore, those unsupervised methods have been proved to have an infinite number of entangled solutions without introducing inductive bias (Locatello et al., 2019).Recent works (Shen & Zhou, 2021;Khrulkov et al., 2021;Karras et al., 2019;H\u00e4rk\u00f6nen et al., 2020;Voynov & Babenko, 2020) show that, for GANs purely trained for image generation, traversing along different directions in the latent space causes different variations of the generated image.This phenomenon indicates that there is some disentanglement property embedded in the latent space of the pretrained GAN.The above observations indicate that training the encoder and generator simultaneous may not be the best choice.We provide an alternative route to learn disentangled representation: fix the pretrained generator, jointly discover the factors in the latent space of the generator and train the encoder to extract disentangled representation, as shown in Figure 1(b).From the intuitive notion of disentangled representation, similar image variations should be caused by changing the same factor, and different image variations should be caused by changing different factors.This provide a novel contrastive learning view for disentangled representation learning and inspires us to propose a framework: Disentanglement via Contrast (DisCo) for disentangled representation learning.In DisCo, changing a factor is implemented by traversing one discovered direction in the latent space.For discovering the factors, DisCo adopts a typical network module, Navigator, to provides candidate traversal directions in the latent space (Voynov & Babenko, 2020;Jahanian et al., 2020;Shen et al., 2020).For disentangled representation learning, to model the various image variations, we propose a novel \u2206-Contrastor to build a Variation Space where we apply the contrastive loss.In addition to the above architecture innovations, we propose two key techniques for DisCo: (i) an entropy-based domination loss to encourage the encoded representations to be more disentangled, (ii) a hard negatives flipping strategy for better optimization of Contrastive Loss.We evaluate DisCo on three major generative models (GAN, VAE, and Flow) on three popular disentanglement datasets.DisCo achieves the state-of-the-art (SOTA) disentanglement performance compared to all the previous discovering-based methods and typical (VAE/InfoGAN-based) methods.Furthermore, we evaluate DisCo on the real-world dataset FFHQ (Karras et al., 2019) to demonstrate that it can discover SOTA disentangled directions in the latent space of pretrained generative models.Our main contributions can be summarized as: (i) To our best knowledge, DisCo is the first unified framework for jointly learning disentangled representation and discovering the latent space of pretrained generative models by contrasting the image variations."}
{"paper_id": 221, "abstract": "In this paper, we delve into the intricate realm of few-shot learning within the challenging landscape of cross-domain scenarios. Here, the stakes are high; the task is to harness a meager handful of support samples to extend the capabilities of an already-trained model into uncharted territory. We propose a bold hypothesis: if a deep learning model can swiftly adapt to diverse domains during its training, utilizing only a scant few examples, it will retain this remarkable capacity when faced with new domains during testing.  This insight drives us to introduce a pioneering framework we call Domain-Switch Learning (DSL). At its core, DSL weaves the cross-domain challenge into the very fabric of the training process through a dynamic \u201cfast switching\u201d mechanism. In practice, this means that each training iteration is anchored in a single domain, only to pivot to a different domain in the next. Yet, this is not merely a matter of switching; DSL imposes two critical constraints: first, the deep model must resist the temptation to overfit to the current domain, and second, it must safeguard the invaluable knowledge gleaned from previous domains. Together, these constraints forge a pathway to rapid generalization across a spectrum of domains.  Our experimental findings robustly support our central hypothesis, revealing that the capacity for cross-domain generalization cultivated during training seamlessly translates to the testing phase. As a result, DSL not only enhances performance in the realm of cross-domain few-shot classification but also establishes a new benchmark, reshaping the landscape of what is possible in this field.", "introduction": "This paper challenges a realistic problem, i.e., the cross-domain scenario, for few-shot learning.Basically, the few-shot learning task uses a classifier learned on the training set to recognize novel classes with very few support samples.In real-world applications, there is usually a domain gap between the training samples and the support samples Tseng et al. (2020); Guo et al. (2020); Chen et al. (2019).The domain gap further imposes a critical challenge: since the support samples are very rare, they do not suffice for mitigating the domain gap between the support set (for novel classes) and the training set.Consequentially, the domain gap significantly compromises the recognition accuracy of the novel classes.Therefore, learning a model with strong cross-modality generalization capacity is important for cross-domain few-shot learning Tseng et al. (2020).We argue that we may enhance the desired cross-modality generalization by \"learning-togeneralize\".To be more specific, we hold a hypothesis / intuition that if a deep model learns to fast generalize itself to different domains (using very few samples) during training, it will maintain the good domain generalization capacity for testing.We model our intuition into a novel Domain-Switch Learning (DSL) framework, as illustrated in Fig. 1.DSL uses multiple (M > 1) domains to construct cross-domain training scenario in a \"fast switching\" manner.Instead of simply mixing all the domains to construct a mini-batch (Fig. 1 (a)), DSL includes only a single domain into every training iteration and switches to another domain for the following iteration (Fig. 1 (b)).Moreover, each iteration contains very few samples per category, so as to imitate the few-shot setting.Therefore, after every switch, the deep model crosses into a domain different from the former one, yielding the cross-domain few-shot scenario.2018) ) as the switchable domains while using mini-ImageNet as a basic training set which appears in each iteration.An important advantage of this domain-switch learning manner is that it can suppress learning from domain-specific knowledge, as illustrated in Fig. 1.Specifically, we consider each domain contains both domain-general and domain-specific knowledge.If we mix all the domains for each training iteration (Fig. 1 (a)), the deep model may memorize all the domain-specific knowledge, as well as the domain-general knowledge.Consequentially, during testing, the domain-specific knowledge hinders generalization towards the novel domain and thus compromises few-shot classification accuracy.In contrast, in DSL (Fig. 1 (b)), the model only learns from one single domain in an iteration.Since the domain-specific knowledge of the current domain does not fit the following domain, the deep model is prone to discarding these domain-specific knowledge during the following iteration.To further promote cross-domain generalization in this domain-switch learning scheme, DSL enforces two constraints as follows:1) The deep model should not over-fit the domain in the current iteration, because over-fitting the current domain makes the model memorize much of the domain-specific knowledge.The first constraint is implemented with a domain-specific prompter module (consisted of multiple prompters).After the model learns from the i-th training domain D i and gets updated, its prediction accuracy on D i increases and is relatively high.We store this edition of model as the prompter for D i .Next time before the training domain switches to D i again, the model just gets updated from D i-1 and becomes relatively inaccurate on D i .Given the accurate prompter and the inaccurate learner (w.r.t.D i ), we average their prediction for supervision so that the penalty on the learner will be suppressed.The details are to be accessed in Section 3.3.2) The deep model should not forget the already-learned knowledge of former domains, so that next time the model crosses into these domains again, it can directly re-use the corresponding knowledge for prediction.The second constraint is implemented with a domain-general teacher module.Specifically, we collect several historical models and average their parameters to get a mean model.During training, we use the softmax prediction of the mean model as the auxiliary supervising signals for the learner (apart from the ground truth label).The mean model serves as a teacher distilling the already-learned knowledge of former domains to the learner.Since this teacher has no obvious bias towards any single domain, we name it as a domain-general teacher.The details of the domaingeneral teacher module are to be accessed in Section 3.4.These two constraints achieve complementary benefits for DSL, and jointly reinforce the crossdomain generalization.Extensive experiments under four cross-domain scenarios show that DSL consistently improves cross-domain few-shot learning and achieves performance on par with the state-of-the-art methods.Our main contributions are summarized as follows:\u2022 We propose a novel Domain-Switch Learning (DSL) framework for cross-domain few-shot learning.DSL uses multiple domains for training and switches the domain in consecutive training iterations.It provides a cross-domain learning scenario where the deep model learns to generalize across different domains.\u2022 Under the DSL framework, we integrate two modules, i.e., the domain-specific prompter and the domain-general teacher.These two modules achieve complementary benefits for DSL and jointly reinforce the cross-domain generalization.\u2022 We conduct extensive experiments to validate the effectiveness of the proposed DSL.Experimental results confirm that the fast generalization capacity can be inherited from training to testing and thus improves cross-domain few-shot classification.On all the four popular benchmarks, DSL achieves performance on par with the state of the art."}
{"paper_id": 222, "abstract": "In the realm of machine learning, a dataset serves as the lifeblood of any task, a tapestry woven from countless threads of data points. Yet, not all threads are created equal; some shimmer with potential, while others fade into obscurity. This disparity in significance can profoundly influence the journey of rehearsal-based continual learning, where we must carefully curate a subset of training examples\u2014a coreset\u2014to revisit and reinforce our model's memory, warding off the specter of catastrophic forgetting.   In this ever-evolving landscape, the caliber of samples we choose to embed in our coreset can spell the difference between triumph and failure. The stakes rise even higher in realistic scenarios, where challenges such as imbalanced datasets or the presence of noise threaten to derail our efforts. To confront these formidable obstacles, we present Online Coreset Selection (OCS), a method both elegant and effective. OCS dynamically identifies the most representative and informative samples at each iteration, enabling us to train in an online fashion.   Our approach not only enhances the model's ability to adapt to new datasets but also ensures that the selected samples maintain a strong affinity with previous tasks, effectively staving off catastrophic forgetting. Through rigorous validation across a spectrum of standard, imbalanced, and noisy datasets, we demonstrate that our coreset selection mechanism outperforms established continual learning baselines, fostering improved task adaptation while maximizing sample efficiency. In this quest for knowledge, OCS stands as a beacon, guiding us toward a future where learning is not just continual, but also resilient.", "introduction": "Humans possess the ability to learn a large number of tasks by accumulating knowledge and skills over time.Building a system resembling human learning abilities is a deep-rooted desire since sustainable learning over a long-term period is essential for general artificial intelligence.In light of this need, continual learning (CL) (Thrun, 1995), or lifelong learning, tackles a learning scenario where a model continuously learns over a sequence of tasks (Kumar & Daume III, 2012;Li & Hoiem, 2016) within a broad research area, such as classification (Kirkpatrick et al., 2017;Chaudhry et al., 2019a), image generation (Zhai et al., 2019), language learning (Li et al., 2019b;Biesialska et al., 2020), clinical application (Lee & Lee, 2020;Lenga et al., 2020), speech recognition (Sadhu & Hermansky, 2020), and federated learning (Yoon et al., 2021).A well-known challenge for continual learning is catastrophic forgetting (McCloskey & Cohen, 1989), where the continual learner loses the fidelity for past tasks after adapting the previously learned knowledge to future tasks.Recent rehearsal-based continual learning methods adapt the continual model to the previous tasks by maintaining and revisiting a small replay buffer (Titsias et al., 2020;Mirzadeh et al., 2020).However, the majority of these methods store random-sampled instances as a proxy set to mitigate catastrophic forgetting, limiting their practicality to real-world applications (see Figure 1a) when all the training instances are not equally useful, as some of them can be more representative or informative for the current task, and others can lead to performance degeneration for previous tasks.Furthermore, these unequal potentials could be more severe under practical scenarios containing imbalanced, streaming, or noisy instances (see Figure 2).This leads to an essential question in continual learning:How can we obtain a coreset to promote task adaptation for the current task while minimizing catastrophic forgetting on previously seen tasks?To address this question, we propose Online Coreset Selection (OCS), a novel method for continual learning that selects representative training instances for the current and previous tasks from arriving streaming data in an online fashion based on our following three selection strategies: (1) Minibatch similarity selects samples that are representative to the current task T t .(2) sample diversity encourages minimal redundancy among the samples of current task T t .(3) Coreset affinity promotes minimum interference between the selected samples and knowledge of the previous tasks T k , \u2200k < t.To this end, OCS minimizes the catastrophic forgetting on the previous tasks by utilizing the obtained coreset for future training, and also encourages the current task adaptation by updating the model parameters on the top-\u03ba selected data instances.The overall concept is illustrated in Figure 1b.Our method is simple, intuitive, and is generally applicable to any rehearsal-based continual learning method.We evaluate the performance of OCS on various continual learning scenarios and show that it outperforms state-of-the-art rehearsal-based techniques on balanced, imbalanced, and noisy continual learning benchmarks of varying complexity.We also show that OCS is general and exhibits collaborative learning with the existing rehearsal-based methods, leading to increased task adaptation and inhibiting catastrophic forgetting.To summarize, our contributions are threefold:\u2022 We address the problem of coreset selection for realistic and challenging continual learning scenarios, where the data continuum is composed of class-imbalanced or noisy instances that deteriorate the performance of the continual learner during training.\u2022 We propose Online Coreset Selection (OCS), a simple yet effective online coreset selection method to obtain a representative and diverse subset that has a high affinity to the previous tasks from each minibatch during continual learning.Specifically, we present three gradient-based selection criteria to select the coreset for current task adaptation while mitigating catastrophic forgetting.\u2022 We demonstrate that OCS is applicable to any rehearsal-based continual learning method and experimentally validate it on multiple benchmark scenarios, where it largely improves the performance of the base algorithms across various performance metrics."}
{"paper_id": 223, "abstract": "In the realm of image processing, the pursuit of dynamic scene deblurring stands as a formidable challenge\u2014one that seeks to banish the spectral remnants of motion blur from our captured moments. Traditional methods, often reliant on learning-based frameworks, strive to bridge the gap between the blurred and the pristine by minimizing the distances, be it L1 or L2, between their outputs and a reference image that embodies clarity. Yet, recent innovations have ventured into the territory of visual recognition features, aiming to enhance perceptual quality. The flaw in this approach lies in its focus on high-level contexts, neglecting the fundamental low-level structures that define blurriness itself.  In response, we put forth a novel strategy that directly confronts the deblurring challenge by embracing the inverse task of reblurring. This technique amplifies the residual blur, reconstructing the original blur with precision. However, therein lies a conundrum: a deblurred image, stripped of its blur, presents a formidable obstacle for reblurring, as it boasts a zero-magnitude blur that resists reconstitution. To navigate this complexity, we introduce two distinct reblurring loss functions tailored for enhanced deblurring performance. The supervised reblurring loss, employed during the training phase, juxtaposes the amplified blur of both deblurred and sharp images. Meanwhile, the self-supervised reblurring loss, utilized during inference, diligently examines the deblurred output for any lingering traces of blur.  Our extensive experiments, conducted across large-scale benchmarks and real-world images, illuminate the efficacy of these reblurring losses. The results not only affirm their capacity to elevate the perceptual quality of deblurred images, as measured by NIQE and LPIPS scores, but also showcase a marked improvement in visual sharpness. In essence, our approach heralds a new dawn in the quest for clarity, where the art of reblurring serves as a powerful ally in the battle against motion blur.", "introduction": "Motion blur commonly arises when the cameras move or scene changes during the exposure in dynamic environments.Dynamic scene deblurring is a challenging ill-posed task finding both the locally-varying blur and the latent sharp image from a large solution space.Traditional approaches (Hirsch et al., 2011;Whyte et al., 2012;Kim et al., 2013;Kim & Lee, 2014) tried to alleviate the ill-posedness by using statistical prior on sharp images such as gradient sparsity.Instead of using such handcrafted knowledge, recent methods take advantage of large-scale datasets as well as deep neural networks (Nah et al., 2017;Su et al., 2017;Noroozi et al., 2017;Nah et al., 2019;Shen et al., 2019).Usually, the learning is driven by minimizing the pixel-wise distance to the ground truth, e.g., L1 or L2, so that the PSNR between the deblurred and the sharp reference can be maximized.By utilizing modern ConvNet architectures and training techniques, state-of-the-art approaches (Nah et al., 2017;Tao et al., 2017;Gao et al., 2019;Yuan et al., 2020;Park et al., 2020;Chi et al., 2021) have been developed toward higher capacity and deblurring accuracy.Still, most methods tend to suffer from the blurry predictions due to the regression-to-mean behavior often witnessed in ill-posed problems with large solution space (Ledig et al., 2017;Menon et al., 2020).To overcome limitations of the conventional objectives, concepts of perceptual (Johnson et al., 2016) and adversarial (Ledig et al., 2017;Nah et al., 2017;Kupyn et al., 2018) loss terms from highlevel semantic tasks have been introduced to improve the visual quality of the deblurred results.Nevertheless, such high-level losses may not serve as optimal goals for blur removal as low-level structural properties, e.g., blurriness, are not the primary features considered in their formulations.As illustrated in Figure 1, results from the previous deblurring methods are still blurry to a degree and the VGG and the adversarial losses are not sufficient to obtain perceptually pleasing and sharp images across different architectures (Tao et al., 2018;Gao et al., 2019;Kupyn et al., 2019).Figure 1: Comparison of the deblurred images and their reblurred counterparts.For each image, we visualize the remaining blur kernel (Cho & Lee, 2009) at the center pixel visualized on the right bottom side.Upper: The kernels from the previous deblurring methods implicate the direction of the original blur.Lower: When the proposed reblurring module is applied, our result does not lose sharpness as we reconstruct the output that is hard to be reblurred.While the deblurred images look less blurry compared with the original input, it is still possible to find nontrivial blur kernels with directional motion information.From the observation, we introduce the concept of reblurring which amplifies the unremoved blur in the given image and reconstructs the original blur.We note that our reblurring operation aims to recover the original motion trajectory in the blurry input, rather than to synthesize arbitrary, e.g., Gaussian, blurs.Therefore, an ideally deblurred clean image is hard to reblur as no noticeable blur can be found to be amplified, making reblurring an ill-posed task.In contrast, it is straightforward to predict the original shape of blur from insufficiently deblurred images as shown in Figure 1.We propose to use the difference between non-ideally deblurred image and the ideal sharp image in terms of reblurring feasibility as the new optimization objective, reblurring loss for the image deblurring problem.The reblurring loss is realized by jointly training a pair of deblurring and reblurring modules.The reblurring module performs the inverse operation of deblurring, trying to reconstruct the original blurry image from a deblurred output.Using the property that the blurriness of a reblurred image depends on the sharpness quality of the deblurred result, we construct two types of loss functions.During the joint training, supervised reblurring loss compares the amplified blurs between the deblurred and the sharp image.Complementing L1 intensity loss, the supervised reblurring loss guides the deblurring module to focus on and eliminate the remaining blur.While our training strategy is similar to the adversarial training of GANs (Goodfellow et al., 2014) in a sense that our deblurring and reblurring modules play the opposite roles, the purposes and effects of the adversary are different.The reblurring loss concentrates on image blurriness regardless of image realism.Furthermore, in contrast to the GAN discriminators that are not often used at test time, our reblurring module can be used to facilitate self-supervised reblurring loss.By making the deblurred image harder to reblur, the deblurring module can adaptively optimize itself without referring to the ground truth.Our reblurring loss functions provide additional optimization directives to the deblurring module and can be generally applied to any learning-based image deblurring methods.With the proposed approach, we can derive sharper predictions from existing deblurring methods without modifying their architectures.We summarize our contributions as follows:\u2022 Based on the observation that clean images are hard to reblur, we propose novel loss functions for image deblurring.Our reblurring loss reflects the preference for sharper images and contributes to visually pleasing deblurring results.\u2022 At test-time, the reblurring loss can be implemented without a ground-truth image.We perform test-time adaptive inference via self-supervised optimization with each input.\u2022 Our method is generally applicable to any learning-based methods and jointly with other loss terms.Experiments show that the concept of reblurring loss consistently contributes to achieving state-of-the-art visual sharpness as well as LPIPS and NIQE across different model architectures."}
{"paper_id": 224, "abstract": "In the realm of Actor-Critic multi-task reinforcement learning (MTRL), a singular value function\u2014our steadfast critic\u2014can sometimes sow discord among the tasks it seeks to guide, leading to detrimental interference that undermines the very essence of learning. Enter Multi-Critic Actor Learning, or MultiCriticAL, a bold new paradigm that champions the idea of distinct critics for each task while employing a unified multi-task actor. By drawing a clear line between tasks, we liberate our critics from the burden of deciphering these distinctions themselves, thereby alleviating the interference that can muddle task-value estimates.  In the crucible of multi-style learning\u2014a fascinating subset of MTRL where agents are sculpted to embody various, unique behavioral styles\u2014MultiCriticAL shines brightly. It demonstrates remarkable prowess, achieving performance enhancements of up to 56% over traditional single-critic approaches. In scenarios where the singular critic falters, MultiCriticAL not only perseveres but triumphs, mastering the art of diverse behavior styles.  To illustrate its practical application, we delve into a simulated real-world scenario: MultiCriticAL empowers agents to fluidly navigate between an array of fighting styles within an experimental iteration of EA\u2019s UFC game. Here, the innovation doesn\u2019t just enhance performance; it transforms the very fabric of how agents learn to fight, blending styles with a grace that echoes the complexities of human combat.", "introduction": "Reinforcement Learning (RL) offers an interesting means by which to develop interesting behaviors in a variety of controls settings.The work presented in this paper is primarily contextualized by our efforts to develop RL techniques for teaching RL-policies to behave with multiple distinct and specific styles for game-play.RL can be useful in developing control policies for game-play and has been demonstrated as being capable of learning human-like or even superhuman play, most notably beating top-ranked human players in games such as Dota 2 (Berner et al., 2019), StarCraft (Vinyals et al., 2019), Chess (Silver et al., 2017), and Go (Silver et al., 2016).RL-based control in games offers a wide array of potential applications, including testing (Zhao et al., 2020;Ariyurek et al., 2021;Gordillo et al., 2021), game design (Gissl\u00e9n et al., 2021), providing competition for human players (Berner et al., 2019), or simply as a means to develop more interesting game-play AI behaviors (Zhao et al., 2020;Alonso et al., 2020).RL algorithms can be notorious however for training aesthetically unappealing policies, but prior work has demonstrated that, with careful shaping of the reward functions, it is possible to derive highly specific desirable behavior (Peng et al., 2018;Mysore et al., 2021b).Machine learning (ML) solutions to game-play AI can however represent a significant increase in compute requirements over their heuristic-based counterparts, especially when using deep learning techniques.If a learned control policy could represent multiple desirable styles and offered a controllable way to transition between the learned styles, this could reduce the total compute burden of the ML-based controller, making it more practically viable.Ideally, developing an ML model would constitute learning a single, low-complexity policy that would model and transition smoothly between multiple behavior styles.Reduced model complexity and deploying just a single policy network enables greater compute efficiency through hardware parallelism and reduced memory costs, which is beneficial in resource-constrained applications, such as videogames.We explore the problem of mutli-style RL, a special case of the more commonly explored problem of multi-task RL (MTRL).Whereas MTRL techniques typically seek to solve a wide array of RL problems under a single learning campaign, multi-style RL adds the extra constraint that the 'tasks' in question all have identical system dynamics but would be characterized by different re-Figure 1: MultiCriticAL breaks from the common practice of using a single unified critic in MTRL and instead uses separate critics for each task learned.The proposed method is used to successfully train multiple distinct behavior styles in various games including Pong and UFC.ward signals corresponding to the different desired styles of behavior.We focus specifically on applications of Actor-Critic RL techniques, which, in addition to being compatible with continuous action-space control, allow for reduced runtime computational cost as only the actor functions are required for inference, while the critics can often be disregarded after training.Mysore et al. (2021a) and Andrychowicz et al. (2021) have also demonstrated that it is possible to train compact and performant policy networks through careful consideration of the actor and critic network architectures.A common baseline approach to MTRL is to employ one-hot task encoding to delineate between tasks.However, in a number of multi-style cases considered this was not sufficient to enable successful multi-style policy-learning.We suspected that the similarity of game-play states visited between each trained style interfered with each style's value optimization.In surveying existing MTRL literature for advancements in the field of multi-task Actor-Critic RL methods, we noticed a surprising hole.There are studies that employ single actors and single critics (Finn et al., 2017;Yang et al., 2020;Sodhani et al., 2021;Nichol et al., 2018), multiple actors and multiple critics (Andreas et al., 2017;Teh et al., 2017;Rusu et al., 2015;2016;Huang et al., 2017), or multiple actors and a single critic (Yang et al., 2017;Dewangan et al., 2018).Curiously, seemingly no prior work explores using a single policy with distinct, per-task value estimation, i.e. a single actor with multiple critics.This paper proposes Multi-Critic Actor Learning (MultiCriticAL), a single-actor, multi-critic framework for multi-task Actor-Critic optimization, and evaluates it on a variety of multi-style learning problems.The core idea of MultiCriticAL is to maintain separate per-style (or task) critic functions for each style being learned.By separating the critic MultiCriticAL would be able to avoid negative interference in the learned values for different styles.Our results show that MultiCriticAL consistently trains more performant policies, compared to its single-critic counterparts, succeeds in cases where the single-critic methods fail and achieves between 20-45% improvement in more traditional multi-task settings involving multi-level game-play.Additionally, we demonstrate the utility of Mul-tiCriticAL in a use-case more reflective of real-world application, in an experimental build for EA's UFC Knockout Mode, where we train policies to act with multiple specific and distinct behavior styles.This work is also amongst the first to study the efficacy of MTRL algorithms across a broad range of multi-style problems, an aspect of MTRL receiving limited prior attention."}
{"paper_id": 225, "abstract": "In the ever-evolving realm of self-supervised learning, a promising path has emerged through the lens of contrastive learning\u2014a method that seeks to distinguish each image, each instance, within the vast expanse of a dataset. Yet, amid this pursuit of clarity, a shadow looms: the intricate tapestry of semantic relationships among instances is often overlooked. This oversight can lead to a perplexing phenomenon, where the anchor\u2014a reference point in our learning\u2014finds itself distanced from semantically similar samples, a situation we have come to recognize as \"false negatives.\"  In our exploration, we unveil a crucial insight: the detrimental impact of these false negatives intensifies as we scale up to larger datasets, rich with a myriad of semantic concepts. To combat this challenge, we introduce a groundbreaking self-supervised contrastive learning framework that operates with an innovative approach: it incrementally identifies and explicitly eliminates these false negatives from the learning process.   Our method follows a dynamic trajectory alongside the training process, progressively detecting an increasing number of high-quality false negatives. This evolution is rooted in the understanding that, as our encoder matures, the embedding space itself becomes more structured and semantically coherent. Furthermore, we delve into two strategic methodologies for the explicit removal of these identified false negatives during the course of contrastive learning.  Through rigorous experimentation, our framework demonstrates its superiority, outshining existing self-supervised contrastive learning methods across multiple benchmarks, even within the constraints of limited resources. In this journey through the intricate landscape of learning, we aim not just to refine our techniques, but to illuminate the path for future explorations in the vast domain of vision tasks.", "introduction": "Self-supervised learning of visual representation (Doersch et al., 2015;Pathak et al., 2016;Noroozi & Favaro, 2016;Oord et al., 2018;Gidaris et al., 2018;Chen et al., 2020a) aims to learn a semanticaware embedding space based on the image data without the supervision of human-labeled annotations.Recently, significant advances have been made by contrastive learning approaches (Hjelm et al., 2019;Tian et al., 2019;Chen et al., 2020b;c;He et al., 2020;Chen et al., 2020d) to reduce the performance gap with the supervised counterparts.Most unsupervised contrastive learning methods are developed based on the instance discrimination assumption.These approaches treat each training image as an individual instance and learn the representations that discriminate every single sample.Specifically, considering an arbitrary image as an anchor, the only positive sample is generated by applying a different data augmentation to the anchor image, while all other images are treated as negative samples.The training objective is to attract the positive to the anchor while repelling the negative samples from the anchor.While instance-level contrastive learning has shown impressive performance, these methods do not take the semantic relationship among images into consideration.Although some images share similar semantic concepts with the anchor, they are still considered as negative samples and are equally pushed away from the anchor with all other negatives.We define these samples as \"false negatives\" in self-supervised contrastive learning, which would adversely affect the representation learning (Saunshi et al., 2019).One might argue that such undesirable effects might be minor for the datasets with diverse semantic concepts since the probabilities of drawing false negatives are relatively low.However, as shown in Figure 1, we empirically find the opposite results that the performance drops due to training with false negative samples are more significant on large-scale datasets with more semantic categories.These results bring up the issue of instance-level contrastive learning when it is applied to datasets with more complex semantic contents.Figure 1: Effect of false negative for contrastive learning.To study the effect of false negatives, we compare two frameworks: Sim-CLR (Chen et al. (2020b), blue) representing instance-level contrastive learning which is trained with false negatives, and SupCon (Khosla et al. (2020), red) benefiting from human-labeled annotations to exclude false negatives.We observe that training with false negatives leads to larger performance gaps (orange) on the datasets with more semantic categories.The proposed approach (green) effectively alleviates such adverse effects by explicitly removing the detected false negatives for self-supervised contrastive learning.See Section 4.3 for experimental details.To handle the false negative issue, we propose a novel framework to incrementally detect and explicitly remove false negatives for self-supervised contrastive learning.During training, we cluster samples in the embedding space and assign the cluster indices as the pseudo labels to the training images.Instances with an identical label to the anchor are then detected as false negatives.However, we notice that the pseudo labels generated in the earlier training stages are comparably unreliable to represent the semantic relationship among instances since the semantic structure of the embedding space is still under development.Therefore, we propose a novel strategy to assign the pseudo labels in an incremental manner.In the early training stage, we only use a small portion of pseudo labels with higher confidence while treating other samples as individual instances.In the later training stage, as the pseudo labels become more reliable, we dynamically include more confident labels to detect and remove false negatives and hence, can benefit semantic-aware representation learning.Furthermore, we discuss two types of training objectives to explicitly address the detected false negatives during contrastive learning: elimination and attraction.We theoretically show that they are the generalization of triplet loss (Schroff et al., 2015).While both objectives avoid discriminating the anchor and its false negatives, the attraction objective further minimizes the distances between them.Empirically, we find that such a more aggressive strategy is less tolerant to noisy pseudo labels and unsuitable to our self-supervised framework.The main contributions of this paper are summarized as follows:\u2022 We highlight the issue of false negatives in contrastive learning, especially on the largescale datasets with more diverse semantic contents.\u2022 We propose a novel contrastive learning framework with incremental false negative detection (IFND) that smoothly bridges instance-level and semantic-aware contrastive learning.Our approach explicitly detects and incrementally removes false negatives as the embedding space becomes more semantically structural through the training process.\u2022 Our approach performs favorably against the existing self-supervised contrastive learning frameworks on multiple benchmarks under a limited training resource setup.Besides, we introduce two metrics: mean true positive/negative rates to evaluate the clustering quality."}
{"paper_id": 226, "abstract": "In the realm of machine learning, where the shadows of uncertainty often loom large, the challenge of Positive and Unlabeled (PU) learning emerges as a formidable quest. This endeavor seeks to forge a binary classifier from the murky waters of weak training datasets, comprised of both positive and unlabeled instances. In this paper, we unveil a groundbreaking approach: Positive and Unlabeled Learning with Partially Positive Mixup, or P3Mix. This innovative method harnesses the dual powers of data augmentation and supervision refinement through a clever mixup technique.  Drawing inspiration from a curious phenomenon we observed\u2014where the learned PU boundary tends to drift towards the positive side of the spectrum\u2014we devised a strategy to address the inherent ambiguity of unlabeled instances. By selecting mixup partners from the positive instances that lie in proximity to this learned boundary, we create augmented instances that are not only closer to the boundary but also endowed with sharper supervision. This transformation acts as a catalyst, nudging the learned PU boundary ever closer to the fully supervised ideal, thereby enhancing our classification prowess.  Our extensive experimental results illuminate the efficacy of this heuristic mixup technique within the PU learning landscape. The findings reveal that P3Mix consistently surpasses the existing state-of-the-art methods, paving the way for a new era in the pursuit of clarity amidst uncertainty.", "introduction": "Positive and Unlabeled (PU) learning refers to a specific binary classification problem, where only a small number of positive training instances are manually annotated but all other instances are unlabeled (Liu et al., 2002).Such kind of datasets naturally arise in many significant real-world scenarios such as product recommendation (Hsieh et al., 2015), deceptive reviews detection (Ren et al., 2014), and medical diagnosis (Yang et al., 2012).For specific example, many diseases, e.g., Alzheimer's disease, Amyotrophic Lateral Sclerosis, and Parkinson's disease, are very infrequent and with long latency, hence only few diagnosed patients are known but a much larger population of undiagnosed individuals may be either diseased or healthy.Treating the diagnosed ones as positive instances and the undiagnosed ones as unlabeled instances results in such PU datasets of medical diagnosis.To meet those practical demands, PU learning has drawn increasing interest from the machine learning community (Bekker & Davis, 2020).Formally, let x \u2208 R d and y \u2208 {0, 1} be the feature representation and category label, respectively, where the positive instance is indicated by y = 1 and the negative one by y = 0.In the context of PU learning, the training dataset is composed of the sets of positive instances P = {(x i , y i = 1)} np i=1and unlabeled instances U = {x i } np+nu i=np+1 , where U contains both positive and negative instances.The target is to learn a binary classifier based on such weak training dataset P \u222a U.During the past decades, many PU learning methods have been proposed, where, naturally, the essential idea is to estimate the negative instances from the set of unlabeled instances U. Generally, most of existing PU learning methods can be divided into two categories, termed as sample-selection  et al., 2017;Krizhevsky, 2016;Chen et al., 2020a).The disambiguation-free objective suffers from much lower PP rates than the real positive prior as well as the fully supervised objective.This implies the decision boundary deviation phenomenon, which results in higher error rates.methods and cost-sensitive methods.The sample-selection methods, as the name suggests, mainly select reliable negative instances from U using various heuristic strategies, e.g., Na\u00efve Bayes (Liu et al., 2002), kNN (Zhang & Zuo, 2009), k-means (Chaudhari & Shevade, 2012), and reinforcement learning (Luo et al., 2021); and then apply supervised methods over positive and those reliable negative instances.In contrast, the cost-sensitive methods treat all unlabeled instances as corrupted negative ones, and correct the estimation bias of the objective by employing well-designed misclassification risks such as unbiased risk estimator (du Plessis et al., 2014;2015;Kiryo et al., 2017) and maximum margin loss (Shi et al., 2018;Gong et al., 2019b;Zhang et al., 2019;Gong et al., 2019a).Orthogonal to the aforementioned techniques, we note that some PU learning methods such as (Chen et al., 2020a;Wei et al., 2020) have made preliminary attempts to integrate with the art of mixup, i.e., an economic-yet-effective data augmentation method (Zhang et al., 2018).Formally, mixup generates an augmented instance ( x, y) with the convex combination of any pair of instances {(x i , y i ), (x j , y j )} drawn from the training dataset:Previous studies have indicated that mixup is approximately equivalent to applying adversarial training (Zhang et al., 2021), enabling to improve robustness with even scarce and noisy supervision (Thulasidasan et al., 2019;Carratino et al., 2020;Zhang et al., 2021).Accordingly, it has been successfully used to solve various learning problems with weak supervision, e.g., semi-supervised learning (Berthelot et al., 2019), noisy label learning (Li et al., 2020b), and partial label learning (Yan & Guo, 2020).Our story and contribution.Inspired by the recent success of mixup in learning with weak supervision, our original goal is to thoroughly investigate the impact of mixup in PU learning.To this end, we begin with a naive disambiguation-free objective of PU learning, where all unlabeled instances are treated as pseudo-negative instances, denoted by U = {(x i , y i = 0)} np+nu i=np+1 , and the binary classifier is trained based on P \u222a U.In preliminary experiments, we found an interesting phenomenon, where the number of training instances predicted as positive by the disambiguationfree classifier tends to be smaller than usual as illustrated in Fig. 1.This phenomenon implies that the disambiguation-free boundary tends to deviate from the fully supervised boundary towards the positive side, expressed by a toy example shown in Fig. 2(a).We consider that the decision boundary deviation is mainly caused by the marginal pseudo-negative instances, which lie between the two boundaries.Such instances are more likely to be positive but actually annotated by negative.Motivated by this observation, we extend mixup to a specific heuristic version for PU learning, enabling to achieve data augmentation and supervision correction simultaneously.Its basic idea is to transform the marginal pseudo-negative instances into augmented instances which are partially positive and yet also lie between the two boundaries, so as to push the learned boundary towards the fully supervised one.This can be achieved by selecting the mixup partners for marginal pseudonegative instances from the positive instances that are around the learned boundary, as expressed in Fig. 2(b).With this insight, we propose a novel PU method, namely Positive and unlabeled learning with Partially Positive Mixup (P 3 Mix).Generally, P 3 Mix is easy-to-implement, where, specifically, we can define the marginal pseudo-negative instances using the predictive results and the positive instances around the boundary using the entropy values of predictive results.To evaluate the effectiveness of P 3 Mix, we conduct a number of experiments on benchmark datasets.Experimental results demonstrate that P 3 Mix can consistently outperform the state-of-the-art PU methods."}
{"paper_id": 227, "abstract": "In the ever-expanding realms of artificial intelligence, we unveil a groundbreaking concept: Synthetic Environments (SEs) and Reward Networks (RNs), both forged from the intricate architecture of neural networks. These entities serve as proxy models, designed to guide Reinforcement Learning (RL) agents through their training journeys. Our exploration reveals a remarkable truth: an agent, having honed its skills within the confines of an SE, can seamlessly navigate the complexities of its corresponding real environment.  The SE operates as a comprehensive surrogate, adept at capturing the nuances of state dynamics and reward structures, while the RN functions as a more selective ally, enhancing or substituting rewards where necessary. To bring these constructs to life, we employ a bi-level optimization framework: the inner loop focuses on the training of the RL agent, while the outer loop meticulously fine-tunes the parameters of the SE and RN using an evolutionary strategy.  Through rigorous evaluation across a diverse spectrum of RL algorithms and classic control environments, we uncover a paradox. In direct comparison, the journey to master an SE proxy demands more interactions with the real environment than training agents solely within that reality. Yet, once an SE has been established, the need for real-world interactions diminishes, paving the way for the training of new agents without further engagement with the tangible world. These learned SE proxies empower agents to achieve their goals with fewer interactions, all while preserving the integrity of performance.  Our empirical findings illuminate the path forward, suggesting that SEs excel by cultivating informed representations that steer agents toward the most pertinent states. Furthermore, we discover that these proxies exhibit a remarkable resilience to variations in hyperparameters and possess the ability to adapt to unseen agents, heralding a new era of training efficiency in the landscape of Reinforcement Learning.", "introduction": "Generating synthetic data addresses the question of what data is required to achieve a rich learning experience in machine learning.Next to increasing the amount of available data, synthetic data can enable higher training efficiency that opens up new applications for Neural Architecture Search (Such et al., 2020), may improve algorithm analysis or facilitate custom datasets (Jhang et al., 2020).In this paper, we consider learning neural synthetic data generators for Reinforcement Learning (RL).We investigate the question of whether we can learn a synthetic Markov Decision Process of a real (target) environment which is capable of producing synthetic data to allow effective and more efficient agent training, that is, to achieve similar or higher performance more quickly compared to when training purely on the real environment.When learning to produce both states and rewards, we refer to these neural network proxies as synthetic environments (SEs).Additionally, we investigate the same question for learning reward proxies that do not learn about the state dynamics and which we refer to as a Reward Networks (RNs).We depict our procedure in Figure 1which resembles a bi-level optimization scheme consisting of an outer and inner loop.The inner loop trains the agent on an SE or RN.Since our method is agnostic to both domain and agent, we can interchangeably adopt standard RL algorithms in the inner loop.In the outer loop, we assess the agent's performance by evaluating it on the real environment; we then take the collected reward as a score to update the SE's or RN's neural parameters used in the inner loop.In this way, the SE/RN is gradually updated such that an agent being trained on it, scores higher on a real environment.For the outer loop we use Evolution Strategies (Rechenberg, 1973;Salimans et al., 2017) with a population of SE/RN parameters.After discussing related work (Section 2), we make the following contributions:agent env action state reward test test freeze policy train agent env action state reward freeze policy Synthetic Environments (SEs) Reward Networks (RNs) rollout on real env rollout on real env agent NN action state reward training on synthetic env \u03a8 action state reward training on reward net \u03a8 train env NN agent cumulative reward of policy trained on NN outer loop ES \u03a8 cumulative reward of policy trained on NN outer loop ES \u03a8 Signals Steps Network Updates 1 1 2 2 3 3Figure 1: We use an agent-agnostic meta-learning approach to learn neural proxy RL environments (left) and reward networks (right) for a target task.In the inner loop, we train RL agents on the proxy and use the evaluation performance on the target task in the outer loop to evolve the proxy.\u2022 We introduce synthetic environments (Section 3), a novel concept that focuses on the environment instead of agent learning using a bi-level optimization scheme, which is guided purely by the agent performance.This concept goes beyond the usual learning of a onetime internal environment model inside an agent (such as in Dyna (Sutton, 1990)).\u2022 As a sub-problem of SEs, we investigate reward networks (Section 4), contrasting several types of potential-based reward shaping (Ng et al., 1999) variants.\u2022 We show that it is possible to learn SEs (Section 5) and RNs (Section 6) that, when used for agent training, yield agents that successfully solve the Gym tasks (Brockman et al., 2016) CartPole and Acrobot (SEs), as well as Cliff Walking, CartPole, MountainCarContinuous, and HalfCheetah (RNs).\u2022 We show that SEs and RNs are efficient and robust in training agents, require fewer training steps compared to training on the real environment and are able to train unseen agents \u2022 We report empirical evidence showing that SEs and RNs achieve their efficiency gains for training new agents through condensed and informed state and reward representations.Overall, we find it noteworthy that it is actually possible to learn such proxies, and we believe our research will improve their understanding.Since these learned proxies can train agents quickly and transfer to unseen agents, we believe this work might open up possible avenues of research in RL.Possible future applications include cheap-to-run environments for AutoML (Hutter et al., 2019) or for robotics when training on targets is expensive, as well as agent and task analysis, or efficient RL agent pre-training.Our PyTorch (Paszke et al., 2019) code and models are made available publicly.foot_0"}
{"paper_id": 228, "abstract": "In the realm of deep learning, the exploration of 3D shape analysis has flourished, birthing a myriad of models tailored to various data representation formats. From the intricate web of MeshCNN for meshes to the innovative PointNet for point clouds and the robust VoxNet for voxels, the landscape is rich with advancements. In this paper, we unveil the Representation-Agnostic Shape Fields (RASF)\u2014a versatile and computationally efficient shape embedding module poised to revolutionize 3D deep learning.  At its core, RASF is constructed upon a learnable 3D grid, replete with multiple channels that elegantly capture local geometry. Through the magic of coordinate indexing, RASF deftly retrieves shape embeddings across a spectrum of 3D representations, be they point clouds, meshes, or voxels. While the optimization of RASF's learnable parameters offers a multitude of pathways, we spotlight two particularly effective pre-training schemes: shape reconstruction and normal estimation.   Once RASF has undergone training, it transforms into a plug-and-play performance enhancer, delivering substantial gains with minimal overhead. Our extensive experiments, spanning a diverse array of 3D representation formats, networks, and applications, underscore the universal prowess of RASF. For those eager to delve deeper, our code and pre-trained models await at your fingertips, ready to unlock new possibilities in the world of 3D shape analysis.", "introduction": "3D shape analysis is the foundation to understand the physical world.It has a wide range of applications in real life including robotics (Liu et al., 2020;Liang et al., 2018), autopilot (Shi et al., 2020;Song et al., 2019;Qi et al., 2018;Zhou & Tuzel, 2018), medical imaging (Yang et al., 2021b;Li et al., 2018a;Yang et al., 2021a) and movie animation (Xu et al., 2019;Aberman et al., 2020;Hertz et al., 2020).In recent years, the research on this topic has prevailed and showed promising results in various tasks, such as object classification, part segmentation, scene segmentation, etc. .Figure 1: Vertical Axis: Performance; Horizontal Axis: Latency.Diverse backbones with (solid symbols) and without RASF (hollow symbols) are evaluated.RASF could be seamlessly plugged into any 3D deep learning pipeline to improve performance across diverse downstream tasks and datasets, with little code modification and computation cost.Shape could be represented in different data formats, among which meshes, point clouds and voxels are most commonly used.In the deep learning era, most studies on these three representations use coordinates or coordinateslike feature as input to feed into the backbone network.For point clouds, the direct input to the network is the point coordinates.For meshes, the node feature of the graph is the vertices coordinates.For volumetric data, the 3D shape is denoted by whether a voxel in a particular position is occupied or not.Using coordinates to characterize a shape is simple and straightforward.However, the major problem with this is that coordinate lacks contextual geometric information.Hence the capacity of the backbone network could be restricted.Even though various operators and backbones are proposed to extract high-level feature from the combination of coordinates by aggregating the local geometry, the effect of the input feature is not clear yet.The practice in Natural Language Processing (NLP) and Data Mining (DM) might shed some light on this issue.Word embeddings (Mikolov et al., 2013;Pennington et al., 2014) is proposed very early in NLP to map words into an embedding space using embedding layers, which work as lookup tables that are indexed by the one-hot encoding of words.The word embeddings retrieved from embedding layers have similar values for words with similar meanings.This technique is widely adopted across the NLP area and notably boosts the overall performance, regardless of how the text data is distributed and which language model is used (Brown et al., 2020;Devlin et al., 2019;Radford et al., 2019;Vaswani et al., 2017).Studies in the field of data mining (DM) also learn continuous embedding representation for graph nodes (Grover & Leskovec, 2016;Perozzi et al., 2014).Embedding learning in NLP and DM indicates that the effect of input feature and the capacity of the backbone network are somehow orthogonal to each other.It raises the question that whether there is a better way to denote a shape instead of vanilla coordinates, so as to facilitate the backbone network to execute downstream tasks, regardless of which backbone model is used.In this work, we introduce Representation-Agnostic Shape Fields (RASF), a shape embedding layer that maps coordinates to shape embeddings with rich geometry information.RASF is implemented using a learnable multi-channel 3D grid.Similar to the lookup table in word embedding layer, coordinates within a local shape index from this 3D grid and retrieve shape embeddings.With simple operation on data, we make RASF compatible with major 3D representations, including point clouds, meshes and voxels.To obtain the weights of RASF, we investigate several pre-training schemes for RASF, among which we find that self-supervised training schemes, i.e., reconstruction and normal estimation, yield the best performance and generalizability.Once trained, RASF could be seamlessly plugged into any 3D deep learning pipeline to improve performance across diverse downstream tasks and datasets, with little code modification and computation cost (See Fig. 1).We empirically show that RASF consistently brings significant improvement under diverse backbones and applications, including object classification, part segmentation and scene segmentation.In this work, we introduce a generalizable (i.e., it could be used in different 3D representations, backbones and downstream tasks) and computation-efficient shape embedding layer for 3D deep learning, named Representation-Agnostic Shape Fields (RASF).It applies a learnable multi-channel 3D grid to store local geometry.Shape embeddings for various 3D shape representations (point clouds, meshes and voxels) are retrieved by coordinates indexing.While there are multiple ways to obtain the coefficients of RASF, we provide two effective schemes among all in this paper for RASF pre-training, that is shape reconstruction and normal estimation.Abundant experiments across different representations, backbones and downstream tasks are conducted to validate the generalization and efficiency of our proposed RASF."}
{"paper_id": 229, "abstract": "In the ever-evolving realm of Natural Language Processing, the towering figure of BERT has emerged, casting a long shadow with its remarkable prowess across a myriad of tasks. Yet, as with many great powers, its might comes at a cost\u2014one that weighs heavily on computational resources and memory constraints. Enter the arcane art of binarization, a powerful technique that wields the ability to compress this behemoth into a form that demands but a fraction of its original might, relying on the simplicity of 1-bit parameters and swift bitwise operations.   However, the quest for full binarization of BERT\u2014a path fraught with peril\u2014often leads to a steep decline in performance, a fate that has been largely overlooked by scholars. In this work, we embark on a journey of discovery, uncovering the root causes of this decline: the degradation of information and the discordance in optimization directions during the intricate dance of forward and backward propagation.   Thus, we unveil BiBERT, a beacon of hope in the landscape of fully binarized models, crafted to transcend these limitations. BiBERT introduces a novel Bi-Attention structure, designed to harness representation information with unparalleled efficiency, alongside a Direction-Matching Distillation (DMD) scheme that meticulously guides the optimization of our fully binarized BERT.   Through rigorous experimentation, we demonstrate that BiBERT not only surpasses straightforward baselines but also eclipses existing state-of-the-art quantized BERTs, achieving remarkable margins on established NLP benchmarks. As the first of its kind, BiBERT achieves a staggering reduction of 56.3 times in FLOPs and 31.2 times in model size, illuminating the tremendous potential of fully binarized BERTs in resource-constrained environments. The future of NLP may very well hinge on such innovations, where power meets efficiency in a world that demands both.", "introduction": "\u20221.8BiBERT (ours) 1-1-1\u2022 13MB Figure 1: Accuracy vs. FLOPs & size.Our BiBERT enjoys most computation and storage savings while surpassing SOTA quantized BERTs on GLUE benchmark with low bit activation.Recently, the pre-trained language models have shown great power in various natural language processing (NLP) tasks (Wang et al., 2018a;Qin et al., 2019;Rajpurkar et al., 2016).In particular, BERT (Devlin et al., 2018) significantly improves the state-of-the-art performance, while the massive parameters hinder their widespread deployment on edge devices in the real world.Therefore, model compression has been actively studied to alleviate resource constraint issues, including quantization (Shen et al., 2020;Zafrir et al., 2019), distillation (Jiao et al., 2020;Xu et al., 2020), pruning (McCarley et al., 2019;Gordon et al., 2020), parameter sharing (Lan et al., 2020), etc.Among them, quantization emerges as an efficient way to obtain the compact model by compressing the model parameters to lower bit-width representation, such as Q-BERT (Shen et al., 2020), Q8BERT (Zafrir et al., 2019), and GOBO (Zadeh et al., 2020).However, the representation limitation and optimization difficulties come as a consequence of applying discrete quantization, triggering severe performance drop in quantized BERT.Fortunately, distillation becomes a common remedy in quantization as an auxiliary optimization approach to tackle the performance drop, which encourages the quantized BERT to mimic the full-precision model to exploit knowledge in teacher's representation (Bai et al., 2020).tivation) further allows the model to utilize extremely compact 1-bit parameters and efficient bitwise operations, which can largely promote the applications of BERT on edge devices in the real world (Wang et al., 2021;Liu et al., 2018).However, although the BERT quantization equipped with distillation has been studied, it is still a significant challenge to binarize BERT to extremely 1-bit, especially for its activation.Compared with the weight and word embedding in BERT, the binarization of activation brings the most severe drop, and the model even crashes in NLP tasks.So far, previous studies have pushed down the weight and word embedding to be binarized (Bai et al., 2020), but none of them have ever achieved to binarize BERT with 1-bit activation accurately.Therefore, we first build a fully binarized BERT baseline, a straightforward yet effective solution based on common techniques.Our study finds that the performance drop of BERT with binarized 1-bit weight, activation, and embedding (called fully binarized BERT) comes from the information degradation of the attention mechanism in the forward propagation and the optimization direction mismatch of distillation in the backward propagation.First, the attention mechanism makes BERT focus selectively on parts of the input and ignore the irrelevant content (Vaswani et al., 2017;Chorowski et al., 2015;Wu et al., 2016).While our analysis shows that direct binarization leads to the almost complete degradation of the information of attention weight (Figure 4), which results in the invalidation of the selection ability for attention mechanism.Second, the distillation for the fully binarized BERT baseline utilizes the attention score, the direct binding product of two binarized activations.However, we show that it causes severe optimization direction mismatch since the non-neglectable error between the defacto and expected optimization direction (Figure 5).This paper provides empirical observations and theoretical formulations of the above-mentioned phenomena, and proposes a BiBERT to turn the full-precision BERT into the strong fully binarized model (see the overview in Figure 2).To tackle the information degradation of attention mechanism, we introduce an efficient Bi-Attention structure based on information theory.Bi-Attention applies binarized representations with maximized information entropy, allowing the binarized model to restore the perception of input contents.Moreover, we developed the Direction-Matching Distillation (DMD) scheme to eliminate the direction mismatch in distillation.DMD takes appropriate activation and utilizes knowledge from constructed similarity matrices in distillation to optimize accurately.Our BiBERT, for the first time, presents a promising route towards the accurate fully binarized BERT (with 1-bit weight, embedding, and activation).The extensive experiments on the GLUE (Wang et al., 2018a) benchmark show that our BiBERT outperforms existing quantized BERT models with ultra-lower bit activation by convincing margins.For example, the average accuracy of BiBERT exceeds 1-1-1 bit-width BinaryBERT (1-bit weight, 1-bit embedding and 1-bit quantization) by 20.4% accuracy on average, and even better than 2-8-8 bit-width Q2BERT by 13.3%.Besides, we highlight that our BiBERT gives impressive 56.3\u00d7 and 31.2\u00d7saving on FLOPs and model size, respectively, which shows the vast advantages and potential of the fully binarized BERT model in terms of fast inference and flexible deployment in real-world resource-constrained scenarios (Figure 1).Our code is released at https://github.com/htqin/BiBERT."}
{"paper_id": 230, "abstract": "In the ever-evolving realm of computer vision, a new revolution is taking shape\u2014one driven by the power of Transformers. These remarkable architectures are reshaping the landscape of recognition tasks, and at the forefront of this transformation are Detection Transformers, the pioneers of fully end-to-end learning systems for object detection. Meanwhile, Vision Transformers stand as the first fully transformer-based design for image classification, heralding a new era in visual understanding.  In this paper, we unveil the Vision and Detection Transformers (ViDT), a groundbreaking synthesis that melds the strengths of both methodologies to forge a remarkably effective and efficient object detector. At the heart of ViDT lies a reimagined attention module, which enhances the capabilities of the Swin Transformer, elevating it to operate as a standalone object detector. This is complemented by a computationally savvy transformer decoder that deftly harnesses multi-scale features and employs auxiliary techniques, all meticulously crafted to elevate detection performance while maintaining a lean computational footprint.  Our extensive evaluations on the Microsoft COCO benchmark dataset reveal that ViDT strikes an unparalleled balance between average precision (AP) and latency, outshining its fully transformer-based counterparts with an impressive 49.2 AP. This achievement speaks to the high scalability of our approach, accommodating even the most substantial models with ease. We invite the community to explore our findings and implementations, as we have made the code and trained models publicly available at https://github.com/naver-ai/vidt. Join us on this journey as we redefine the boundaries of what is possible in object detection.", "introduction": "Object detection is the task of predicting both bounding boxes and object classes for each object of interest in an image.Modern deep object detectors heavily rely on meticulously designed components, such as anchor generation and non-maximum suppression (Papageorgiou & Poggio, 2000;Liu et al., 2020).As a result, the performance of these object detectors depend on specific postprocessing steps, which involve complex pipelines and make fully end-to-end training difficult.Motivated by the recent success of Transformers (Vaswani et al., 2017) in NLP, numerous studies introduce Transformers into computer vision tasks.Carion et al. (2020) proposed Detection Transformers (DETR) to eliminate the meticulously designed components by employing a simple transformer encoder and decoder architecture, which serves as a neck component to bridge a CNN body for feature extraction and a detector head for prediction.Thus, DETR enables end-to-end training of deep object detectors.By contrast, Dosovitskiy et al. (2021) showed that a fully-transformer backbone without any convolutional layers, Vision Transformer (ViT), achieves the state-of-theart results in image classification benchmarks.Approaches like ViT have been shown to learn effective representation models without strong human inductive biases, e.g., meticulously designed components in object detection (DETR), locality-aware designs such as convolutional layers and pooling mechanisms.However, there is a lack of effort to synergize DETR and ViT for a better object detection architecture.In this paper, we integrate both approaches to build a fully transformer-based, end-to-end object detector that achieves state-of-the-art performance without increasing computational load.A straightforward integration of DETR and ViT can be achieved by replacing the ResNet backbone  (body) of DETR with .This naive integration, DETR (ViT)foot_0 , has two limitations.First, the canonical ViT suffers from the quadratic increase in complexity w.r.t.image size, resulting in the lack of scalability.Furthermore, the attention operation at the transformer encoder and decoder (i.e., the \"neck\" component) adds significant computational overhead to the detector.Therefore, the naive integration of DETR and ViT show very high latency -the blue lines of Figure 1.Recently, Fang et al. (2021) propose an extension of ViT to object detection, named YOLOS, by appending the detection tokens [DET] to the patch tokens [PATCH] (Figure 2(b)), where [DET] tokens are learnable embeddings to specify different objects to detect.YOLOS is a neck-free architecture and removes the additional computational costs from the neck encoder.However, YOLOS shows limited performance because it cannot use additional optimization techniques on the neck architecture, e.g., multi-scale features and auxiliary loss.In addition, YOLOS can only accommodate the canonical transformer due to its architectural limitation, resulting in a quadratic complexity w.r.t. the input size.In this paper, we propose a novel integration of Vision and Detection Transformers (ViDT) (Figure 2(c)).Our contributions are three-folds.First, ViDT introduces a modified attention mechanism, named Reconfigured Attention Module (RAM), that facilitates any ViT variant to handle the appended [DET] and [PATCH] tokens for object detection.Thus, we can modify the latest Swin Transformer (Liu et al., 2021) backbone with RAM to be an object detector and obtain high scalability using its local attention mechanism with linear complexity.Second, ViDT adopts a lightweight encoder-free neck architecture to reduce the computational overhead while still enabling the additional optimization techniques on the neck module.Note that the neck encoder is unnecessary because RAM directly extracts fine-grained representation for object detection, i.e., [DET] tokens.As a result, ViDT obtains better performance than neck-free counterparts.Finally, we introduce a new concept of token matching for knowledge distillation, which brings additional performance gains from a large model to a small model without compromising detection efficiency.ViDT has two architectural advantages over existing approaches.First, similar to YOLOS, ViDT takes [DET] tokens as the additional input, maintaining a fixed scale for object detection, but constructs hierarchical representations starting with small-sized image patches for [PATCH] tokens.Second, ViDT can use the hierarchical (multi-scale) features and additional techniques without a significant computation overhead.Therefore, as a fully transformer-based object detector, ViDT facilitates better integration of vision and detection transformers.Extensive experiments on Microsoft COCO benchmark (Lin et al., 2014) show that ViDT is highly scalable even for large ViT models, such as Swin-base with 0.1 billion parameters, and achieves the best AP and latency trade-off."}
{"paper_id": 231, "abstract": "In the realm of reinforcement learning, a new horizon beckons\u2014a horizon illuminated by the concept of curriculum value-based reinforcement learning (RL). This innovative approach navigates the complexities of mastering a challenging target task by deftly leveraging action-values across a carefully curated sequence of tasks, each one more formidable than the last. Yet, the intricacies of effectively reusing these action-values remain shrouded in mystery, a puzzle yet to be fully unraveled.  In this paper, we unveil a transformative idea: the infusion of boosting into the fabric of curriculum value-based RL. By conceptualizing the action-value function as a tapestry woven from the threads of residuals trained on individual tasks, we introduce what we call boosted curriculum reinforcement learning (BCRL). This method not only enhances the representativeness of the functional space but does so with a flourish, as each new task adds a fresh residual to our growing collection. Thus, we achieve a dual purpose: the reuse of prior action-values and an elevation in the expressiveness of the action-value function itself.  Our theoretical exploration of BCRL positions it as an approximate value iteration algorithm, revealing its advantages over traditional curriculum RL. We delve into discussions of approximation accuracy and the pathway to convergence toward the optimal action-value function, illuminating the strengths of our approach. To bolster our claims, we present a wealth of empirical evidence showcasing the remarkable benefits of BCRL in scenarios where precise action-value estimation and targeted exploration are paramount. Join us as we chart this new course in the landscape of reinforcement learning, where the power of curriculum and boosting converge to create a brighter future for intelligent agents.", "introduction": "The combination of reinforcement learning (RL) (Sutton & Barto, 2018) algorithms with powerful function approximators, i.e., deep RL (Franc \u00b8ois-Lavet et al., 2018;Mnih et al., 2015), is a breakthrough towards solving complex decision making and control problems that were impractical for previous shallow RL methods.However, the outstanding performance of deep RL techniques is obtained at the cost of a massive amount of data needed from the interaction with the environment, hindering the practicality of deep RL methods.Curriculum RL is a biologically inspired approach that frames the problem of learning a complex target task into learning a sequence of simplified, increasingly difficult, versions of it (Florensa et al., 2017;Shao et al., 2018;Ivanovic et al., 2019;Narvekar et al., 2020).Commonly, establishing the complexity of a task in RL is not trivial, as an indisputable notion of complexity is still missing.However, from a practical point of view, we can consider a task complex if it requires targeted exploration in the environment or complex policies to be solved.In curriculum RL, the notion of task complexity is key, and it is assumed that tailored tasks in a sequence of increasing complexity are presented to the learning agent.Such an appropriate design is, however, a difficult problem, requiring either human expertise (Narvekar et al., 2020) or solutions to automatize the selection of tasks following the learning of the agent (Jiang et al., 2015;Svetlik et al., 2017;Klink et al., 2020).Regardless of using a handcrafted or an automatically generated curriculum, a common approach adopted by most value-based curriculum RL methods is to use the same function approximator throughout all the tasks (Narvekar et al., 2020): given a task, an approximation of the action-value function is learned, and used as an initialization for learning the action-value function of the next task in the curriculum.For the success of this procedure, the function approximator needs to be powerful enough to handle the complexity of the target task.However, this requirement raises the key issue of adequately designing the function approximator.A complex function approximator is prone to overfitting, and sensitive to hyper-parameters, being detrimental for learning simple tasks in the curriculum; conversely, using an overly simple function approximator can hinder the learning of the target task.Moreover, choosing an adequate function approximator a priori is not trivial, as the target task is only visited at the end, and it is commonly unsolvable by non-curriculum RL methods.In this paper, we introduce a novel approach that models the action-value function of a task as a sum of residuals learned on the previous tasks in the curriculum.Our method increases the representativeness of the function approximator as new tasks are presented to the agent, leading to a procedure that increases the size of the functional space as the curriculum proceeds.This results in a learning procedure that increases the power of the function approximator according to the complexity of the task at hand, using small models in simple problems to avoid overfitting, and using large models (resulting from the sum of small ones) to handle the dimensionality of complex problems.Notably, if the curriculum is sufficiently fine-grained, the residuals become smoother functions, close to zero, that are easy to learn, even if the respective tasks are harder than the previous ones.We call our method boosted curriculum reinforcement learning (BCRL), for its resemblance with the boosting technique in supervised learning (Freund, 1995).We provide a theoretical analysis of our approach under the lens of the approximate value iteration (AVI) framework (Farahmand, 2011).In AVI, an estimate of the optimal action-value function is obtained as an iterative process that starts from an arbitrary estimate and applies the optimal Bellman operator until convergence.It is shown that two sources of error exist in AVI: (i) computation of the optimal Bellman operator, (ii) representation of the action-value function (Farahmand, 2011).While the former is due to the need of using samples to approximate the unknown optimal Bellman operator, the latter depends on the representativeness of the functional space chosen to approximate the action-value function.In this work, we formalize the curriculum RL problem in the AVI setting to investigate the representation of the action-value function obtained by BCRL and cases in which this representation may result in a tighter bound on the approximation error compared to regular curriculum RL.We complement this analysis with an empirical evaluation of BCRL in AVI and RL settings, resorting to the fitted Q-iteration algorithm (Ernst et al., 2005), least-squares policy iteration (Lagoudakis & Parr, 2003), and deep Q-networks (Mnih et al., 2015), demonstrating advantages of BCRL over regular curriculum RL.For a space \u03a3, we define M(\u03a3) as the set of probability measures over the \u03c3-algebra \u03c3 \u03a3 .We denote B(\u03a3, B) as the space of bounded measurable functions w.r.t.\u03c3 \u03a3 with bound 0<B<\u221e.A finite-action discounted MDP is defined as a tuple T = S, A, P, R, \u03b3 , where S is a measurable state space, A is a finite set of actions, P : S \u00d7 A \u2192 M(S) is a transition kernel, R : S \u00d7 A \u00d7 S \u2192 R is a reward function, and \u03b3 \u2208 [0, 1) is a discount factor.A policy \u03c0 is a mapping from the state space S to a probability distribution over the action space A. A policy induces an action-value functioncorresponding to the expected cumulative discounted reward obtained performing action a in state s and following the policy \u03c0 thereafter.RL aims at solving an MDP by finding an optimal policy \u03c0 * which induces an optimal action-value function Q * (s, a) = sup \u03c0 Q \u03c0 (s, a).For immediate rewards uniformly bounded by R max , the action-value function is bounded byFrom now on, we consider the L 2 -norm when the subscript p is omitted.Operators.We define the optimal Bellman operator T * : B(S \u00d7 A) \u2192 B(S \u00d7 A) asThe fixed point of the optimal Bellman operator is Gy\u00f6rfi et al. (2006), we define the truncation operator"}
{"paper_id": 232, "abstract": "In the realm of visual interpretation, humans possess an innate ability to discern relationships between objects, a skill that has proven elusive for even the most sophisticated deep learning algorithms. This intricate task hinges on three pivotal challenges: the identification of object entities along with their attributes, the deduction of semantic connections between these entities, and the capacity to adapt to novel combinations of objects and their relationships\u2014a feat known as systematic generalization.   In this study, we harness the power of vision transformers (ViTs) as the foundation for our visual reasoning endeavors, enhancing their capabilities by leveraging well-defined concepts of object entities and their interrelations. To this end, we introduce an innovative concept-feature dictionary that facilitates dynamic retrieval of image features during training, utilizing concept keys for enhanced flexibility. This dictionary paves the way for two groundbreaking auxiliary tasks: a global task designed to refine relational reasoning, and a local task aimed at bolstering semantic object-centric correspondence learning.  To rigorously assess the systematic generalization of our visual reasoning models, we propose systematic splits for the established HICO and GQA benchmarks. Our findings reveal that the Concept-guided Vision Transformer\u2014affectionately dubbed RelViT\u2014substantially eclipses previous methodologies, achieving performance improvements of 16% and 13% on HICO and GQA in the original splits, and an astonishing 43% and 18% in the systematic splits. Furthermore, our ablation studies demonstrate RelViT\u2019s remarkable adaptability across various ViT variants and its resilience to fluctuations in hyper-parameters. Thus, we stand on the precipice of a new era in visual reasoning, where the synergy of concepts and advanced architecture unlocks unprecedented potential.", "introduction": "Deep neural networks have achieved great success in visual recognition.However, their ability for visual relational reasoning, i.e. reasoning with entities and their relationships in a visual scene, still falls short of human-level performances, especially in real-world domains.The challenges of common visual relational reasoning tasks, e.g.HICO and GQA benchmarks (Chao et al., 2015;Hudson & Manning, 2019) are manifested in three aspects: 1) object-centric learning to identify objects (including humans) as well as their visual properties; 2) relational reasoning to infer all pairwise relationships between the object entities; and 3) systematic generalization to reason with visual entities and relations on novel object-relation combinations and extrapolate to longer reasoning hops (Bahdanau et al., 2018;Hupkes et al., 2020).While existing models have leveraged pre-trained object detectors (Ren et al., 2015;Jiang et al., 2020) and/or explicit symbolic reasoning methods (Yi et al., 2018) to tackle these challenges, they leave ample space for improvement.More recently, vision transformers (ViTs) have become the new paradigm for visual recognition and have made great strides in a broad range of visual recognition tasks (Dosovitskiy et al., 2020;Wang et al., 2021a;Liu et al., 2021).Several properties of ViT make it a compelling model choice for visual relational reasoning.First, the self-attention mechanism in ViT offers a strong relational inductive bias, explicitly modeling the relations between input entities.Second, the design of image as patches facilitates the learning of object-centric representations, as evidenced by recent works, e.g.DINO and EsViT (Caron et al., 2021;Li et al., 2021), that demonstrate ViTs trained with self-supervised learning (SSL) capture objects in the image without label annotations.Figure 1: An overview of our method.Red+Green: the learning pipeline of DINO (Caron et al., 2021) and EsViT (Li et al., 2021); Red+Blue: our pipeline.We introduce a concept-feature dictionary, where the key is a concept c and its value is a queue of image features f with the same concept, to allow flexible feature retrieval with the concept keys.With the proposed dictionary, we further develop our concept-guided global and local tasks.EMA denotes the exponential moving average.To investigate the efficacy of the ViT backbone for visual relational reasoning, in particular on systematic generalization, we introduce new systematic splits to canonical benchmarks and compare the ViT backbone with the CNN backbone.Results on GQA show that switching to ViTs in MCAN model (Yu et al., 2019) brings an immediate 11% gain in accuracy.However, the performance gap between the original GQA testing split and the new systematic split remains considerable (15% in accuracy) for both backbones.It suggests that generic ViTs still need to be improved to tackle the reasoning task, especially on systematic generalization.Recent works have shown that neural networks can learn representations with better generalization, by learning certain auxiliary tasks of predicting human-specified concepts (Hill et al., 2020;Koh et al., 2020).A natural question emerges: can we exploit these concepts to improve the reasoning ability of ViTs?Our approach is to make better use of concepts (e.g. the labels in the original training dataset) in the ViT training for better relational reasoning.To this end, we first introduce a novel concept-feature dictionary, where each key is a concept and its value is a queue of image features with the same concept, as shown in Figure 1.It allows dynamic and flexible training-time image feature retrieval during training.Based on this dictionary, we then augment the canonical ViT training pipeline with two auxiliary tasks: First, to facilitate high-level reasoning about relationships, we design a global task that helps cluster images with the same concept together to produce semantically consistent relational representations.Second, to learn better object-centric representations, we develop a local task that guides the model to discover object-centric semantic correspondence across images (Liu et al., 2010).Thanks to the plug-and-play feature of our concept-feature dictionary, our auxiliary tasks can be easily incorporated into existing ViT training pipelines without additional input preprocessing.We term the resulting model concept-guided vision transformer (or RelViT for short).Figure 2: Results on HICO.Our method improves the best baseline by 16%, 43%, and 7% on the original non-systematic and two new systematic splits.Sys.: systematic.We evaluate our method on two standard visual relational reasoning benchmarks: HICO and GQA.Beyond the original independent and identically distributed (I.I.D.) training-testing split, we introduce new systematic splits for each dataset to examine the ability of systematic generalization, i.e., recognizing novel object-relation combinations.Our results show that RelViT significantly outperforms previous approaches.On HICO, it improves the best baseline by 16%, 43%, and 7% on the original non-systematic and two new systematic splits, respectively, as shown in Figure 2. On GQA, it further closes the gap of overall accuracy between models using visual backbone feature only and models using additional bounding box features (obtained from pre-trained object detectors) by 13% and 18% on the two splits.We also show that our method is compatible with various ViT variants and robust to hyperparameters.Finally, our qualitative inspection indicates that RelViT does improve ViTs on learning relational and object-centric representations.Our main contributions are summarized as follows:\u2022 We propose RelViT, by incorporating visual relational concepts to the ViT training with the newlyintroduced concept-guided global and local auxiliary tasks, where a concept-feature dictionary is proposed to enable dynamic and flexible image feature retrieval with the concept keys.\u2022 In extensive experiments on the original non-systematic and new systematic split of the HICO and GQA datasets, we demonstrate the advantages of RelViT over various strong baselines for visual relational reasoning.\u2022 We perform ablation studies on RelViT to show the contributions of its key components, its compatibility to various ViT architectures, and its robustness to hyper-parameters.We provide qualitative results to confirm our improved learning of relational and object-centric representations."}
{"paper_id": 233, "abstract": "In the realm of meta-learning, where the quest for adaptability reigns supreme, Model-Agnostic Meta-Learning (MAML) stands as a beacon of ingenuity, celebrated for its prowess across a multitude of learning challenges. Yet, beneath its elegant surface lies a complexity\u2014a duality of inner-loop and outer-loop updates that orchestrate the dance between task-specific learning and the overarching meta-model. This intricate design obscures the very essence of MAML\u2019s learning objective, leaving many to grapple with its underlying principles.  In this paper, we embark on a journey to illuminate the inner workings of MAML, unveiling a fresh perspective that likens it to a meta-learner wielding a supervised contrastive objective within the realm of classification. Picture this: query features gravitating toward their support counterparts within the same class while repelling those from disparate classes. Through rigorous experimentation, we substantiate this contrastive behavior, employing an analysis grounded in the principles of cosine similarity.  However, our exploration does not end there. We uncover a lurking interference term within vanilla MAML, a byproduct of random initialization and the interplay between tasks that can cloud the learning process. To combat this, we introduce a straightforward yet potent technique known as the \"zeroing trick,\" designed to mitigate this interference and sharpen MAML's focus.  Our extensive experiments, conducted on the esteemed mini-ImageNet and Omniglot datasets, reveal a consistent enhancement in performance, affirming the efficacy of our proposed method. In this way, we not only clarify the enigmatic workings of MAML but also pave the way for its continued evolution in the ever-expanding landscape of meta-learning.", "introduction": "Humans can learn from very few samples.They can readily establish their cognition and understanding of novel tasks, environments, or domains even with very limited experience in the corresponding circumstances.Meta-learning, a subfield of machine learning, aims at equipping machines with such capacity to accommodate new scenarios effectively (Vilalta & Drissi, 2002;Grant et al., 2018).Machines learn to extract task-agnostic information so that their performance on unseen tasks can be improved (Hospedales et al., 2020).One highly influential meta-learning algorithm is Model Agnostic Meta-Learning (MAML) (Finn et al., 2017), which has inspired numerous follow-up extensions (Nichol et al., 2018;Rajeswaran et al., 2019;Liu et al., 2019;Finn et al., 2019;Jamal & Qi, 2019;Javed & White, 2019).MAML estimates a set of model parameters such that an adaptation of the model to a new task only requires some updates to those parameters.We take the few-shot classification task as an example to review the algorithmic procedure of MAML.A few-shot classification problem refers to classifying samples from some classes (i.e.query data) after seeing a few examples per class (i.e.support data).In a meta-learning scenario, we consider a distribution of tasks, where each task is a few-shot classification problem and different tasks have different target classes.MAML aims to meta-train the base-model based on training tasks (i.e., the meta-training dataset) and evaluate the performance of the base-model on the testing tasks sampled from a held-out unseen dataset (i.e. the meta-testing dataset).In meta-training, MAML follows a bi-level optimization scheme composed of the inner loop and the outer loop, as shown in Appendix A (please refer to Section 2 for detailed definition).In the inner loop (also known as fast adaptation), the base-model \u03b8 is updated to \u03b8 \u2032 using the support set.In the outer loop, a loss is evaluated on \u03b8 \u2032 using the query set, and its gradient is computed with respect to \u03b8 to update the base-model.Since the outer loop requires computing the gradient of gradient (as the update in the inner loop is included in the entire computation graph), it is called second-order MAML (SOMAML).To prevent computing the Hessian matrix, Finn et al. (2017) propose first-order MAML (FOMAML) that uses the gradient computed with respect to the inner-loop-updated parameters \u03b8 \u2032 to update the base-model.The widely accepted intuition behind MAML is that the models are encouraged to learn generalpurpose representations which are broadly applicable not only to the seen tasks but also to novel tasks (Finn et al., 2017;Raghu et al., 2020;Goldblum et al., 2020).Raghu et al. (2020) confirm this perspective by showing that during fast adaptation, the majority of changes being made are in the final linear layers.In contrast, the convolution layers (as the feature encoder) remain almost static.This implies that the models trained with MAML learn a good feature representation and that they only have to change the linear mapping from features to outputs during the fast adaptation.Similar ideas of freezing feature extractors during the inner loop have also been explored (Lee et al., 2019;Bertinetto et al., 2019;Liu et al., 2020), and have been held as an assumption in theoretical works (Du et al., 2021;Tripuraneni et al., 2020;Chua et al., 2021).While this intuition sounds satisfactory, we step further and ask the following fundamental questions:(1) In what sense does MAML guide any model to learn general-purpose representations?(2) How do the inner and outer loops in the training mechanism of MAML collaboratively prompt to achieve so? (3) What is the role of support and query data, and how do they interact with each other?In this paper, we answer these questions and give new insights on the working mechanism of MAML, which turns out to be closely connected to supervised contrastive learning (SCL)foot_1 .Here, we provide a sketch of our analysis in Figure 1.We consider a setting of (a) a 5-way 1-shot paradigm of few-shot learning, (b) the mean square error (MSE) between the one-hot encoding of groundtruth label and the outputs as the objective function, and (c) MAML with a single inner-loop update.At the beginning of the inner loop, we set the linear layer w 0 to zero.Then, the inner loop update of w 0 is equivalent to adding the support features to w 0 .In the outer loop, the output of a query sample q 1 is actually the inner product between the query feature \u03d5(q 1 ) and all support features (the learning rate is omitted for now).As the groundtruth is an one-hot vector, the encoder is trained to either minimize the inner product between the query features and the support features (when they are from different classes, as shown in the green box), or to pull the inner product between the query features and the support features to 1 (when they have the same label, as shown in the red box).Therefore, the inner loop and the outer loop together manifest a SCL objective.Particularly, as the vanilla implementation of MAML uses non-zero (random) initialization for the linear layer, we will show such initialization leads to a noisy SCL objective which would impede the training.In this paper, we firstly review a formal definition of SCL, present a more general case of MAML with cross entropy loss in classification, and show the underlying learning protocol of vanilla MAML as an interfered SCL in Section 2. We then experimentally verify the supervised contrastiveness of MAML and propose to mitigate the interference with our simple but effective technique of the zeroinitialization and zeroing trick (cf.Section 3).In summary, our main contributions are three-fold:\u2022 We show MAML is implicitly an SCL algorithm in classification and the noise comes from the randomly initialized linear layer and the cross-task interaction.\u2022 We verify the inherent contrastiveness of MAML based on the cosine similarity analysis.\u2022 Our experiments show that applying the zeroing trick induces a notable improvement in testing accuracy during training and that that during meta-testing, a pronounced increase in the accuracy occurs when the zeroing trick is applied."}
{"paper_id": 234, "abstract": "In the realm of continual learning, a formidable challenge looms: the specter of catastrophic forgetting, where the knowledge of the past is all too easily lost. To combat this, the concept of memory replay has emerged as a beacon of hope, utilizing representative samples from prior training to forge a path toward state-of-the-art performance. Yet, most existing strategies are constrained by the limitations of a small memory buffer, housing only a scant few original data points\u2014insufficient to capture the rich tapestry of the old data distribution.  In our quest to enhance this process, we introduce a groundbreaking approach that marries memory replay with data compression, deftly reducing the storage burden of old training samples while simultaneously expanding the volume that can be retained within the memory buffer. We recognize that the delicate balance between the quality and quantity of compressed data is a complex dance, one that is pivotal to the success of memory replay. To navigate this intricacy, we unveil a novel method rooted in determinantal point processes (DPPs), allowing us to judiciously determine the optimal compression quality for incoming training samples.  By employing a straightforward data compression algorithm, paired with a carefully calibrated quality selection, we can significantly elevate the performance of recent strong baselines, maximizing the amount of compressed data stored within our limited confines. Our extensive validation across various benchmarks of class-incremental learning, as well as in the demanding arena of object detection for autonomous driving, underscores the efficacy of our approach. In this way, we illuminate a new path forward in the ongoing battle against forgetting, ensuring that the wisdom of the past remains a vital part of our learning journey.", "introduction": "The ability to continually learn numerous tasks and infer them together is critical for deep neural networks (DNNs), which needs to mitigate catastrophic forgetting (McCloskey & Cohen, 1989) of the past.Memory replay of representative old training samples (referred to as memory replay) has been shown as an effective solution, and achieves the state-of-the-art (SOTA) performance (Hou et al., 2019).Existing memory replay approaches are mainly built on a small memory buffer containing a few original data, and try to construct and exploit it more effectively.However, due to the low storage efficiency of saving original data, this strategy of building memory buffer will lose a lot of information about the old data distribution.On the other hand, this usually requires huge extra computation to further mitigate catastrophic forgetting, such as by learning additional parameters (Liu et al., 2021a) or distilling old features (Hu et al., 2021).Different from \"artificial\" memory replay in DNNs, a significant feature of biological memory is to encode the old experiences in a highly compressed form and replay them to overcome catastrophic forgetting (McClelland, 2013;Davidson et al., 2009;Carr et al., 2011).Thus the learned information can be maintained in a small storage space as comprehensively as possible, and flexibly retrieved.Inspired by the compression feature of biological memory replay, we propose memory replay with data compression (MRDC), which can largely increase the amount of old training samples that can be stored in the memory buffer by reducing their storage cost in a computationally efficient way.Given a limited storage space, data compression introduces an additional degree of freedom to explicitly balance the quality and quantity for memory replay.With a properly selected quality, using a naive JPEG compression algorithm (Wallace, 1992) can achieve comparable or better performance than recent strong approaches with less extra computation (Fig. 1, purple arrow), and can further improve their performance (Fig. 1, gray arrow).However, to empirically determine the compression quality is usually inefficient and impractical, since it requires learning a task sequence or sub-sequence repeatedlyfoot_0 .We propose a novel method based on determinantal point processes (DPPs) to efficiently determine it without repetitive training.Further, we demonstrate the advantages of our proposal in realistic applications such as continual learning of object detection for autonomous driving, where the incremental data are extremely large-scale.Our contributions include: (1) We propose memory replay with data compression, which is both an important baseline and a promising direction for continual learning; (2) We empirically validate that the trade-off between quality and quantity of compressed data is highly nontrivial for memory replay, and provide a novel method to efficiently determine it without repetitive training; (3) Extensive experiments show that using a naive data compression algorithm with a properly selected quality can largely improve memory replay by saving more compressed data in a limited storage space."}
{"paper_id": 235, "abstract": "In the realm of conditional score-based data generation, many existing methods lean heavily on Bayes' theorem, unraveling the gradients of a log posterior density into a blend of scores. This approach streamlines the training of conditional score models, allowing us to separately estimate these scores through a combination of a score model and a classifier. Yet, upon closer inspection, we uncover a critical flaw: the training objectives for the classifier can inadvertently create a significant score mismatch. This discrepancy means that the scores we estimate can stray dangerously far from their true counterparts, leading our samples astray during the diffusion process and ultimately compromising the quality of our generated data.  To tackle this challenge, we introduce a groundbreaking training objective known as Denoising Likelihood Score Matching (DLSM) loss. This innovative framework guides the classifier to align its gradients with those of the true log likelihood density, effectively bridging the gap between estimated and actual scores. Our rigorous experiments reveal that this method not only addresses the score mismatch issue but also significantly enhances performance across key evaluation metrics on the Cifar-10 and Cifar-100 benchmarks. In conclusion, by embracing DLSM, we can accurately model conditional scores, paving the way for superior data generation and a more reliable diffusion process.", "introduction": "Score-based generative models are probabilistic generative models that estimate score functions, i.e., the gradients of the log density for some given data distribution.As described in the pioneering work (Hyv\u00e4rinen, 2005), the process of training score-based generative models is called Score Matching (SM), in which a score-based generative model is iteratively updated to approximate the true score function.Such a process often incurs heavy computational burdens, since the calculation of the score-matching objective involves the explicit computation of the partial derivatives of the score model during training.Therefore, a branch of study in this research domain (Vincent, 2011;Song et al., 2019) resorts to reformulating the score-matching objective to reduce the training cost.Among these works, the author in (Vincent, 2011) introduced the Denoising Score-Matching (DSM) method.This method facilitates the training process of score-based generative models, and thus lays the foundation for a number of subsequent researches.Recently, the authors in (Song & Ermon, 2019) proposed an unified framework based on DSM, and achieved remarkable performance on serval real-world datasets.Their success inspired several succeeding works (Song & Ermon, 2020;Ho et al., 2020;Song et al., 2021a;b;Dhariwal & Nichol, 2021), which together contribute to making score-based generative models an attractive choice for contemporary image generation tasks.A favorable aspect of score-based generative models is their flexibility to be easily extended to conditional variants.This characteristic comes from a research direction that utilizes Bayes' theorem to decompose a conditional score into a mixture of scores (Nguyen et al., 2017).Recent endeavors followed this approach and further extended the concept of conditional score-based models to a number of application domains, including colorization (Song et al., 2021b), inpainting (Song et al., 2021b), and source separation (Jayaram & Thickstun, 2020).In particular, some recent researchers (Song et al., 2021b;Dhariwal & Nichol, 2021) applied this method to the field of classconditional image generation tasks, and proposed the classifier-guidance method.Different from the classifier-guidance-free method adopted by (Ho et al., 2021), they utilized a score model and a classifier to generate the posterior scores (i.e., the gradients of the log posterior density), with which the data samples of certain classes can be generated through the diffusion process.The authors in (Dhariwal & Nichol, 2021) showed that the classifier guidance method is able to achieve improved performance on large image generation benchmarks.In spite of their success, our analysis indicates that the conditional generation methods utilizing a score model and a classifier may suffer from a score mismatch issue, which is the situation that the estimated posterior scores deviate from the true ones.This issue causes the samples to be guided by inaccurate scores during the diffusion process, and may result in a degraded sampling quality consequently.To resolve this problem, we first analyze the potential causes for the score mismatch issue through a motivational low-dimensional example.Then, we formulate a new loss function called Denoising Likelihood Score-Matching (DLSM) loss, and explain how it can be integrated into the current training method.Finally, we evaluate the proposed method under various configurations, and demonstrate its advantages in improving the sampling quality over the previous methods in terms of several evaluation metrics."}
{"paper_id": 236, "abstract": "In the ever-evolving landscape of neural network design, trainable layers\u2014those intricate convolutional building blocks\u2014have long been the cornerstone, learning parameters to weave together the global context through a series of spatial operations. Yet, as we strive for efficiency, the depthwise convolution, often heralded as a beacon of reduced parameters and FLOPs, has revealed a troubling truth: it offers little in the way of practical speed improvements.   In this paper, we challenge the prevailing wisdom that ties efficiency solely to these trainable layers. Instead, we propose a radical shift toward the embrace of simple, built-in parameter-free operations, presenting them as potent alternatives to the conventional spatial operations that dominate network architectures. Our goal is to dismantle the stereotype that organizes these spatial operations solely within the realm of trainable layers.  Through extensive experimental analyses, we delve into layer-level studies with fully-trained models and conduct neural architecture searches to uncover the true potential of parameter-free operations, such as max-pooling. Our findings reveal a straightforward yet powerful approach to reimagining network architectures, where these parameter-free operations serve as the foundational building blocks, maintaining model accuracy while enhancing efficiency.  The results, particularly on the ImageNet dataset, illuminate a promising path forward: architectures that leverage parameter-free operations not only achieve greater efficiency in model speed but also reduce the number of parameters and FLOPs. For those eager to explore this innovative frontier, the code and ImageNet pretrained models are available at https://github.com/naver-ai/PfLayer.", "introduction": "Image classification has been advanced with deep convolutional neural networks (Simonyan & Zisserman, 2015;Huang et al., 2017;He et al., 2016b) with the common design paradigm of the network building blocks with trainable spatial convolutions inside.Such trainable layers with learnable parameters effectively grasp attentive signals to distinguish input but are computationally heavy.Rather than applying pruning or distillation techniques to reduce the computational cost, developing new efficient operations has been another underlying strategy.For example, a variant of the regular convolution, such as depthwise convolution (Howard et al., 2017) has been proposed to bring more efficiency by reducing the inter-channel computations.The operation has benefits in the computational budgets, including the number of parameters and FLOPs.However, the networks heavily using the depthwise convolution (Howard et al., 2017;Sandler et al., 2018;Tan & Le, 2019) have an inherent downside of the latency, which generally do not reach the speed of the regular convolution.In a line of study of efficient operations, there have been many works (Wang et al., 2018;Wu et al., 2018;Han et al., 2020;Tan & Le, 2021) based on the regular convolution and the depthwise convolution.Most methods utilize the depthwise convolution's efficiency or target FLOPs-efficiency but are slow in the computation.Meanwhile, parameter-free operations were proposed; a representative work is the Shift operation (Wu et al., 2018).Its efficiency stems from the novel operation without learning spatial parameters by letting the feature mixing convolutions learn from the shifted features.However, the implementation does not bring about the actual speed-up as expected.This is because the operation-level optimization is still demanding compared to the regular convolution with highly optimized performance.Another parameter-free operation feature shuffling (Zhang et al., 2018) is a seminal operation that reduces the computational cost of the feature mixing layer.However, it hardly plays the role of a spatial operation.In this paper, we focus on efficient parameter-free operations that actually replace trainable layers for network design.We revisit the popular parameter-free operations, the max-pool and the avg-pool op-erations (i.e., layers), which are used in many deep neural networks (Simonyan & Zisserman, 2015;Huang et al., 2017) restricted to perform downsampling (Simonyan & Zisserman, 2015;Howard et al., 2017;Sandler et al., 2018;He et al., 2016b).Can those simple parameter-free operations be used as the main network building block?If so, one can reduce a large portion of the parameters and the overall computational budget required during training and inference.To answer the question, the max-pool and the avg-pool operations are chosen to be demonstrated as representative simple parameter-free operations.We first conduct comprehensive studies on the layer replacements of the regular convolutions inside networks searched upon the baseline models with model training.Additionally, we incorporate a neural architecture search (Liu et al., 2019) to explore effective architectures based on the operation list with the parameter-free operations and convolutional operations.Based on the investigations, we provide a simple rule of thumb to design an efficient architecture using the parameter-free operations upon primitive network architectures.The design guide is applied to popular heavy networks and validated by the performance trained on ImageNet (Russakovsky et al., 2015).It turns out that our models have apparent benefits to the computational costs, particularly the faster model speeds.In addition, ImageNet-C (Hendrycks & Dietterich, 2019) and ImageNet-O (Hendrycks et al., 2021) results show our models are less prone to be overfitting.We further propose a novel deformable parameter-free operation based on the max-pool and the avg-pool to demonstrate the future of a parameter-free operation.Finally, we show that the parameter-free operations can successfully replace the self-attention layer (Vaswani et al., 2017), thus attaining further efficiency and speed up.We summarize our contributions as follows:\u2022 We study whether parameter-free operations can replace trainable layers as a network building block.To our knowledge, this is the first attempt to investigate a simple, builtin parameter-free layer as a building block for further efficiency ( \u00a73).\u2022 We provide a rule of thumb for designing a deep neural network including convolutional neural networks and vision transformers with parameter-free layers ( \u00a74).\u2022 Experimental results show that our efficient models outperform the previous efficient models and yield faster model speeds with further robustness ( \u00a75)."}
{"paper_id": 237, "abstract": "In the ever-evolving realm of image super-resolution, a myriad of networks have emerged, each promising to elevate the clarity of our visual experiences. Yet, despite their impressive capabilities, these architectures often remain cumbersome, falling short of the lightweight efficiency we crave. Compounding this challenge, traditional model compression methods\u2014such as neural architecture search and knowledge distillation\u2014tend to demand substantial computational resources, leaving us yearning for a more elegant solution.  Enter network pruning, a cost-effective and potent technique for model compression. However, its application to super-resolution networks is fraught with difficulty, particularly when it comes to the notorious filter pruning within residual blocks. The intricacies of this process have long posed a challenge for researchers seeking to streamline their models without sacrificing performance.  To navigate these turbulent waters, we introduce structure-regularized pruning (SRP), a novel approach that imposes a guiding hand on the pruned structure. By ensuring that the locations of pruned filters remain consistent across layers, we create a harmonious balance within the network. Specifically, for layers interconnected by the same residual, we identify filters of equivalent indices as unimportant, allowing us to prune with purpose.  To safeguard the expressive power of these unimportant filters, we employ $L_2$ regularization, gently nudging their weights toward zero. This careful orchestration ensures that their absence minimally impacts performance, allowing the network to flourish in their stead.  Through the application of SRP, we unveil two remarkable architectures: the lightweight SRPN-Lite and the impressively deep SRPN. Our extensive evaluations against both nimble and expansive networks reveal that SRPN-Lite and SRPN not only hold their ground but excel, delivering superior results both quantitatively and visually compared to other recent efficient super-resolution methodologies. In this journey of refinement, we have forged a path toward efficiency without compromise, illuminating the future of image super-resolution.", "introduction": "Given a low-resolution (LR) image, single image super-resolution (SR) aims to reconstruct a highresolution (HR) output.Essentially, as a many-to-one mapping problemimage SR is ill-posed.To tackle this problem, plenty of deep convolutional neural networks (CNNs) (Dong et al., 2014;2016;Kim et al., 2016b;Zhang et al., 2018c;2020;2021) have been investigated to learn the accurate mapping from LR input to the corresponding HR target.Deep CNN for image SR is first investigated in SRCNN (Dong et al., 2014) and has continuously shown promising SR performance.SRCNN consists of there convolutional (Conv) layers, constraining its expressivity.Kim et al. adopted residual learning to increase network depth in VDSR (Kim et al., 2016a) and obtained notable improvements over SRCNN.Lim et al. (Lim et al., 2017) simplified residual blocks and built a much deeper network EDSR.Zhang et al. (Zhang et al., 2018b) proposed a even much deeper one RCAN with the residual in residual structure.Empowered by increased network size, deep SR models like EDSR (Lim et al., 2017) and RCAN (Zhang et al., 2018b) have seen remarkable SR performance.However, as a cost, the large model size brings about problems such as excessive memory footprint, slow inference speed.It is thereby impractical to deploy them on resource-constrained platforms directly (Lee et al., 2020).Aiming for efficient SR, more and more works introduce lightweight network architectures (Ahn et al., 2018;Luo et al., 2020).Ahn et al. proposed cascading residual network (CARN) (Ahn et al., 2018).Hui et al. proposed information multi-distillation network (IMDN) (Hui et al., 2019).Lee et al. introduced knowledge distillation (Hinton et al., 2014) for image SR (Lee et al., 2020).Besides, neural architecture search (NAS) (Zoph & Le, 2017;Elsken et al., 2019) was also utilized for lightweight SR model (Chu et al., 2019a).However, there are still several downsides to these networks: (1) There is still an obvious performance gap between those lightweight models and the very deep models; (2) These methods can consume considerable computation resources for training.For example, (Chu et al., 2019a)  takes about three days;(3) It is hard for most lightweight SR network designs to generalize to more large-scale networks and achieve superior performance at the same time.So, it is needed to explore more resource-friendly, effective, and generic lightweight SR networks.On the other hand, neural network pruning is well-known as an effective technique to reduce model complexity (Reed, 1993;Sze et al., 2017).For acceleration, filter pruning (a.k.a.structured pruning) (Li et al., 2017) attracts more attention than weight-element pruning (a.k.a.unstructured pruning) (Han et al., 2015;2016b).Introducing filter pruning into image SR is a promising way to achieve a good trade-off between performance and complexity.However, it is not easy to apply filter pruning techniques to image SR networks directly.This is mainly because residual connections are well-known difficult to prune in structured pruning (Li et al., 2017).On the other hand, they are extensively used in state-of-the-art (SOTA) image SR methods (e.g., EDSR (Lim et al., 2017) has 32 residual blocks; RCAN (Zhang et al., 2018b) even has nested residual blocks).To tackle the above issue, we propose Structure-Regularized Pruning (SRP), which imposes regularization on the pruned structure to ensure the locations of pruned filters are aligned across different layers.Specifically, for the layers connected by the same residual, we select the filters of the same indices as unimportant filters (i.e., those we will remove finally).To transfer the expressive power in the unimportant filters to the remainder of the network, we employ L 2 regularization to drive the weights towards zero gradually so that eventually, their absence will incur negligible performance loss.To the best of our knowledge, our SRP is one of the leading works (Zhang et al., 2021) to leverage structured pruning for efficient image SR.Our main contributions are as follows:\u2022 We propose a network structure-regularized pruning (SRP) method to learn efficient image SR networks.We try to jointly train image SR models with network pruning simultaneously to achieve high reconstruction performance as well as efficiency.\u2022 Our SRP provides a general idea to structurally prune networks, which consists of extensive residual connections.The introduction of regularization as a pruning tool manages to maintain the expressivity of the original network while peeling off the unnecessary redundancy.\u2022 We employ SRP to train efficient image SR networks, resulting in a lightweight network (named SRPN-Lite) and a very deep one (named SRPN).We achieve superior performance gains on both lightweight and large image SR networks."}
{"paper_id": 238, "abstract": "In the realm of generative learning, discrete variational auto-encoders (VAEs) have emerged as powerful tools, capable of mapping the intricate landscapes of semantic latent spaces. Yet, in many practical scenarios, these latent spaces are not merely simple; they are high-dimensional labyrinths. Navigating through these structures often demands the daunting task of enumerating an exponentially vast array of possibilities, a feat that can overwhelm even the most robust algorithms.   Recently, innovative strategies have surfaced, enabling us to approximate gradients without the need to traverse the entirety of these latent spaces. In our exploration, we harness the power of Natural Evolution Strategies (NES)\u2014a family of gradient-free, black-box optimization techniques\u2014to advance the learning of discrete structured VAEs. The brilliance of NES lies in its computational efficiency; it estimates gradients through mere forward evaluations, circumventing the need to propagate gradients through the complexities of discrete structures.  Through rigorous empirical testing, we reveal that optimizing discrete structured VAEs with NES achieves results on par with traditional gradient-based methods. Moreover, we establish a theoretical foundation showing that NES converges even for non-Lipschitz functions, which are often encountered within the realm of discrete structured VAEs. In this work, we illuminate a path forward, merging the elegance of evolutionary strategies with the nuanced demands of discrete latent spaces, and paving the way for future advancements in generative learning.", "introduction": "Discrete variational auto-encoders (VAEs) are able to represent structured latent spaces in generative learning.Consequently VAEs drive extensive research in machine learning applications, including language classification and generation [60,17,54,9,13], molecular synthesis [28,15,48], speech and visual understanding [36,55,3].Compared to their continuous counterparts, they can improve interpretability by illustrating which terms contributed to the solution [48,40], and they can facilitate the encoding of inductive biases in the learning process, such as images consisting of a small number of objects [12] or tasks requiring intermediate alignments [36,42,1,2].Learning VAEs with discrete n-dimensional latent variables is computationally challenging since the size of the support of the posterior distribution may be exponential in n.This is particularly common under the structured settings, when the latent variables represent complex structures such as trees or graphs.The Gumbel-max reparametrization trick trades enumeration with optimization using efficient dynamic programming algorithms and enables a computation of the model value.Unfortunately, the resulting mapping remains non-differentiable due to the presence of arg max operations.In order to propagate gradients efficiently, Jang et al. [19], Maddison et al. [34] proposed the Gumbel-softmax reformulation that uses a smooth relaxation of the reparametrized objective, replacing the arg max operation with a softmax operation.Following such an approach may bring back the need for enumerating over a large search space.This is due to the partition function of the softmax operator, which relies on a summation over all possible latent assignments, which may be exponential in n.To better deal with the computational complexity in the structured setting, sophisticated stochastic softmax tricks were devised to learn discrete structured VAEs [48] (e.g., perturb-and-parse for dependency parsing by [9], Gumbel-Sinkhorn for bi-partite matching [36]).In this work we propose to use the Natural Evolution Strategy (NES) [57,58] algorithm for learning discrete structured VAEs.The NES algorithm is a gradient-free black-box optimization method that does not need to propagate gradients through discrete structures.Instead, the NES algorithm estimates gradients by forward-pass evaluations only.We experimentally show that gradient-free methods are as effective as sophisticated gradient based methods, such as perturb-and-parse.NES is conceptually appealing when considering discrete structured VAEs since NES does not require to construct complex solutions to propagate gradients through the arg max operation, as it only requires to evaluate the model.Moreover, the proposed approach is highly parallelizable, hence computationally appealing.Our contributions: (1) We suggest using black-box, gradient-free based optimization methods, specifically NES, to optimize discrete structured VAEs.(2) We experimentally demonstrate that NES, which uses the models' output in a black-box manner, is as effective as gradient based approximations although being more general and simpler to use.(3) We rigorously describe the connection between NES and previous gradient based optimization methods (i.e, REINFORCE) as well as prove that the NES algorithm converges for non-Lipschitz functions."}
{"paper_id": 239, "abstract": "In the realm of machine learning, the quest for harnessing the profound potential of deep learning within Bayesian frameworks has long been fraught with challenges. Enter Prior-Data Fitted Networks (PFNs), a groundbreaking approach that marries the strength of large-scale machine learning with the elegance of Bayesian inference. At the heart of PFNs lies a simple yet powerful premise: all that is needed is the ability to sample from a prior distribution over supervised learning tasks.   We redefine the task of posterior approximation as a supervised classification problem, employing a set-valued input that allows us to draw tasks from the prior, gather a collection of data points and their corresponding labels, and then strategically mask one label. The network learns to predict the masked label probabilistically, drawing upon the context of the remaining data points. When faced with a new supervised learning task, PFNs can swiftly generate probabilistic predictions for any other data points in a single forward pass, effectively emulating the intricate dance of Bayesian inference.  Our results are nothing short of remarkable. PFNs not only achieve near-perfect replication of Gaussian processes but also facilitate efficient Bayesian inference in scenarios previously deemed intractable, boasting speedups exceeding 200 times compared to existing methods. We showcase the versatility of PFNs across a spectrum of applications, from Gaussian process regression and Bayesian neural networks to classification tasks involving small tabular datasets and few-shot image classification. The implications are profound, as PFNs demonstrate a remarkable generality that opens new avenues for exploration. For those eager to delve deeper, our code and trained PFNs await at https://github.com/automl/TransformersCanDoBayesianInference.", "introduction": "In the last decade, supervised machine learning (ML) methods using deep learning architectures have made substantial progress on machine learning tasks where a large amount of training data is available (Vaswani et al., 2017;He et al., 2016;Krizhevsky et al., 2012).A very important problem in ML is thus to transfer these successes to smaller-scale setups with less data available.In this paper, we propose a way to build models that approximate posteriors with flexible and replaceable priors using deep learning models.It makes specifying a prior as simple as defining a sampling scheme of supervised learning tasks.Train the PFN by minimizing -K i=1 log q \u03b8 (y) train ) Actual training dataset and test input (Dtrain, xtest) Bayesian inference via the trained PFN, with the actual training data and a test point as input: q \u03b8 * (y test |x test , Dtrain) \u2248 p(y test |x test , Dtrain)PFN with parameters \u03b8 * Figure 1: A visualization of Prior-Data Fitted Networks (PFNs).We sample datasets from a prior and fit a PFN on hold-out examples of these datasets.Given an actual dataset, we feed it and a test point to the PFN and obtain an approximation to Bayesian inference in a single forward propagation.While the success of deep learning on large datasets can be attributed to the capacity of neural networks to approximate any function, there is still a need for encoding prior knowledge, for example through model architecture (e.g., Convolutional Neural Networks (LeCun et al., 1989)) or regularizers (e.g., data augmentations (Hendrycks et al., 2019;Cubuk et al., 2020)).Otherwise, the no free lunch theorems show that there are no good methods to solve the class of prediction problems (Wolpert & Macready, 1997).Thus, a large number of specialized algorithms have been developed for different small-scale tasks (LeCun et al., 1989;Kadra et al., 2021;Chen & Guestrin, 2016).Encoding prior information into a machine learning model can, however, be challenging.A well-defined way to bias a model is to use Bayesian inference.The foundation of Bayesian inference is an assumption on the distribution of the data to appear in a real-world application.This assumption gives rise to a prior belief over the probability for the data to follow a particular model.One might, e.g., implement a prior, encoding the data as created by a neural network (Bayesian Neural Networks, (MacKay, 1992)), by a polynomial, the likelihood of a Gaussian mixture or random code in a pre-defined programming language (Solomonoff, 1997).Using Bayesian inference for prediction in supervised learning has the advantages that (i) it has a theoretical foundation that makes it valid in setups where the prior p(t) fits; (ii) it can thus better account for the actual likelihood of different events; (iii) it is well calibrated and (iv) it is interpretable as the prior describes the expectations of the model.However, retrieving the posterior predictive distribution for a given prior is intractable in most cases (Blei et al., 2017;MacKay, 1992).Figure 1 outlines Prior-Data Fitted Networks (PFNs), for approximating such Bayesian models.We assume a given representative prior distribution over supervised learning tasks (or functions), which provides our inductive bias.To train a PFN, we use supervised learning with set-valued inputs representing entire datasets: we repeatedly sample a meta-train task (or function) from the given prior, draw a set of data points and their labels from it, mask one of the labels and learn to make probabilistic predictions for it based on the set-valued input of the rest of the data points.Given an actual real-world dataset, we feed it together with a test point as inputs to the PFN and that outputs its prediction distribution for the test point, conditioned on the dataset.As we will demonstrate, this distribution approximates exact Bayesian posterior prediction.We refer to this step as (Bayesian) inference, as opposed to the training of the PFN itself.Our PFNs thus allow us to approximate the posterior predictive distribution for any prior from which we are able to sample data.This is a very weak requirement compared to the standard assumptions of other approximations for Bayesian inference (Hoffman et al., 2014;2013;Jordan et al., 1999).This allows a simple approximation of a large set of priors, including priors that are very hard to approximate with currently available tools.We make the following contributions:\u2022 We present architectural changes to successfully use Transformers for posterior predictive distribution (PPD) approximation, including a novel predictive distribution for regression tasks.The proposed method is simple, cheap and generally applicable to a large set of priors.\u2022 We demonstrate that PFNs can approximate the PPD of Gaussian processes and Bayesian neural networks (BNNs) orders of magnitude faster than MCMC with NUTS or SVI with Bayes-by-Backprop (Blundell et al., 2015).\u2022 We demonstrate that PFNs can have an impact on real-world tasks.(i) We implement BNNs with priors over architectures on PFNs that enable tuning free predictions in a single forward pass and outperform all baselines on a large benchmark of small tabular datasets.(ii) Additionally, we show that simple handwriting priors enable few-shot learning on Omniglot (Lake et al., 2015)."}
{"paper_id": 240, "abstract": "In a world increasingly shadowed by concerns over data privacy, particularly in the realm of face recognition, the federated learning (FL) paradigm emerges as a beacon of hope. Yet, the traditional methods of FL often falter, hindered by the unique challenges of the task at hand. Specifically, the necessity of sharing class centers among clients, while crucial for enhancing recognition performance, inadvertently opens the door to privacy breaches. To navigate this treacherous terrain of the privacy-utility paradox, we introduce PrivacyFace\u2014a groundbreaking framework that elevates federated learning for face recognition by facilitating the exchange of auxiliary, privacy-preserving information among clients.  PrivacyFace is constructed upon two pivotal pillars. The first is a novel Differentially Private Local Clustering (DPLC) mechanism, designed to extract sanitized clusters from local class centers, ensuring that privacy remains intact. The second component is a consensus-aware recognition loss that fosters a spirit of collaboration among clients, ultimately refining the discriminative power of the features learned. Our rigorous mathematical proofs confirm that these innovations uphold differential privacy, all while imposing minimal overhead and delivering remarkable performance enhancements\u2014specifically, boosts of +9.63% and +10.26% for TAR@FAR=1e-4 on the IJB-B and IJB-C datasets, respectively.  Through extensive experiments and thorough ablation studies conducted on a large-scale dataset, we demonstrate not only the efficacy but also the practicality of our method. PrivacyFace stands as a testament to what can be achieved when we prioritize both privacy and performance, forging a new path in the realm of federated learning face recognition.", "introduction": "Face recognition technique offers great benefits when used in right context, such as public safety, personal security and convenience.However, misuse of this technique is a concern as it involves unique and irrevocable biometric data.The rapid commercial applications based on face recognition and facial analysis techniques have stimulated a global conversation on AI ethics, and have resulted in various actors from different countries issuing governance initiatives and guidelines.EU's General Data Protection Regulation (GDPR) (Voigt & Von dem Bussche, 2017), California Consumer Privacy Act (CCP) and Illinois Personal Information Protection Act (IPI) enforces data protection \"by design and by default\" in the development of any new framework.On the nationwide \"315 show\" of year 2021, the China Central Television (CCTV) called out several well-known brands for illegal face collection without explicit user consent.As researchers, it is also our duty to prevent the leakage of sensitive information contained in public datasets widely used by the research community.Therefore, faces in ImageNet (Deng et al., 2009) were recently all obfuscated (Yang et al., 2021) and a large face dataset called MS-Celeb-1M (Guo et al., 2016) was pulled of the Internet.In the wake of growing social consensus on data privacy, the field of face recognition calls for a fundamental redesign about model training while preserving privacy.A potential solution is the paradigm called Federated Learning (FL) (McMahan et al., 2017a).Given C clients with local datasets {D 1 , D 2 , \u2022 \u2022 \u2022 , D C } as shown in Fig. 1a, FL decentralizes the training process by combining local models fine-tuned on each client's private data and thus hinders privacy breaches.Typical examples of these clients include personal devices containing photo collections of a few family members, or open-world scenarios such as tourist attractions visited by tens of thousands of people.In most circumstances, we can safely assume very few classes would co-exist in two or more clients.Despite the numerous FL-based applications in various domains (Kairouz et al., 2019) ranging from health to NLP, there are very little progress (Aggarwal et al., 2021;Bai et al., 2021;Liu et al., 2021) in training face recognition models with FL schemes.Unlike other tasks, parameters of the last classifier for a face recognition model are crucial for recognition performance but strongly associated with privacy.These parameters can be regarded as mean embeddings of identities (Wang et al., 2018; !\"#$%\" &'%()*' +,-(*,$)(,#.In contrast, the proposed PrivacyFace framework learns an improved face embedding by aggregating discriminative embedding clusters that are proved to achieve differential privacy.Meng et al., 2021b;Shen et al., 2020) (also called as class centers), from where individual privacy could be spied out as studied by plenty of works (Kumar Jindal et al., 2018;Boddeti, 2018;Mai et al., 2020;Dusmanu et al., 2021).That prevents the FL approach from broadcasting the whole model among clients and the central server, and consequently leads to conflicts in the aggregation of local updates.As depicted in the global feature distribution of Fig. 1a, both clients try to spread out their own classes in the same area (pointed by the arrow) of the normalized feature space.Thus, the training loss could oscillate to achieve consensus given the sub-optimal solutions from multiple clients.On the other hand, a large batch with sufficient negative classes is necessary to learn a discriminative embedding space for advanced face recognition algorithms.During the conventional FL updates, each class is only aware of local negative classes while those from other clients are untouchable.This further limits performances of FL approaches in face recognition.The privacy-utility paradox motivates us to introduce PrivacyFace, a framework improves federated learning face recognition by broadcasting sanitized information of local class globally.In the framework, a novel algorithm called Differentially Private Local Clustering (DPLC) first generates privacy-agnostic clusters of class centers while any specific individual in the cluster cannot be learned, irrespective of attacker's prior knowledge, information source and other holds.Recall that the privacy cost of a differential privacy scheme is propotional to the l 2 -sensitivity while inversely propotional to the query number.Our DPLC reaches a low l 2 -sensitivity by restricting the cluster size as well as covering sufficient class centers.In addition, the number of necessary centers to communicate in DPLC is irrelevant to the number of classes in the training data.These characteristics jointly equip DPLC with much smaller privacy cost than the naive alternative, which sanitizes each class center individually by Gaussian noise.In our experiments, DPLC's privacy cost is only 1.7e-7 of that of the naive approach.That persuasively reveals the high security level of our approach.The second part of PrivacyFace is the consensus-aware face recognition loss.Following principles of Federated Averaging (FedAvg) (McMahan et al., 2017a), a server iteratively gathers feature extractors and privacy-agnostic clusters from clients, averages parameters of feature extractors and distributes them to clients.Accordingly, the consesus-aware loss notifies each client not to embed samples in the inappropriate zone (differential private clusters marked by DP) of the feature space during the local optimization, as shown in Fig. 1b.This process aids each client to train more discriminative features as well as align all consensuses.Compared to the conventional approach, our PrivacyFace boosts performances by +9.63% and +10.26% for TAR@FAR=1e-4 on IJB-B and IJB-C respectively with only single-digit privacy cost.Moreover, the additional computational cost as well as communication cost are negligible (e.g., the extra clusters to broadcast only occupy 16K storage while the backbone already takes 212M).In a word, PrivacyFace is an efficient algorithm which improves conventional federated learning face recognition by a large margin on performances, while requires little privacy cost as well as involves lightweight computational/communication overheads."}
{"paper_id": 241, "abstract": "In the realm of federated learning, where a multitude of edge devices join forces to forge a model under the vigilant guidance of a central server, communication emerges as the formidable bottleneck. Each device, bound by the constraints of privacy, refrains from sharing its training data, yet must engage in a relentless exchange of information with the server. Enter local Stochastic Gradient Descent (SGD), a beacon of hope that aims to curtail the number of learning rounds, yet its relentless demands for communication remain a staggering burden, particularly in bandwidth-constrained networks.  Recognizing the untapped potential of device-to-device (D2D) and device-to-server (D2S) collaboration within contemporary communication landscapes, we unveil a novel federated optimization strategy: the hybrid local SGD (HL-SGD). In this innovative framework, devices are organized into distinct clusters, each benefiting from robust D2D communication bandwidth. HL-SGD elegantly encompasses prior methodologies, such as local SGD and gossip SGD, allowing us to deftly navigate the delicate balance between model accuracy and computational efficiency.  Our analysis delves into the convergence properties of HL-SGD amidst the challenges posed by heterogeneous data in general nonconvex scenarios. Through rigorous experimentation, we demonstrate that the synergistic approach of hybrid model aggregation\u2014leveraging both D2D and D2S communications\u2014significantly accelerates the training process in federated learning. In this dance of devices, we find not just speed, but a harmonious convergence toward a more efficient future.", "introduction": "Federated learning (FL) is a distributed machine learning paradigm in which multiple edge devices or clients cooperate to learn a machine learning model under the orchestration of a central server, and enables a wide range of applications such as autonomous driving, extended reality, and smart manufacturing (Kairouz et al., 2021).Communication is a critical bottleneck in FL as the clients are typically connected to the central server over bandwidth-limited networks.Standard optimization methods such as distributed SGD are often not suitable in FL and can cause high communication costs due to the frequent exchange of large-size model parameters or gradients.To tackle this issue, local SGD, in which clients update their models by running multiple SGD iterations on their local datasets before communicating with the server, has emerged as the de facto optimization method in FL and can largely reduce the number of communication rounds required to train a model (McMahan et al., 2017;Stich, 2019).However, the communication benefit of local SGD is highly sensitive to non-iid data distribution as observed in prior work (Rothchild et al., 2020;Karimireddy et al., 2020).Intuitively, taking many local iterations of SGD on local dataset that is not representative of the overall data distribution will lead to local over-fitting, which will hinder convergence.In particular, it is shown in (Zhao et al., 2018) that the convergence of local SGD on non-iid data could slow down as much as proportionally to the number of local iteration steps taken.Therefore, local SGD with a large aggregation period very slow on the large and sparse network topology that is typically found in FL settings.Unlike previous works, HL-SGD leverages both D2S and D2D communications in the system.Some recent studies aim to encapsulate variants of SGD under a unified framework.Specifically, a cooperative SGD framework is introduced in (Wang & Joshi, 2021) that includes communication reduction through local SGD steps and decentralized mixing between clients under iid data distribution.A general framework for topology-changing gossip SGD under both iid and non-iid data distributions is proposed in (Koloskova et al., 2020).Note that all of the above works assume undirected network topology for communications in every iteration.In comparison, our proposed HL-SGD is different: the D2S communication is asymmetric due to the use of device sampling and model broadcasting in each global aggregation round and cannot be modeled in an undirected graph.Therefore, the convergence analysis of HL-SGD does not fit into the prior frameworks and is much more challenging.Moreover, our major focus is on the runtime of the algorithm rather than its convergence speed in iterations."}
{"paper_id": 242, "abstract": "In the realm of optimization, where the dance of algorithms unfolds, the tale of stochastic gradient descent (SGD) has often been one of triumph. Yet, beneath the surface of its celebrated success lies a more complex narrative\u2014one that we aim to unravel in this work. We embark on a journey through the shadowy landscapes of worst-case optimization problems, revealing the peculiar and sometimes troubling behaviors of SGD when it strays from the well-trodden paths of previous analyses.  Imagine a world where SGD, rather than gracefully finding its way to the global zenith, becomes ensnared in local maxima, languishing in saddle points with a lethargy that defies expectation. Picture it favoring sharp minima, like a traveler drawn to treacherous cliffs instead of the gentle valleys below. In our exploration, we construct intricate landscapes and data distributions that give rise to these phenomena: (1) SGD's tendency to settle in local maxima, (2) its sluggish escape from saddle points, (3) its preference for sharp minima over their flatter counterparts, and (4) the curious case of AMSGrad also converging to local maxima.  To ground our findings, we illustrate these concepts through a minimal neural network-like example, shedding light on the intricate interplay between minibatch sampling, discrete-time update rules, and the realistic landscapes that shape SGD's journey. Our results serve as a clarion call, emphasizing the need for a holistic examination of these elements to truly grasp the role of SGD in the grand saga of deep learning.", "introduction": "SGD is the main optimization algorithm for training deep neural networks, and understanding SGD is widely regarded as a key step on the path to understanding deep learning (Bottou, 2012;Zhang et al., 2018;Xing et al., 2018;Mori et al., 2021;Du et al., 2018;Allen-Zhu et al., 2018;Wojtowytsch, 2021a;b;Gower et al., 2021;Ziyin et al., 2022b;Gurbuzbalaban et al., 2021;Zou et al., 2021;Li et al., 2021;Feng and Tu, 2021).Despite its algorithmic simplicity (could be described by only two lines of equations), SGD is hard to understand.The difficulty is threefold: (1) SGD is discrete-time in nature, and discrete-time dynamics are typically much more complicated to solve than their continuous-time counterparts (May, 1976); (2) SGD noise is state-dependent (Ziyin et al., 2022b;Hodgkinson and Mahoney, 2020); and (3) the loss landscape can be nonlinear and non-convex, involving many local minima, saddle points, and degeneracies (the Hessian is not full-rank) in the landscape (Xing et al., 2018;Wu et al., 2017).Each of these difficulties is so challenging that very few works attempt to deal with all of them simultaneously.Most previous works on SGD are limited to the cases when the loss landscape is strongly convex (Ziyin et al., 2022b;Liu et al., 2021;Hoffer et al., 2017;Mandt et al., 2017), or when the noise is assumed to be Gaussian and time-independent (Zhu et al., 2019;Jastrzebski et al., 2017;Xie et al., 2021); for the works that tackle SGD in a non-convex setting, often strong conditions are assumed.The reliance on strong assumptions regarding each of the challenges means that our present understanding of SGD for deep learning could be speculative.This work aims to examine some commonly held presuppositions about SGD and show that when all the three challenging factors are taken together, many counter-intuitive phenomena may arise.Since most of these phenomena are potentially undesired, this work can also be seen as a worst-case analysis of SGD.In this work, we study the behavior of SGD in toy landscapes with non-convex loss and multi-minima.We approach the problem of SGD convergence from a different angle from many of the related works: instead of studying when SGD will converge, our result helps answer the question of when SGD might fail.In particular, the problem setting considers discrete-time SGD close to saddle points, where the noise is due to minibatch sampling, and the learning rate is held constant throughout training.In the next section, we define the SGD algorithm and the necessary notations.In Sec. 3, we discuss the related works.A warmup example is provided in Sec. 4. Sec. 5 introduces the main results.Sec.6 presents the numerical simulations, including a minimal example involving a neural network.We also encourage the readers to examine the appendix.Sec.A presents additional experiments.Sec.B presents a continuous-time analysis of the problems in the main text and is relevant for discussing the unique discrete-time features of SGD.Sec.C presents all the proofs."}
{"paper_id": 243, "abstract": "In the ever-shifting landscape of spatio-temporal forecasting, we unveil a groundbreaking approach tailored for the enigmatic nature of arbitrarily distributed points. Picture a system governed by an elusive partial differential equation\u2014one whose intricate dance we seek to understand. By wielding the finite element method, we conjure a continuous-time model that captures the very essence of the data's dynamics.  At the heart of our endeavor lies a graph neural network, a powerful construct that deftly estimates the instantaneous effects of these unknown dynamics across a meticulously crafted mesh of the spatial domain. What sets our model apart is its ability to weave in prior knowledge, allowing us to impose assumptions about the form of the unknown PDE. This, in turn, creates a structural bias that guides our learning toward specific processes, much like a skilled artisan shaping clay into a masterpiece.  As we delve deeper, we derive a transport variant of our model from the convection equation, revealing its prowess in enhancing transfer performance to higher-resolution meshes. Our results shine in the realms of sea surface temperature and gas flow forecasting, where our model outshines traditional methods in the spatio-temporal forecasting arena.  Yet, it is not merely the numbers that tell our story. A qualitative analysis unveils the model's unique ability to disentangle the complex dynamics of the data into their fundamental components. This interpretability transforms our work into a beacon of clarity, illuminating the path through the intricate web of spatio-temporal phenomena. In this journey, we invite you to explore the depths of our findings and witness the unfolding of a new chapter in forecasting.", "introduction": "Figure 1: Finite Element Networks predict the instantaneous change of each node by estimating the effect of the unknown generating dynamics on the domain volume that the node shares with its neighbors.The laws driving the physical world are often best described by partial differential equations (PDEs) that relate how a magnitude of interest changes in time with its change in space.They describe how the atmosphere and oceans circulate and interact, how structures deform under load and how electromagnetic waves propagate (Courant & Hilbert, 2008).Knowledge of these equations lets us predict the weather (Coiffier, 2011), build sturdier structures, and communicate wirelessly.Yet, in many cases we only know the PDEs governing a system partially (Isakov, 2006) or not at all, or solving them is too computationally costly to be practical (Ames, 2014).Machine learning researchers try to fill in these gaps with models trained on collected data.For example, neural networks have been trained for weather forecasts (Shi et al., 2015) and fluid flow simulations (Belbute-Peres et al., 2020), both of which are traditionally outcomes of PDE solvers.Even the dynamics of discrete dynamical systems such as traffic (Li et al., 2018) and crowds (Zhang et al., 2017) have been learned from data.A challenge facing these models is the high cost of acquiring training data, so the data is usually only available sparsely distributed in space.Since graphs are a natural way to structure sparse data, models incorporating graph neural networks (GNNs) have been particularly successful for spatio-temporal forecasting (Yu et al., 2018;Wu et al., 2019).In the domain of physical processes we can reasonably assume that the observed system follows a PDE.There are mainly two ways to incorporate this assumption as a-priori knowledge into a model.First, we can encode a known PDE into a loss function that encourages the model to fulfill the equation (Raissi et al., 2019).Another way to go about this is to derive the model structure itself from known laws such as the convection-diffusion equation (de B\u00e9zenac et al., 2018).We will follow the second approach.Consider a dynamical system on a bounded domain \u2126 \u2282 R d that is governed by the PDE \u2202 t u = F t, x, u, \u2202 x u, \u2202 2 x u, ...(1) on functions u : [0, T ] \u00d7 \u2126 \u2192 R m .If we have a dense measurement u 0 : \u2126 \u2192 R m of the current state of the system and a solution u that satisfies Eq. ( 1) for all t \u2208 [0, T ] and also fulfills the initial condition u(0, x) = u 0 (x) at all points x \u2208 \u2126, we can use u as a forecast for the state of the system until time T .From a spatio-temporal forecasting perspective, this means that we can forecast the evolution of the system if we have a continuous measurement of the state, know the dynamics F , and can find solutions of Eq. ( 1) efficiently.Unfortunately, in practice we only have a finite number of measurements at arbitrary points and only know the dynamics partially or not at all.Contributions.An established numerical method for forecasts in systems with fully specified dynamics is the finite element method (FEM) (Brenner et al., 2008).In this paper, we introduce the first graph-based model for spatio-temporal forecasting that is derived from FEM in a principled way.Our derivation establishes a direct connection between the form of the unknown dynamics and the structure of the model.Through this connection our model can incorporate prior knowledge on the governing physical processes via assumptions on the form of the underlying dynamics.We employ this mechanism to derive a specialized model for transport problems from the convection equation.The way that the model structure arises from the underlying equation makes our models uniquely interpretable.We show that our transport model disentangles convection and the remainder of the learned dynamics such as source/sink behavior, and that the activations of the model correspond to a learned flow field, which can be visualized and analyzed.In experiments on multi-step forecasting of sea surface temperature and gas flow, our models are competitive against baselines from recurrent, temporal-convolutional, and continuous-time model classes and the transport variant improves upon them in the transfer to higher-resolution meshes."}
{"paper_id": 244, "abstract": "In the realm of 3D reconstruction, we unveil a method of shape completion that dances gracefully with the complexities of continuous geometry, all while navigating the vast landscapes of large-scale scenes. Real-world scans, those intricate tapestries of our environment, often bear the scars of missing data, interwoven with objects that refuse to be neatly categorized. The challenge of shape completion is as elusive as a shadow in the twilight\u2014ill-posed and fraught with uncertainty. To rise to this challenge, we harness the power of Generative Cellular Automata, a framework that learns to embrace the myriad possibilities of shape, transforming our approach to accommodate the sprawling expanse of continuous geometry.  Our method incrementally conjures local shapes, crafting them as a sparse voxel embedding that captures the essence of each occupied cell through its latent code. We rigorously establish that our training objective for this sparse voxel embedding maximizes the variational lower bound of the complete shape distribution, ensuring that our progressive generation stands as a legitimate generative model. Through a series of experiments, we reveal the prowess of our approach, demonstrating its ability to conjure diverse and plausible scenes that remain true to the original input, even when faced with significant gaps in data. Remarkably, our method outshines deterministic models, even in scenarios where missing data is minimal, underscoring the vital role of a probabilistic framework in achieving high-quality geometry completion, regardless of the completeness of the input scans.", "introduction": "High-quality 3D data can create realistic virtual 3D environments or provide crucial information to interact with the environment for robots or human users (Varley et al. (2017)).However, 3D data acquired from a real-world scan is often noisy and incomplete with irregular samples.The task of 3D shape completion aims to recover the complete surface geometry from the raw 3D scans.Shape completion is often formulated in a data-driven way using the prior distribution of 3D geometry, which often results in multiple plausible outcomes given incomplete and noisy observation.If one learns to regress a single shape out of multi-modal shape distribution, one is bound to lose fine details of the geometry and produce blurry outputs as noticed with general generative models (Goodfellow (2017)).If we extend the range of completion to the scale of scenes with multiple objects, the task becomes even more challenging with the memory and computation requirements for representing large-scale high resolution 3D shapes.In this work, we present continuous Generative Cellular Automata (cGCA), which generates multiple continuous surfaces for 3D reconstruction.Our work builds on Generative Cellular Automata (GCA) (Zhang et al. (2021)), which produces diverse shapes by progressively growing the object surface from the immediate neighbors of the input shape.cGCA inherits the multi-modal and scalable generation of GCA, but overcomes the limitation of discrete voxel resolution producing high-quality continuous surfaces.Specifically, our model learns to generate diverse sparse voxels associated with their local latent codes, namely sparse voxel embedding, where each latent code encodes the deep implicit fields of continuous geometry near each of the occupied voxels (Chabra et al. (2020); Jiang et al. ( 2020)).Our training objective maximizes the variational lower bound for the log-likelihood of the surface distribution represented with sparse voxel embedding.The stochastic formulation is modified from the original GCA, and theoretically justified as a sound generative model.We demonstrate that cGCA can faithfully generate multiple plausible solutions of shape completion even for large-scale scenes with a significant amount of missing data as shown in Figure 1.To the best of our knowledge, we are the first to tackle the challenging task of probabilistic scene completion,Figure 1: Three examples of complete shapes using cGCA given noisy partial input observation.Even when the raw input is severely damaged (left), cGCA can generate plausible yet diverse complete continuous shapes.which requires not only the model to generate multiple plausible outcomes but also be scalable enough to capture the wide-range context of multiple objects.We summarize the key contributions as follows:(1) We are the first to tackle the problem of probabilistic scene completion with partial scans, and provide a scalable model that can capture largescale context of scenes.(2) We present continuous Generative Cellular Automata, a generative model that produces diverse continuous surfaces from a partial observation.(3) We modify infusion training (Bordes et al. (2017)) and prove that the formulation indeed increases the variational lower bound of data distribution, which verifies that the proposed progressive generation is a valid generative model."}
{"paper_id": 245, "abstract": "In the realm of machine learning, a dark threat looms\u2014model extraction attacks. Here, adversaries, like cunning thieves in the night, exploit the vulnerabilities of a model exposed through public APIs. They launch a barrage of queries, each one a calculated strike, gathering predictions to craft their own imitation. Traditional defenses rise to meet this challenge, but they often tread a precarious line, sacrificing the integrity of the model's utility for those who seek to use it legitimately.   But what if we could turn the tables? We propose a novel solution, a barrier crafted from the very essence of computational effort\u2014a proof-of-work. By demanding that users undertake this challenge before accessing the model's predictions, we transform the landscape of model extraction. Suddenly, the path for attackers becomes steep and treacherous, their computational burden multiplying\u2014up to a staggering 100 times. Yet, fear not, for our clever calibration ensures that the regular user feels only a whisper of this added weight, with an overhead of merely up to 2 times.  To weave this intricate tapestry, we harness the principles of differential privacy, measuring the delicate balance of information revealed with each query. Our approach requires no alterations to the victim model itself, providing a robust shield for machine learning practitioners. With this method, we empower them to safeguard their publicly exposed models, turning the tide against those who would seek to steal their hard-earned creations. In this battle of wits and will, we offer a glimmer of hope, a way to fortify the defenses of the digital realm.", "introduction": "Model extraction attacks (Tram\u00e8r et al., 2016;Jagielski et al., 2020;Zanella-Beguelin et al., 2021) are a threat to the confidentiality of machine learning (ML) models.They are also used as reconnaissance prior to mounting other attacks, for example, if an adversary wishes to disguise some spam message to get it past a target spam filter (Lowd & Meek, 2005), or generate adversarial examples (Biggio et al., 2013;Szegedy et al., 2014) using the extracted model (Papernot et al., 2017b).Furthermore, an adversary can extract a functionally similar model even without access to any real input training data (Krishna et al., 2020;Truong et al., 2021;Miura et al., 2021) while bypassing the long and expensive process of data procuring, cleaning, and preprocessing.This harms the interests of the model owner and infringes on their intellectual property.Defenses against model extraction can be categorized as active, passive, or reactive.Current active defenses perturb the outputs to poison the training objective of an attacker (Orekondy et al., 2020).Passive defenses try to detect an attack (Juuti et al., 2019) or truncate outputs (Tram\u00e8r et al., 2016), but these methods lower the quality of results for legitimate users.The main reactive defenses against model extraction attacks are watermarking (Jia et al., 2020b), dataset inference (Maini et al., 2021), and proof of learning (Jia et al., 2021).However, reactive approaches address model extraction post hoc, i.e., after the attack has been completed.We design a pro-active defense that prevents model stealing before it succeeds.Specifically, we aim to increase the computational cost of model extraction without lowering the quality of model outputs.Our method is based on the concept of proof-of-work (PoW) and its main steps are presented as a block diagram in Figure 1.Our key proposal is to wrap model outputs in a PoW problem to force users to expand some compute before they can read the desired output.The standard PoW techniques, Figure 1: Machine Learning API with calibrated proof-of-work.Answering queries with PoW consists of the following steps: 1 An attacker sends queries drawn from an unlabeled dataset to a machine learning service. 2 The information leakage estimator computes the privacy cost of the queries.3 The privacy cost of queries is sent to the proof-of-work (PoW) server, which adds the new cost to the total user's (attacker's) query cost.4 The PoW server generates a puzzle, whose difficulty is based on the total cost incurred by the user.5 The attacker solves the puzzle using its PoW Client and 6 sends the solution back to the server.7 The PoW server verifies the solution and if it is correct, it notifies the victim model, which 8 predicts the labels.The victim model returns the labels to the client 9 , which can potentially use the returned labels to train its own model.which initially were used in the security domain to prevent a server from Denial-of-Service attacks (DoS)foot_0 , provide users with a small puzzle to be solved before giving access to the service (Dwork & Naor, 1993).In the case of Machine-Learning-as-a-Service (MLaaS), the answer to a query (e.g., the label predicted by a model) can similarly be released to a user once his or her computational or other system resources are spent.This can prevent a malicious user from obtaining enough examples labeled by a victim model to steal the model.Intuitively, our method shifts the trade-off between the quality of answers and robustness, that was introduced by previous defenses, to a trade-off between the computational cost and robustness to model stealing.To minimize the impact of our defense on legitimate users, we tie the difficulty of the PoW problem (Back, 2020) to an estimate of how much information about a model has been extracted so far by a given user.We compute the privacy cost per query as an information leakage metric used in the differential privacy literature (Dwork et al., 2014).We choose to work with the privacy based metric because it is (1) agnostic to the learning task and model architecture, and (2) practical to compute for the model owner.We tune the per-query difficulty of the required proof-of-work based on the measured information gained by a user.PoW does not deteriorate the accuracy of answers and adds only a small increase (less than 2x) in the cost of querying public models for legitimate users.The cost is usually much higher (up to 100x) for attackers.To summarize, our contributions are as follows:\u2022 We introduce a new class of pro-active defenses against model stealing attacks that use proofof-work mechanisms to adaptively increase the computational cost of queries with a negligible increase in the cost for legitimate users, and a high increase in cost for many attackers.Our method is the first active defense strategy that leaves the model's accuracy entirely intact.\u2022 We propose a novel application of differential privacy budgets as a way to measure the amount of information leakage from answered queries.We calibrate the difficulty of the proof-of-work problem based on a user's query cost as estimated through its differential privacy budget.\u2022 Our rigorous evaluation on four datasets and eight attacks validates that adversaries attempting to steal a model issue queries whose privacy cost accumulates up to 7x faster than for benign queries Table 1: Taxonomy of model extraction attacks.We use IND for in-distribution and OOD for out-of-distribution data.The MixMatch attack is the best performing one with very few queries run against the ML API and a high accuracy of the extracted model, mainly due to the abundant and efficient usage of the problem domain (i.e., task-specific) data.At the other end of the spectrum is DataFree attack with only synthetic OOD data.\u2022 We design adaptive attacks against our defense and show that they require simultaneous optimization of two opposing goals: (1) minimizing the privacy cost of queries run against the ML API and (2) maximizing the information leakage incurred by the queries.These conflicting targets render such attacks less effective, making them less attractive to adversaries."}
{"paper_id": 246, "abstract": "In the ever-evolving landscape of recommendation systems, the art of capturing feature relationships with both precision and efficiency stands as a cornerstone for click-through rate (CTR) prediction. Traditional methods have often relied on cumbersome, manually-crafted low-order interactions or rigid, high-order interactions that demand additional DNN modules for implicit modeling\u2014both of which can stifle innovation and adaptability.   In this paper, we unveil a groundbreaking approach known as the Dynamic Parameterized Operation (DPO), a versatile plug-in designed to learn interactions\u2014both explicit and implicit\u2014on an instance-by-instance basis. By integrating DPO into Deep Neural Network (DNN) and Attention modules, we enhance the adaptability of feature-based modeling while simultaneously refining user behavior modeling through an instance-wise locality lens.   Our Dynamic Parameterized Networks not only surpass the performance of state-of-the-art methodologies in rigorous offline experiments across public datasets and real-world production environments, but they also shine in an online A/B test. Most notably, these networks have been successfully deployed within the ranking systems of one of the world's largest e-commerce platforms, efficiently handling the vast traffic of hundreds of millions of active users. In this way, we bridge the gap between theoretical advancement and practical application, paving the way for the next generation of recommendation systems.", "introduction": "Click-through rate (CTR) prediction, which aims to estimate the probability of a user clicking an item, is of great importance in recommendation systems and online advertising systems (Cheng et al., 2016;Guo et al., 2017;Rendle, 2010;Zhou et al., 2018b).Effective feature modeling and user behavior modeling are two critical parts of CTR prediction.Deep neural networks (DNNs) have achieved tremendous success on a variety of CTR prediction methods for feature modeling (Cheng et al., 2016;Guo et al., 2017;Wang et al., 2017).Under the hood, its core component is a linear transformation followed by a nonlinear function, which models weighted interaction between the flattened inputs and contexts by fixed kernels, regardless of the intrinsic decoupling relations from specific contexts (Rendle et al., 2020).This property makes DNN learn interaction in an implicit manner, while limiting its ability to model explicit relation, which is often captured by feature crossing component (Rendle, 2010;Song et al., 2019).Most existing solutions exploit a combinatorial framework (feature crossing component + DNN component) to leverage both implicit and explicit feature interactions, which is suboptimal and inefficient (Cheng et al., 2016;Wang et al., 2017).For instance, wide & deep combines a linear module in the wide part for explicit low-order interaction and a DNN module to learn high-order feature interactions.Follow-up works such as Deep & Cross Network (DCN) follows a similar manner by replacing the wide part with more sophistic networks, however, posits restriction to input size which is inflexible.Above-mentioned methods pay little attention to user behavior modeling.Recently, attention-based methods like DIN and DIEN have attracted many interests that attempt to capture user preferences based on users' historical behaviors (Zhou et al., 2018b;2019;Feng et al., 2019).With regard to the interaction of characteristics, the use of attention mechanisms in these methods can be treated as an explicit modelling of the interaction of characteristics while neglecting the modelling of implicit interactions of characteristics.The methods mentioned above either model implicit and explicit feature interactions isolated or adopt a suboptimal way to combine them, which can be inefficient.In this work, we aim to address these problems by introducing a small MLP layer that dynamically generates kernels conditioned by the current instance to capture both implicit and explicit feature interactions.The core idea is to first generate context weights and biases from the context stream, and then aggregate them with the input stream adaptively.We formulate a generic function and implement it with an efficient dynamic parameterized operation (DPO).The first weight generator projects contextual features into high-dimensional representation, which models implicit conditional bias.The second feature aggregator aims to fuse input features and projected contextual representation in a multiplicative way, e.g., matrix multiplication and convolution, maintaining both low-and high-order information.For feature-based modeling, we introduce feature-based DPO where the weight-generate operation dynamically produces instance-wise filters conditioned on the embedded context.The featureaggregate function then applies instance-wise filters to the flattened input by matrix multiplication, allowing to learn multiplicative features.In particular, we further propose a new class of DPO, called field-based DPO, which is not only instance-specific but also field-specific.In that case, the filters vary from field to field and from instance to instance, allowing more complex interactions along the field dimension.For user behavior modeling, we introduce sequence-based DPO that consists of two variants: behaviorbehavior dynamic operation and query-behavior dynamic operation.A representative method of dynamic convolution (Chen et al., 2020;Yang et al., 2019) shares the convolution kernel, which is generated by the global average of the inputs.Similarly, (Wu et al., 2019) proposed DyConv, a lightweight fine-grained convolution that depends only on time-step, reinforcing the encoder-based language modeling framework.However, our methods incorporate both local and global information as they jointly use locality-aware methods (e.g., convolution or separable convolution) followed by a global average pooling layer to produce instance-wise weights.The query-behavior dynamic operation is specialized designed for the decoder-based framework in CTR prediction, aiming to capture target-behavior dependency.To our best knowledge, this is the first attempt to extend the business of dynamic neural networks to CTR prediction with extensive experiments on two fundamental scenarios.The comprehensive study against existing solutions validates the superiority of our proposed method.Moreover, we demonstrate that incorporating DPO into the real-world ranking system is beneficial.Our contribution can be summarized as followed:\u2022 We propose a generic formulation for capturing multiplicative interaction via weightgenerate and feature-aggregate function, termed DPO.\u2022 For feature-based modeling, we propose two variants, named field-based and feature-based DPO, offering a unifying view of implicit and explicit feature interaction.Decomposing these operations, we find they implicitly inherit low-and second-order representation.\u2022 For user behavior modeling, we propose two sequence-based variants: behavior-behavior and query-behavior DPO.The first one computes locally perceptual dynamic filters and the second one learns target-behavior dependency in a multiplicative manner.We demonstrate that such operations can benefit the self-attention layers by higher computational efficiency through modeling locality as inductive bias.\u2022 The proposed dynamic parameterized networks outperform state-of-the-art methods by a significant margin on both public and real-world production datasets.We also give a comprehensive study about the relationship of our proposed methods to previous Factorization Machine (Rendle, 2010) and CrossLayer (Wang et al., 2017).We further demonstrate the effectiveness and superiority of our method with an online A/B test in real-world applications by incorporating it into the fine-rank stage of the real-world ads system."}
{"paper_id": 247, "abstract": "For decades, the quest to unravel the noise parameters of the Kalman Filter (KF) has captivated researchers, spawning a vast tapestry of studies dedicated to noise estimation under myriad conditions. This pursuit stems from a fundamental truth: accurate noise estimation is akin to wielding a powerful weapon against filtering errors. Yet, we unveil a critical insight\u2014when the foundational assumptions of the KF are even slightly breached, the effective noise can shift dramatically, shattering the presumed equivalence between noise estimation and error minimization. Such violations are not merely rare anomalies; they are insidious and often evade notice, complicating the landscape of filtering.  In light of this revelation, we advocate for a paradigm shift\u2014a robust solution that transcends the need for a bespoke model tailored to each unique problem. We harness the power of gradient-based optimization, directly targeting filtering errors while deftly navigating the intricate parameterization of the KF's symmetric and positive-definite parameters. Through a diverse array of state-estimation and tracking challenges, our findings reveal that this optimization not only enhances the accuracy of the KF but also fortifies its resilience against design missteps.  Moreover, we illustrate a compelling narrative: an optimized neural network model may initially appear to eclipse the KF in error reduction. However, this advantage dissipates once the KF is subjected to a similar optimization process. This phenomenon underscores a vital lesson in the realm of complex models\u2014they can easily be misjudged as superior, when in truth, they merely reflect a higher degree of optimization.", "introduction": "The Kalman Filter (KF) (Kalman, 1960) is a celebrated method for linear filtering and prediction, with applications in many fields including tracking, guidance, navigation and control (Zarchan and Musoff, 2000;Kirubarajan, 2002).Due to its simplicity and robustness, it remains highly popular -with over 10,000 citations in the last 5 years alone (Google Scholar, 2021) -despite the rise of many non-linear sequential prediction models (e.g., recurrent neural networks).The KF relies on the following model for a dynamic system:where X t is the state of the system at time t (whose estimation is usually the goal), and its dynamics are modeled by the linear operator F t up to the random noise \u03c9 t with covariance Q; and Z t is the observation, which is modeled by the linear operator H t up to the noise \u03bd t with covariance R. When the operators F t , H t are not assumed to depend on time, the notation may be simplified to F, H.To use the KF, one must determine the noise parameters Q, R. The filtering errors (i.e., estimation errors of the states {X t }) are minimized when Q and R correspond to the true covariance matrices of the noise (Humpherys et al., 2012).Thus, these parameters are usually determined by noise estimation.In absence of system state data {x t } (the \"ground truth\"), many methods have been suggested to determine Q and R from observations data {z t } alone (Abbeel et al., 2005;Odelson et al., 2006;Zanni et al., 2017;Park et al., 2019).When ground-truth data is available, however, noise estimation trivially reduces to calculation of the sample covariance matrices (Lacey, 1998): R := Cov({z t -H t x t } t ), Q := Cov({x t+1 -F t x t } t ).(2) Indeed, as stated by Odelson et al. (2006), \"the more systematic and preferable approach to determine the filter gain is to estimate the covariances from data\".Our work focuses on such problems with ground-truth available for learning (but not for inference after the learning, of course), which was motivated by a real-world Doppler radar estimation problem.Noise estimation is often not optimal: The equivalence between noise estimation and errors minimization can be proved under the standard KF assumptions -that is, known and linear dynamics and observation models (F t , H t ), with i.i.d and normally-distributed noises ({\u03c9 t }, {\u03bd t }) (Humpherys et al., 2012).However, as put by Thomson (1994), \"experience with real-world data soon convinces one that stationarity and Gaussianity are fairy tales invented for the amusement of undergraduates\"and linearity and independence can be safely added to this list.Therefore, under realistic assumptions, the covariance of the noise does not necessarily correspond to optimal filtering.We introduce a case study in the context of radar tracking, where we demonstrate that even using the true covariance of the noise (\"oracle\" noise-estimation) is sub-optimal in a variety of scenarios -including very simple scenarios with relatively minor violation of the KF assumptions.In Appendices E and F, we also analyze this phenomenon analytically for two private cases (non-linearity in Doppler radar and non-i.i.d noise in lidar), where the violation of a single KF assumption is shown to modify the effective noise.By providing this extensive evidence that noise-estimation is a sub-optimal way to tune a KF even in presence of system-states ground-truth data, we re-open a problem that was considered solved for decades (Kalman, 1960).We also show that seemingly small changes in the properties of the scenario may lead to major changes in the desired design of the KF, e.g., whether to use a KF or an Extended KF (Sorenson, 1985).In certain cases, the design choices are easy to overlook (e.g., Cartesian vs. spherical coordinates), and are not trivial to make even if noticed.As a result, it is impractical to manually choose or develop a variant of the KF for every problem.Rather, we should assume that our model is sub-optimal, and leverage data to deal with the sub-optimality as robustly as possible.Optimization is optimal: We consider Q and R as model parameters that should be optimized with respect to the filtering errors (i.e., system-state estimation errors) -rather than estimating the noise.While both noise estimation and errors optimization rely on exploitation of data, only the latter explicitly addresses the actual goal of solving the filtering problem.Gradient-based optimization methods are usually effective in the field of machine learning, but applying them naively to the entries of Q and R may violate the symmetry and positive-definiteness (SPD) constraints of the covariance matrices.Indeed, even works that come as far as optimizing Q and R (instead of estimating the noise) usually apply limited optimization methods, e.g., gridsearch (Coskun et al., 2017) or diagonal restriction of the covariance matrices (Formentin and Bittanti, 2014;Li et al., 2019).To address this issue, we use a parameterization based on Cholesky decomposition (Horn and Johnson, 1985), which allows us to apply gradient-based optimization to SPD matrices.This method is computationally efficient compared to other general gradient-based methods for SPD optimization (Tsuda et al., 2005;Tibshirani, 2015).We demonstrate that the optimization reduces the errors of the KF consistently: over different variants of the KF, over different violations of KF assumptions, over different domains (tracking from radar, video or lidar), over small and large training datasets, and even under distributional shifts between train and test datasets.Furthermore, we show that optimization improves the robustness to design decisions, by shrinking the performance gaps between different variants of the KF.As explained above, we extensively justify the need to optimize the KF in many practical problems, and suggest a simple solution which is effective, robust, computationally efficient, and relies on standard tools in supervised machine learning.As a result, we believe that in the scope of filtering problems with available ground-truth, whenever the KF assumptions are not strictly-guaranteed, the suggested optimization method should become the new standard procedure for the KF tuning.Unfair comparison: Many learning algorithms have been suggested to address non-linearity in filtering problems, e.g., based on Recurrent Neural Networks (RNN).Such works often use a linear tool such as the KF as a baseline for comparison -with tuning parameters being sometimes ignored (Gao et al., 2019), sometimes based on noise estimation (fa Dai et al., 2020), and sometimes optimized in a limited manner using trial-and-error (Jamil et al., 2020) or grid-search (Coskun et al., 2017).Our findings imply that such a methodology yields over-optimistic conclusions, since the baseline is not optimized to the same level as the learning model.This may result in adoption of over-complicated algorithms with no actual added value.Instead, any learning algorithm should be compared to a baseline that is optimized using a similar method (e.g., gradient-descent with respect to the errors).Indeed, we consider an extension of the KF based on LSTM, which is the key component in many SOTA algorithms for non-linear sequential prediction in recent years (Neu et al., 2021).For radar tracking with non-linear motion, we demonstrate how the LSTM seems to provide a significant improvement over the KF.Then, we show that the whole improvement comes from optimization of parameters, and not from the expressive non-linear architecture.In particular, this result demonstrates the competitiveness of our suggested method versus SOTA sequential prediction models.Recent works in the area of machine learning have already shown that advanced algorithms often obtain most of their improvement from implementation nuances (Engstrom et al., 2019;Andrychowicz et al., 2020;Henderson et al., 2017).Our work continues this line of thinking and raises awareness to this issue in the domain of filtering problems.We show (empirically and analytically) that the gold-standard noise estimation method to tune the KF given ground-truth data is often sub-optimal; demonstrate how this leads to underevaluation of the KF compared to optimized filtering models such as neural networks; suggest a simple method to optimize the KF, using gradient-based optimization with Cholesky parameterization; and extensively demonstrate its improved accuracy and its robustness to model misspecification.Limitations: This work relies on the availability of ground-truth system-states in the training data to allow supervised learning.Ground-truth data is often available from simulations, controlled experiments or manual labeling.Within this scope, we show that although noise estimation is straightforward (Equation 2), it is often not the right task to address.Another limitation is in the theoretical guarantees of gradient-based optimization (see Appendix L), despite its wide success in many fields.The paper is organized as follows: Section 2 reviews the KF.Section 3 introduces our method for efficient KF optimization.Section 4 justifies the necessity of KF optimization through a detailed case study.Section 5 presents a neural version of the KF which reduces the errors compared to a standard KF -but not when compared to an optimized KF.Section 6 discusses related works.The KF algorithm (Kalman, 1960;Humpherys et al., 2012) relies on Equation 1 for a dynamic system model.It keeps an estimate of the state X t , represented as the mean x t and covariance P t of a normal distribution.As shown in Figure 1, it alternately predicts the next state using the dynamics model (prediction step), and processes new information from incoming observations (update or filtering step)."}
{"paper_id": 248, "abstract": "Are all bits truly essential? In this exploration, we unveil SimpleBits, a groundbreaking method designed to distill inputs into their most meaningful essence by strategically trimming away unnecessary information. The brilliance of SimpleBits lies in its independence from domain-specific expertise, allowing it to autonomously identify and eliminate the least relevant features for any given task. By harnessing the power of a pretrained generative model, we embark on a dual optimization journey\u2014seeking not only to minimize the bits per dimension of our inputs but also to enhance classification performance.   Our approach spans a diverse array of scenarios: from traditional training methods to dataset condensation and post-hoc analysis. Through this lens, we delve into the revelations that simplified inputs provide regarding the decision-making processes of classification networks. Our findings reveal that SimpleBits adeptly discards extraneous information, particularly in tasks burdened by distracting elements. Additionally, when employed in a post-hoc context, our method sheds light on the underlying reasons behind misclassifications made by conventionally trained classifiers. In the realm of dataset condensation, we discover that inputs can be simplified with only a negligible impact on accuracy. Ultimately, our innovative, learning-driven simplification technique emerges as a powerful new tool for unraveling the intricacies of network decisions, offering fresh insights into the complex tapestry of machine learning.", "introduction": "A better understanding of the information deep networks learn can lead to new scientific discoveries (Raghu & Schmidt, 2020), inform our understanding of differences between human and model behaviors (Makino et al., 2020) and can serve as powerful auditing tools (Geirhos et al., 2020).Removing information from the input manually is one way to understand what information content is relevant for learning.For example, researchers have occluded parts of the input or removed specific frequency ranges from the input to see which input regions and frequency ranges are relevant for the network's prediction (Zintgraf et al., 2017;Makino et al., 2020;Banerjee et al., 2021).These ablation techniques use simple heuristics such as removing at random (Hooker et al., 2019) or exploit domain knowledge about interpretable aspects of the input (input regions, frequency range) to create simpler versions of the input and analyze the network's prediction on these simpler inputs.What if, instead of using heuristics, one would learn how to simplify inputs that contain predictionrelevant information?This way, one could synthesize simpler inputs and gain intuition into model behavior without relying on domain knowledge about what information may be relevant for the network.For this, one needs to define the precise meaning of \"simplify an input\", including useful metrics for the simplification of the inputs and for the retention of task-relevant information.In this work, we propose SimpleBits -an information-reduction method that learns to synthesize simplified inputs which contain less information but still remain informative for the task.To measure simplicity, we use a finding initially reported as a problem for density-based anomaly detection -generative image models tend to assign higher probability densities and hence lower bits to visually simpler inputs (Kirichenko et al., 2020;Schirrmeister et al., 2020).Here, we use this to our advantage and minimize the encoding bit size given by a generative network trained on a general image distribution to simplify inputs.At the same time, we optimize that the simplified inputs still contain the task-relevant information.We then investigate what information is retained in the simplified inputs and how the simplification affects network behavior in a variety of settings.We apply SimpleBits both in a per-instance setting, where each image is processed to be a simplified version of itself, and the size of training set remains We apply SimpleBits to a variety of tasks to aid neural network understanding.As a per-image simplifier, applied during training (left), it investigates the trade-off curve between simplification and accuracy.It can also be used as a post-hoc analysis tool after training (center), illuminating features of images that are crucial to the trained classifier.When combined with data condensation (right), the original data set can be effectively reduced both in size and in complexity.unchanged, as well as in a condensation setting, where the dataset is compressed to only a few samples per class, with the condensed samples simplified at the same time.Applied during training, SimpleBits can be used to investigate the trade-off between the information content and the task performance.After training, SimpleBits can act as an analysis tool to understand what information a trained model uses for its decision making.Figure 1 summarizes tasks covered in this paper.Our evaluation provides the following insights in the investigated scenarios:1. Per-instance simplification during training.SimpleBits successfully removes superfluous information for tasks with injected distractors.On natural image datasets, Sim-pleBits highlights plausible task-relevant information (shape, color, texture).Increasing simplification leads to accuracy decreases and we report the trade-off between simplifying inputs and task level performance for different datasets.2. Dataset simplification with condensation.We evaluate SimpleBits applied to a condensation setting that processes the training data into a much smaller set of synthetic images.SimpleBits simplifies these images to drastically reduce the encoding size without substantial task performance decrease.On a chest radiograph dataset (Johnson et al., 2019a;b), SimpleBits can uncover known radiologic features for pleural effusion and gender.3. Post-training simplification.For a trained model, SimpleBits provides intuition into the prediction-relevant information in an image.For example, by visualizing simplifications of mispredicted images, we can form hypotheses of why these images were mispredicted."}
{"paper_id": 249, "abstract": "In the realm of optimization, where the quest for efficiency is ever-present, importance sampling emerges as a formidable ally, wielding the potential to dramatically enhance the convergence rate of stochastic gradient methods. Traditionally, this technique has been employed to precondition optimization challenges, yet its lesser-known power lies in its ability to diminish the variance of gradient estimators. Alas, this perspective has not yet birthed practical methodologies that tangibly improve the asymptotic error of stochastic gradient methods.  In this manuscript, we unveil the stochastic reweighted gradient (SRG), a groundbreaking variance-reduced stochastic gradient method that harnesses the pure essence of importance sampling. This innovative approach promises to surpass the asymptotic error of the conventional stochastic gradient descent (SGD), particularly in scenarios characterized by strong convexity and smoothness.   Moreover, we reveal that SRG is not merely a standalone solution; it can be seamlessly integrated to leverage the advantages of both importance-sampling-based preconditioning and variance reduction. When juxtaposed with SGD, our novel algorithm exhibits the remarkable ability to concurrently diminish the condition number and the asymptotic error\u2014each by a factor that corresponds to the number of component functions.  Through rigorous experimentation, we illustrate SRG's enhanced convergence performance in the context of $\\ell_2$-regularized logistic regression problems, paving the way for a new era of optimization where speed and accuracy can coexist harmoniously.", "introduction": "Unconstrained optimization of finite-sum objectives is a core algorithmic problem in machine learning.The prototypical way of solving such problems is by viewing them through the lens of stochastic optimization, where the source of stochasticity resides in the choice of the index in the sum.Stochastic gradient descent (SGD) (Robbins & Monro, 1951) remains the standard algorithm for this class of problems.A natural way to improve on SGD is by considering importance sampling schemes.This idea is not new and dates back to (Needell et al., 2014) who uses importance sampling as a preconditioning technique.They propose sampling the indices with probabilities proportional to the smoothness constants of the corresponding component functions, and show that this sampling scheme provably reduces the condition number of the problem.In another line of work, variance-reduced methods were found to achieve linear convergence in the strongly convex and smooth case (Roux et al., 2012;Schmidt et al., 2017).Many of these methods rely on control variates to reduce the variance of the gradient estimator used by SGD (Johnson & Zhang, 2013;Defazio et al., 2014).While very successful, the applicability of these methods is limited by the large memory overhead that they introduce, or the periodic full-gradient recomputation that they require (Defazio & Bottou, 2019).Despite strong progress in this research area, an importance-sampling-based analogue to these algorithms, which is potentially free from these drawbacks, has yet to emerge.In this work, we propose such an analogue.We introduce stochastic reweighted gradient (SRG), an importance-sampling-based variance-reduced optimization algorithm.Similar to SGD, SRG requires a single gradient oracle call per iteration, and only requires O(n) additional memory, and O(log n) additional floating point operations per iteration.We analyze the convergence rate of SRG in the strongly-convex and smooth case, and show that it can provably improve the asymptotic error of SGD.Finally, we show how our importance sampling strategy can be combined with smoothnessbased importance sampling, and prove that the resulting algorithm simultaneously performs variance reduction and preconditioning.We demonstrate improved convergence in practice on 2 -regularized logistic regression problems."}
{"paper_id": 250, "abstract": "In the ever-evolving landscape of machine learning, the art of multimodal fusion stands out as a beacon of potential, promising to elevate model performance across a myriad of tasks. Yet, amidst this promise, the resilience of such fusion techniques remains largely unexplored in the existing body of research. In this groundbreaking paper, we unveil a pioneering approach: a training-free robust late-fusion method that deftly leverages the conditional independence assumption alongside Jacobian regularization.   At the heart of our innovation lies the minimization of the Frobenius norm of a Jacobian matrix, transforming the optimization challenge into a manageable Sylvester equation. This clever reformation not only simplifies the process but also opens new avenues for understanding the interplay of modalities. We further enrich our contribution with a theoretical error bound, illuminating the role of the additional modality in enhancing robustness.  Through a series of rigorous numerical experiments conducted on datasets such as AV-MNIST, RAVDESS, and VGGsound, we demonstrate the formidable efficacy of our method, showcasing its resilience against both adversarial attacks and random corruptions. Join us as we embark on this journey to fortify multimodal fusion, paving the way for more robust and adaptable machine learning models.", "introduction": "Deep fusion models have recently drawn great attention of researchers in the context of multimodal learning (Vielzeuf et al., 2018;Baltru\u0161aitis et al., 2018;P\u00e9rez-R\u00faa et al., 2019;Wang et al., 2020;Xue et al., 2021) as it provides an easy way to increase model accuracy and robustness.For instance, RGB cameras and LiDARs are usually deployed simultaneously on an autonomous vehicle, and the resulting RGB images and point clouds are referred to as two modalities, respectively.When RGB images are blurry at night, point clouds could provide complementary information and help to make decisions in vision tasks (Kim & Ghosh, 2019).Over the past few years, numerous multimodal fusion methods have been proposed at different levels: early-, middle-, and late-fusion (Chen et al., 2021).In early-fusion, input feature vectors from different modalities are concatenated and fed into one single deep neural network (DNN), while in middle-fusion, they go into DNNs independently and exchange information in feature space.Unlike the previous two cases, late-fusion is realized by merging distinct DNNs at their output layers via concatenation, element-wise summation, etc.These three levels of fusion possess different pros and cons.For instance, late-fusion, the primary concern of our paper, is (i) privacy-friendly and (ii) convenient to deploy.Specifically, assume that a hospital wants to have an AI agent to judge whether a patient has a certain disease or not (Sun et al., 2020).It has to divide the complete training feature (e.g., medical records, X-ray images) of every patient and deliver them to different AI companies, otherwise, the patients' identities will be exposed and their privacy are unprotected.This, in turn, directly rules out the possibility of applying early-or middle-fusion methods.On the other hand, the hospital could still exploit latefusion technique to generate the ultimate AI agent after several unimodal DNNs are trained by AI companies.Moreover, unlike early-or middle-fusion, many late-fusion methods could tolerate missing modality information (i.e., no need for paired data) and thus are convenient to deploy.Although late-fusion is a mature topic in the literature, its performance under adversarial attacks (Madry et al., 2018;Tsipras et al., 2019) and random corruptions (Zheng et al., 2016;Kim & Ghosh, 2019) is rather under-explored.In this paper, we address the problem of robust late-fusion by utilizing Jacobian regularization (Varga et al., 2017;Jakubovitz & Giryes, 2018;Hoffman et al., 2019;Chan et al., 2019) and conditional independence assumption (Sun et al., 2020).The key is to minimize the Frobenius norm of a Jacobian matrix so that the multimodal prediction is stabilized (see Figure 1).Our main contributions are as follows:\u2022 To the best of our knowledge, we are the first to propose a training-free robust late-fusion method.The involving optimization problem is relaxed to a Sylvester equation (Jameson, 1968), and the solution is obtained with only a little computational overhead.\u2022 We provide a theoretical error bound of our proposed robust late-fusion method and an illustrative explanation about the function of the extra modality via the TwoMoon example.\u2022 Thorough numerical experiments demonstrate that our method outperforms other latefusion methods and is capable to handle both adversarial attacks and random corruptions.Figure 1: Illustration of the proposed robust late-fusion method.Before unimodal raw logit z A and z B are fused, the Jacobian regularization technique is applied.Roughly speaking, it will enforce the derivative of p with respect to z A becomes smaller.Thus, when z A is perturbed to z A (due to random corruption or adversarial attack), the change of multimodal prediction ||p -p|| will be limited to some extent.For the illustration purpose, all variables here (e.g., p, z A ) are drawn in one-dimensional space."}
{"paper_id": 251, "abstract": "In the realm of machine learning, where data scarcity can feel like a formidable foe, transfer-learning methods emerge as a beacon of hope. These techniques harness the power of models pretrained on abundant data from a source domain, channeling that knowledge to enhance performance in a target domain that lacks the same wealth of information. Among the strategies available, linear probing stands out for its efficiency; it involves freezing the source model while training a new classification head tailored to the target domain. Yet, this approach often finds itself overshadowed by the more resource-intensive fine-tuning method, which adjusts all parameters of the source model. This latter technique unlocks the potential of intermediate layers, allowing the model to tap into valuable insights that linear probing might overlook.  Intrigued by this disparity, we delve into the possibility of directly harnessing those intermediate layers even within the confines of linear probing. Thus, we introduce a novel approach: Head-to-Toe probing, or Head2Toe. This innovative method selects features from every layer of the source model, constructing a classification head finely attuned to the target domain\u2019s nuances. Our evaluations on the VTAB dataset reveal that Head2Toe not only matches the performance of fine-tuning on average but also, crucially, excels in scenarios involving out-of-distribution transfer. In this way, Head2Toe stands as a testament to the power of ingenuity in the face of challenge, bridging the gap between efficiency and effectiveness in the pursuit of knowledge transfer.", "introduction": "Tranfer learning is a widely used method for obtaining strong performance in a variety of tasks where training data is scarce-see Zhu et al. (2020); Alyafeai et al. (2020); Zhuang et al. (2020) for recent application-specific surveys.A well-known recipe for transfer learning involves the supervised or unsupervised pretraining of a model on a source task with a large training dataset (also referred to as upstream training).After pretraining, the model's output head is discarded, and the rest of the network is used to obtain a feature embedding, i.e., the output of what was formerly the penultimate layer of the network.When transferring to a target task, a new output head is trained on top of the feature extractor (downstream training).This approach makes intuitive sense: if a linear combination of embedding features performs well on the source task, we expect a different linear combination of features to generalize to the target domain, provided the source and target tasks are similar.This approach of training a new output head, referred to as linear probing (LINEAR), often yields significant improvements in performance on the target task over training the network from scratch (Kornblith et al., 2019).An alternative to LINEAR is fine-tuning (FINETUNING), which uses targetdomain data to adapt all weights in the feature extractor together with the new output head.This procedure requires doing forward and backward passes through the entire network at each training step and therefore its per-step cost is significantly higher than LINEAR.Furthermore, since the entire network is fine-tuned, the entire set of new weights needs to be stored for every target task, making FINETUNING impractical when working on edge devices or with a large number of target tasks.However, FINETUNING is often preferred over LINEAR since it consistently leads to better performance on a variety of target tasks even when data is scarce (Zhai et al., 2019).FINETUNING's superior generalization in the low-data regime is counterintuitive given that the number of model parameters to be adapted is often large relative to the amount of available training data.How does FINETUNING learn from few examples successfully?We conjecture that FINETUNING better leverages existing internal representations rather than discovering entirely new representations; FINETUNING exposes existing features buried deep in the net for use by the classifier.Under this hypothesis, features needed for transfer are already present in the pretrained network and might be identified directly without fine-tuning the backbone itself.In this work, we propose and explore methods for selecting useful features embedded in internal layers of a pretrained net, concatenating them to the embedding produced by the pretrained net, and then applying the LINEAR transfer approach to the augmented representation (Fig. 1-left).Our approach leads to significant improvements over LINEAR as shown in Fig. 1-right.Our key contributions are as follows:1. We perform analyses to better understand two factors that affect the benefit of incorporating intermediate representations: feature selection and the degree to which a target domain is out-of-distribution (OOD).2. We introduce head-to-toe probing (HEAD2TOE), a simple recipe for selecting relevant features from intermediate representations for few-shot generalization.On the VTAB collection of data sets, we show that HEAD2TOE outperforms LINEAR and matches the performance of more computationally costly FINETUNING.3. Critically, HEAD2TOE outperforms FINETUNING on OOD target domains.If a practitioner can make an educated guess about whether a target domain is OOD with respect to a source, using HEAD2TOE improves on the state of the art for transfer learning"}
{"paper_id": 252, "abstract": "In the vast realm of artificial intelligence, where the intricacies of high-dimensional data dance like shadows in the night, artificial neural networks have emerged as powerful allies. Yet, these networks often find themselves shackled by their design, tailored to specific tasks like a master craftsman honing a blade for a singular purpose. Take, for instance, the challenge of classification\u2014be it \"fair\" or \"hierarchical.\" Once a particular architecture is chosen, the path to adapting it for new endeavors can feel as treacherous as navigating a labyrinth.  In this work, we unveil a groundbreaking architecture: the Concept Subspace Network (CSN). This innovative model transcends the limitations of its predecessors, forging a bridge between specialized classifiers and a more unified approach. The CSN is not merely a tool; it is a versatile entity capable of grasping the intricate tapestry of multi-concept relationships. With our experiments, we reveal that CSNs not only achieve state-of-the-art results in fair classification while upholding the principle of concept independence, but they also possess the remarkable ability to transform into hierarchical classifiers. Even more astonishing, they can harmonize the seemingly conflicting ideals of fairness and hierarchy within a single, cohesive framework.  Inspired by the interpretability of prototype-based classifiers, the CSN stands as a testament to what is possible when innovation meets intention. It invites us to envision a future where adaptability and clarity reign supreme, empowering us to navigate the complexities of classification with newfound ease.", "introduction": "Neural networks are able to learn rich representations of data that support highly accurate classification; however, understanding or controlling what neural nets learn remains challenging.Some techniques offer insight into pre-trained models by uncovering directions within latent spaces that correspond to particular concepts, image manipulations, or more (Goetschalckx et al., 2019;Kim et al., 2018), while approaches focused on interpretability provide techniques that are more comprehensible to humans (Li et al., 2018;Chen et al., 2019).While these methods provide insight, they fail to offer control: humans observe learned patterns but are unable to guide models such that learned relationships are useful for a particular setting or task.Another line of work has advanced the design of models for particular types of classification tasks (such as fair or hierarchical classification) but these techniques are often developed with only one problem in mind (Zemel et al., 2016;Xie et al., 2017;Hase et al., 2019).For example, models built for fair classification (predicting an outcome regardless of information about a protected field) are only used to enforce independence of concepts rather than hierarchy.Thus, humans may exert control over learned representations by selecting an appropriate technique rather than tuning training parameters within the same technique.We have designed a new neural network architecture, the concept subspace network (CSN), which generalizes existing specialized classifiers to produce a unified model capable of learning a spectrum of multi-concept relationships.CSNs use prototype-based representations, a technique employed in interpretable neural networks in prior art (Li et al., 2018;Chen et al., 2019;Garnot & Landrieu, 2020).A single CSN uses sets of prototypes in order to simultaneously learn multiple concepts; classification within a single concept (e.g., \"type of animal\") is performed by projecting encodings into a concept subspace defined by the prototypes for that concept (e.g., \"bird,\" \"dog,\" etc.).Lastly, CSNs use a measure of concept subspace alignment to guide concept relationships such as independence or hierarchy.In our experiments, CSNs performed comparably to state-of-the art in fair classification, despite prior methods only being designed for this type of problem.In applying CSNs to hierarchical classification tasks, networks automatically deduced interpretable representations of the hierarchical problem structure, allowing them to outperform state-of-the-art, for a given neural network backbone, in terms of both accuracy and average cost of errors on the CIFAR100 dataset.Lastly, in a human-motion prediction task, we demonstrated how a single CSN could enforce both fairness (to preserve participant privacy) and hierarchy (to exploit a known taxonomy of tasks).Our findings suggest that CSNs may be applied to a wide range of problems that had previously only been addressed individually, or not at all."}
{"paper_id": 253, "abstract": "In the ever-evolving realm of machine learning, the quest for precision in anomaly detection has led us to the intriguing landscape of Generative Adversarial Networks (GANs). Among these, the Bad-GAN has emerged as a beacon of promise, adept at crafting pseudo anomalies from the shadowy fringes of inlier distributions. Yet, like any powerful tool, it is not without its flaws. The existing Bad-GANs often fall prey to a dual curse: they produce anomalies that lack the rich diversity necessary for robust detection, and they sometimes stray too far from the true essence of real anomalies, casting doubt on their generalizability.  In our pursuit of a more refined solution, we introduce Taichi-GAN\u2014a model forged to transcend the limitations of its predecessors. At its core, Taichi-GAN employs a novel orthogonal loss function, designed to regulate the cosine distances among the decentralized samples generated by the Bad-GAN, ensuring a vibrant array of anomalies. Moreover, when the stars align and a few genuine anomaly samples are available, we harness the power of a traditional GAN\u2014our Good-GAN\u2014to draw the pseudo anomalies closer to their authentic counterparts.  Thus, Taichi-GAN emerges as a harmonious synthesis of Good-GAN and Bad-GAN, engaging in a delicate dance of adversarial training. This synergy not only enhances the quality of the generated anomalies but also fortifies the discriminator\u2019s ability to score anomalies with newfound resilience. Our results speak volumes, showcasing significant advancements across a variety of simulated and real-world anomaly detection challenges. In this journey, Taichi-GAN stands as a testament to innovation, guiding us toward a future where anomaly detection is as precise as it is powerful.", "introduction": "Anomaly detection (AD), a.k.a.outlier detection, refers to detecting uncommon samples (usually heterogeneous) out of inlier distribution (ChandolaVarun et al., 2009;Pang et al., 2021a).AD can be performed either in an unsupervised manner when only inliers are involved during the training process (Schlegl et al., 2019;Ngo et al., 2019;Ruff et al., 2018), or in a semi-supervised manner when few labeled anomalies are available (Ruff et al., 2020b;Liznerski et al., 2021).Generative adversarial nets (GAN) (Goodfellow et al., 2014) are considered the most effective generative models nowadays, which have also been applied for AD (Schlegl et al., 2017;2019;Zenati et al., 2018b;Siddiquee et al., 2019;Perera et al., 2019).Likewise, one-class classification and its variants are often used as the backbone of AD approaches (Sch\u00f6lkopf et al., 2001;Ruff et al., 2018;Liznerski et al., 2021;Wu et al., 2019).Recently, the adversarial training and one-class classification have been leveraged together to discriminate inlier/outlier in an end-to-end manner (Zheng et al., 2019;Ngo et al., 2019).Such efforts share a similar spirit with the work introduced in Dai et al. (2017) and here we call them Bad-GANs.Instead of generating samples to match the training data distribution as a conventional GAN (Goodfellow et al., 2014) that we refer as Good-GAN in this paper, a Bad-GAN pushes its generated samples towards the peripheral area of the training data distribution and considers them as pseudo anomalies.Rather than defining anomaly scores based on 1) the residual between reconstructed and original samples such as AnoGAN series (Schlegl et al., 2017;2019), or 2) existing one-class classification-based measures such as Deep SVDD (Ruff et al., 2018) and FCDD (Liznerski et al., 2021), Bad-GANs directly leverage the learned discriminator to identify unseen anomalies from normal cases.Although Bad-GANs have shown great potentials, they suffer from several disadvantages.Dis1: the generated samples can converge to limited patterns rather than being heterogeneous as real anomalies; Dis2: it is hard to ensure generated samples to resemble real anomalies, while abundance of irrelevant pseudo anomalies may negatively affect the discrimination of real anomalies; Dis3: no application has been found to integrate labeled anomalies in prior-art Bad-GANs.In this work, we propose a new Taichi-GAN to address aforementioned disadvantages.First, based on a state-of-the-art Bad-GAN (FenceGAN (Ngo et al., 2019)) framework, an orthogonal loss is proposed to increase the angular diversity, which addresses Dis1, and partially relieves Dis2 by increasing the overlap between generated pseudo anomalies and real anomalies.Second, we are the first to incorporate Good-GAN within a Bad-GAN framework, where the Good-GAN contributes to generating pseudo anomalies guided by few real anomalies, which addresses Dis2 and Dis3.Our model is termed Taichi-GAN, as the Good-GAN and Bad-GAN are integrated in a competing yet balanced manner to generate pseudo anomalies towards better AD.We illustrate our ideas with 2D synthetic data and validate the proposed model with five datasets including clinical applications."}
{"paper_id": 254, "abstract": "In the ever-evolving realm of software development, where collaboration is the lifeblood of innovation, the intricate dance of multiple developers weaving their code together often leads to a formidable foe: the merge conflict. These conflicts, arising from simultaneous alterations to the same lines of code, can halt the momentum of pull requests and bring continuous integration pipelines to a grinding halt, sometimes for hours or even days. Such delays are not just inconveniences; they can severely undermine the productivity of even the most seasoned developers.  Enter MergeBERT, a groundbreaking framework designed to tackle this challenge head-on. Drawing upon the powerful capabilities of a transformer encoder model and employing a token-level three-way differencing approach, MergeBERT reimagines the resolution of merge conflicts. By recognizing the inherent patterns within the chaos, we redefine the problem of generating resolution sequences as a classification task\u2014one that leverages a set of primitive merge patterns gleaned from the rich tapestry of real-world merge commit data.  Our results speak volumes: MergeBERT achieves an impressive accuracy of 63-68% in synthesizing merge resolutions, delivering nearly a threefold enhancement over traditional structured methods and doubling the performance of existing neural program merge tools. But the true magic lies in its versatility; MergeBERT seamlessly adapts to source code written in Java, JavaScript, TypeScript, and C#, and demonstrates a remarkable ability to generalize to previously unseen programming languages without missing a beat. With MergeBERT, we stand on the precipice of a new era in collaborative software development, where the specter of merge conflicts is no longer a hindrance, but a challenge we can confidently overcome.", "introduction": "Collaborative software development relies on version control systems such as git to track changes across files.In most projects, developers work primarily in a branch of a software repository, periodically synchronizing their code changes with the main branch via pull requests (Gousios et al., 2016).When multiple developers make concurrent changes to the same line of code, a merge conflict may occur.According to an empirical study of four large software projects by Zimmermann (2007) up to 46% of all merge commits result in conflicts.Resolving merge conflicts is a time-consuming, complicated, and error-prone activity that requires understanding both the syntax and program semantics, often taking more time than developing a code feature itself (Bird & Zimmermann, 2012).Modern version control systems such as git utilize the diff3 algorithm for performing unstructured line-based three-way merge of input files (Smith, 1998).This algorithm aligns the two-way diffs of two versions of the code A and B over the common base O into a sequence of diff \"slots\".At each slot, a change from either A or B is selected.If both program versions introduce a change at the same slot, a merge conflict is produced, and manual resolution of the conflicting modifications is required.A versatile, production-level merge conflict resolution system should be aware of programming language syntax and semantics yet be sufficiently flexible to work with any source code files, irrespective of the programming language.It should generalize to a wide variety of real-world merge conflicts beyond a specific merge type or a domain of software artifacts.Inspired by the exceptional performance of transformer models and self-supervised pretraining in natural language understanding and generation tasks (Devlin et al., 2019;Radford et al., 2019;Liu et al., 2019;Lewis et al., 2019;Raffel et al., 2020) as well as in the programming language domain (Feng et al., 2020;Svyatkovskiy et al., 2020;Clement et al., 2020;Tufano et al., 2020;Ahmad et al., 2021), we introduce MergeBERT: a neural program merge framework based on token-level three-way differencing and transfer learning.We select a bidirectional transformer encoder (BERT) as our encoder implementation.As a bidirectional encoder, BERT allows to attend to code context surrounding the conflicting chunks, which is a key advantage over left-to-right language models.To endow our model with a basic knowledge of programming language syntax and semantics, we adopt a two-step training procedure: (1) unsupervised masked language model pretraining on a massively multilingual source code corpus, (2) supervised finetuning for the sequence classification task.We transfer weights of the pretrained encoder into a multi-input model architecture that encodes all inputs that a standard diff3 algorithm takes (two two-way diffs of input programs) as well as the edit sequence information, then aggregate them for learning.While MergeBERT utilizes BERT, other encoder architectures such as LSTM (Hochreiter & Schmidhuber, 1997), or efficient transformer variants like Poolingformer (Zhang et al., 2021) could also be utilized for this task.The paper contributions are as follows: (1) we introduce MergeBERT a novel transformer-based program merge framework that leverages token-level three-way differencing and formulates the task of generating the resolution sequence as a classification task over a set of primitive merge patterns extracted from real-world merge commit data, (2) we effectively transfer knowledge about program syntax and types of source code identifiers from millions of software programs to downstream sequence classification task of merge conflict resolution by using self-supervised pretraining (see section 5), which also makes this approach computationally more feasible, (3) we overcome several limitations of the existing neural program merge models Dinella et al. (2021) and semi-structured program merge tools like jsFSTMerge and JDime to improve upon the state-of-the-art by 2-3\u00d7 (see sections 7 and 8), and finally, (4) we demonstrate that multilingual MergeBERT is sufficiently versatile to work with programs in Java, JavaScript, TypeScript, and C#, and can generalize to unseen languages without retraining."}
{"paper_id": 255, "abstract": "In the realm of image retrieval, the quest to find images that resonate with a given query image hinges on the art of feature extraction. Traditionally, approaches have revolved around a two-step process: first, the broad sweep of global features to narrow down the candidates, followed by a meticulous re-ranking using local feature matching. While these methods have shone brightly across various datasets, they are not without their shadows. The local feature matching, for instance, is a voracious consumer of both time and memory. Moreover, the re-ranking phase often diminishes the prominence of global features, and the local feature learning itself suffers from a lack of precision and semantic depth, hampered by overly simplistic designs.  In response to these challenges, we introduce a novel approach: the Unifying Global and Attention-based Local Features Retrieval method, or UGALR for short. This method stands as a beacon of innovation\u2014a seamless, end-to-end pipeline that reimagines the image retrieval process. UGALR harnesses the power of two key advancements: first, it accelerates the speed of feature extraction and trims down memory usage by eliminating the re-ranking phase, opting instead for the efficiency of convolutional neural networks over the cumbersome RANSAC algorithm for local feature matching. Second, it elevates the accuracy and semantic richness of local information by deftly combining spatial and channel attention mechanisms, bolstered by intermediate supervision.  Our rigorous experiments on the Revisited Oxford and Paris datasets have demonstrated the remarkable efficacy of UGALR, achieving state-of-the-art performance that outshines other leading methods. The codes for this groundbreaking work will be made available soon, inviting others to delve into the possibilities we\u2019ve unveiled.", "introduction": "Image retrieval is a classic task in computer vision.Its primary purpose is to search images that are similar to the given image through extracting features.Learning image representations well is the key factor to superior performance for image retrieval.Global and local features are two typical image representations in image retrieval.According to (Cao et al., 2020), global features summarize image content and could learn similarity across very different poses, but lack spatial arrangement information.Local features contain geometric information and descriptors about specific regions, but lack global understanding of image content.For better performance, popular approaches first search with global features, then re-rank with local feature matching.This kind of method could achieve state-of-the-art results according to (Cao et al., 2020) (Taira et al., 2018) (Sarlin et al., 2019).Nevertheless, they have substantial drawbacks when put into practical use.First, the processes of extracting local features, local feature matching, and re-ranking are of low efficiency either in memory or in latency.For example, extracted local features often occupy too much memory.The RANSAC algorithm used in local feature matching is extremely time-consuming and also unstable since it depends heavily on iteration times.What's more, it needs to rank twice to get the final retrieval results and re-ranking with local features would erase the influence of global features in top results; Second, the local attention module is too trivial to capture local information good enough in both accuracy and semantics.Besides, previous works mostly treat attention and features equally (Ng et al., 2020) (Woo et al., 2018) (Hu et al., 2018), but rigit balance factor between attention and features would limit attention learning.A question arises: facing the problem that retrieval results are correct in the category but lack attention to detail, is there any other way to solve it without the downsides mentioned above?Our work gives an appropriate answer.With a different view, we could interpret this problem as that the learned global features are not powerful enough to capture local information, while local fea-tures perform better in distinguish details.Since local feature matching is to find the appropriate homography matrix between images, which is a process of learning linear transformation, we think it could be learned by the convolutional neural network.Based on these ideas, we intend to learn a more powerful global feature that contains merits of local features with the aid of multi-attentive local attention, CNN-based homography transformation, and information fusion.First, we focus on local feature attention.We work on enhancing it in both module design and training strategy.In terms of module design, we proposed a LALM (Location Attention Learning Module) block that contains both spatial attention mechanism and channel attention mechanism to enhance semantics and accuracy.In terms of training strategy, we introduced intermediate supervision into the process of training attention modules with the aim of giving different importance to attention and features.Second, we focus on learning CNN-based homography transformation and information fusion.We concatenate the output of LALM with the global feature branch, let subsequent blocks learn homography transformation in local feature matching, and naturally fuse it with global features.Instead of extracting global and local features respectively from the trained model, we only use the enhanced features that contain both global and local information to perform image retrieval, which reduces time and space costs caused by local features storing, matching, and re-ranking.In general, our main contributions are summarized as follows:\u2022 We innovatively utilised CNN network to learn the homography transformation in local feature matching and generate a more powerful feature that contains both global information and local information.\u2022 We proposed a LALM(Local Attention Learning Module) block which learns local attention in both spatial and channel dimension.We introduced intermediate supervision into image retrieval to help the network treat attention and feature differently and adaptively, which effectively avoids attention degradation.\u2022 We proposed a single-stage model named UGALR(Unify Global and Attention-based Local Features Retrieval methods) that can be trained end-to-end.It achieved state-of-the-art performance on two typical datasets with less memory and faster extraction speed compared with other popular methods."}
{"paper_id": 256, "abstract": "In the ever-evolving landscape of artificial intelligence, Deep Reinforcement Learning (DRL) stands as a beacon of potential, illuminating pathways for solving intricate sequential decision-making challenges across a myriad of applications. Yet, as with all powerful tools, it is not without its shadows. The practical deployment of DRL in the real world is fraught with obstacles, chief among them the specter of overfitting, which can shackle the learned policies, rendering them less adaptable to new environments. This issue is particularly pronounced in the realm of offline DRL, where the absence of a definitive ground truth complicates model selection\u2014a stark contrast to the more forgiving online settings of simulated environments.  In this paper, we unveil a novel approach: the Pessimistic Model Selection (PMS) framework, designed specifically for the offline DRL landscape. Our method offers a theoretical guarantee, providing a tuning-free mechanism to sift through a collection of candidate models and unearth the most effective policy. To further refine our approach, we introduce two innovative strategies aimed at mitigating the biases that often plague DRL models in their quest for optimality. Through rigorous numerical studies, we demonstrate that our PMS approach not only meets the challenges head-on but also outshines existing methodologies, paving the way for more robust and reliable applications of DRL in the real world.", "introduction": "The success of deep reinforcement learning (Mnih et al., 2013;Henderson et al., 2018) (DRL) often leverages upon executive training data with considerable efforts to select effective neural architectures.Deploying online simulation to learn useful representations for DRL is not always realistic and possible especially in some high-stake environments, such as automatic navigation (Kahn et al., 2018;Hase et al., 2020), dialogue learning (Jaques et al., 2020), and clinical applications (Tang et al., 2020a).Offline reinforcement learning (Singh & Sutton, 1996;Levine et al., 2020;Agarwal et al., 2020) (OffRL) has prompted strong interests (Paine et al., 2020;Kidambi et al., 2020) to empower DRL toward training tasks associated with severe potential cost and risks.The idea of OffRL is to train DRL models with only logged data and recorded trajectories.However, with given observational data, designing a successful neural architecture in OffRL is often expensive (Levine et al., 2020), requiring intensive experiments, time, and computing resources.Unlike most aforementioned applications with online interaction, Offline tasks for reinforcement learning often suffer the challenges from insufficient observational data from offline collection to construct a universal approximated model for fully capturing the temporal dynamics.Therefore, relatively few attempts in the literature have been presented for provide a pipeline to automate the development process for model selection and neural architecture search in OffRL settings.Here, model selection refers to selecting the best model (e.g., the policy learned by a trained neural network) among a set of candidate models (e.g.different neural network hyperparameters).In this work, we propose a novel model selection approach to automate OffRL development process, which provides an evaluation mechanism to identify a well-performed DRL model given offline data.Our method utilizes statistical inference to provide uncertainty quantification on value functions trained by different DRL models, based on which a pessimistic idea is incorporated to select the best model/policy.In addition, two refined approaches are further proposed to address the possible biases of DRL models in identifying the optimal policy.In this work, we mainly focus on deep Q-network (Mnih et al., 2013;2015) (DQN) based architectures, while our proposed methods can be extended to other settings.Figure 1 demonstrates the superior performance of the proposed pessimistic model selection (PMS) method in identifying the best model among 70 DRL models of different algorithms on one navigation task (See Appendix C for details), compared with the model selection method by (Tang & Wiens, 2021) which uses three offline policy evaluation (OPE)  (Gottesman et al., 2018); (c) approximate model (AM) (Voloshin et al., 2019); (d) fitted Q evaluation (FQE) (Le et al., 2019).In this figure, the algorithms are trained and evaluated in a navigation task (E2) discussed in Section 7 and Appendix C. estimates for validation.Specifically, based on the derived confidence interval of the OPE value for each candidate model, the final selected model by our PMS method is the one that has the largest lower confidence limit, which exactly has the largest true OPE value among all candidate models.In contrast, none of three OPE estimates used for model selection by Tang & Wiens (2021) can identify the best model due to the inevitable overfitting issue during the validation procedure.To close this section, we summarize the contributions of this work as follows:\u2022 We propose a novel PMS framework, which targets finding the best policy from given candidate models (e.g., neural architecture, hyperparameters, etc) with offline data for DQN learning.Unlike many existing methods, our approach essentially does not involve additional hyperparameter tuning except for two interpretable parameters.\u2022 Leveraging asymptotic analysis in statistical inference, we provide uncertainty quantification on each candidate model, based on which our method can guarantee that the worst performance of finally selected model is the best among all candidate models.See Corollary 1 for more details.\u2022 To address potential biases of candidate models in identifying the optimal policy, two refined approaches are proposed, one of which can be shown to have regret bounded by the smallest error bound among all candidate models under some technical conditions (See Corollary 2).To the best of our knowledge, this is the first model-selection method in offline DRL with such a guarantee.\u2022 The numerical results demonstrate that the proposed PMS shows superior performance in different DQN benchmark environments."}
{"paper_id": 257, "abstract": "In the realm of hyperparameter optimization (HPO), we often find ourselves grappling with a complex bi-level optimization challenge. Here, the task is to construct a probabilistic surrogate model that captures the essence of observed hyperparameter responses, such as validation loss. The true magic lies in the subsequent maximization of an acquisition function, which guides us toward promising hyperparameter candidates ripe for evaluation. Yet, the journey doesn\u2019t end there; we can enhance our choice of surrogate and acquisition functions through the wisdom gleaned from related tasks.  In this paper, we unveil a groundbreaking transfer learning strategy, intricately woven into the fabric of model-based reinforcement learning. We propose an ensemble of probabilistic models that not only serves as our surrogate but also allows for trajectory sampling\u2014an innovation that breathes new life into HPO. Additionally, we introduce a novel variant of model predictive control, employing a straightforward look-ahead strategy. This policy deftly optimizes a sequence of actions that correspond to hyperparameter candidates, effectively accelerating the HPO process.  Our experiments, conducted across three diverse meta-datasets, stand as a testament to our approach's superiority. When pitted against state-of-the-art HPO algorithms, including a model-free reinforcement learning method, our technique consistently outshines the competition. By harnessing the power of a simple planning-based policy, we illuminate a path forward in the intricate landscape of hyperparameter optimization.", "introduction": "Hyperparameter optimization (HPO) is a ubiquitous problem within the research community and an integral aspect of tuning machine learning algorithms to ensure generalization beyond the training data.HPO is often posed as a sequential decision-making process, however, it can be seen as a special use-case of model-based reinforcement learning (MbRL) (Sutton, 1991a;Henaff et al., 2017) developed under the guise of some idiosyncratic terms.In MbRL the objective is to train a transition model to approximate an underlying transition function via interactions with an environment governed by some policy, e.g.random shooting (Nagabandi et al., 2018), to improve sample efficiency and learn from more useful interactions whereby the learned models are used as a simulator for sampling trajectories.An agent navigates the simulated environment to optimize a pre-defined reward function, while the transition model remains unchanged.Conventionally in HPO, a surrogate model is trained to estimate some black-box function, e.g.validation loss of a machine learning algorithm under investigation (Rasmussen, 2003;Snoek et al., 2015b;Springenberg et al., 2016).An acquisition function (Wilson et al., 2017b) interacts with the surrogate model to propose potential hyperparameters that optimize the black-box response, viewed as a reward function.Effectively, the surrogate model is the only unknown component for a transition model, that prevents HPO from being framed fully as MbRL problem.In this paper, we present a novel formulation for HPO defined within the context of MbRL.Namely, we learn an ensemble of probabilistic neural network models (Lakshminarayanan et al., 2017) and show that using model predictive control (MPC) (Kamthe & Deisenroth, 2018) and a novel lookahead variant to navigate the simulated black-box environment, we can outperform conventional Bayesian optimization techniques with heuristic acquisition functions in both transfer and nontransfer learning settings.Thus, we elaborate on the importance of explicit planning in HPO that has been largely overlooked by the community.We also formally define HPO as a Markov decision process (MDP) with a simple, yet novel, state representation as the set of previously evaluated hyperparameters and their corresponding responses.We argue that with a clearly defined transition model, we can replace the acquisition function with a simple policy that maximizes the reward across the simulated trajectories, and achieve better results through proper planning, as shown in Figure 1.LookAhead MPC-5 is our look-ahead strategy with MPC that simulates 5 states ahead.We focus here on the motivation with more details in Section 6.Our main contributions are summarized as i) a formal definition of HPO as an MDP which does not depend on any engineered heuristics, ii) a new transfer learning surrogate model represented by an ensemble of probabilistic neural networks, iii) a novel acquisition function that implements a look-ahead strategy paired with model predictive control, iv) clear motivation that highlights the impact of planning in HPO, which to the best of our knowledge, has never been addressed before."}
{"paper_id": 258, "abstract": "In the vast and intricate landscape of neural networks, the minima of the loss function serve as hidden sanctuaries\u2014local optimal sets of weights that deftly extract and process the myriad threads of input data, weaving them into predictions of outcomes. Yet, within the realm of underparameterized networks, these weights often lack the capacity to encompass every nuance of the information at hand.   Our exploration reveals a fascinating truth: distinct local minima develop unique specializations, each honing in on different facets of the learning conundrum and interpreting input data through their own lens. This phenomenon can be harnessed through the creation of a meta-network\u2014an ingenious construct that amalgamates the predictive prowess of multiple minima from the loss function landscape, culminating in a more formidable classifier.   With this innovative approach, we observe a remarkable enhancement in performance, boosting the area under the receiver operating characteristic curve (AUC) by approximately 20% in the face of a complex learning challenge. We lay forth a theoretical framework for the synthesis of these minima, illustrating how a meta-network can be meticulously trained to discern which representative to invoke for the classification of a given data item.   Lastly, we delve into an analysis of symmetry-equivalent solutions to machine learning dilemmas, offering a systematic avenue to amplify the efficiency of our methodology. In this confluence of theory and practice, we unveil a path toward greater understanding and capability within the realm of neural networks.", "introduction": "Deep learning with neural networks (NNs) is a high-dimensional, non-convex optimisation problem for a loss function landscape (LFL).The coordinates of a minimum in the LFL are a set of weights for the machine learning model and a locally optimal solution to the learning problem, and these terms will therefore be used interchangeably throughout.It follows that the coordinates of the global minimum of the LFL are the weights that produce the lowest possible value of the loss function for the training data.The aim of machine learning is usually for the model to find a set of weights that fit the training data, but also generalise well to unseen testing data.Our approach extends this view.Instead of looking at just one minimum of the LFL, we are interested in the expressive power of multiple minima.To analyse how different minima extract and process information from the input data, we survey numerous low-lying minima of the LFL.Here, we employ tools from the energy landscape approach (Wales, 2003) to gain new insight into machine learning LFLs (Ballard et al., 2017).We note that the concept of a minimum is somewhat abstract in machine learning landscapes compared to molecular systems.While in a molecular energy landscape only minima provide valid configurations for a stable molecule, this restriction does not apply to LFLs for machine learning.In fact, some low-lying non-minima will have a smaller loss value and higher classification accuracy than a high-lying minimum.Here, we are interested in developing a better understanding of the capacity of diverse minima of the LFL, and showing that by combining the expressive power of different minima, we can build a better classifier.The compact form of this predictor provides a balance between accuracy and efficiency as required in applications where evaluation is a computational bottleneck.1.1 BACKGROUND Machine learning models are structurally limited in the amount of data they can fit: their capacity is finite.The most commonly known measure of capacity is perhaps the Vapnik-Chervonenkis (VC) dimension (Vapnik & Chervonenkis, 1971;Vapnik et al., 1994).The higher the VC dimension, the more complex are the data can be fitted.More rigorously, VC dimension is defined as the largest cardinality of a set of data points that the NN can shatter (for our purpose, shatter means classify correctly).Thus, the weights of an underparameterised model (i.e.fewer parameters than training data points) may be incapable of fitting the entire test data set, but instead fit just parts of it.The approach we employ to study the expressive power of combinations of individual minima is a variation of ensemble learning, where the results of multiple different predictors are combined to improve the overall accuracy of an approximation problem (Dong et al., 2020).The idea of combining multiple sources of information, specifically the output predictions of multiple classifiers, has been considered for over two decades (Breiman, 1996;Hashem, 1997;Jin & Lu, 2009).Two of the most important questions in ensemble learning are: which classifiers to consider, and how to combine the individual predictions (Wang, 2008).For a detailed review see Kuncheva (2014)."}
{"paper_id": 259, "abstract": "In the realm of neural network classifiers, one metric reigns supreme: the area under the receiver operating characteristic curve, or AUC. Yet, despite its prominence, the direct optimization of AUC as a loss function during training remains an uncharted territory. In this exploration, we embark on a journey to compare the traditional minimization of cross-entropy (CE) loss against the audacious pursuit of optimizing AUC itself.   Our investigation delves deep into the loss function landscape (LFL) of approximate AUC (appAUC) loss functions, revealing the intricate architecture of this solution space. We scrutinize various surrogates for AUC approximation, illuminating their unique characteristics and divergences. What we uncover is striking: the appAUC landscape is a vastly different realm compared to that of CE. While the approximate AUC loss function does indeed enhance the testing AUC, it presents a landscape teeming with more minima\u2014yet these minima are frail, exhibiting greater average Hessian eigenvalues.  To ground our findings in theory, we provide a robust foundation that elucidates these phenomena. Finally, we broaden our scope, offering insights on how the LFL can serve as a beacon, guiding the analysis and selection of loss functions in future endeavors. In this way, we hope to illuminate paths previously obscured, revealing the potential for deeper understanding and innovation in the field of neural network optimization.", "introduction": "The area under the curve of the receiver operating characteristic curve (AUC) is a commonly used method to evaluate the accuracy and reliability of a neural network classifier.However, for mathematical reasons, the AUC cannot be used as as the loss function to be minimised during neural network training (Menon & Elkan, 2011).Instead, other functions such as cross-entropy are commonly employed.The stepwise nature of the AUC function is the reason that it is nondifferentiable and hence cannot simply be optimised.However, the AUC can be approximated using surrogate losses such as the sigmoid function.This approach can lead to a formulation equivalent to the soft-AUC described by Calders & Jaroszewicz (2007), which is referred to here as appAUC.We find it intuitive to optimise a function that it as close as possible to the one used to evaluate the model.Our main contributions in this paper are therefore:\u2022 To understand the use of different approximation to the AUC as the loss function employed in training of a neural network \u2022 To explore the organisation of the appAUC landscape and compare it to a 'standard' crossentropy landscape \u2022 A theoretical foundation of the differences between appAUC and cross-entropy landscapes \u2022 To outline how a loss function landscape analysis can be useful for loss function selection To better understand the advantages and disadvantages of the approximated AUC loss function, we study the functional space, commonly referred to as LFL, using tools from the theoretical study of energy landscapes in molecular and condensed matter systems (Wales, 2003).The usefulness of the energy landscape approach has previously been demonstrated in the context of neural network LFLs (Ballard et al., 2017).We will employ methods from this approach to gain insights about geometric features of the LFL, including the number of minima, their curvatures, and their connectivity.By repeatedly surveying large parts of the LFL, we are, with high probability, able to find the true global minimum.Additionally, due to a Metropolis criterion in the global optimisation approach described below, we do not get stuck in a local minimum, and explore the full LFL, hence learning more about the functional surface.Instead of a single minimum, we aim to find a large number of minima that, together with transition states, provide a faithful coarse-grained representation of the loss function landscape.We believe that this approach will yield valuable insights into the use of appAUC as a loss function in neural networks.Various interesting questions arise from the use of an appAUC loss function.Besides a comparison of properties between landscapes, and the effects of hyperparameter changes, we are especially interested in the differences between appAUC and CE landscapes.Both loss functions, for the same neural network architecture, address the learning problem of finding a mapping f from input data to class label.Does this common foundation imply that minima of the CE loss function are also minima of the appAUC function, or are they at least very similar to each other?We will show below that this condition does not hold, and explain why.Furthermore, to the best of our knowledge, there has been no previous research into the functional properties of AUC surrogates.We believe that quantifying inherent, geometric properties of loss functions will provide a more fundamental understanding of the applicability of particular loss functions to distinct machine learning problems."}
{"paper_id": 260, "abstract": "The quest to unravel a textured 3D mesh from the depths of a single image presents a formidable challenge, particularly when faced with the unpredictable nature of real-world objects that lack definitive 3D references. Previous endeavors have leaned on the crutch of weak supervision, relying on 2D silhouette annotations derived from monocular images. However, this approach creates a dissonance between the 2D guidance and the 3D output, inadvertently skewing the reconstruction towards the visible aspects of the mesh while neglecting the holistic integrity of the model. Though various hand-crafted heuristics have emerged in attempts to bridge this gap, the problem remains stubbornly unresolved.   In this paper, we unveil a novel framework, **MeshInversion**, which seeks to close this divide by harnessing the power of a **generative prior** from a 3D GAN pre-trained specifically for the synthesis of textured meshes. Our method embarks on a journey through the latent space of the 3D GAN, meticulously searching for representations that align with the target mesh as dictated by the single-view observation. This pre-trained GAN is imbued with a wealth of 3D semantics, capturing the intricacies of mesh geometry and texture, thus allowing us to navigate its manifold in a way that inherently regularizes the authenticity and fidelity of our reconstructions.   Crucially, this regularization operates directly within the 3D realm, offering indispensable guidance for mesh components that remain hidden from the 2D perspective. Our experiments, conducted on established benchmarks, demonstrate that **MeshInversion** yields remarkably faithful 3D reconstructions, maintaining consistent geometry and texture across both visible and occluded regions. Furthermore, it showcases impressive adaptability, successfully generalizing to meshes that are less frequently encountered, including the extended articulations of deformable objects.", "introduction": "Single-view 3D object reconstruction aims at recovering the shape and texture of the object from a single image.This long-standing problem is fundamental to various applications such as robotics navigation, 3D scene understanding, and augmented/virtual reality.A key challenge is the lack of 3D or multi-view supervision due to the prohibitive cost of data collection and annotation for object instances in the wild.Leveraging more readily available 2D supervisions such as keypoints, Kanazawa et al. (2018b) first introduce CMR, a deep learning framework of textured mesh reconstruction that is trained with a collection of real-world monocular images of an object category.To further relax the supervision constraint, several follow-up studies propose to learn the 3D manifold in a self-supervised manner, only requiring single-view images and their corresponding masks for training (Li et al., 2020;Goel et al., 2020;Bhattad et al., 2021;Hu et al., 2021).This further complicates the problem as minimizing the reconstruction error in the 2D domain often tends to ignore the overall 3D geometry and back-side appearance, which may end up with a shortcut solution that looks plausible only from the input viewpoint.While these methods compensate the relaxed supervision by exploiting various forms of prior information in the CMR-like framework, e.g., semantic invariance in the UV space (Li et al., 2020) and interpolated consistency of the predicted 3D attributes (Hu et al., 2021), this task remains challenging and worth further exploration.In this work, we propose an alternative approach that is built upon generative prior possessed by Generative Adversarial Networks (GANs), named MeshInversion.GANs are typically known for their exceptional ability to capture comprehensive knowledge (Brock et al., 2019;Karras et al., 2019;Shu et al., 2019), empowering the success of GAN inversion in various image restoration tasks (Gu et al., 2020;Pan et al., 2020b) and in point cloud completion (Zhang et al., 2021).By training a GAN to synthesize 3D shapes in the form of a topology-aligned texture and deformation map, one could enable the generator to capture rich prior knowledge of a certain object category, including high-level semantics, object geometries, and texture details.We, therefore, propose to exploit this appealing generative prior through GAN inversion.Formally, our framework finds the latent code of the GAN manifold that best recovers the 3D object corresponding to the input image with a pre-trained 3D GAN.Given the single-view observation, the latent code is updated towards minimizing 2D reconstruction losses by rendering the 3D object onto the image plane.Hence, the 3D GAN manifold implicitly constrains the reconstructed 3D shape within the realistic boundaries, whereas minimization of the existing 2D losses explicitly drives the 3D shape towards a faithful reflection of the input image.Searching for the optimal latent code in the GAN manifold for single-view 3D object reconstruction is not trivial.Specifically, reprojection misalignment easily leads to a local minimum, and the impact of discretization-induced information loss is amplified by the smooth manifold.To address the misalignment issue, we propose a Chamfer Texture Loss, which relaxes the one-to-one pixel correspondences in existing low-level losses and allows the match to be found within a local region.By jointly considering the appearance and positions of image pixels or feature vectors, it provides a robust appearance distance despite inaccurate camera poses and in the presence of high-frequency textures.To avoid information loss, we propose a Chamfer Mask Loss, which intercepts the rasterization process and computes the Chamfer distance between the projected vertices before discretization to retain information, with the foreground pixels of the input image projected to the same continuous space.Hence, it becomes more sensitive to small variations in shape and offers a more accurate gradient for geometric learning.Our proposed method demonstrates compelling performance for 3D reconstruction from real-world monocular images.Overall, MeshInversion gives highly plausible and faithful 3D reconstruction in terms of both appearance and 3D shape.It achieves state-of-the-art results on the perceptual metric, i.e., FID, when evaluating the textured mesh from various viewpoints, and is on-par with the existing CMR-based methods in terms of geometric accuracy.In addition, MeshInversion benefits from a holistic understanding of the objects given the generative prior.As a result, it not only gives a realistic recovery of the back-side texture but also generalizes well in the presence of occlusion.Furthermore, MeshInversion also shows fairly remarkable generalization for 3D shapes that are less commonly seen, such as birds with open wings and long tails.Some representative reconstruction results are depicted in Fig. 1.Single-view 3D Reconstruction.It recovers the 3D information of an object, such as its shape and texture, from a single-view observation.Several methods use image-3D object pairs (Wang et al., 2018;Pan et al., 2019;Mescheder et al., 2019;Rematas et al., 2021) or multi-view images (Niemeyer et al., 2020;Liu et al., 2019;Yariv et al., 2020;Wang et al., 2021;Oechsle et al., 2021) for training, which limit the scenarios to synthetic data.Another line of work fits the parameters of a 3D prior morphable model, e.g., SMPL for humans and 3DMM for faces (Gecer et al., 2019;Sanyal et al., 2019;Kanazawa et al., 2018a), which are expensive to build and difficult to extend to many different natural object categories.To relax the constraints on supervision, CMR (Kanazawa et al., 2018b) reconstructs category-specific textured mesh by training with a collection of monocular images and associated 2D supervisions, i.e., 2D key-points, camera poses, and silhouette masks.Thereafter, several follow-up works further relax the supervision, e.g., masks only, and improves the reconstruction results mainly by exploiting forms of prior information across the object category or specific to the CMR-like framework.Specifically, they incorporate the prior by enforcing various types of cycle consistencies, such as texture cycle consistency (Li et al., 2020;Bhattad et al., 2021), rotation adversarial cycle consistency (Bhattad et al., 2021), and interpolated consistency (Hu et al., 2021).Some of these methods also leverage external information, e.g., category-level mesh templates (Goel et al., 2020;Bhattad et al., 2021), and semantic parts provided by an external SCOPS model (Li et al., 2020).For texture modeling, a direct regression of pixel values in the UV texture map often leads to blurry images, e.g., Goel et al. (2020).Therefore, the mainstream approach is to regress pixel coordinates, i.e., learning texture flow from the input image to the texture map.Although texture flow is easier to regress and usually provides a vivid front view result, it is often unable to capture the high-level semantics, and thus fails to generalize well to novel views or occluded regions.Our approach directly predicts the texture pixel values by incorporating a pre-trained GAN.In contrast to the texture flow approach, it benefits from a holistic understanding of the objects given the generative prior and offers high plausibility and fidelity at the same time.GAN Inversion.A well-trained GAN usually captures useful statistics and semantics underlying the training data.In the 2D domain, GAN prior has been explored extensively in various image restoration and editing tasks (Bau et al., 2019b;Pan et al., 2020b;Gu et al., 2020).GAN inversion, the common method in this line of work, finds a latent code that best reconstructs the given image using the pre-trained generator.Typically, the target latent code can be obtained via gradient descent (Ma et al., 2018;Lipton & Tripathi, 2017), projected by an additive encoder that learns the inverse mapping of a GAN (Bau et al., 2019a), or a combination of them (Zhu et al., 2020).Recently, there are attempts to apply GAN inversion in the 3D domain.Zhang et al. (2021) use a pre-trained point cloud GAN to address shape completion in the canonical pose, giving remarkable generalization for out-of-domain data such as real-world partial scans.Pan et al. (2020a) first explore to recover the geometric cues from pre-trained 2D GANs and achieve exceptional reconstruction results, but the reconstructed shapes are limited to 2.5D due to limited poses that 2D GANs can synthesize.In this work, we directly exploit the prior from a 3D GAN to reconstruct the shape and texture of complete 3D objects."}
{"paper_id": 261, "abstract": "In the ever-expanding realm of machine learning, where datasets burgeon like the vast, uncharted territories of a fantasy world, the ability to generalize to unseen regions of data remains a quest of utmost importance. This endeavor, by its very nature, is fraught with uncertainty, guided not by the sheer volume of data but by the inductive biases that shape a learner's understanding of the world. Unfortunately, the biases of machine learning systems often diverge from our own, leading to extrapolations that can feel as alien as a creature from a distant realm.   In our exploration, we delve into two distinct realms of inductive bias: the feature-level bias, which dictates the ease with which certain features are learned, and the exemplar-vs-rule bias, which governs how these learned features are wielded in the art of generalization. While the nuances of exemplar- versus rule-based generalization have been the subject of much scrutiny in the annals of cognitive psychology, we introduce a novel protocol, inspired by these empirical explorations, designed to probe this intricate trade-off within learning systems.  Our proposed measures illuminate the shifts in extrapolation behavior as we manipulate feature coverage in a combinatorial landscape. Through a series of empirical investigations spanning a diverse array of models and both illustrative and real-world domains\u2014encompassing the realms of image and language\u2014we unveil that a nuanced understanding of the exemplar-rule trade-off, when set against the backdrop of feature-level bias, offers a richer narrative of extrapolation behavior than existing frameworks provide.   Our findings reveal a striking tendency among standard neural network models towards exemplar-based extrapolation, prompting a deeper discussion on the implications of these insights for the realms of data augmentation, fairness, and the pursuit of systematic generalization. In this ever-evolving journey, we stand at the cusp of understanding how to better align our machine learning constructs with the intricate tapestry of human cognition.", "introduction": "Extrapolation or generalization-decisions on unseen datapoints-is always underdetermined by data; which particular extrapolation behavior an algorithm exhibits is determined by the algorithm's inductive biases (Mitchell, 1980).For modern deep learning systems, these inductive biases often deviate from those in humans.When the inductive biases of ML systems are opaque, and guarantees on extrapolation are not possible-as is often the case with many modern ML systems (D'Amour et al., 2020)-we can instead turn to empirical study of the behavior of a system to derive principles about the system's operation.Cognitive psychology provides a rich basis for experimental designs to study the often-opaque human cognitive system via its external behavior.These can be leveraged to distinguish between competing hypotheses about a machine learning system's inductive biases in the same manner (Ritter et al., 2017b;Lake et al., 2018;Dasgupta et al., 2019).In this paper, we draw on methods from cognitive psychology to define a protocol that teases apart the different inductive biases that go into informing how an opaque learning system extrapolates outside its training distribution.We focus in particular on combinatorial generalization for classification in the presence of spurious correlation.Our protocol goes significantly beyond existing work by controlling for various confounds.We isolate two distinct kinds of inductive bias-feature-level bias and exemplar-rule bias-that have different effects on model extrapolation.We examine these inductive biases across models in an expository points-in-a-plane setting, as well as in naturalistic image and language domains.Finally, we discuss the implications of these inductive biases and their relation to previous work on data augmentation and spurious correlation.Feature-level bias measures which features a system finds easier or harder to learn.This informs which feature a system will generalize on the basis of when both features are correlated or confounded.This kind of feature-level bias has been studied extensively in human cognition (Landau et al., 1988;Hudson Kam & Newport, 2005).There has also been recent work-directly inspired by theseobservations ratio of predictions condition training examples extrapolation humans rule-based exemplar-based (shape-biased) (no feature bias) (no feature bias)cue conflict \"dax\" \"fep\" ?\"dax\" \"fep\" \"dax\" \"fep\" \"dax\" \"fep\" zero shot \"dax\" \"fep\" ?\"dax\" \"fep\" \"dax\" \"fep\" \"dax\" \"fep\" partial exposure \"dax\" \"fep\" \"fep\" ?\"dax\" \"fep\" \"dax\" \"fep\" \"dax\" \"fep\" cognitive psychology studies-that examines similar biases in artificial neural networks, most notably the \"shape-bias\", the tendency to generalize image category labels according to shape rather than according to color or texture (Ritter et al., 2017a;Hermann et al., 2019;Geirhos et al., 2018).While there exists previous work examining specific feature biases in deep learning, we present a more general measure of feature-level bias as well as demonstrate how it interacts with-but is distinct from-another kind of inductive bias, viz.exemplar-vs-rule bias.Exemplar-vs-rule bias measures how a system uses features to inform decisions by trading off between exemplar-and rule-based generalization.A rule-based categorization decision is made on the basis of minimal features that support the category boundary (e.g., Ashby & Townsend, 1986), while an exemplar-based decision-maker generalizes a category on the basis of similarity to category exemplars (e.g., Shepard & Chang, 1963), and therefore may invoke many or all features that underlie a category.Extensive empirical work in cognitive psychology has found evidence of both kinds of generalization in humans (Nosofsky et al., 1989;Rips, 1989;Allen & Brooks, 1991;Rips & Collins, 1993;Smith & Sloman, 1994).This trade-off can be understood intuitively as a continuum that varies the number of features employed to discriminate between categories (Pothos, 2005). 1 This continuum also plays a role in representation learning systems such as deep neural networks (Hinton & Salakhutdinov, 2006), where feature selection is automated."}
{"paper_id": 262, "abstract": "In the intricate realm of model-based reinforcement learning (RL), a familiar pattern emerges: first, we build a model from the data we've gathered, and then we leverage that model for RL or planning. Yet, there lies a paradox at the heart of this process. Models that excel in training\u2014boasting low mean squared error (MSE)\u2014do not always translate into superior control. The RL agent, in its relentless pursuit of mastery, may gravitate toward those elusive states where the model falters, or it might maneuver in ways that cleverly conceal the model's inaccuracies. This dissonance, as highlighted in previous studies, reveals a fundamental mismatch in objectives: while models are crafted to enhance their precision, their true value lies in the effectiveness of the policies they inform.  In this work, we propose a novel solution: a unified objective that harmonizes the training of both the model and the policy. This synergy ensures that updates to either component bolster a lower bound on expected returns, effectively bridging the gap that has long plagued the field. Our objective serves as a global lower bound on expected returns, tightening under specific assumptions to enhance its utility. The algorithm we introduce, MnM, draws inspiration from the mechanics of a Generative Adversarial Network (GAN). Here, a classifier discerns between genuine and fabricated transitions, prompting the model to refine its outputs to mirror reality, while the policy evolves to sidestep regions where the model's predictions falter. In this way, we forge a path toward a more coherent and effective integration of model and policy, paving the way for advancements in RL that are as profound as they are necessary.", "introduction": "Much of the appeal of model-based RL is that model learning is a simple and scalable supervised learning problem.Unfortunately, the accuracy of the learned model does not directly correlate with whether the model-based RL algorithm will receive high reward (Farahmand et al., 2017;Lambert et al., 2020).For example, a model might make small mistakes in critical states that cause a policy to take suboptimal actions.Alternatively, a model with large errors may yield a policy that attains high return if the model errors occur in states that the policy never visits.The underlying problem is that dynamics models are trained differently from how they are used.Typical model-based methods train a model using data sampled from the real dynamics (e.g., using maximum likelihood), but apply these models by using data sampled from the learned dynamics (Deisenroth & Rasmussen, 2011;Williams et al., 2017;Janner et al., 2019;Hafner et al., 2019).Prior work has identified this objective mismatch issue (Farahmand et al., 2017;Luo et al., 2019;Lambert et al., 2020): the model is trained using one objective, but the policy is trained using a different objective.Designing an objective for model training that is guaranteed to improve the expected reward remains an open problem.This paper aims to answer the following question: How should we train a dynamics model so that it produces high-return policies when used for model-based RL?In this paper, we propose a model-based RL algorithm where the model and policy are jointly optimized with respect to the same objective.Our objective is a lower bound on the expected return under the true environment dynamics; a slightly more complicated version of this lower bound becomes tight under certain assumptions.Structurally, our algorithm resembles a generative adversarial network (a GAN), in that the model is trained using a discriminator that distinguishes between real and fake transitions.This same discriminator is included in the objective for the policy, and both the model and policy are jointly trained to maximize reward and minimize discriminator accuracy.Thus, the model and policy cooperate to produce realistic and high-reward trajectories.Our method stands in contrast to standard model-based RL methods, where it is more common to pit the model against the policy (Bagnell et al., 2001;Nilim & El Ghaoui, 2003;Ross & Bagnell, 2012).An consequence of maximizing the lower bound is that the dynamics model does not learn the true dynamics, but rather learns optimistic dynamics that facilitate exploration.The main contribution of this work is an algorithm, Mismatched no More (MnM), for model-based RL that provably maximizes a lower bound on expected reward.Importantly, this bound becomes tight at optimality under certain assumptions.To the best of our knowledge, this is the first model-based RL objective that is a global lower bound on expected return, and that involves optimizing the model and policy using the same objective.Our algorithm has the unique property of jointly optimizing the policy and model using the same objective.Across a range of tasks, we demonstrate that our method is competitive with prior state-of-the-art methods on benchmark tasks; on certain hard exploration tasks, our method outperforms prior methods based on maximum likelihood estimation."}
{"paper_id": 263, "abstract": "In the vast and intricate realm of Reinforcement Learning (RL), challenges such as meta RL, robust RL, and the quest for generalization often manifest as POMDPs\u2014Partially Observable Markov Decision Processes. Theoretically, the solution seems straightforward: by infusing model-free RL with the power of memory, particularly through recurrent neural networks, one could tackle the myriad complexities of POMDPs. Yet, the landscape has shown us a different truth. Previous investigations have revealed that these recurrent model-free methods frequently lag behind their more specialized counterparts, which are meticulously crafted for specific POMDP scenarios.  In this paper, we embark on a journey to reassess this prevailing notion. Our exploration uncovers a pivotal insight: with a thoughtful approach to architecture and hyperparameter tuning, a recurrent model-free RL implementation can not only match but, in some instances, surpass the performance of more advanced techniques tailored to their particular domains. To foster further exploration and innovation in this field, we also present an accessible and efficient implementation of our recurrent model-free RL approach, establishing a robust baseline for future endeavors in the realm of POMDPs.", "introduction": "Figure 1: Implementation Matters for Recurrent Model-Free RL.This paper identifies critical design decisions for recurrent model-free RL that outperforms not only prior implementations (e.g.PPO-GRU and A2C-GRU from Kostrikov (2018)), but also purposedesigned methods (e.g.VRM from Han et al. (2020)).We also show Markovian policies as lower bounds for reference.The y-axis is normalized return given the return of oracle policy (Raffin et al., 2021).While reinforcement learning (RL) is often cast as the problem of learning a single fully observable task, also known as MDP, training and testing on that same task, most real-world applications of RL demand some degree of transfer and partial observability.For example, visual navigation (Zhu et al., 2017) requires adaptation to unseen scenes with occlusion in observations, and human-robot collaboration requires that robots infer the intentions of human collaborators.(Chen et al., 2018).Many subareas in RL study problems that are special cases of POMDPs, and we summarize them in Table 1.For example, meta RL (Duan et al., 2016;Schmidhuber, 1987;Thrun & Pratt, 2012;Wang et al., 2017) is a POMDP where certain aspects of the reward function or (less commonly) dynamics function are unobserved but held constant through one episode.The robust RL problem (Bagnell et al., 2001;Pattanaik et al., 2018;Pinto et al., 2017;Rajeswaran et al., 2017a) assumes that certain aspects of the dynamics or reward function are unknown, aiming at finding optimal policies that perform against adversarially-chosen perturbations.Generalization in RL (Cobbe et al., 2019;Packer et al., 2018;Whiteson et al., 2011;Zhang et al., 2018a) focuses on unobserved aspects of the dynamics or reward function that are novel during testing, using an average-case objective instead of a worst-case objective like robust RL.Recent work has proposed efficient and performant algorithms for solving these specialized problem settings.However, these algorithms often make assumptions that preclude their application to other classes of POMDPs.For example, methods for robust RL are rarely used for the meta RL setting due to objective mismatch; methods for meta RL are rarely used for general POMDPs due to the stationarity assumption in meta RL.Nonetheless, many prior works have used a simple baseline that is applicable to all POMDPs: model-free RL equipped with a recurrent policy and (sometimes) value function (Duan et al., 2016;Fakoor et al., 2020;Igl et al., 2018;Packer et al., 2018;Rakelly et al., 2019;Wang et al., 2017;Yu et al., 2019).We will refer to this approach as recurrent model-free RL.This baseline is simultaneously simple (requiring changing only a few lines of code from a model-free RL algorithm) and general.However, prior work has consistently found that recurrent model-free RL performs poorly across a wide range of problem settings, including meta RL (Rakelly et al., 2019;Zintgraf et al., 2020), general POMDPs (Han et al., 2020;Igl et al., 2018), robust RL (Zhang et al., 2021), and generalization in RL (Packer et al., 2018).One common explanation is that specialized algorithms that are tailored to specific types of POMDPs are very likely to outperform recurrent model-free RL because they (implicitly) encode inductive biases for solving these specific tasks.For example, algorithms for meta RL may leverage the assumption that the underlying dynamics (while unknown) are fixed, and the underlying goals are fixed within one episode (Rakelly et al., 2019;Zintgraf et al., 2020); algorithms for robust RL may assume that the dynamics parameters are known (Rajeswaran et al., 2017a) and dynamics is Lipschitz continuous (Jiang et al., 2021).This paper challenges this explanation.We argue that, contrary to popular belief, recurrent modelfree RL is competitive with recent state-of-the-art algorithms across a range of different POMDP settings.Similar to prior work in Markovian on-policy RL methods (Andrychowicz et al., 2021;Engstrom et al., 2020), our experiments show that implementation in recurrent model-free RL matters.Fig. 1 shows a typical scenario in PyBullet occlusion environments (Coumans & Bai, 2016) to support this argument.Through extensive experiments, we show that the careful design and implementation of recurrent model-free RL is critical to its performance.Design decisions, such as the actor-critic architecture, conditioning on previous actions and/or rewards, the underlying model-free RL algorithms, and context length in RNNs, are especially crucial.The main contribution of this paper is a performant implementation of recurrent-model free RL.We demonstrate that simple yet important design decisions, such as the underlying RL algorithm and the context length, yield a recurrent model-free RL algorithm that performs on par with prior specialized POMDP algorithms on the environments those algorithms were designed to solve.Ablation experiments identify the importance of these design decisions.We also open-sourced our code that is easy to use and memory-efficient."}
{"paper_id": 264, "abstract": "In the realm of data representation, attention mechanisms serve as a beacon, illuminating the most significant features by weighing their importance through probabilistic expectations. Recent explorations by Martins et al. (2020, 2021) have unveiled a novel approach with continuous attention mechanisms, honing in on unimodal attention densities derived from the exponential and its deformed cousin\u2014where the latter exhibits a remarkable sparsity of support. Building on this foundation, Farinhas et al. (2021) introduced Gaussian mixture attention densities, a versatile class that boasts dense support and adaptability.  In this paper, we embark on an ambitious journey to broaden the horizons of attention mechanisms by introducing two new flexible classes: the kernel exponential families and our innovative counterpart, the kernel deformed exponential families. Through rigorous theoretical exploration, we unveil new existence results for both families, revealing that the deformed variant possesses approximation capabilities akin to those of its kernel exponential relatives. Our experimental findings further illuminate the potential of kernel deformed exponential families, showcasing their ability to deftly attend to non-overlapping intervals of time. In this way, we push the boundaries of understanding in the field, paving the way for future innovations in attention mechanisms.", "introduction": "Attention mechanisms take weighted averages of data representations (Bahdanau et al., 2015), where the weights are a function of input objects.These are then used as inputs for prediction.Discrete attention 1) cannot easily handle data where observations are irregularly spaced 2) attention maps may be scattered, lacking focus.Martins et al. (2020;2021) extended attention to continuous settings, showing that attention densities maximize the regularized expectation of a function of the data location (i.e.time).Special case solutions lead to exponential and deformed exponential families: the latter has sparse support.They form a continuous data representation and take expectations with respect to attention densities.Using measure theory to unify discrete and continuous approaches, they show transformer self-attention (Vaswani et al., 2017) is a special case of their formulation.Martins et al. (2020;2021) explored unimodal attention densities: these only give high importance to one region of data.Farinhas et al. (2021) extended this to use multi-modal mixture of Gaussian attention densities.However 1) mixture of Gaussians do not lie in either the exponential or deformed exponential families, and are difficult to study in the context of Martins et al. (2020;2021) 2) they have dense support.Sparse support can say that certain regions of data do not matter: a region of time has no effect on class probabilities, or a region of an image is not some object.We would like to use multimodal exponential and deformed exponential family attention densities, and understand how Farinhas et al. (2021) relates to the optimization problem of Martins et al. (2020;2021).This paper makes three contributions: 1) we introduce kernel deformed exponential families, a multimodal class of densities with sparse support and apply it along with the multimodal kernel exponential families (Canu & Smola, 2006) to attention mechanisms.The latter have been used for density estimation, but not weighting data importance 2) we theoretically analyze normalization for both kernel exponential and deformed exponential families in terms of a base density and kernel, and show approximation properties for the latter 3) we apply them to real world datasets and show that kernel deformed exponential families learn flexible continuous attention densities with sparse support.Approximation properties for the kernel deformed are challenging: similar kernel exponential family results (Sriperumbudur et al., 2017) relied on standard exponential and logarithm properties to bound the difference of the log-partition functional at two functions: these do not hold for deformed analogues.We provide similar bounds via the functional mean value theorem along with bounding the Frechet derivative of the deformed log-partition functional.The paper is organized as follows: we review continuous attention (Martins et al., 2020;2021).We then describe how mixture of Gaussian attention densities, used in Farinhas et al. (2021), solve a different optimization problem.We next describe kernel exponential families and give novel normalization condition relating the kernel growth to the base density's tail decay.We then propose kernel deformed exponential families, which can have support over disjoint regions.We describe normalization and prove approximation capabilities.Next we describe use of these densities for continuous attention, including experiments where we show that the kernel deformed case learns multimodal attention densities with sparse support.We conclude with limitations and future work."}
{"paper_id": 265, "abstract": "In the ever-evolving realm of machine learning, the Invariant Risk Minimization (IRM) principle emerged like a beacon of hope for tackling the elusive domain generalization challenge, first introduced by Arjovsky et al. in 2019. This principle boldly strives to unearth a data representation that ensures an optimal classifier maintains its steadfastness across diverse domains\u2014each shaped by its unique experimental conditions. Yet, as the tides of research have turned, the originally proposed invariance penalty has faced scrutiny, its efficacy questioned by stylized experiments and counterexamples that reveal its vulnerabilities.  In our exploration, we delve deep into the intricate interplay between data representation, invariance penalty, and risk. With the insight of a seasoned scholar, we introduce a novel invariance penalty, crafting an adaptive rule that fine-tunes the coefficient of the original penalty proposed by Arjovsky et al. (2019). But we do not stop there; we also provide practical guidance to navigate the pitfalls highlighted by recent counterexamples that threaten the IRM\u2019s promise.  Our journey culminates in a series of numerical experiments, traversing both synthetic landscapes and the complexities of real-world data. Among our endeavors, we set our sights on a particularly pressing challenge: predicting human health status. Drawing from a diverse array of studies that illuminate the connection between the human gut microbiome and specific diseases, we validate the potency of our proposed approach. Through these efforts, we not only substantiate the effectiveness of our innovations but also pave the way for the broader adoption of the IRM principle across various real-world applications, igniting new possibilities in the ever-expanding universe of machine learning.", "introduction": "Under the learning paradigm of Empirical Risk Minimization (ERM) (Vapnik, 1992), data is assumed to consist of independent and identically distributed (iid) samples from an underlying generating distribution.As the data generating distribution is often unknown in practice, ERM seeks predictors with minimal average training error (i.e., empirical risk) over the training set.However, shuffling and treating data as iid risks possibly losing important information about the underlying conditions of the data generating process.Despite becoming a ubiquitous paradigm in machine learning, a growing body of literature (Arjovsky et al., 2019;Teney et al., 2020) has revealed that ERM and the the common practice of shuffling data inadvertently results in capturing all correlations found in the training data, whether spurious or causal, and produces models that fail to generalize to test data.The potential variation of experimental conditions that can exist at training time and during deployment in real-world applications, manifests in discrepancies between training and testing distributions.This, in turn, highlights the need for machine learning algorithms to generalize out-of-distribution (OoD).Shuffling and treating data as iid risks possibly losing important information about the underlying conditions of the data generating process.As will be demonstrated in this work, partitioning training data into environments, e.g., based on the conditions under which data is generated, can exploit these differences to enhance generalization.One promising approach based on this observation is that of Arjovsky et al. (2019), in which the principle of Invariant Risk Minimization (IRM) is introduced.The objective of IRM is to find a predictor that is invariant across all training environments (see Definition 1 and Equation 1).Because of the conceptually appealing nature of IRM and its potential to address the OoD-generalization problem, there is a stream of literature scrutinizing various facets of the original framework, e.g., extensions to other settings including online learning (Javed et al., 2020) and treatment effect estimation (Shi et al., 2020), fairness (Adragna et al., 2020), introducing game-theoretic interpretations (Ahuja et al., 2020), and raising concerns on the drawbacks and limitations of current IRM implementations (Rosenfeld et al., 2021;Kamath et al., 2021).For an indepth overview of the broader generalization literature, we refer the interested reader to (Arjovsky, 2020) and the references therein, and for an empirical evaluation of the performance of a number of the state-of-the-art methods on various test cases, we refer the reader to (Gulrajani & Lopez-Paz, 2020).In this paper, we introduce two practical implementations of the IRM principal to increase its appeal and applicability in real-world settings.First, we propose an invariance penalty that is directly related to risk.More precisely, we show that the risk in each environment under an arbitrary classifier equals to the risk under the optimal classifier for that environment plus the newly proposed invariance penalty between the said classifier and the optimal one.Second, we build on these initial findings to provide practitioners who currently use the original IRMv1 implementation with an adaptive rule by which to choose the penalty coefficient appropriately.In doing so, we characterize the difference between our proposed invariance penalty and the one proposed by Arjovsky et al. (2019) in terms of the eigenvalues of the Gram matrix of the data representation.This eigenstructure plays a significant role in the failure of invariance penalties including the one proposed by Arjovsky et al. (2019).In addition to providing practitioners with two valid approaches for implementing IRM, this work serves to illustrate the importance of the eigenstructure of the Gram matrix of the data representation for IRM.In particular, we revisit the counterexample of Rosenfeld et al. (2021) where the invariance penalty of Arjovsky et al. (2019) can be made arbitrarily small for a non-invariant representation.We show that the Gram matrix is ill-conditioned in such cases.We then provide a practical solution to alleviate such behavior, in particular for the case where the representation is parameterized by a neural network.Moreover, we show that the proposed framework finds an invariant predictor for the setting in which the data is generated according to a linear Structural Equation Model (SEM) when provided a sufficient number of training environments under a mild non-degeneracy condition, which is similar in nature to the ones considered in (Arjovsky et al., 2019;Rosenfeld et al., 2021).Finally, we evaluate our method on various test cases including InvarianceUnitTests (Aubin et al., 2020) and HealthyGutTests.InvarianceUnitTests is a test bed with three synthetically generated data sets capturing different structures of spurious correlations.HealthyGutTests is a curated collection of biomedical data sets based on a prior meta-analysis of various microbiome studies (Gupta et al., 2020) in which the relationships between human gut microbiome composition and various disease phenotypes can be investigated.The remainder of the paper is organized as follows.In Section 2, we formally define the notion of invariant prediction, the invariant risk minimization principle, and its relaxation proposed by Arjovsky et al. (2019).In Sections 3 and 4, we introduce our more practical implementation and the rationale for its design.In Section 5, we evaluate the efficacy of our proposed model and compare it with other variations of IRM over a series of experiments.We conclude the paper in Section 6.All mathematical proofs are presented in the Appendix."}
{"paper_id": 266, "abstract": "In the realm of deep neural networks, depth reigns supreme, yet it brings forth the burdens of increased sequential computation and latency. This raises a pivotal question: can we forge high-performing neural networks that defy the conventions of depth? Our exploration reveals a resounding yes. By harnessing the power of parallel subnetworks rather than the traditional layered approach, we effectively trim depth while preserving exceptional performance. In a groundbreaking demonstration, we reveal that a network with a mere depth of 12 can achieve remarkable top-1 accuracy\u2014over 80% on ImageNet, 96% on CIFAR10, and 81% on CIFAR100. Furthermore, this low-depth backbone excels in achieving an impressive average precision of 48% on MS-COCO. Our analysis delves into the scaling rules governing our design, uncovering methods to enhance performance without altering depth. Ultimately, we present a compelling proof of concept for utilizing non-deep networks in the creation of low-latency recognition systems. To foster further innovation, we will open-source our code, inviting the community to explore the possibilities.", "introduction": "Deep Neural Networks (DNNs) have revolutionized the fields of machine learning, computer vision, and natural language processing.As their name suggests, a key characteristic of DNNs is that they are deep.That is, they have a large depth, which can be defined as the length of the longest path from an input neuron to an output neuron.Often a neural network can be described as a linear sequence of layers, i.e. groups of neurons with no intra-group connections.In such cases, the depth of a network is its number of layers.It has been generally accepted that large depth is an essential component for high-performing networks because depth increases the representational ability of a network and helps learn increasingly abstract features (He et al., 2016a).In fact, one of the primary reasons given for the success of ResNets is that they allow training very deep networks with as many as 1000 layers (He et al., 2016a).As such, state-of-the-art performance is increasingly achieved by training models with large depth, and what qualifies as \"deep\" has shifted from \"2 or more layers\" in the early days of deep learning to the \"tens or hundreds of layers\" routinely used in today's models.For example, as shown in Figure 1, competitive benchmarks such as ImageNet are dominated by very deep models (He et al., 2016a;Huang et al., 2017;Tan & Le, 2019) with at least 30 layers, whereas models with fewer than 30 layers perform substantially worse.The best-performing model with fewer than 20 layers has a top-1 accuracy of only 75.2, substantially lower than accuracies achievable with 30 or more layers when evaluated with a single image crop (He et al., 2015;Tan & Le, 2019).But is large depth always necessary?This question is worth asking because large depth is not without drawbacks.A deeper network leads to more sequential processing and higher latency; it is harder to parallelize and less suitable for applications that require fast responses.In this paper, we study whether it is possible to achieve high performance with \"non-deep\" neural networks, especially networks with \u223c 10 layers.We find that, contrary to conventional wisdom, this is indeed possible.We present a network design that is non-deep and performs competitively against its deep counterparts.We refer to our architecture as ParNet (Parallel Networks).We show, for the first time, that a classification network with a depth of just 12 can achieve accuracy greater than 80% on ImageNet, 96% on CIFAR10, and 81% on CIFAR100.We also show that a detection network with a low-depth (12) backbone can achieve an AP of 48% on MS-COCO.Note that the number of parameters in ParNet is comparable to state-of-the-art models, as illustrated in Figure 1.A key design choice in ParNet is the use of parallel subnetworks.Instead of arranging layers sequentially, we arrange layers in parallel subnetworks.This design is \"embarrassingly parallel\", Figure 1: Top-1 accuracy on ImageNet vs. depth (in log scale) of various models.ParNet performs competitively to deep state-of-the-art neural networks while having much lower depth.Performance of prior models is as reported in the literature.Size of the circle is proportional to the number of parameters.Models are evaluated using a single 224\u00d7224 crop, except for ViTB-16 and ViTB-32 (Dosovitskiy et al., 2021), which fine-tunes at 384\u00d7384 and PReLUnet (He et al., 2015), which evaluates at 256\u00d7256.Models are trained for 90 to 120 epochs, except for parameterefficient models such as MNASNet (Tan et al., 2019), Mo-bileNet (Sandler et al., 2018), and EfficientNet (Tan & Le, 2019), which are trained for more than 300 epochs.For fairness, we exclude results with longer training, higher resolution, or multi-crop testing. in the sense that there are no connections between the subnetworks except at the beginning and the end.This allows us to reduce the depth of the network while maintaining high accuracy.It is worth noting that our parallel structures are distinct from \"widening\" a network by increasing the number of neurons in a layer.ParNet not only helps us answer a scientific question about the necessity of large depth, but also offers practical advantages.Due to the parallel substructures, ParNet can be efficiently parallelized across multiple processors.We find that ParNet can be effectively parallelized and outperforms ResNets in terms of both speed and accuracy.Note that this is achieved despite the extra latency introduced by the communication between processing units.This shows that in the future, with possibly specialized hardware to further mitigate communication latency, ParNet-like architectures could be used for creating extremely fast recognition systems.We also study the scaling rules for ParNet.Specifically, we show that ParNet can be effectively scaled by increasing width, resolution, and number of branches, all while keeping depth constant.We observe that the performance of ParNet does not saturate and increases as we increase computational throughput.This suggests that by increasing compute further, one can achieve even higher performance while maintaining small depth (\u223c10) and low latency.To summarize, our contributions are three-fold:\u2022 We show, for the first time, that a neural network with a depth of only 12 can achieve high performance on very competitive benchmarks (80.7% on ImageNet, 96% on CIFAR10, 81% on CIFAR100).\u2022 We show how parallel structures in ParNet can be utilized for fast, low-latency inference.\u2022 We study the scaling rules for ParNet and demonstrate effective scaling with constant low depth."}
{"paper_id": 267, "abstract": "In the realm of structured neural network pruning, a number of recent investigations have cast doubt on the efficacy of inheriting weights. These studies suggest that training a model from scratch can rival, or even surpass, the performance of fine-tuning a pruned network. However, we contend that this perspective is fundamentally flawed, primarily due to the use of inappropriately low learning rates during fine-tuning. Our research unveils compelling evidence that with appropriately larger learning rates, pruning consistently outshines training from scratch across a variety of networks\u2014such as ResNets and VGG11\u2014and datasets, including MNIST, CIFAR10, and ImageNet, across most pruning ratios.  To unravel the critical importance of the fine-tuning learning rate, we delve into the theoretical underpinnings through the lens of dynamical isometry\u2014a fascinating property of neural networks that ensures gradient signals maintain their norm throughout propagation. Our findings indicate that the act of pruning disrupts this dynamical isometry, which fundamentally elucidates the performance disparity observed between high and low fine-tuning learning rates. Thus, restoring dynamical isometry prior to fine-tuning emerges as a necessity.  In light of this, we introduce a straightforward yet effective regularization-based technique designed to recover dynamical isometry in modern residual convolutional neural networks. This approach not only enhances the performance of pruned models but also contributes to a deeper understanding of the intricate dynamics at play in neural network training.", "introduction": "Pruning is a time-honored methodology to reduce parameters in a neural network without seriously compromising its performance (Reed, 1993;Sze et al., 2017).The prevailing pipeline of pruning comprises three steps: 1) pretraining: train a dense model; 2) pruning: prune the dense model based on certain rules; 3) finetuning: retrain the pruned model to regain performance.Most existing research focuses on the second step, seeking the best criterion to select unimportant weights so as to incur as less performance degradation as possible.This 3-step pipeline has been practiced for more than 30 years (Mozer & Smolensky, 1989;LeCun et al., 1990) and is still extensively adopted in today's pruning methods (Sze et al., 2017).These said, several recent works (Crowley et al., 2018;Liu et al., 2019) questioned the necessity of inheriting weights from a pretrained model because they empirically found the small model trained from scratch can match (or sometimes outperform) the counterpart pruned from the pre-trained large model.This acutely challenges the past wisdom as well as our common belief about pruning.As far as we know, there is no formal response to this critical conflict.A theoretical-level understanding of this problem is even more elusive.Meanwhile, the pruning community has been observing even more open questions.Specifically, (Renda et al., 2020;Le & Hua, 2021) found that the learning rate (LR) in finetuning holds a critical role in the final performance.A proper learning rate schedule (e.g., a larger initial LR 10 -2 vs. 10 -3 with step-decay schedule) can improve the top-1 accuracy of a pruned ResNet-34 model (He et al., 2016) by more than 1% on ImageNet (Deng et al., 2009).This discovery calls for more attention being paid to the finetuning step when comparing different pruning methods.Unfortunately, they did not present more theoretical insights to explain its occurrence.This also remains an open question in the community up to date.In this paper, we will show these two open questions actually point to the same one.Specifically, we rerun the experiments of (Crowley et al., 2018;Liu et al., 2019) and find simply using a larger finetuning LR (10 -2 vs. 10 -3 and decay it) can significantly improve the final performance.Compared to the improved pruning performance, training from scratch does not compete or surpass pruning anymore (see Tab. 1 and Tab. 2 on ImageNet).This observation invites many questions immediately:(1) Theoretical understanding: Why does this happen?What is the theoretical reason behind it?(2) Practical solution: If this is a problem, how to fix it?Can the understanding of this problem lead us to better pruning algorithms?This paper will present answers to all these questions.The key tool we employ to unveil the mysteries is dynamical isometry (Saxe et al., 2014), which describes a kind of nice property in neural networks that are easy to optimize.We carefully design an explanatory experiment using a linear MLP (multi-layer perceptron) network to demonstrate how finetuning LR affects the final performance by affecting dynamical isometry.In brief, we observe the finetuning process can recover dynamical isometry; a larger LR can help recover it faster (or better), hence the better final performance.The proposed explanation is validated by our empirical results and resonates with many empirical observations.Furthermore, by the explanation, we learn dynamical isometry recovery is rather imperative.To achieve so, we present a very simple regularization-based method for pruning and show its effectiveness in recovering dynamical isometry on modern residual convolutional neural networks (CNNs).(1) We empirically demonstrate the questioning about the value of inheriting weights in structured pruning in previous works is inaccurate and point out that the direct cause is improperly using a small finetuning LR.Our finding justifies the value of inheriting weights in structured pruning.(2) On top of the empirical finding, more importantly, we present a theoretical explanation through examining the dynamical isometry of networks in pruning.This explanation is empirically validated by our carefully designed control experiments.(3) In addition to the theoretical understanding, we also propose a regularization-based method for dynamical isometry recovery.Despite its brutal simplicity, it is shown effective to recover the broken dynamical isometry on modern residual convolutional neural networks."}
{"paper_id": 268, "abstract": "In the ever-evolving landscape of deep learning, recent studies have illuminated a critical insight: fine-tuning the learning rate is paramount for achieving optimal performance in structured neural network pruning. This phenomenon can be traced back to the disruption of what we call *dynamical isometry*, a delicate balance that pruning tends to shatter. Yet, the quest for a filter pruning technique that not only preserves this dynamical isometry but also scales gracefully to the grand architectures of modern deep networks has remained a formidable challenge\u2014until now.  Enter *Orthogonality Preserving Pruning* (OPP), our innovative structured pruning method that deftly navigates these complexities. OPP employs a regularization strategy that meticulously constrains the Gram matrix of convolutional kernels, fostering orthogonality among essential filters while simultaneously nudging the less important weights toward oblivion. But we don\u2019t stop there; we also introduce a regularization approach for batch-normalization parameters, further safeguarding the dynamical isometry across the entire network.  The results speak volumes. On linear networks, OPP stands shoulder to shoulder with the gold standard for dynamical isometry recovery. When pitted against non-linear giants like ResNet56 and VGG19 on the CIFAR datasets, OPP doesn\u2019t just compete\u2014it triumphs, outpacing existing methods by a significant margin. And as we venture into the realm of modern deep networks, such as ResNets on ImageNet, OPP continues to shine, yielding impressive results that rival many contemporary filter pruning techniques.  To the best of our knowledge, OPP is the *first* method to successfully uphold dynamical isometry during the pruning process for *large-scale* deep neural networks, marking a pivotal moment in the ongoing saga of neural network optimization.", "introduction": "Neural network pruning aims to remove the parameters without seriously compromising the performance.It normally consists of three steps (Reed, 1993;Han et al., 2015;2016b): pretrain a dense model; prune the unnecessary connections or neurons with some criteria; finetune to regain performance.Pruning is usually categorized into two groups, unstructured pruning (a.k.a.element-wise pruning) and structured pruning (a.k.a.filter pruning or channel pruning).The former chooses a scalar weight as the basic pruning element; the latter chooses a 3d filter as the basic pruning element.In general, structured pruning is more favored for acceleration on commodity hardware because of its consequent regular sparsity; unstructured pruning results in irregular sparsity, which can be considerable without performance degradation but hard to exploit for acceleration if not using customized hardware (Han et al., 2016a;2017).Recent structured pruning works (Renda et al., 2020;Le & Hua, 2021;Wang et al., 2021a) showed an interesting phenomenon: in the finetuning step, using a larger learning rate (LR) helps to achieve a significantly better final performance (e.g., ResNet34 pruned at speedup 1.32\u00d7 can be improved by over 1% top-1 accuracy on ImageNet (Deng et al., 2009) using finetuning LR 1e-2 vs. 1e-3).The reason behind is shown by (Wang et al., 2021a) relevant to dynamical isometry (Saxe et al., 2014), a nice property of neural networks that are easy to train without gradient vanishing or explosion (Glorot & Bengio, 2010;Pascanu et al., 2013).They mainly made two observations for explanation.First, the weights removal operation immediately breaks the dynamical isometry of the pretrained network.Second, SGD training in finetuning can help recover it; a larger LR help recover it faster and better, thus making the final performance stronger.Although (Wang et al., 2021a) provided a sound explanation, a more practical issue is how to recover the broken dynamical isometry or maintain it during pruning.In this regard, (Wang et al., 2021a) proposed to apply weight orthogonalization based on QR decomposition (Trefethen & Bau III, 1997;Mezzadri, 2006) to the pruned model.However, the method was shown to only work for linear networks.On modern deep convolutional neural networks (CNNs), the method is still far from satisfactory.In this paper, we present orthogonality preserving pruning (OPP), a new filter pruning method that maintains dynamical isometry well during pruning.The main idea of OPP is to promote kernel orthogonality among the kept filters meanwhile pushing the weights to be pruned rather close to zero.By doing so, the subsequent weights removal operation will barely hurt the dynamical isometry of the network.Specifically, we propose to regularize the gram matrix of weights: all the entries representing the correlation between the pruned filters and the others are encourage to diminish to zero.This is the first technical contribution of our method.The second one lies in how to treat the cross-correlation entries of kept filters.Inspired by the proposed orthogonalization initialization in (Saxe et al., 2014), we add a kernel orthogonality term to the kept filters, which promotes dynamical isometry during pruning.In addition, modern deep models are typically equipped with batch normalization (BN) (Ioffe & Szegedy, 2015).However, previous filter pruning papers rarely explicitly take BN into account (except two (Liu et al., 2017;Ye et al., 2018); the differences between our work and theirs will be discussed in Sec.3.2) to mitigate the side effect when it is removed because its associated filter is removed.By the idea of preserving dynamical isometry, they should not be ignored since their removal breaks dynamical isometry as well.Therefore, we propose to regularize the learnable parameters of BN to minimize the influence of its absence.Empirically, the proposed pruning algorithm is easy to implement and delivers encouraging results compared to many existing filter pruning methods.Notably, this is the first method that effectively maintains dynamical isometry during pruning on modern large-scale deep networks.Contributions.We make three contributions in this paper:\u2022 We present the first filter pruning method (orthogonality preserving pruning) that can effectively maintain dynamical isometry when pruning modern deep networks, through a customized weight gram matrix as regularization target.\u2022 Apart from weight regularization, we also propose to regularize the batch normalization parameters to better maintain dynamical isometry.This has been overlooked by most previous pruning papers, while we show it is an indispensable part if we aim to maintain dynamical isometry for the whole network.\u2022 Practically, the proposed method is scalable to modern large-scale deep neural networks (e.g., ResNets) and datasets (e.g., ImageNet).It achieves promising pruning performance in comparison to many state-of-the-art filter pruning methods."}
{"paper_id": 269, "abstract": "In the realm of neural networks, a fascinating dance unfolds between teacher and student\u2014a process known as knowledge distillation (KD). This artful guidance allows the student model to absorb wisdom from its more experienced counterpart. While many have pondered the reasons behind KD's success, the intricate relationship it shares with data augmentation (DA) has remained shrouded in mystery.   In this exploration, we are propelled by a striking revelation within the domain of classification: the KD loss, when nurtured through extended training iterations, harnesses the power of data augmentation far more effectively than traditional cross-entropy loss. To illuminate this synergy, we introduce a versatile framework that elucidates the dynamic interplay between KD and DA.   Drawing inspiration from our findings, we embark on a quest to elevate KD through innovative data augmentation techniques we term TLmixup and TLCutMix. But we do not stop there; we forge an even more potent and efficient DA strategy tailored specifically for KD, rooted in the principles of active learning.   Our rigorous experiments across CIFAR-100, Tiny ImageNet, and ImageNet datasets validate the prowess of our methods, revealing a new pinnacle of accuracy achieved with the original KD loss, now enhanced by our robust augmentation techniques. This surpasses the performance of existing state-of-the-art methods that rely on more complex distillation losses. Furthermore, we demonstrate that by amalgamating our approaches with these advanced distillation losses, we can push the boundaries of accuracy even further.  Beyond the impressive results, our work sheds crucial light on the mechanics behind knowledge distillation's success. The interactions we've uncovered between KD and DA not only deepen our understanding but also pave the way for the development of even more formidable KD algorithms. In this ever-evolving landscape of machine learning, we stand on the brink of new possibilities, inspired by the powerful synergy of these two methodologies.", "introduction": "Deep neural networks (DNNs) are the best performing machine learning method in many fields of interest (LeCun et al., 2015;Schmidhuber, 2015).How to effectively train a deep network for classification has been a central topic for decades.In the past several years, efforts have mainly focused on better architecture design (e.g., batch normalization (Ioffe & Szegedy, 2015), residual blocks (He et al., 2016), dense connections (Huang et al., 2017)) and better loss functions (e.g., label smoothing (Szegedy et al., 2016;M\u00fcller et al., 2019), contrastive loss (Hinton, 2002), largemargin softmax (Liu et al., 2016)) than the standard cross-entropy (CE) loss.Knowledge distillation (KD) (Hinton et al., 2014) is a training framework that falls in the second group.In KD, a stronger network -called teacher -is introduced to guide the learning of the original network -called student -by minimizing the discrepancy between the representations of the two networks, LKD = (1 -\u03b1)LCE(y, p (s) ) + \u03b1\u03c4 2 DKL(p (t) /\u03c4, p (s) /\u03c4 ),where D KL represents KL divergence (Kullback, 1997); \u03b1 \u2208 (0, 1) is a factor to balance the two loss terms; L CE denotes the cross-entropy loss; y is the one-hot label and p (t) , p (s) stands for the teacher's output and student's output, respectively (which are probability distributions over the classes); \u03c4 is a temperature constant (Hinton et al., 2014) to smooth predicted probabilities.KD allows us to train smaller, more efficient neural networks without compromising on accuracy, which facilitates deploying deep learning in resource constrained environments (e.g., on mobile devices).The effectiveness of KD has been seen in many tasks (Chen et al., 2017;Wang et al., 2020;Jiao et al., 2019;Wang & Yoon, 2021).Meanwhile, many works have investigated the reason behind its success, such as class similarity structure (Hinton et al., 2014) and regularization (Yuan et al., 2020).However, few works have paid attention to its interplay with the input image data augmentation (DA), a technique to obtain more data through various transformations (Shorten & Khoshgoftaar, 2019).In this paper, we will show that data augmentation is also an important dimension to explain the success of KD.Moreover, our findings show we can achieve much better performance simply using the original KD loss equipped with a stronger data augmentation scheme.Figure 1: Test error rate of ResNet20 on CIFAR-100 when trained for different numbers of epochs (the teacher is ResNet56 for KD).Each result is obtained by averaging 3 random runs (shaded area indicates the std) \"Flip\" refers to horizontal flip; \"Crop\" refers to random crop.Both are standard data augmentation schemes in classification.The optimal number of training epochs and its test error are highlighted in red.Our proposed algorithms are inspired from interesting observations shown in Fig. 1, where we plot the student test error curves when the model is trained for different numbers of epochs using KD loss vs. CE lossfoot_0 .Three data augmentation scenarios are examined: not using DA at all (Without DA), only using the horizontal flip (Flip), and using both the horizontal flip and random crop (Flip+Crop).We have the following observations.(1) Within each plot, KD loss delivers lower test error than CE loss.(2) When DA is used (comparing the middle or right plot to the left), both CE and KD curves are improved.(3) When DA is used (comparing the middle or right plot to the left), the optimal number of training epochs is postponed and the postponement is greater for KD than CE (the optimal number of epochs is postponed from 180 to 480 for KD versu from 60 to 120 for CE).(4) When a stronger DA is employed (comparing the right plot to the middle), the optimal number of epochs is further postponed with even lower test error.The first two observations are well-recognized by existing works (they simply reiterate the effectiveness of KD and DA, respectively), while the last two observations are new discoveries of this work, concerning the interaction between KD and DA.In other words, KD and DA, as two common techniques to improve the performance of DNNs, are actually not independent.This paper explains the interplay between KD and DA and leverages it for stronger KD performance using only the standard KD loss.Specifically, we explain why KD is able to exploit DA more than the CE loss.Owing to the random transformations in data augmentation, the input data are not fixed over different epochs.Different views of each image are presented over the training process.When KD loss is used, the teacher maps these different views to different targets.As illustrated in Fig. 2(a), these targets with different probability structures can reveal more information of the data, thus helping the student more.In contrast, when CE loss is adopted, the target is fixed regardless of the different views of the input.The extra information is thus lost.This observation inspired us to develop two stronger data augmentation techniques (TLmixup, TLCutmix) that are tailored for KD.We further tap into the idea of active learning to make TLCutmix even better, that is, our TLCutmix+pick method.In summary, these are the main contributions of this work:\u2022 We make novel observations of the interaction between KD and DA.We explain how DA methods are more suited to be exploited by KD which uses teacher outputs as labels instead of the ground-truth one-hot labels used by CE loss.\u2022 Inspired by the above, we propose to enhance the original KD loss with stronger data augmentation schemes (by adapting mixup (Zhang et al., 2018) and CutMix (Yun et al., 2019) to KD).It is shown that these methods are more reasonably applied in the KD case than in the CE case.\u2022 We further propose an even stronger data augmentation method specifically for knowledge distillation using the idea of active learning (Settles, 2011).\u2022 We show empirically better results simply by using the original KD loss combined with the proposed DA scheme, compared to state-of-the-art KD methods, which adopt more advanced distillation losses."}
{"paper_id": 270, "abstract": "In the ever-evolving landscape of cross-domain object detection, we find ourselves at a crossroads\u2014a place where the balance between transferability and discriminability hangs in the balance. Recent advancements have illuminated the path forward, with state-of-the-art methods diligently working to weave together the intricate tapestry of local regions, aligning cross-channel and spatial information to enhance model generalization. Yet, in their quest for harmony, these approaches often inadvertently steer networks towards a focus on shared attributes, neglecting the rich, nuanced features that define individual domains.   To navigate this challenge, we present a groundbreaking adaptation framework that seeks to restore equilibrium. At its heart lies a style-aware feature fusion technique, complemented by two innovative plug-and-play regularization modules designed to recalibrate the model\u2019s focus toward the domain-specific elements that truly matter. Our insight is simple yet profound: while extracting discriminative features in the target domain may pose a formidable challenge, we can harness the power of feature style transfer to imbue the model with the essential details it needs.  With no unnecessary embellishments, our method emerges as a beacon of improvement, propelling the performance of existing Domain Adaptive Faster R-CNN detectors to new heights. The results speak for themselves, as we achieve state-of-the-art benchmarks across several datasets, illuminating the path forward in the realm of cross-domain object detection.", "introduction": "Along with the advances in deep convolutional neural networks (Krizhevsky et al., 2012), remarkable progresses have been achieved in various visual tasks (He et al., 2016;Long et al., 2015;Ren et al., 2017).Such outstanding achievements heavily rely on the assumption that training and test data are drawn from an independent identical distribution.Nevertheless, due to the changes of environmental condition in real-world, a phenomenon known as \"domain shift\" occurs and challenges this assumption, thus leading to a significant performance decay.Although collecting more training data can alleviate such problems to some extent, it is non-trivial because of the high cost of image annotation.An appealing solution to alleviate the shift is introducing domain adaptation (DA) methodology, which aims at bridging the gap between domains by learning invariant representations.In this paper, we focus on the unsupervised scenarios, called unsupervised domain adaptation (UDA), where the training set is formed from labeled source data and unlabeled target data.Recently, a variety of UDA methods have been well-studied for classification (Tzeng et al., 2014;Ganin & Lempitsky, 2015;Saito et al., 2018;Wang et al., 2020;Li et al., 2021b) and semantic segmentation (Hoffman et al., 2018;Zou et al., 2018;Dong et al., 2020;Li et al., 2020;Jaritz et al., 2021), which are mostly based on minimizing domain discrepancy measurement or adversarial training.These strategies have made great progress but are hard to be applied to object detection directly because of the local nature of detection tasks (Zhu et al., 2019).To address the adaptation dilemma for detection tasks, instance-level feature alignment has been widely used in Chen et al. (2018b); Saito et al. (2019); Zheng et al. (2020).Most of them strive to handle the relation between local regions by calibrating cross-channel and spatial information.However, it may implicitly drive networks to pay more attention on shared attributes and ignore the domain-specific feature.As shown in Figure 1, DA reduces the domain shift by feature alignment between source and target domain, which highlights the common components at the cost of domain-specific features.We review the feature alignment of domain adaptation and suppose that image-level style and instancelevel content play a vital role in detection tasks.The style information should be globally consistent within its own domain while local homogeneous content information between different domains ought to be analogous, and vice versa.Building upon the above findings, we propose a novel adaptation framework for cross-domain object detection, which supplements the domain-specific representation by restructuring the content and style information.Specifically, we adopt the Domain Adaptive Instance Normalization (DAdaIN) on shallow layers to generate synthesis features, which contributes on encoding the domain-specific and domain-invariant features in source and target domain.Furthermore, Global Style Alignment (GSA) module and Local Content Alignment (LCA) module are exploited to regularize the consistency of style and content between multiple domains.GSA consists of two sub-modules, i.e., style similarity alignment and style consistency alignment, which are adopted in multi-layer to align global style representation.Moreover, LCA is proposed to calibrate the content information of local regions, which enhances the model's response to foreground objects of various styles.In summary, the main contributions of this work are three-fold:\u2022 We review the feature alignment in domain adaptation and point out that it may implicitly drives networks to focus more on public component and ignore the domain-specific feature.Furthermore, we suppose that global style and local content informations play a vital role in detection tasks.\u2022 We propose a novel adaptation framework for cross-domain object detection.A style-aware feature fusion method and two plug-and-play adaptation modules are designed to reposition the focus of the model to domain-specific features by restructuring the style and content of feature.\u2022 We conduct extensive experiments in adaptation between both similar and dissimilar domains to validate the effectiveness of our approach.The compelling experimental results and visualizations demonstrate that our method significantly boost the performance of existing Domain Adaptive Faster R-CNN detectors under various domain-shift scenarios."}
{"paper_id": 271, "abstract": "In this paper, we unveil a groundbreaking framework known as the Collaboration of Experts (CoE), a brilliant tapestry woven from the distinct skills of multiple networks, united toward a singular purpose. Each expert within this framework represents an individual network, specializing in a unique segment of the dataset, thereby amplifying the collective prowess of the ensemble. When presented with a sample, a delegator selects an expert, which not only provides a preliminary prediction but also offers a pathway for early termination of the process, optimizing efficiency.  To ensure that each model within the CoE fulfills its designated role, we introduce an innovative training algorithm comprising three pivotal components: the Weight Generation Module (WGM), the Label Generation Module (LGM), and the Selection Reweighting Module (SRM). This triad of modules works in harmony to elevate the performance of our framework, culminating in state-of-the-art results on ImageNet\u2014an impressive 80.7% top-1 accuracy achieved with 194 million FLOPs.   Moreover, when combined with the PWLU activation function and Conditional Convolutions (CondConv), CoE reaches a remarkable milestone, attaining 80.0% accuracy with a mere 100 million FLOPs for the first time in history. Perhaps most strikingly, CoE is designed with hardware efficiency in mind, delivering a 3 to 6 times speedup compared to existing conditional computation methods. Our experimental findings on translation tasks further underscore the robust generalizability of the CoE framework, heralding a new era of collaborative intelligence in machine learning.", "introduction": "There are many approaches for model collaboration, among which ensemble learning (Hansen & Salamon, 1990;Wen et al., 2020;Wenzel et al., 2020) is a popular one.Ensemble learning uses a consensus scheme to decide the collective result by vote.However, it requires multiple forward passes, leading to a significant runtime cost.MIMO (Havasi et al., 2021) draws inspiration from model sparsity (Frankle & Carbin, 2019) and tries to ensemble several subnetworks within one regular network.It only needs one single forward pass of the regular network but is incompatible with compact models.Conditional computation methods (Cheng et al., 2020;Yan et al., 2015;Shazeer et al., 2017) alleviates this issue via delegation scheme, i.e. assigning one or several, rather than all models, conditionally to make the prediction.Some recently proposed conditional computation methods (Zhang et al., 2020b;Yang et al., 2019;Zhang et al., 2021) have achieved remarkable performance based on dynamic convolution.Nonetheless, they usually have high memory access cost (MAC) and a low degree of parallelism, which increases the real latency (Ma et al., 2018).Motivated by this, we propose the Collaboration of Experts (CoE) framework to both eliminate the need for multiple forward passes and keep hardware-friendly.CoE consists of one delegator and multiple experts.Firstly, delegator gives a rough prediction and makes the expert selection.If the rough prediction is unreliable, the selected expert will make the refined prediction.Otherwise, the procedure will be early terminated to save FLOPs.Moreover, we only need to load the selected expert into memory, thus keep the ratio of MAC to FLOPs as a constant.By contrast, dynamic convolution methods (Zhang et al., 2020b;2021) need to load a large number of parameters, namely basis models or experts, to synthesize the input-dependent ones.It enlarges MAC and reduces the degree of parallelism, resulting in a significant deceleration.To make each model in CoE play its role, we propose a novel training algorithm (as shown in Fig. 1) which consists of three components: weight generation module (WGM), label generation module (LGM) and selection reweighting module (SRM).LGM generates the label (selection label) to constitute loss of delegator for expert selection (selection loss).Selection label is a one-hot vector, indicating the suitable expert for each given input.Due to the random initialization of experts, selection labels are irregular in the early training stage.Nonetheless, delegator tends to learn generalizable patterns first, since networks learn gradually more complex hypotheses during training (Arpit et al., 2017).With delegator as the bridge, WGM can partition the training data into portions based on generalizable patterns, and reweight expert losses to impel each expert to focus on one portion.This procedure makes selection labels more regular in return.Thus, delegator avoids overfitting to the irregular labels.SRM aims to promote delegator to select experts better.It is achieved by enforcing delegator to focus on samples whose recognition results are sensitive to expert selection.Experimental results on ImageNet demonstrate the superiority of CoE.It achieves 78.2/80.7%top-1 accuracy with only 100/194M FLOPs, while the accuracy for ensembled models (Hansen & Salamon, 1990) is 79.6% with 920M FLOPs.Compared with dynamic network approaches, CoE is more hardware-friendly.It not only outperforms the SOTA dynamic method BasisNet which achieves 80.0% accuracy with 198M FLOPs (Zhang et al., 2021), but also accomplishes a 3.1x speedup on hardware.Besides, CoE can be equipped with CondConv and further improve the accuracy to 79.2/81.5% with 102/214M FLOPs.More surprisingly, we further boost the accuracy to 80.0% with only 100M FLOPs for the first time by using PWLU activation function (Zhou et al., 2021).Experimental results on the translation task also demonstrate the strong generalizability of CoE.The contributions of this paper can be summarized as follows:\u2022 We propose a collaboration framework named Collaboration of Experts (CoE) and demonstrate that it can lead to outstanding performance with little computation cost.Moreover, it is hardware-friendly and able to achieve real speedup.\u2022 We present a novel optimization strategy for CoE.The core insight is to promote diversity within experts by distributing their expertise over different portions of the dataset.\u2022 We update the state-of-the-art on ImageNet for mobile setting, achieving 80.0% top-1 accuracy with only 100M FLOPs for the first time.2 RELATED WORK 2.1 ENSEMBLE LEARNING AND MODEL SELECTION Ensemble learning (Hansen & Salamon, 1990) aims at combining the predictions from several models to get a more robust one.Some recently proposed literatures (Wen et al., 2020;Wenzel et al., 2020) demonstrate that significant gains can be achieved with negligible additional parameters compared to the original model.However, these methods still require multiple (typically, 4-10) forward passes for prediction, leading to a significant runtime cost.Differently, CoE utilizes a delegator to select only one expert for the refined prediction, thus at most two forward passes are needed.MIMO (Havasi et al., 2021) draws inspiration from model sparsity (Frankle & Carbin, 2019) and holds the view that multiple independent subnetworks can be concurrently trained within one regular network because of the heavy parameter redundancy.Therefore, those subnetworks can be ensembled with a single forward pass of the regular model.However, MIMO cannot be applied to compact models which have already been pruned or the ones constructed by AutoML methods (Zhong et al., 2018;Zhang et al., 2020a;Cai et al., 2020).It is because these models are compact enough and have few redundant parameters.By contrast, CoE is free from the compactness of experts and compatible with various models.Recently, some works about model selection are proposed (Li et al., 2021b;You et al., 2021).These methods are concerned with ranking a number of pre-trained models and finding the one transfers best to a downstream task of interest.Therefore, they select models task-wisely.By contrast, CoE aims at improving the task performance via selecting the most suitable expert for each sample instance-wisely.Moreover, these methods conduct model selection based on a set of samples (training set) which cannot be adopted in CoE.2.2 DYNAMIC NETWORKS Dynamic networks achieve high performance with low computation cost by conditionally varying the network parameters (Zhang et al., 2020b;Yang et al., 2019) or network architectures (Yuan et al., 2020).HD-CNN (Yan et al., 2015) and HydraNet (Mullapudi et al., 2018) select branches based on the category, they cluster all categories into n groups, where n is the number of branches.While CoE learns the model selection pattern automatically, it can be based on any property, rather than limited to the category.MoE (Shazeer et al., 2017) and Switch Transformer (Fedus et al., 2021) enable the direct training of Router by scaling the output feature of experts with the predicted gate-values of Router.These methods aim at conditionally selecting a specific layer or block.Differently, CoE can take more advantage of conditional computation as the selection of whole network makes every parameter input-dependent.The recently proposed Dynamic Convolution methods (Zhang et al., 2020b;Yang et al., 2019;Chen et al., 2020) share the same idea and achieve remarkable performance with low FLOPs but high latency.It is because these methods need to load many basis models or experts to synthesize the dynamic parameters, causing high MAC and low degree of parallelism (Ma et al., 2018).By contrast, CoE only needs to load the selected expert into memory, avoiding these problems.Finally yet importantly, batch processing is an important method to enhance the degree of parallelism.Because of the input-dependent parameters (Zhang et al., 2021) or architectures (Yuan et al., 2020), these methods cannot process samples in batch.Differently, CoE supports batch processing because the number of experts is limited and each one of them corresponds to many test samples."}
{"paper_id": 272, "abstract": "In the realm of deep learning, where the vastness of data can be as daunting as a stormy sea, decentralized adaptive gradient methods emerge as a beacon of efficiency. These methods, where each node only communicates with its immediate neighbors, hold the key to reducing both communication overhead and wall-clock training time. While the specific recursions of these methods may differ, they are bound by a common thread: each node scales its gradient using the historical squared gradients\u2014a process we call the adaptive step\u2014before or during its exchanges with neighboring nodes.  However, we have uncovered a critical flaw in this adapt-then-while-communicate framework. This structure renders the algorithms overly sensitive to the disparities in data distributions, causing their convergence points to drift away from the desired stationary solutions. To navigate these treacherous waters, we introduce a novel approach: a communicate-then-adapt structure. In this framework, each node completes its neighborhood communications before undertaking the adaptive step.   The implications of this shift are profound. Our method is not only theoretically sound, guaranteeing convergence to the stationary solution even in non-convex landscapes, but it also excels in practice. Through rigorous experimentation across a spectrum of computer vision and natural language processing tasks, we demonstrate that our approach significantly outperforms existing decentralized adaptive methods, charting a clearer course toward effective and efficient deep learning.", "introduction": "Decentralized SGD (Lopes & Sayed, 2008;Nedic & Ozdaglar, 2009;Chen & Sayed, 2012;Lian et al., 2017;Assran et al., 2019) is an emerging training approach for deep learning known for its much less communication overhead.In contrast to parallel SGD in which a global averaging across all computing nodes is required per iteration, decentralized SGD does not involve any global operations.Building upon partial averaging, in which each node only needs to compute the locally averaged model within its neighborhood, decentralized SGD can save remarkable communications and training time in large-scale distributed deep learning tasks compared to parallel SGD.Although simple to use, the vanilla decentralized SGD sometimes suffers from the slow convergence.Inspired by the well-documented success of adaptive methods such as AdaGrad (Duchi et al., 2011a), Adam (Kingma & Ba, 2014) and AMSGrad (Reddi et al., 2019), several decentralized adaptive methods (Nazari et al., 2019;Lin et al., 2021) have been proposed to accelerate decentralized SGD training.While these algorithms have achieved remarkable success in several practical applications, they have also been observed to not converge to the desired solution (i.e., global optimal solution in the convex scenario or stationary solution in the non-convex scenario) in some other settings.For example, it has been observed in the convex setting (see Sec. 3) that DAdam (Nazari et al., 2019) and QG-DAdam (Lin et al., 2021) do not converge to the global optimal solution.This paper studies this situation in detail.We rigorously uncover the reason why DAdam and QG-DAdam fail to achieve the desired solution, and propose a novel decentralized adaptive method to resolve the convergence issue.In particular, we make the following key contributions: \u2022 We find the algorithms DAdam and QG-DAdam, while different in concrete recursions, share a similar structure: each node scales its gradient with the past squared gradients (which is referred to as the adaptive step) before or while it communicates with neighbors.We identify the limitation of such adapt-then/while-communicate structure: it will make the developed algorithms highly sensitive to data heterogeneity, and hence deviate their limiting points from the desired solution.\u2022 To overcome these limitations, we propose a novel communicate-then-adapt algorithm structure, in which each node will conduct the adaptive step after all neighborhood communications.It is not just trivially switching the order of communication step and adaptive step.The key component to guarantee the effectiveness of the communicate-then-adapt structure is the utilization of the decentralized augmented gradient (DAG) rather than the standard stochastic gradient in algorithm development.The newly proposed algorithm, which is coined as DAG-Adam, can provably converge to the desired solution.While this paper mainly focuses on the vanilla Adam algorithm, the core idea behind DAG-Adam can be easilty extended to AMSGrad or other adaptive methods.\u2022 Experimental results on a variety of computer vision and natural language processing tasks show that DAG-Adam outperforms various existing state-of-the-art decentralized adaptive baselines under different network and data configurations.Furthermore, the performance of our proposed algorithm is persistently competitive to the centralized counterpart.Related work on decentralized optimization and deep training.Decentralized optimization was extensively studied in the control and signal processing community.The first decentralized algorithms on general optimization problems include decentralized gradient descent (Nedic & Ozdaglar, 2009), diffusion (Lopes & Sayed, 2008;Chen & Sayed, 2012;Sayed, 2014), and dual averaging (Duchi et al., 2011b).After that, various primal-dual algorithms come out to further speed up the convergence, and they are based on alternating direction method of multipliers (Adam) (Shi et al., 2014), explicit bias-correction (Shi et al., 2015;Yuan et al., 2019;Li et al., 2019), gradient tracking (Xu et al., 2015;Di Lorenzo & Scutari, 2016;Nedic et al., 2017;Qu & Li, 2018;Lu et al., 2019), and dual acceleration (Scaman et al., 2017;Uribe et al., 2020).In deep learning tasks, decentralize SGD, which was established in (Lian et al., 2017) to achieve the same linear speedup as parallel SGD in convergence rate, has attracted a lot of attentions.Many efforts have been made to extend the algorithm to directed topologies (Assran et al., 2019), time-varying topologies (Koloskova et al., 2020), asynchronous settings (Lian et al., 2018), and data-heterogeneous scenarios (Tang et al., 2018;Xin et al., 2020;Lin et al., 2021;Yuan et al., 2021).With careful consensus control (Kong et al., 2021) or periodic global averaging (Chen et al., 2021b), decentralize SGD can achieve 1.3 \u223c 2\u00d7 training time speedup without severe performance degradation.Techniques such as quantization/compression (Alistarh et al., 2017;Bernstein et al., 2018;Koloskova et al., 2019a;b;Tang et al., 2019;Liu et al., 2020), periodic updates (Stich, 2019;Koloskova et al., 2020;Yu et al., 2019), and lazy communication (Chen et al., 2018a;Liu et al., 2019b) were also integrated into decentralized SGD to further reduce communication overheads.There are also a few studies on accelerated variants of decentralized SGD, and most of them are on (static) momentum acceleration.(Assran et al., 2019;Gao & Huang, 2020) propose to run a local momentum SGD step first before the partial averaging is conducted.Another work (Yu et al., 2019) imposes an additional partial averaging over momentum to increase stability.Recent works (Lin et al., 2021;Yuan et al., 2021) developed strategies that can remove the momentum-incurred bias in decentralized momentum SGD.All these methods are not with adaptive strategies to scale gradients.Related work on adaptive gradient method.Adaptive gradient methods, with AdaGrad (Duchi et al., 2011a) and Adam (Kingma & Ba, 2014) as two representatives, have shown strong performance in training deep neural networks.With adaptive adjustment in the gradient direction and automatic tune in the learning rate, adaptive gradient methods can boost the performance of SGD training significantly.In spite of its remarkable empirical success, Adam suffers from a convergence issue: it may not converge to the desired solution with a fixed mini-batch size (Reddi et al., 2019;Zaheer et al., 2018).Many algorithms have been proposed to resolve this issue.AMSGrad (Reddi et al., 2019) preserves the long-term memory of past gradients to improve convergence, and (Zaheer et al., 2018) suggests the usage of increasing mini-batch sizes in convergence guarantees.(Chen et al., 2018b) has studied the convergence of a family of Adam-type algorithms.(Zhou et al., 2018) provides a better convergence rate for AMSGrad.From the empirical side, RAdam (Liu et al., 2019a) and AdamW (Loshchilov & Hutter, 2018) can significantly improve Adam performance.The exploration in decentralized adaptive methods are rather limited.DAdam (Nazari et al., 2019) is the first consensus-based adaptive methods for distributed optimization to our knowledge.(Lin et al., 2021) proposes QG-DAdam, which utilizes quasi-global momentum to locally approximates the globally descent direction.A recent work (Chen et al., 2021a) proposes a unified framework that incorporates various adaptive methods into decentralized setting.While these methods have shown strong empirical performance in several practical applications, they either suffer from unstable convergence or heavy communications.We leave more discussion on their limitations in Sec. 2. Adaptive gradient methods have also been extended to the federated learning setting in which multiple clients cooperate to learn a model under the supervision of a central server (McMahan et al., 2017).Useful references in this direction can be found in (Xie et al., 2019;Reddi et al., 2020)."}
{"paper_id": 273, "abstract": "In the realm of artificial intelligence, spiking neural networks (SNNs) stand as a beacon of promise, showcasing remarkable strengths in information encoding, energy efficiency, and computational prowess. Yet, the shadow of underdeveloped supervised learning algorithms looms large, casting doubt on the potential of SNNs for effective training. At the heart of this challenge lies the crucial matter of weight initialization\u2014a key that unlocks the door to efficient training. The initial conditions set the stage for gradient generation through back-propagation over time, making it a pivotal factor during the nascent phases of learning.  Drawing inspiration from the unique properties of spiking neurons, we embark on a journey to derive an asymptotic formula that captures the essence of their response curve, closely mirroring the actual distribution of neuronal responses. Armed with this insight, we unveil a novel initialization strategy, rooted in the slant asymptote, designed to combat the pernicious issue of gradient vanishing.  Our experiments, conducted across various coding schemes in classification tasks, reveal a compelling narrative: our method not only accelerates training speed but also enhances the final model accuracy, surpassing both conventional deep learning initialization techniques and existing SNN methods. Further explorations into diverse neuron types and training hyperparameters showcase our approach's remarkable versatility and superiority over its counterparts. As we conclude, we offer strategic insights for SNN training, drawn from our analyses, paving the way for future advancements in this captivating field.", "introduction": "To date, deep learning has contributed to outstanding performance on various tasks (Hinton et al., 2012;Hosu & Rebedea, 2016;He et al., 2016;LeCun et al., 2015).Most state-of-the-art models are based on analog neurons.Each analog value conveys activation; thus, the inference processes of analog neural networks (ANNs) consist of massive matrix manipulations.To overcome this, spiking neural networks (SNNs) are increasing attracting from researchers (Maass, 1997;Maass & Markram, 2004;Yang et al., 2019).The spiking neurons in an SNN emit sparse 0,1 spikes when activated.Compared with existing ANNs, SNNs are considered to encode spatiotemporal patterns and have better power efficiency.Hence, implementations of SNNs are highly appealing in powerlimited scenarios.In addition, SNNs have exhibited computational capabilities as powerful as those of conventional artificial neural networks (Lee et al., 2016).Despite SNNs' powerful capabilities, these networks lack efficient supervised learning algorithms.The discrete spikes prevent the direct use of backpropagation (BP) training.Recently proposed supervised algorithms have mainly focused on two methodologies: ANN-SNN conversion and BP through time (BPTT) with surrogate functions.Compared to ANN-SNN conversion (Diehl et al., 2015;Sengupta et al., 2019;Deng & Gu, 2021;Ho & Chang, 2020;Ding et al., 2021), BPTT is more friendly to the event-based datasets (Lee et al., 2016;Zheng et al., 2021;Neftci et al., 2019).However, Wu et al. (2018) has reported that BPTT is only feasible to shallow architectures.Therefore, BPTT still cannot be regarded as a mature SNN training algorithm.We ascribe the immaturity of BPTT training to the influence of the gradients.One common reason is gradient vanishing and explosion.This is natural since SNNs can be regarded as a type of recurrent neural networks (RNNs) (He et al., 2020;Fang et al., 2020).In the wisdom of RNN methodology, researchers have managed to provide a simple solution by rethinking weight initialization (Le et al., 2015).The success of RNN training inspires consideration of whether there may also be a simple solution for SNNs.Another reason is early gradient generation, which has rarely been mentioned in the previous literature.Most surrogate functions produce gradients when the membrane potential is in the neighborhood of the threshold.Some produce gradients only when spikes are fired (Lee  et al., 2020a).Thus, the flow of the gradients strongly depends on the number of firing neurons and will influence the optimization of the network.A small experiment can help illustrate how early gradient generation affects training.As shown in Fig. 1, with an inadequate weight initialization method, the neurons receive inputs that are too small and cannot fire initially, leading to stall of the training process because gradients fail to be produced.Until some neurons fire, the loss does not begin to decrease, and the neurons must suffer a latter uncoordinated training schedule.Therefore, in general, good weight initialization is also important for efficient SNN training.At present, initialization for SNN training is mainly achieved by following and adapting methods originally developed for ANNs.However, unlike the rectified linear unit (ReLU) activation function conventionally used in ANNs, spiking neurons respond very differently, and recent improvements in models have rarely focused on the unique properties of spiking neurons.In this paper, we investigate the theoretical response of spiking neurons to develop a better SNN initialization strategy.The main contributions of this paper are summarized as follows:\u2022 We use iterative systems to model first-order integrate-and-fire neurons and theoretically investigate the response curve.A slant asymptote is derived to approximate the actual neuron response.\u2022 We examine the difficulty of training deep SNNs and develop an initialization method based on the slant asymptote.The proposed initialization method relies on adequate scaled weights and nonzero biases to enable early gradient training.\u2022 Experiments with rate coding and event coding in classification tasks show that our method can effectively improve the model training speed and accuracy.In addition, empirical analyses suggest that our asymptote initialization method offers versatility and superiority over a vast range of training hyperparameters, such as different neuron types, surrogate functions, and optimizers."}
{"paper_id": 274, "abstract": "The human mind possesses an innate ability to detect anomalies, a skill that has remained elusive for artificial intelligence\u2014until now. The challenge lies in the very nature of anomalies; they exist outside the bounds of what is deemed normal, rendering their distribution nearly impossible to model. As a result, much of the research has focused on defining the contours of normality and identifying deviations, a path fraught with the tedious necessity of extensive data collection for each unique task. In this paper, we unveil our innovative approach to overcoming these obstacles. By harnessing the power of Energy-Based Models (EBMs), we associate low energies with correct values and assign higher energies to their incorrect counterparts. At the heart of our method lies Langevin Dynamics (LD), a process that generates incorrect samples through an iterative optimization strategy, effectively sidestepping the intractable nature of anomaly modeling. To further streamline the process, we introduce an adaptive sparse coding layer, designed as a plug-and-play feature that enables rapid updates to what is considered normal during inference. This adaptability allows us to sidestep the burdensome data collection typically required, as our sparse coding updates can be achieved with minimal data\u2014thanks to a meta-learning framework that simulates few-shot scenarios during training. Our findings, bolstered by robust empirical evidence, pave the way for a more agile and efficient approach to anomaly detection, one that aligns closely with the intuitive capabilities of the human mind.", "introduction": "Anomaly detection is an important area of study in the field of artificial intelligence.It has found utility in computer vision applications such as industrial inspection (Bergmann et al., 2019) and video surveillance (Liu et al., 2018;Zhao et al., 2011;Nguyen & Meunier, 2019), in the context of abuse prevention such as misinformation, fraud and network intrusion detection (Zhang et al., 2019;Bolton & Hand, 2002;Mukherjee et al., 1994), and others such as system health monitoring and fault detection (Bao et al., 2019;Purarjomandlangrudi et al., 2014).In this paper, we propose an approach for detecting anomaly in images, where we have carefully designed steps to handle some of the bigger issues that have prevented the deployment of image anomaly detection in the real-world.Image anomaly detection can generally be defined as the identification of abnormalities in a given image.An exact definition of abnormality in this case is elusive because abnormality can be derived from any unknown distribution outside of a normal population.Many studies have hence focused on modeling the normal population instead of learning irregularities, where the goal is to capture the shared concept among all of the normal data as one or several reference models.This process usually will require investing significant efforts in curating a large enough set of normal samples for each task, after which anomaly is detected as deviations from the reference model(s) (An & Cho, 2015;Xu et al., 2018).Recent work from (Sheynin et al., 2021) provides algorithms that utilize only a few normal samples to train models from scratch.However, the models still have to be provisioned for each new task, which requires considerable human efforts and expertise, and thus lack the fast deployment criterion that is often time critical for real-world applications.In view of these challenges, our goals for this work are threefold.We are interested in designing an anomaly detection system that is capable of: (G1) modeling the normal population while at the same time has a principled approach towards modeling the abnormalities; (G2) quickly adapting to a new task at inference time; and (G3) requiring only a few normal shots to update itself to the new task at hand.For (G1), we introduce the class of Energy Based Model (EBM), which is an important family of generative models (Zhao et al., 2016;Du & Mordatch, 2019;Xie et al., 2016).EBMs have been shown to demonstrate superior capability on modeling data density and localizing anomaly (Genc et al., 2021).For our purpose, the EBM we adopted learn to assign low energy to normal samples but high energy to abnormal samples.More importantly, the abnormal samples are generated with a procedure known as Langevin Dynamics (LD) (Welling & Teh, 2011), which, in its original form, starts with a noise image (see App. Fig C) and gradually samples from the distribution along the direction of lower energy.This lends itself gracefully to utilizing the generated intermediate samples as negative/abnormal.The LD procedure is then coupled with a contrastive divergence loss (Hinton, 2002) that aims to maximize the energy differences between the normal and abnormal samples.To achieve (G2), we propose an adaptive sparse coding layer that is attached to the deep feature extractor in the EBM.The deep features are projected into a set of feature vectors along the spatial axes, after which each vector is forwarded to the sparse coding layer, where the dictionary is constructed with the features of a few normal samples of the given task.In essence, the input representation has been decomposed into a linear combination of normal features with the sparsity constraint imposed.The final energy score is measured by the distance between the original and the reconstructed features (after the sparse coding layer).Under this scheme, the dictionary for a particular task is not obtained by learning, but instead is constructed by the feature representations of a few normal samples during inference.As a result, this simple \"plug-and-play\" trick allows the model to be adapted to novel tasks promptly without re-training.Further, we expect that the dictionary, which is formed by normal features, will not be able to explain the abnormal samples well, causing relatively high reconstruction error that lends itself for subsequent detection.As a bonus, a backward pass from the reconstruction error to the image is also additionally useful for localizing the abnormal regions.Towards (G3), we utilize meta learning (Vilalta & Drissi, 2002;Finn et al., 2017) to simulate the scenario of being given a new task with a few normal shots to update the dictionary, followed by training the EBM.This is accomplished by episodic training, where in each episode the model is adapted to a held back task that is given a few normal samples.To accelerate the EBM training, we introduce \"learning from inpainting\", a simple yet effective strategy for synthesizing hard abnormal samples quicker by starting the LD procedure with a synthesized image that is simply a normal sample with a noise patch injected as opposed to a noise image that is traditionally what is used.We show the proposed framework is able to efficiently adapt to a novel task (e.g., a new object category or scenes from a new camera) with a few normal samples on both industrial inspection and video surveillance tasks.We provide both qualitative and quantitative results to demonstrate that our method outperforms other adaptive frameworks and is comparable to methods that rely on large amount of normal samples."}
{"paper_id": 275, "abstract": "In the ever-evolving realm of few-shot image classification, we unveil Meta-OLE, a groundbreaking approach that harnesses the power of geometry to swiftly adapt to new tasks. Imagine a method that not only learns but also refines a feature space tailored for each unique classification challenge, achieving a delicate balance of inter-class orthogonality and intra-class low-rankness. At its core, our deep feature extractor is meticulously trained to cultivate orthogonal low-rank subspace structures, ensuring that features from distinct classes coexist harmoniously within a task.  But we don\u2019t stop there. To tackle the complexities of novel tasks with unseen categories, we embark on a journey of meta-learning, crafting a lightweight transformation that sharpens the inter-class margins. This clever innovation not only enhances our adaptability but also empowers us to leverage query data for seamless label propagation from labeled to unlabeled data\u2014without the need for cumbersome auxiliary networks.  The geometry-regularized feature subspaces we create allow classifiers for new tasks to be inferred in an elegant closed form, employing an adaptive subspace truncation that judiciously discards non-discriminative dimensions. Through rigorous experimentation on benchmark few-shot image classification tasks, we demonstrate that Meta-OLE outperforms the current state-of-the-art in meta-learning. Join us as we redefine the boundaries of rapid adaptation in the world of few-shot learning.", "introduction": "Meta learning, also referred to as learning to learn, aims at acquiring knowledge from a distribution of tasks, and learn to quickly solve novel tasks sampled from the same or similar underlying task distribution.Meta learning is extensively studied under the context of few-shot learning (FSL), and realized by models that adapt efficiently when given only a few labeled samples of novel tasks.The research is mainly driven by how to design the adaptation for acquiring task-specified models efficiently and robustly.Prototypical networks (ProtoNets) (Snell et al., 2017) adapt to new tasks by computing the prototype of each class simply as the average of feature vectors, with all the network parameters shared across tasks.MAML (Finn et al., 2017) adapts to new tasks by a few iterations of gradient descent, and this approach has inspired many subsequent methods (Antoniou et al., 2018;Finn et al., 2018;Nichol et al., 2018;Yoon et al., 2018).The adaptation of the entire network makes it hard to be scaled to large networks, and many recent efforts focus on adapting the last classification layer only (Gordon et al., 2019;Bertinetto et al., 2019), while assuming a universal feature extractor that is shared across all tasks.In this paper, we attempt to attack few-shot image classification from a new perspective of geometry regularization of the feature space.As observed in (Lezama et al., 2018), training deep networks with softmax and cross-entropy loss does not simultaneously enforce intra-class similarity and interclass margins.On the other hand, encouraging features to be in a low-rank subspace in each class as well as orthogonal across classes can significantly improve the robustness of deep classification networks.While such explicit orthogonal low-rank geometry regularization has been proved successful in classical classification tasks Lezama et al. (2018); Qiu et al. (2018); Lezama et al. (2017), it remains highly non-trivial to extend this geometry regularization approach to tasks with novel classes involved at the testing stage.In few-shot image classification, further challenges arise from the demand of robust generalization to novel unseen classes.As we will show in this work, large class margins resulting from explicit geometric regularization can potentially allow novel knowledge to be represented by a composition of existing knowledge, and can reduce interference across classes.An illustration is presented in Figure 1.Seen classes (a) Feature space trained with standard softmax and cross-entropy loss.While linear boundaries are learned for the seen classes, the lack of enforcing intra-class similarity and inter-class separation leaves little space for novel classes (purple and and yellow dots) to be well represented in the feature space without interference.Motivated by the maximal-margin feature space geometry, we introduce meta-learned orthogonal low-rank embedding (Meta-OLE) to combine the simplicity of the prototype based methods and the adaptivity of the parameter adaptation based methods.Specifically, we encourage an orthogonal low-rank structure to the feature space across classes.Thus feature vectors of the same class reside in a subspace with imposed low-rankness, while subspaces across classes are encouraged to be as orthogonal as possible.While imposing geometry-regularization to the feature space over seen classes has been investigated Lezama et al. (2018;2017); Wen et al. (2016), the induced feature extractor does not guarantee to generalize well to novel object classes that are unseen during training.To extend an orthogonal low-rank embedding to a few-shot learning scenario, we introduce a metalearning framework with a light-weight adaptive orthogonal low-rank transformation that is able to adapt efficiently to novel classes with very few examples.We then show that, given the imposed lowrank orthogonal geometry, the final classification of query samples can be performed by subspace projections, where the projection matrices are directly inferred from the few labeled examples in a closed form.And we show that, to adaptively adjust the dimension of the projections based on the compactness of the feature subspace, the robustness of the classifier to outlier examples can be further improved.The closed-form inference of class labels allows unlabeled samples to be easily involved in the learning of the adaptive orthogonal low-rank transformation for label propagation, and improved performance is observed without any auxiliary parametric components to infer the pseudo labels.Despite being simple and geometry-motivated, the proposed method achieves on public FSL datasets superior performance to state-of-the-art methods that often involve more sophisticated components.In summary, our contributions are as follows:\u2022 We propose to impose low-rank orthogonal geometry in feature space for few-shot learning.\u2022 We introduce meta-learned adaptive orthogonal low-rank transformations for efficient adaptations to novel tasks with unseen classes.\u2022 Geometry-motivated classifier based on subspace projections with adaptive dimension selection is introduced for fast and robust class inference.\u2022 The effectiveness of the proposed Meta-OLE is validated with extensive experiments on few-shot image classification."}
{"paper_id": 276, "abstract": "In the ever-evolving realm of machine learning, hyperparameter optimization (HPO) stands as a pivotal challenge, often shrouded in the fog of resource limitations and the vast expanse of possible evaluations. Despite the strides made in this field, the quest for efficient HPO remains an unsolved riddle, prompting researchers to delve into the intriguing territory of transfer learning as a means to mitigate the inefficiencies inherent in HPO.   In a bold departure from conventional methodologies, we introduce the Deep Kernel Gaussian Process surrogate with Landmark Meta-features (DKLM), a groundbreaking approach designed to harness the power of joint meta-training across a diverse array of source tasks, allowing for seamless and efficient transfer to new, unseen target tasks. At the heart of DKLM lies an innovative end-to-end meta-feature network, meticulously crafted to capture the nuanced similarities between hyperparameter configurations. This network not only embeds the evaluated configurations but also their corresponding performance, culminating in a rich tapestry of contextualized, dataset-specific similarity representations.  Through rigorous experimentation across a broad spectrum of HPO meta-datasets sourced from OpenML, we validate the prowess of DKLM, showcasing its empirical superiority over a suite of cutting-edge baselines. In doing so, we illuminate a path forward in the quest for more efficient hyperparameter optimization, one that promises to reshape the landscape of machine learning deployment.", "introduction": "Hyperparameter optimization (HPO) is an essential open problem in machine learning (ML) due to the black-box nature of methods' empirical performances as a function of their hyperparameters.The major challenge lies in the computational infeasibility of training and evaluating a large sample of hyperparameters in order to identify the best generalization performances.As a result, transfer learning lends itself as a promising direction for improving the sample efficiency of HPO methods (Wistuba et al., 2016;Perrone et al., 2018;Wistuba & Grabocka, 2021).Prior approaches for transfer learning in HPO rely on exploring existing evaluations on a pool of datasets where the model under investigation is evaluated.The similarity between datasets is often captured via features describing their characteristics (a.k.a.meta-features), such as descriptive statistics of dataset features (Michie et al., 1994;Wistuba et al., 2016), or landmark measures in the form of the accuracies gathered from a set of basic classifiers (nearest neighbors, decision trees, SVMs, etc.) on the datasets (Pfahringer et al., 2000;Feurer et al., 2014).A recent trend highlights the potential of learning parametric meta-feature extractors for tabular datasets (Jomaa et al., 2021a), which are further meta-trained (Finn et al., 2017) to improve the HPO transferability to new datasets (Jomaa et al., 2021b).Unfortunately, typical transfer learning from a set of unrelated source tasks suffers from the negative transfer phenomenon (Wang et al., 2019), which implies a poor generalization performance on target tasks that are dissimilar to the source tasks, according to a predefined dissimilarity measure, e.g.similarity of response curves.This can happen, for example, when a model is learned jointly across tasks without task-specific attributes.To resolve the negative transfer of HPO performance predictors (a.k.a.surrogates) we introduce a novel direction that conditions Gaussian Process (GP) surrogates in Bayesian Optimization on the meta-features of datasets.In that manner, we can transfer knowledge only from similar datasets, and hence conditioned on the similarity of meta-features.However, in contrast to ad-hoc dataset meta-features that are hand-crafted by domain experts, we propose a novel architecture for deep GP kernels (Wilson et al., 2016a) that are enriched with novel end-to-end neural network components that generate meta-features only from the tuples of past hyperparameter configurations and their evaluated performances.Our meta-feature network is a set-based neural network that is invariant to the permutation/sequence of past hyperparameter evaluations.We jointly train Deep Kernel Gaussian Process surrogate with Landmark Meta-features (DKLM) through HPO meta-learning (Wistuba & Grabocka, 2021).To validate the empirical performance of our method we present extensive results on a large-scale benchmark that involves 16 different search spaces and 101 datasets from OpenML for a total of 3.4 million hyperparameter evaluations (Pineda-Arango et al., 2021).Detailed experiments against a series of traditional HPO methods, as well as recent transfer HPO baselines, demonstrate the superiority of meta-learning the initialization of DKLM.Overall, we make the following contributions:\u2022 Introduce the first paper that tackles the negative transfer phenomenon in Bayesian HPO, by conditioning GP surrogates on meta-features, i.e. on dataset characteristics;\u2022 Propose an end-to-end deep GP which implicitly learns networks that generate metafeatures, with no ad-hoc inductive bias from experts on manually designing meta-features;\u2022 Demonstrate the empirical superiority of our method on a very-large-scale experimental protocol (3.4 million hyperparameter evaluations), against a large number of baselines."}
{"paper_id": 277, "abstract": "In this paper, we embark on a journey through the intricate landscape of collaborative learning within a diverse research network, where each client stands as a guardian of their own unique sample population, shielded by the barriers of privacy. Our quest is to forge models for each client that not only surpass the limitations of their individual data but also thrive through the power of secure alliances with their peers in the network.  However, as we navigate this realm, we discover that the variances in sample distributions among clients are not simply obstacles; they are the very fabric of our challenge. Collaborating with every client may not yield the most optimal local models. Thus, we introduce a novel framework\u2014Learning to Collaborate\u2014where clients can strategically select their allies within the network. This approach seeks to establish a \"collaboration equilibrium,\" fostering smaller coalitions that empower each client to attain models of unparalleled utility.  To illuminate this path, we unveil the concept of the benefit graph, a dynamic representation of how each client can harness the strengths of collaboration with others. Through the lens of Pareto optimization, we chart a course to maximize these benefits and derive collaboration coalitions based on sophisticated graph operations.  Our framework not only redefines the architecture of collaboration within research networks but also offers practical insights. We present compelling experiments on both synthetic and real-world datasets, showcasing the transformative effectiveness of our method. Join us as we explore this new frontier of collaborative learning, where the synergy of shared knowledge leads to groundbreaking advancements.", "introduction": "Effective learning of machine learning models over a collaborative network of data clients has drawn considerable interest in recent years.Frequently, due to privacy concerns, we cannot simultaneously access the raw data residing on different clients.Therefore, distributed (Li et al., 2014) or federated learning (McMahan et al., 2017) strategies have been proposed, where typically model parameters are updated locally at each client with its own data and the parameter updates, such as gradients, are transmitted out and communicate with other clients.During this process, it is usually assumed that the participation in the network comes at no cost, i.e., every client is willing to participate in the collaboration.However, this is not always true in reality.One example is the clinical research network (CRN) involving multiple hospitals (Fleurence et al., 2014).Each hospital has its own patient population.The patient data are sensitive and cannot be shared with other hospitals.If we want to build a risk prediction model with the patient data within this network in a privacy-preserving way, the expectation from each hospital is that a better model can be obtained through participating in the CRN compared to the one built from its own data collected from various clinical practice with big efforts.In this scenario, there has been a prior study showing that the model performance can decrease when collaborating with hospitals with very distinct patient populations due to negative transfer induced by sample distribution discrepancies (Wang et al., 2019;Pan & Yang, 2009).With these considerations, in this paper, we propose a novel learning to collaborate framework.We allow the participating clients in a large collaborative network to form non-overlapping collaboration coalitions.Each coalition includes a subset of clients such that the collaboration among them can benefit their respective model performance.We aim at identifying the collaboration coalitions that can lead to a collaboration equilibrium, i.e., there are no other coalition settings that any of the individual clients can benefit more (i.e., achieve better model performance).To obtain the coalitions that can lead to a collaboration equilibrium, we propose a Pareto optimization framework to identify the necessary collaborators for each client in the network to achieve its maximum utility.In particular, we optimize a local model associated with a specific client on the I 1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" Z r o C L 0 J J b p l Y Y x K F 0 T u I b B 7 Z 7 x k = \" > A A A C y H i c j V H L S s N A F D 2 N r 1 p fn z r 3 n z N x 7 v S T g q b S s l 4 I x M z s 3 v 1 B c L C 0 t r 6 y u l d c 3 m m m c C Z 8 5 f h z E o u 2 5 K Q t 4 x B z J Z c D a i W B u 6 A W s 5 Q 2 P V b x 1 y 0 T K 4 + h C j h L W D d 1 B x P v c d y V R z u n V 2 J 5 c l y t W 1 d L L n A Z 2 D i r I V y M u P + M S P c T w k S E E Q w R J O I C L l J 4 O b F h I i O t i T J w g x H W c Y Y I S a T P K Y p T h E j u k 7 4 B 2 n Z y N a K 8 8 U 6 3 2 6 Z S A X k F K E z u k i S l P E F a n m T q e a W f F / u Y 9 1 p 7 q b i P 6 e 7 l X S K z E D b F / 6 T 4 z / 6 t T t U j 0 c a h r 4 F R T o h l V n Z + 7 Z L o r 6 u b m l 6 o k O S T E K d y j u C D s a + V n n 0 2 t S X X t q r e u j r / q T M W q v Z / n Z n h T t 6 Q B 2 z / H O Q 2 a e 1 W 7 V t 0 / r 1 X q R / m o i 9 j C N n Z p n g e o 4 w Q N O O T N 8 Y B H P B l n R m L c G a O P V K O Q a z b x b R n 3 7 z l h k Q U = < / l a t e x i t > I 2 < l a t e x i t s h a 1 _ b a s e 6 4 = \" 2 + + c L 5 T E X u m v U + o a h U s T V n d Z R 8 E = \" > A A A C y H i c j V H L T s J A F D 3 U F + I L d e m m k Z i 4 I i 3 B 6 J L o R l 1 h Y o E E 0 b T D g B N K 2 7 R T D S F s / A G 3 + m X G P 9 C / 8 M 5 Y E p U Y n a b t m X P v O T P 3 X i / y R S I t 6 z V n z M 0 v L C 7 l l w s r q 2 v r G 8 X N r U Y S p j H j D g v 9 M G 5 5 b s J 9 E X B H C u n z V h R z d + j 5 v O k N T l S 8 e c f j R I T B p R x F v D N 0 + 4 H o C e Z K o p y z 6 3 F l c l M s W W V L L 3 M W 2 B k o I V v 1 s P i C K 3 Q R g i H F E B w B J G E f L h J 6 2 r B h I S K u g z F x M S G h 4 x w T F E i b U h a n D J f Y A X 3 7 t G t n b E B 7 5 Z l o N a N T f H p j U p r Y I 0 1 I e T F h d Z q p 4 6 l 2 V u x v 3 m P t q e 4 2 o r + X e Q 2 J l b g l 9 i / d N P O / O l W L R A 9 H u g Z B N U W a U d W x z C X V X V E 3 N 7 9 U J c k h I k 7 h L s V j w k w r p 3 0 2 t S b R t a v e u j r + p j M V q / Y s y 0 3 x r m 5 J A 7 Z / j n M W N C p l u 1 o + u K i W a s f Z q P P Y w S 7 2 a Z 6 H q O E U d T j k L f C I J z w b 5 0 Z k 3 B u j z 1 Q j l 2 m 2 8 W 0 Z D x 8 7 w p E G < / l a t e x i t > I 3 < l a t e x i t s h a 1 _ b a s e 6 4 = \" G 0 6 m X S C W + K 1 1 x S P K J + c v 3 b G Q h M E = \" > A A A C y H i c j V H L T s J A F D 3 U F + I L d e m m k Z i 4 I k U x u i S 6 U V e Y W C B B N O 0 w 4 I S + 0 k 4 1 h L D x B 9 z q l x n / Q P / C O 2 N J V G J 0 m r Z n z r 3 n z N x 7 3 c g T i b S s 1 5 w x M z s 3 v 5 B f L C w t r 6 y u F d c 3 G k m Y x o z b L P T C u O U 6 C f d E w G 0 p p M d b U c w d 3 / V 4 0 x 2 c q H j z j s e J C I N L O Y x 4 x 3 f 6 g e g J 5 k i i 7 L P r 0 f 7 4 p l i y y p Z e 5 j S o Z K C E b N X D 4 g u u 0 E U I h h Q + O A J I w h 4 c J P S 0 U Y G F i L g O R s T F h I S O c 4 x R I G 1 K W Z w y H G I H 9 O 3 T r p 2 x A e 2 V Z 6 L V j E 7 x 6 I 1 J a W K H N C H l x Y T V a a a O p 9 p Z s b 9 5 j 7 S n u t u Q / m 7 m 5 R M r c U v s X 7 p J 5 n 9 1 q h a J H o 5 0 D Y J q i j S j q m O Z S 6 q 7 o m 5 u f q l K k k N E n M J d i s e E m V Z O + m x q T a J r V 7 1 1 d P x N Z y p W 7 V m W m + J d 3 Z I G X P k 5 z m n Q 2 C t X q u W D i 2 q p d p y N O o 8 t b G O X 5 n m I G k 5 R h 0 3 e A o 9 4 w r N x b k T G v T H 8 T D V y m W Y T 3 5 b x 8 A E + I 5 E H < / l a t e x i t > I 4< l a t e x i t s h a 1 _ b a s e 6 4 = \" 0 7 8 T l P i n Z n b p 3 G 8 Y D O a C G 0 4 E 8 G 4 = \" > A A A C y H i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R K p 6 L L o R l 1 V M G 2 h V k n S a R 2 a F 8 l E K a U b f 8 C t f p n 4 B / o X 3 h l H U I v o h C R n z r 3 n z N x 7 v S T g m b C s l 4 I x M z s 3 v 1 B c L C 0 t r 6 y u l d c 3 m l m c p z 5 z / D i I 0 7 b n Z i z g E X M E F w F r J y l z Q y 9 g L W 9 4 L O O t W 5 Z m P I 4 u x C h h 3 d A d R L z P f V c Q 5 Z x e j W u T 6 3 L F q l p q m d P A 1 q A C v R p x + R m X 6 C G G j x w h G C I I w g F c Z P R 0 Y M N C Q l w X Y + J S Q l z F G S Y o k T a n L E Y Z L r F D + g 5 o 1 9 F s R H v p m S m 1 T 6 c E 9 K a k N L F D m p j y U s L y N F P F c + U s 2 d + 8 x 8 p T 3 m 1 E f 0 9 7 h c Q K 3 B D 7 l + 4 z 8 7 8 6 W Y t A H 4 e q B k 4 1 J Y q R 1 f n a J V d d k T c 3 v 1 Q l y C E h T u I e x V P C v l J + 9 t l U m k z V L n v r q v i r y p S s 3 P s 6 N 8 e b v C U N 2 P 4 5 z m n Q 3 K v a t e r + e a 1 S P 9 K j L m I L 2 9 i l e R 6 g j h M 0 4 J A 3 x w M e 8 W S c G Y l x Z 4 w + U o 2 C 1 m z i 2 z L u 3 w F A h J E I < / l a t e x i t > I 5< l a t e x i t s h a 1 _ b a s e 6 4 = \" r x H 3 e G L 3 j l L z h Y J G R a o k r V T V v j c = \" > A A A C y H i c j V H L T s J A F D 3 U F + I L d e m m k Z i 4 I s V A d E l 0 o 6 4 w s U C C a N o y 4 I S + M p 1 q C G H j D 7 j V L z P + g f 6 F d 8 a S q M T o N G 3 P n H v P m b n 3 u r H P E 2 l Z r z l j b n 5 h c S m / X F h Z X V v f K G 5 u N Z M o F R 6 z v c i P R N t 1 E u b z k N m S S 5 + 1 Y 8 G c w P V Z y x 2 e q H j r j o m E R + G l H M W s G z i D k P e 5 5 0 i i 7 L P r c W 1 y U y x Z Z U s v c x Z U M l B C t h p R 8 Q V X 6 C G C h x Q B G E J I w j 4 c J P R 0 U I G F m L g u x s Q J Q l z H G S Y o k D a l L E Y Z D r F D + g 5 o 1 8 n Y k P b K M 9 F q j 0 7 x 6 R W k N L F H m o j y B G F 1 m q n j q X Z W 7 G / e Y + 2 p 7 j a i v 5 t 5 B c R K 3 B L 7 l 2 6 a + V + d q k W i j y N d A 6 e a Y s 2 o 6 r z M J d V d U T c 3 v 1 Q l y S E m T u E e x Q V h T y u n f T a 1 J t G 1 q 9 4 6 O v 6 m M x W r 9 l 6 W m + J d 3 Z I G X P k 5 z l n Q P C h X q u X a R b V U P 8 5 G n c c O d r F P 8 z x E H a d o w C Z v j k c 8 4 d k 4 N 2 L j 3 h h 9 p h q 5 T L O N b 8 t 4 + A B C 5 Z E J < / l a t e x i t > I 6< l a t e x i t s h a 1 _ b a s e 6 4 = \" r X 3 1 m 9 p L 4 T D F i t z g 9 F O b u h v E h n 0 = \" > A A A C y H i c j V H L T s J A F D 3 U F + I L d e m m k Z i 4 I s X g Y 0 l 0 o 6 4 w s U C C a N p h w A l 9 p Z 1 q C G H j D 7 j V L z P + g f 6 F d 8 a S q M T o N G 3 P n H v P m b n 3 u p E n E m l Z r z l j Z n Z u f i G / W F h a X l l d K 6 5 v N J I w j R m 3 W e i F c c t 1 E u 6 J g N t S S I + 3 o p g 7 v u v x p j s 4 U f H m H Y 8 T E Q a X c h j x j u / 0 A 9 E T z J F E 2 W f X o 4 P x T b F k l S 2 9 z G l Q y U A J 2 a q H x R d c o Y s Q D C l 8 c A S Q h D 0 4 S O h p o w I L E X E d j I i L C Q k d 5 x i j Q N q U s j h l O M Q O 6 N u n X T t j A 9 o r z 0 S r G Z 3 i 0 R u T 0 s Q O a U L K i w m r 0 0 w d T 7 W z Y n / z H m l P d b c h / d 3 M y y d W 4 p b Y v 3 S T z P / q V C 0 S P R z p G g T V F G l G V c c y l 1 R 3 R d 3 c / F K V J I e I O I W 7 F I 8 J M 6 2 c 9 N n U m k T X r n r r 6 P i b z l S s 2 r M s N 8 W 7 u i U N u P J z n N O g s V e u V M v 7 F 9 V S 7 T g b d R 5 b 2 M Y u z f M Q N Z y i D p u 8 B R 7 x h G f j 3 I i M e 2 P 4 m W r k M s 0 m v i 3 j 4 Q N F R p E K < / l a t e x i t > (a) Benefit Graph (b) Finding stable coalition I 4< l a t e x i t s h a 1 _ b a s e 6 4 = \" 0 7 8 T l P i n Z n b p 3 G 8 Y D O a C G 0 4 E 8 G 4 = \" > A A A C y H i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R K p 6 L L o R l 1 V M G 2 h V k n S a R 2 a F 8 l E K a U b f 8 C t f p n 4 B / o X 3 h l H U I v o h C R n z r 3 n z N x 7 v S T g m b C s l 4 I x M z s 3 v 1 B c L C 0 t r 6 y u l d c 3 m l m c p z 5 z / D i I 0 7 b n Z i z g E X M E F w F r J y l z Q y 9 g L W 9 4 L O O t W 5 Z m P I 4 u x C h h 3 d A d R L z P f V c Q 5 Z x e j W u T 6 3 L F q l p q m d P A 1 q A C v R p x + R m X 6 C G G j x w h G C I I w g F c Z P R 0 Y M N C Q l w X Y + J S Q l z F G S Y o k T a n L E Y Z L r F D + g 5 o 1 9 F s R H v p m S m 1 T 6 c E 9 K a k N L F D m p j y U s L y N F P F c + U s 2 d + 8 x 8 p T 3 m 1 E f 0 9 7 h c Q K 3 B D 7 l + 4 z 8 7 8 6 W Y t A H 4 e q B k 4 1 J Y q R 1 f n a J V d d k T c 3 v 1 Q l y C E h T u I e x V P C v l J + 9 t l U m k z V L n v r q v i r y p S s 3 P s 6 N 8 e b v C U N 2 P 4 5 z m n Q 3 K v a t e r + e a 1 S P 9 K j L m I L 2 9 i l e R 6 g j h M 0 4 J A 3 x w M e 8 W S c G Y l x Z 4 w + U o 2 C 1 m z i 2 z L u 3 w F A h J E I < / l a t e x i t > I 5< l a t e x i t s h a 1 _ b a s e 6 4 = \" r x H 3 e G L 3 j l L z h Y J G R a o k r V T V v j c = \" > A A A C y H i c j V H L T s J A F D 3 U F + I L d e m m k Z i 4 I s V A d E l 0 o 6 4 w s U C C a N o y 4 I S + M p 1 q C G H j D 7 j V L z P + g f 6 F d 8 a S q M T o N G 3 P n H v P m b n 3 u r H P E 2 l Z r z l j b n 5 h c S m / X F h Z X V v f K G 5 u N Z M o F R 6 z v c i P R N t 1 E u b z k N m S S 5 + 1 Y 8 G c w P V Z y x 2 e q H j r j o m E R + G l H M W s G z i D k P e 5 5 0 i i 7 L P r c W 1 y U y x Z Z U s v c x Z U M l B C t h p R 8 Q V X 6 C G C h x Q B G E J I w j 4 c J P R 0 U I G F m L g u x s Q J Q l z H G S Y o k D a l L E Y Z D r F D + g 5 o 1 8 n Y k P b K M 9 F q j 0 7 x 6 R W k N L F H m o j y B G F 1 m q n j q X Z W 7 G / e Y + 2 p 7 j a i v 5 t 5 B c R K 3 B L 7 l 2 6 a + V + d q k W i j y N d A 6 e a Y s 2 o 6 r z M J d V d U T c 3 v 1 Q l y S E m T u E e x Q V h T y u n f T a 1 J t G 1 q 9 4 6 O v 6 m M x W r 9 l 6 W m + J d 3 Z I G X P k 5 z l n Q P C h X q u X a R b V U P 8 5 G n c c O d r F P 8 z x E H a d o w C Z v j k c 8 4 d k 4 N 2 L j 3 h h 9 p h q 5 T L O N b 8 t 4 + A B C 5 Z E J < / l a t e x i t > I 6< l a t e x i t s h a 1 _ b a s e 6 4 = \" r X 3 1 m 9 p L 4 T D F i t z g 9 F O b u h v E h n 0 = \" > A A A C y H i c j V H L T s J A F D 3 U F + I L d e m m k Z i 4 I s X g Y 0 l 0 o 6 4 w s U C C a N p h w A l 9 p Z 1 q C G H j D 7 j V L z P + g f 6 F d 8 a S q M T o N G 3 P n H v P m b n 3 u p E n E m l Z r z l j Z n Z u f i G / W F h a X l l d K 6 5 v N J I w j R m 3 W e i F c c t 1 E u 6 J g N t S S I + 3 o p g 7 v u v x p j s 4 U f H m H Y 8 T E Q a X c h j x j u / 0 A 9 E T z J F E 2 W f X o 4 P x T b F k l S 2 9 z G l Q y U A J 2 a q H x R d c o Y s Q D C l 8 c A S Q h D 0 4 S O h p o w I L E X E d j I i L C Q k d 5 x i j Q N q U s j h l O M Q O 6 N u n X T t j A 9 o r z 0 S r G Z 3 i 0 R u T 0 s Q O a U L K i w m r 0 0 w d T 7 W z Y n / z H m l P d b c h / d 3 M y y d W 4 p b Y v 3 S T z P / q V C 0 S P R z p G g T V F G l G V c c y l 1 R 3 R d 3 c / F K V J I e I O I W 7 F I 8 J M 6 2 c 9 N n U m k T X r n r r 6 P i b z l S s 2 r M s N 8 W 7 u i U N u P J z n N O g s V e u V M v 7 F 9 V S 7 T g b d R 5 b 2 M Y u z f M Q N Z y i D p u 8 B R 7 x h G f j 3 I i M e 2 P 4 m W r k M s 0 m v i 3 j 4 Q N F R p E K < / l a t e x i t > (d) Collaboration Equilibrium (c) Re-build Benefit Graph I 1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" Z r o C L 0 J J b p l Y Y x K F 0 T u I b B 7 Z 7 x k = \" > A A A C y H i c j V H L S s N A F D 2 N r 1 p fn z r 3 n z N x 7 v S T g q b S s l 4 I x M z s 3 v 1 B c L C 0 t r 6 y u l d c 3 m m m c C Z 8 5 f h z E o u 2 5 K Q t 4 x B z J Z c D a i W B u 6 A W s 5 Q 2 P V b x 1 y 0 T K 4 + h C j h L W D d 1 B x P v c d y V R z u n V 2 J 5 c l y t W 1 d L L n A Z 2 D i r I V y M u P + M S P c T w k S E E Q w R J O I C L l J 4 O b F h I i O t i T J w g x H W c Y Y I S a T P K Y p T h E j u k 7 4 B 2 n Z y N a K 8 8 U 6 3 2 6 Z S A X k F K E z u k i S l P E F a n m T q e a W f F / u Y 9 1 p 7 q b i P 6 e 7 l X S K z E D b F / 6 T 4 z / 6 t T t U j 0 c a h r 4 F R T o h l V n Z + 7 Z L o r 6 u b m l 6 o k O S T E K d y j u C D s a + V n n 0 2 t S X X t q r e u j r / q T M W q v Z / n Z n h T t 6 Q B 2 z / H O Q 2 a e 1 W 7 V t 0 / r 1 X q R / m o i 9 j C N n Z p n g e o 4 w Q N O O T N 8 Y B H P B l n R m L c G a O P V K O Q a z b x b R n 3 7 z l h k Q U = < / l a t e x i t > I 2 < l a t e x i t s h a 1 _ b a s e 6 4 = \" 2 + + c L 5 T E X u m v U + o a h U s T V n d Z R 8 E = \" > A A A C y H i c j V H L T s J A F D 3 U F + I L d e m m k Z i 4 I i 3 B 6 J L o R l 1 h Y o E E 0 b T D g B N K 2 7 R T D S F s / A G 3 + m X G P 9 C / 8 M 5 Y E p U Y n a b t m X P v O T P 3 X i / y R S I t 6 z V n z M 0 v L C 7 l l w s r q 2 v r G 8 X N r U Y S p j H j D g v 9 M G 5 5 b s J 9 E X B H C u n z V h R z d + j 5 v O k N T l S 8 e c f j R I T B p R x F v D N 0 + 4 H o C e Z K o p y z 6 3 F l c l M s W W V L L 3 M W 2 B k o I V v 1 s P i C K 3 Q R g i H F E B w B J G E f L h J 6 2 r B h I S K u g z F x M S G h 4 x w T F E i b U h a n D J f Y A X 3 7 t G t n b E B 7 5 Z l o N a N T f H p j U p r Y I 0 1 I e T F h d Z q p 4 6 l 2 V u x v 3 m P t q e 4 2 o r + X e Q 2 J l b g l 9 i / dr y p S s 3 P s 6 N 8 e b v C U N 2 P 4 5 z m n Q 3 K v a t e r + e a 1 S P 9 K j L m I L 2 9 i l e R 6 g j h M 0 4r y p S s 3 P s 6 N 8 e b v C U N 2 P 4 5 z m n Q 3 K v a t e r + e a 1 S P 9 K j L m I L 2 9 i l e R 6 g j h M 0 4a a a O p 9 p Z s b 9 5 j 7 S n u t u Q / m 7 m 5 R M r c U v s X 7 p J 5 n 9 1 q h a J H o 5 0 D Y J q i j S j q m O Z S 6 q 7 o m 5 u f q Pareto front of the learning objectives of all clients.Through the analysis of the geometric location of such an optimal model on the Pareto front, we can identify the necessary collaborators of each client.The relationships between each client and its necessary collaborators can be encoded in a benefit graph, in which each node denotes a client and the edge from I j to I i represents I j is one of the necessary collaborators for I i , as exemplified in Figure 1 (a).Then we can derive the coalitions corresponding to the collaboration equilibrium through an iterative process introduced as follows.Specifically, we define a stable coalition as the minimum set such that its all involved clients can achieve its maximal utility.From the perspective of graph theory, these stable coalitions are actually the strongly connected components of the benefit graph.For example, C = I 1 , I 2 , I 3 in Figure 1 (b) is a stable coalition as all clients can achieve their best performance by collaborating with the clients in C (compared with collaborating with other clients in the network).By removing the stable coalitions and re-building the benefit graph of the remaining client iteratively as shown in Figure 1 (b) and (c), we can identify all coalitions as in Figure 1 (d) and prove that the obtained coalitions can lead to a collaboration equilibrium.We empirically evaluate our method on synthetic data, UCI adult (Kohavi, 1996), a classical FL benchmark data set CIFAR10 (Krizhevsky et al., 2009), and a real-world electronic health record (EHR) data repository eICU (Pollard et al., 2018), which includes patient EHR data in ICU from multiple hospitals.The results show our method significantly outperforms existing relevant methods.The experiments on eICU data demonstrate that our algorithm is able to derive a good collaboration strategy for the hospitals to collaborate."}
{"paper_id": 278, "abstract": "In a world where the stakes of classification and Out-of-Distribution (OoD) detection are higher than ever\u2014impacting safety, security, and defense\u2014researchers grapple with the daunting challenges posed by the few-shot setting. The rarity of samples and the specter of adversarial attacks loom large, complicating our efforts to achieve reliable outcomes. Traditional deep neural networks often misplace their confidence, assigning high certainty to samples that lie far from the training data, which only exacerbates the problem of OoD detection.  To confront these formidable obstacles, we introduce the Few-shot ROBust (FROB) model\u2014a beacon of innovation in classification and few-shot OoD detection. Our approach is built upon a foundation of enhanced robustness and dependable confidence predictions tailored specifically for the few-shot paradigm. At the heart of FROB lies a novel methodology that generates a support boundary for the normal class distribution, seamlessly integrating it with few-shot Outlier Exposure (OE).  FROB employs a self-supervised learning strategy to establish a confidence boundary that is informed by both generative and discriminative models. This groundbreaking contribution lies in the way FROB combines the generated boundary with self-supervised learning, imposing a deliberate low confidence at this learned threshold. In doing so, FROB not only crafts formidable adversarial samples along the boundary but also compels the classifier to assign reduced confidence to both OoD samples and those straddling the boundary.  Our model demonstrates a remarkable capacity for generalization, effectively handling unseen anomalies and adversarial attacks, even in real-world test sets that diverge from the training data. To bolster robustness further, FROB reimagines and refines the Outlier Exposure framework, ensuring its efficacy even in zero-shot scenarios. By incorporating our learned boundary into the architecture, FROB adeptly lowers the threshold associated with few-shot robustness, keeping OoD performance stable and largely unaffected by the number of few-shot samples.  Extensive evaluations of FROB across various image datasets and One-Class Classification (OCC) data reveal its prowess, showcasing competitive state-of-the-art performance. In particular, FROB excels in its resilience against the outlier OoD few-shot sample population and its inherent variability, setting a new benchmark in the quest for robust classification and detection in challenging environments.", "introduction": "In real-world settings, it is crucial to robustly perform classification and OoD detection with high levels of confidence.The problem of detecting whether a sample is in-distribution, from the training distribution, or OoD is critical for adversarial attacks.This is crucial nowadays in many applications in safety, security, and defence.However, deep neural networks produce overconfident predictions and do not distinguish in-and out-of-data-distribution.Adversarial examples, when small modifications of the input appear, can change the classifier decision.It is an important property of a classifier to address such limitations with high level of confidence, and provide robustness guarantees for neural networks.In parallel, OoD detection is a challenging aim since classifiers set high confidence to OoD samples away from the training data.The state-of-art models are overconfident in their predictions, and do not distinguish in-and OoD.The setting that our proposed Few-shot ROBust (FROB) model addresses is robust few-shot Out-of-Distribution (OoD) detection and few-shot Outlier Exposure (OE).To address rarity and the limited samples in the few-shot setting, we aim at reducing the number of the few-shots of the OoD samples, while maintaining accurate and robust performance.Diverse data are available today in large quantities.Deep learning magnifies the difficulty of distinguishing OoD from in-distribution.It is possible to use such data to improve OoD detection by training detectors with auxiliary outlier sets (Hendrycks et al., 2019).OE enables detectors to generalize to detect unseen OoD samples with improved robustness and performance.Models trained with different outliers can detect unmodelled data and improve OoD detection by learning cues for whether inputs are unmodelled.By exposing models to different OoD, the complement of the support of the normal class distribution is modelled and the detection of new types of anomalies is enabled.OE improves the calibration of deep neural network classifiers in the setting where a fraction of the data is OoD, addressing the problem of classifiers being overconfident when applied to OoD (Bitterwolf et al., 2020).Aiming at solving the few-shot robustness problem with classification and OoD detection, the contribution of our FROB methodology is the development of an integrated robust framework for self-supervised few-shot negative data augmentation on the distribution confidence boundary, combined with few-shot OE, for improved OoD detection.The combination of the generated boundary in a self-supervised learning way and the imposition of low confidence at this learned boundary is the main contribution of FROB, which greatly and decisively improves robustness for few-shot OoD detection.To address the rarity of relevant outliers during training using OoD samples, we propose to use even few-shots to improve the OoD detection performance.FROB achieves significantly better robustness and resilience to few-shot OoD detection, while maintaining competitive in-distribution accuracy.FROB achieves generalization to unseen anomalies, with applicability to new, in the wild, test sets that do not correlate to the training sets.FROB's evaluation on different sets, CIFAR-10, SVHN, CIFAR-100, and low-frequency noise, using cross-dataset and One-Class Classification (OCC) evaluations, shows that our self-supervised model with few-shot OE on the confidence boundary and few-shot adaptation improves the few-shot OoD detection performance and outperforms benchmarks.The robustness performance analysis of FROB to the number of few-shots and to outlier variation shows that it is robust to few-shots and outperforms baselines."}
{"paper_id": 279, "abstract": "In this paper, we delve into the intriguing realm of Non-symmetric Determinantal Point Processes (NDPPs), presenting a fresh perspective on the challenges of online and streaming MAP inference and learning. Picture a scenario where data points cascade in an unpredictable sequence, and our algorithms must navigate this chaotic flow with the grace of a skilled swordsman\u2014executing a single pass and wielding only sub-linear memory. The stakes are raised in the online setting, where the demand for a valid solution at every moment transforms our task into a high-wire act, balancing precision and efficiency.  To tackle these formidable challenges, we unveil a suite of algorithms, each crafted with rigorous theoretical underpinnings. We put our creations to the test against a backdrop of real-world datasets, revealing that they perform with a proficiency that rivals the best offline algorithms\u2014those that hoard entire datasets and indulge in multiple passes. Join us as we explore this innovative approach, where the constraints of time and memory spark ingenuity, leading to solutions that are not just effective but also elegantly efficient.", "introduction": "Determinantal Point Processes (DPPs) were first introduced in the context of quantum mechanics (Macchi, 1975) and have subsequently been extensively studied with applications in several areas of pure and applied mathematics like graph theory, combinatorics, random matrix theory (Hough et al., 2006;Borodin, 2009), and randomized numerical linear algebra (Derezinski & Mahoney, 2021).Discrete DPPs have gained widespread adoption in machine learning following the seminal work of Kulesza & Taskar (2012) and there has been a recent explosion of interest in DPPs in the machine learning community.For instance, some of the very recent uses of DPPs include automation of deep neural network design (Nguyen et al., 2021), deep generative models (Chen & Ahmed, 2021), document and video summarization (Perez-Beltrachini & Lapata, 2021), image processing (Launay et al., 2021), and learning in games (Perez-Nieves et al., 2021).A DPP is a probability distribution over subsets of items and is characterized by some kernel matrix such that the probability of sampling any particular subset is proportional to the determinant of the submatrix corresponding to that subset in the kernel.Until very recently, most prior work on DPPs focused on the setting where the kernel matrix is symmetric.Due to this constraint, DPPs can only model negative correlations between items.Recent work has shown that allowing the kernel matrix to be nonsymmetric can greatly increase the expressive power of DPPs and allows them to model compatible sets of items (Gartrell et al., 2019;Brunel, 2018).To differentiate this line of work from prior literature on symmetric DPPs, the term Nonsymmetric DPPs (NDPPs) has often been used.Modeling positive correlations can be useful in many practical scenarios.For instance, an E-commerce company trying to build a product recommendation system would want the system to increase the probability of suggesting a router if a customer adds a modem to a shopping cart.State-of-the-art algorithms for learning and inference on NDPPs (Gartrell et al., 2021) require storing the full data in memory and take multiple passes over the complete dataset.Therefore, these algorithms take too much memory to be useful for large scale data, where the size of the entire dataset can be much larger than the random-access memory available.These algorithms are also not practical in settings where data is generated on the fly, for example, in E-commerce applications where new items are added to the store over time, and more importantly, added to the carts of users instantaneously.Streaming and Online Inference: We formulate streaming and online versions of maximum a posteriori (MAP) inference on fixed-size NDPPs and provide algorithms for solving these problems.In the streaming setting, data points arrive in an arbitrary order and the algorithms are constrained to use a single-pass over the data as well as sub-linear memory (i.e.memory that is substantially smaller than the size of the data stream).The online setting we consider has an additional restriction that we need to maintain a valid solution at every time step.For both these settings, we provide algorithms which have comparable or even better solution quality than the offline greedy algorithm while taking only a single pass over the data and using a fraction of the memory used by the offline algorithm.Online Learning: We introduce the online learning problem for NDPPs and provide an algorithm which solves this problem using a single-pass over the data and memory that is constant in m, the number of baskets in the training data (or equivalently the length of the stream).In comparison, the offline learning algorithm takes a large number of passes over the entire data and uses memory linear in m.Strikingly, our online learning algorithm shows comparable performance (log-likelihood) to the state-of-the-art offline learning algorithm, while converging significantly faster in all cases (Figure 2).This is notable, since our algorithm uses only a single pass over the data, while using a tiny fraction of the memory."}
{"paper_id": 280, "abstract": "In the realm of real-world applications, an agent\u2019s observations can often be tainted by measurement errors or insidious adversarial noises, leading it down a path of suboptimal decisions or, worse yet, a complete training collapse. In this exploration, we delve into the robustness of distributional Reinforcement Learning (RL)\u2014a cutting-edge methodology that seeks to capture the entire distribution of potential returns rather than merely the average.   To frame our investigation, we introduce the State-Noisy Markov Decision Process (SN-MDP) within a tabular context, designed to accommodate both stochastic and adversarial disturbances in state observations. Here, we derive the contraction properties for both expectation-based and distributional Bellman operators, shedding light on their underlying mechanics.   As we extend our focus to SN-MDP with function approximation, we rigorously characterize the bounded gradient norm of histogram-based distributional loss, elucidating the enhanced training robustness inherent in distributional RL. Furthermore, we establish more stringent convergence conditions for Temporal-Difference (TD) learning amidst a broader spectrum of state noise, complemented by a sensitivity analysis leveraging influence functions.  Through a comprehensive suite of experimental evaluations across various gaming scenarios, we demonstrate that distributional RL consistently outperforms its expectation-based counterparts in terms of training robustness, even when faced with a multitude of state observation disturbances. The findings underscore the resilience of distributional methods in navigating the unpredictable landscape of real-world challenges.", "introduction": "Learning robust and high-performance policies for continuous state-action reinforcement learning (RL) domains is crucial to enable the successful adoption of deep RL in robotics, autonomy, and control problems.However, recent works have demonstrated that deep RL algorithms are vulnerable either to model uncertainties or external disturbances (Huang et al., 2017;Pattanaik et al., 2017;Ilahi et al., 2020;Chen et al., 2019;Zhang et al., 2020;Shen et al., 2020;Singh et al., 2020;Guan et al., 2020).Particularly, model uncertainties normally occur in a noisy reinforcement learning environment where the agent often encounters systematic or stochastic measurement errors on state observations, such as the inexact locations and velocity obtained from the equipped sensors of a robot.On the other hand, external disturbances are normally adversarial in nature.For instance, the adversary can construct adversarial perturbations on state observations to degrade the performance of deep RL algorithms.These two factors lead to noisy state observations that influence the performance of algorithms, precluding the success of reinforcement learning in real-world applications.Existing works mainly focus on improving the robustness of algorithms in the test environment with noisy state observations.Smooth Regularized Reinforcement Learning (Shen et al., 2020) introduced a regularization to enforce smoothness in the learned policy, and thus improved its robustness against measurement errors in the test environment.Similarly, the State-Adversarial Markov decision process (SA-MDP) (Zhang et al., 2020) was proposed and the resulting principled policy regularization enhances the adversarial robustness of various kinds of RL algorithms against adversarial noisy state observations.However, both of these works assumed that the agent can access clean state observations during training, which is normally not feasible when the environment is inherently noisy, such as unavoidable measurement errors.Thus, the maintenance and formal analysis of policies robust to noisy state observations during training is a worthwhile area of research.On the other hand, recent distributional reinforcement learning algorithms, including C51 (Bellemare et al., 2017), Quantile-Regression DQN (QRDQN) (Dabney et al., 2018b), Implicit Quantile Networks (Dabney et al., 2018a) and Moment-Matching DQN (MMD) (Nguyen et al., 2020), constantly set new records in Atari games, gaining huge attention in the research community.However, existing literature mainly focuses on the performance of algorithms, other benefits, including the robustness in the noisy environment, of distributional RL algorithms are less studied.As distributional RL can leverage additional information about distribution that captures the uncertainty of the environment more accurately, it is natural to expect that distributional RL with this better representation capability can be less vulnerable to the noisy environment while training, which motivates our research.In this paper, we investigate the robustness of distributional RL against various kinds of state observation noises encountered during training.Firstly, we propose a general State-Noisy MDP in the tabular setting, in which we prove the convergence of distributional Bellman operator.We further extend SN-MDP to the function approximation case by considering more complex noisy state observations.Notably, we analyze the vulnerability of classical RL and in contrast characterize the Lipschitz continuity blessing resulting from the Histogram distributional loss in distributional RL, which leads to a bounded gradient norm.This better behaved gradient mitigates the impact of noisy states on the objective function, accounting for the less vulnerability of distributional RL while training.Finally, extensive experiments demonstrate that both expectation-based and distributional RL algorithms can converge in SN-MDP-like settings.More importantly, distributional RL algorithms tend to achieve better robust performance in the presence of more complex state observation noises compared with its expectation-based counterpart that may even diverge in some cases.These empirical results in Section 5 echo our previous theoretical results in both Section 3 and 4. Overall, the training robustness advantage of distributional RL algorithms we revealed facilitates their deployment especially in the noisy environment."}
{"paper_id": 281, "abstract": "In a world where the demands of machine learning systems grow ever more insatiable, the quest for innovation in the frameworks that support them has become a formidable challenge\u2014akin to navigating a labyrinthine realm of complexity and scale. As the computational hunger of these systems drives a relentless surge in advancements across compilers, networking, and hardware, the integration of these innovations into machine learning tools lags behind, trapped in the grip of outdated paradigms. This stagnation stems, in part, from the cumbersome nature of prototyping new computational approaches within the confines of existing frameworks, which often cater predominantly to machine learning researchers and practitioners, sidelining the equally vital voices of systems researchers who hold the keys to progress.  Enter Flashlight\u2014a beacon of hope in this intricate landscape. This open-source library is crafted with the express purpose of igniting a renaissance in machine learning tools and systems. It champions an ethos of openness, modularity, and customization, all while housing state-of-the-art, research-ready models and training setups that span a diverse array of domains. With Flashlight, systems researchers are empowered to swiftly prototype and experiment with groundbreaking ideas in machine learning computation, all while enjoying low overhead that allows it to compete with, and often surpass, the giants of the field. We envision Flashlight not merely as a tool, but as a bridge\u2014one that brings machine learning and systems researchers closer together, fostering collaboration that will ultimately enrich the libraries and frameworks that countless others rely upon in their own quests for knowledge and innovation.", "introduction": "The recent rise of deep learning-based techniques has been accompanied and sustained by the wide availability of dedicated frameworks such as TensorFlow (Abadi et al., 2016) and PyTorch (Paszke et al., 2019).These frameworks have enabled the democratization of machine learning research by providing extensive collections of high level primitives to support common use cases.Lowering the barrier to entry for end users has boosted the popularity of both neural networks and the frameworks in which they are implemented.However, in order to support what are now vast ecosystems and a diverse user base, framework size and complexity have increased dramatically over time.As a result, deep, groundbreaking framework research has become extremely onerous and time consuming, precluding rapid innovation.Given these barriers, major deep learning frameworks have become stuck in their existing operating modes.Innovation in this area remains as important as ever.Indeed, framework innovation accelerates machine learning (ML) and artificial intelligence (AI) research.Frameworks that are easier to use reduce the engineering burden on researchers, and frameworks that are higher-performance decrease the time required to iterate on experimental work and validate hypotheses.Even more critically, tooling plays a fundamental role in deciding which ideas succeed or fail.For example, LeCun et al. (1989) pioneered the use of convolutional neural networks (CNNs) (Fukushima & Miyake, 1982) trained using backpropagation for computer vision tasks in the late 1980s, which was subsequently applied to handwriting recognition.However, widespread success for CNNs was achieved two decades later when Krizhevsky et al. (2012a) leveraged the CUDA programming model to take advantage of graphics processing units (GPUs) to train a much deeper model (AlexNet).While deep learning frameworks have been optimized to leverage existing hardware paradigms for common neural network architectures, they often fail to deliver similar efficiencies on designs that diverge from the mainstream.For example, Barham & Isard (2019) explain how the design of these frameworks results in poor hardware utilization for a novel type of neural network, known as a capsule network (Hinton et al., 2018), that leverages new components such as squashing operations and routing by agreement.More generally, what are now unconventional approaches to modern problems in machine learning require highly-specialized additions to popular frameworks.As a result of narrowly-optimized systems, research beyond deep learning may be discounted due to purported computational infeasibility given modern frameworks' capabilities.Furthermore, the waning of Moore's law (Theis & Wong, 2017) coupled with the ever-growing computational demands of deep learning are prompting several shifts in hardware.Massive-scale distributed computing is now required to train leading models -a process that established frameworks remain unable to handle truly automatically.In parallel, multiple specialized hardware products are now available to better support deep learning applications: Nvidia's TensorCores (Markidis et al., 2018), Google's TPUs (Jouppi et al., 2017), Graphcore's IPUs (Jia et al., 2019), Apple's Neural Enginefoot_0 , and others have been developed to improve total float-pointing operations (FLOPs), cost per-FLOP, or energy consumption.Additionally, numerous efforts are underway to move away from conventional von Neumann computing architectures in which memory and processing units are physically separated, either by storing data closer to compute units or by switching to in-memory computing altogether.While tooling innovation is alive and well given these incentives for progress, working within large, well-established frameworks has become more and more challenging as framework size and scope grows.As a result, many recent innovations have required the development of ad-hoc tools.For example, efforts in machine learning-driven compilation of neural networks are largely built on top of Halide (Adams et al., 2019;Steiner et al., 2021) and TVM (Chen et al., 2018;Zheng et al., 2020); FlexFlow (Jia et al., 2018;2020) underpins recent work aimed at improving the use of distributed computing to accelerate the training of large neural networks; and PET (Wang et al., 2021) provides a framework that enables graph-level neural network optimizations.With ad-hoc approaches, researchers are required to start from scratch for new directions or adapt their ideas to fit into the scaffolding these frameworks provide -resulting in significant technical burdens.To sustain framework innovation, we introduce Flashlight, an open source minimalist ML library designed to support research in machine learning frameworks, facilitate rapid iteration on ideas, reduce the engineering burden on researchers, and remove the need for new tools.Flashlight includes:\u2022 A modular, component-based architecture that makes every aspect of the implementation fully customizable with simple internal APIs.\u2022 A compact yet highly-performant reference implementation of each component.\u2022 A comprehensive set of benchmarks representative of the state-of-the-art in machine learning on which to evaluate alternative implementations."}
{"paper_id": 282, "abstract": "In the realm of deep learning, where the clash of algorithms often overshadows the subtler intricacies of data flow, it is the movement of data\u2014rather than the arithmetic operations themselves\u2014that emerges as the true titan of energy consumption during inference. In this work, we embark on a quest to tame these data movement costs by curtailing the diversity of weights within a neural network. The guiding principle is simple: by constraining the number of unique weights, we can weave a tapestry of efficiency, allowing the entire network to nestle within the processing elements (PEs) of accelerator designs, thereby slashing the energy costs associated with weight retrieval.  To this end, we unveil a method we have dubbed Weight Fixing Networks (WFN). Our design is meticulously crafted to achieve four pivotal objectives: i) a mere handful of unique weights, ii) weight encodings that boast low entropy, iii) unique weight values that lend themselves to energy-efficient hardware multiplication, and iv) the preservation of task performance without loss. Yet, as in any grand tale, these objectives often find themselves at odds. To navigate this web of conflict, we employ a blend of innovative strategies\u2014some novel, others drawn from the well-worn paths of research; a unique regularization term, a perspective on clustering costs as relative distance changes, and an emphasis on the holistic reuse of weights across the network.  Our experiments on Imagenet reveal a triumph of lossless compression, achieving a staggering 56-fold reduction in unique weights and a 1.9-fold decrease in weight-space entropy compared to state-of-the-art quantization methods. In this endeavor, we not only illuminate a path toward greater efficiency but also carve a new niche in the landscape of deep learning optimization.", "introduction": "The Importance of Data Movement Costs.Although there has been a significant amount of attention exploring both algorithmic (Sze et al., 2020) and hardware-based (Chen et al., 2020) approaches to reducing the energy costs of deep learning inference, there is often a noted disconnect between the two (Sze et al., 2017).The most expensive energy costs lie in memory reads (Horowitz, 2014;Gao et al., 2017).For every off-chip DRAM data read, you pay the equivalent of over two hundred 32-bit multiplications in energy costs 1 (Horowitz, 2014).Algorithmic techniques that hope to reduce energy consumption have focussed predominantly on metrics like floating-point operations (FLOPs); intuitively, fewer FLOPs should translate into smaller energy costs from the reduced number of multiplications and potentially, fewer parameters to read from memory.However, it has been observed that this is a weak proxy for the energy consumed in running a deep learning model for inference (Sze et al., 2017).Data movement costs can still be high if we do not consider the re-use of weights, the delay between their re-use, and filter shape effects.Simply reducing the FLOPs alone does not guarantee energy cost reductions -particularly those incurred through data read and write activity.Accelerator Design Considerations.These data movement costs are difficult to reduce in von Neumann architectures (Li et al., 2015;Sebastian et al., 2020), and so there is a move towards co-design of algorithm-hardware inference accelerators.A core consideration of accelerator design lies in exploring dataflow mappings.These mappings determine how data used in computation is distributed across memory components and optimised to take advantage of the re-use of weights, matrix multiplication partial-sums (psums), and data inputs (Han et al., 2016;Chen et al., 2017;2015;2020).Modern deep learning accelerators also make use of hundreds of processing elements (Chen et al., 2020) (PE's) for computation.Each PE contains a small amount of cheap and fast access memory to store a few pieces of information (weights, psums, inputs, etc.).A dataflow mapping cuts down data movement costs by making the best use of each PE and ensuring that the information they contain is recycled as much as possible.A weight/input/psum that cannot be used quickly after its latest use will be written to storage and require a re-read later.Less re-use leads to increased data movement costs.Consider, for example, weight-stationary dataflow, commonly used in SOTA accelerator designs (Farshchi et al., 2019;Jouppi et al., 2017).Here the items stored  ResNet-18 ResNet34ResNet-34 Figure 1: WFN has more weight re-use opportunities than existing quantisation approaches which can be used to reduce data movement costs.Left: The total number of unique parameters left after quantisation is 56x fewer than APoT for ResNet-18 trained on the Imagenet dataset and 71x for the ResNet-34 model.Right: The entropy of the parameters across the network is 1.9x and 1.65x smaller when using the WFN approach over APoT.statically in the PE are the model weights.Input data is then fed into the relevant PE's where multiplication is conducted locally.The ideal situation for a dataflow mapping would be to pay a single data movement cost for each unique weight value and then reference these weight values using indexing.The indexing costs can be kept low using an appropriate encoding scheme such as Huffman coding and approximated by the entropy of the weight space.Thus, the indexing access costs plus unique value access costs can be much smaller than the unquantised network (Mao & Dally, 2016;Wu et al., 2018).Objectives.So we ask ourselves what we could do algorithmically to maximise the benefit of accelerator dataflows?We think an easy win is to reduce the number of unique weights a network uses.Fewer unique weights whilst fixing the network topology and the total number of parameters will mean that more weights are re-used more often.This additional re-use gives more opportunity to dataflows to maintain often-used weights in PE's.To further enhance the compressibility, it is desirable for the distribution of the unique weights to be concentrated around a handful of values.The high probability density weights would then be used more often and could then be reliably stored inside PE's, saving both the cost of overwriting these weight values and re-fetching them when needed later.Finally, we ask what the ideal values of these weights would be.From a computational perspective, not all multiplications are created equal.Integer powers-of-two, for example, can be implemented as simple bit-shifts.Mapping the weights used most to these values offers potential further energy reductions.Putting these three requirements together: few unique weights; a low-entropy encoding with a distribution of weights highly concentrated around a tiny subset of values; and a focus on powers-of-two values for weights -all motivated to reduce computation costs in accelerator designs -we present our contribution.Weight Fixing Networks.Our work's overarching objective is to transform a network comprising many weights of any value (limited only by value precision) to one with the same number of weights but just a few unique values.Rather than selecting the unique weights a priori, we let the optimisation guide the process in an iterative cluster-then-train approach.In each iteration, we cluster an ever-increasing subset of weights to one of a few cluster centroids.We map the pre-trained network weights to these cluster centroids, which are the pool of unique weights.The training stage follows standard gradient descent optimisation to minimise performance loss with two key additions.Firstly, only an ever decreasing subset of the weights are free to be updated.And secondly, we use a new regularisation term to penalise weights in proportion to their nearest clusters' relative distance.We iteratively cluster subsets of weights to their nearest cluster centre, as has been demonstrated successfully previously (Zhou et al., 2017).The way we determine which subset to move is a core component of our contribution which leads to the superior compression results we achieve.Small Relative Distance Change.Rather than selecting subsets with small Euclidean distances to cluster centres, or those that have small magnitude (Zhou et al., 2017), we make the simple Figure 2: We explore adding relative vs absolute noise to each of the layers (x-axis).The layer index indicates which layer was selected to have noise added.Each layer index is a separate experiment with the 95% confidence intervals shaded.observation that the relative -as opposed to absolute -weight change matters.The distance a weight is moved when quantised is dependent on the distance between the weight w i and its new value w i + \u03b4w i .When the new value is zero -as is the case for pruning methodsthen the magnitude of the weight is the distance.Yet, this is not the case more generally.We demonstrate the importance of optimising quantisation for small relative changes with simple empirical observations.Using a pre-trained ResNet-18 model, we test adding relative vs absolute noise to the layers' weights and measure the accuracy change.For relative noise we set a noise level \u03b2 and adjust all weights w i in a layer l as:The standard deviation of noise added is determined by the original value w l i .We contrast this with absolute noise experiments, where we instead set w l i \u2190 w l i + N (0, \u03b2|w l |) where |w l | corresponds to the mean absolute weight value in layer l.We run each layer-\u03b2 combination experiment multiple times -to account for fluctuation in the randomised noise -and present the results in Figure 1.Even though the mean variation of noise added is the same, noise relative to the original weight value (multiplicative noise) is much better tolerated than absolute (additive noise).Since moving weights to quantisation centres is analogous to adding noise, we translate these results into our approach and prioritise weights that have small relative distances to be clustered first.We find avoiding significant quantisation errors requires ensuring that |\u03b4wi| |wi| is small for all weights.If this is not possible, then performance could suffer.In this case, we create an additional cluster centroid in the vicinity of an existing cluster to reduce this relative distance.Our work also challenges the almost universal trend in the literature (Yuhang Li, Xin Dong, 2020;Jung et al., 2019;Zhang et al., 2018a;Zhou et al., 2016;Yamamoto, 2021;Oh et al., 2021) of leaving the first and last layers either at full precision or 8-bit; we attempt a full network quantisation.The cost of not quantising the first layer -which typically requires the most re-use of weights due to the larger resolution of input maps -and the final linear layer -which often contains the largest number of unique weight values -is too significant to ignore.With multiple stages of training and clustering, we finish with a significantly reduced set of unique weights.The regularisation term encourages high probability regions in the weight distribution and a lower-entropy weight-space.The initial choice of cluster centroids as powers-of-two helps us meet our third objective -energy-saving multiplication.Overall we find four distinct advantages over the works reviewed:\u2022 We assign a cluster value to all weights -including the first and last layers.\u2022 We emphasise a low entropy encoding with a regularisation term, achieving entropies smaller than even those seen using 3-bit quantisation approaches -over which we report superior performance.\u2022 We require no additional layerwise scaling; the unique weights are shared across all layers.\u2022 WFN substantially reduces the number of unique parameters in a network when compared to existing SOTA quantisation approaches."}
{"paper_id": 283, "abstract": "In the realm of computational fluid dynamics, where the intricate dance of fluid flow unfolds, we find ourselves at the intersection of innovation and complexity. Physics Informed Neural Networks (PINNs) have emerged as a beacon of hope, adeptly capturing the elusive nature of these phenomena. With the advent of parallel processing algorithms harnessed on GPUs, we have dramatically accelerated our ability to solve the formidable Navier-Stokes Equations, yet this speed often comes at a price\u2014a sacrifice in accuracy as traditional CFD approaches lean on approximations to simplify the modeling process.  In this paper, we unveil a groundbreaking architecture, the Attention-Augmented Physics Informed Neural Network (AA-PINN), designed to delve deeper into the partial differential equations that govern fluid dynamics. Our architecture employs a sophisticated blend of channel and spatial attention mechanisms, allowing the network to focus on the most critical aspects of the flow. Furthermore, we introduce a novel loss function, meticulously crafted to robustly accommodate both initial and boundary conditions, ensuring that our model adheres closely to the physical realities it seeks to represent.  Through rigorous evaluation using metrics such as Root Mean Square Error (RMSE), divergence, and thermal kinetic energy, we demonstrate that our AA-PINN not only meets but exceeds the performance of previous PINN approaches in modeling the Navier-Stokes and Burgers Equations. In doing so, we pave the way for a new era of fluid dynamics modeling, where precision and computational efficiency can coexist harmoniously.", "introduction": "Computational Fluid Dynamics (CFD) has become the core technology behind almost every fluid simulation.Fluid mechanics has been traditionally concerned with big data, thus making deep learning an obvious choice in modelling the inherent complexity of the problem.Neural Networks of late has been quite successful in understanding, predicting, optimizing, and controlling fluid flows.Neural Network has proven to improve optimization performance and reduce convergence time drastically.Neural network is also used for turbulence modelling and identifying low and high dimensional flow regimes.Deep learning algorithms are able to take into account inherent complexity of the problem thus optimizing for the performance, robustness or convergence for complex tasks.Understanding the physics behind fluid flows is a complex problem which can be solved using neural networks by feeding lots of training data.It helps in providing a general purpose framework for interpretability, generalizability and explainability of the results achieved."}
{"paper_id": 284, "abstract": "In the realm of medical imaging, where precision is paramount and every pixel can hold the weight of life and death, deep learning\u2014particularly through the lens of convolutional neural networks\u2014has emerged as a beacon of hope. It has triumphed across a spectrum of challenges, from image classification to the intricate dance of image segmentation and synthesis. Yet, amidst these triumphs, a crucial question lingers: how do we gauge the confidence of these models in their predictions? This inquiry is not merely academic; it is essential for fostering trust in safety-critical applications where human lives hang in the balance.  In this study, we delve into the heart of uncertainty itself, employing an encoder-decoder architecture steeped in the principles of variational inference to tackle the formidable task of segmenting brain tumor images. Our exploration is grounded in the esteemed BRATS dataset, where we measure our model's prowess using the Dice Similarity Coefficient (DSC) and Intersection Over Union (IOU) metrics\u2014tools that illuminate our path to understanding performance.  What sets our approach apart is its dual embrace of uncertainty: we account for both aleatoric and epistemic uncertainties, navigating these complexities with a principled Bayesian framework. In doing so, we not only aim to segment brain tumors with remarkable accuracy but also to illuminate the shadows of uncertainty that accompany such vital predictions, forging a path toward safer, more reliable medical imaging.", "introduction": "Medical image segmentation is a challenging task for medical practitioners.It is costly, takes time and is prone to error.Hence there is a need to automate the manually done segmentation.Lately Neural Networks have shown great potential on a variety of medical image segmentation problems.The challenge with the approaches used in literature is that the model doesn't predict the uncertainty associated.This is where Bayesian methods come into play as it gives a principled way of measuring uncertainty from the model predictions.Measuring uncertainty in the output predictions made by neural networks is important for interpretation and validation.Rather than learning the point estimates, Bayesian Neural Networks (BNN) learns the distribution over the weights.The training process of BNN involves first initializing the parameters of the neural network.Next the weights are sampled from some distribution (like gaussian with zero mean and unit variance) and both the forward pass and backward pass is done to update the weights using the conventional backpropagation algorithm.Monte Carlo dropout networks (Kingma et al., 2015) use dropout layers to approximate deep Gaussian processes which still lack theoretical understanding.Bayesian Convolutional Neural Network (Gal and Ghahramani, 2015) use variational inference to learn the posterior distribution over the weights given the dataset.The problem with this approach is that it requires a lot of computation involving a lot of parameters, making this technique not scalable in practice.Variational Autoencoder (Kingma et al., 2015) which is based on generative models solves the above problems and has been successful in a number of tasks like generating images, texts, recommender systems etc.This approach comes with several challenges in its own right which have been successfully tackled in the literature.A random variable sampled from posterior distribution has no gradient so the conventional backpropagation techniques can't be applied to it.Local Reparameterization Trick (Kingma et al., 2015) was proposed to tackle this by converting the random variable to a deterministic one for computation.The second challenge was the huge computational requirement since it required weight updates in every iteration.Bayes by Backprop algorithm (Blundell et al., 2015) tackled this by calculating gradients in backpropagation using a scale and shift approach by updating the posterior distribution in the backward pass."}
{"paper_id": 285, "abstract": "In this paper, we unveil a groundbreaking architecture known as the Attention Aware Network (AASeg), designed to tackle the challenges of real-time semantic image segmentation. Picture a network that not only perceives the world around it but also understands the nuances of spatial and channel information through the clever integration of Spatial Attention (SA) and Channel Attention (CA) modules. But we didn't stop there; our design harnesses the power of dense local multi-scale context with the Multi Scale Context (MSC) module, weaving together a rich tapestry of feature maps that culminates in a precise segmentation map.  Through rigorous analysis, quantitative experiments, and a detailed ablation study on the Cityscapes, ADE20K, and Camvid datasets, we illustrate the prowess of our method. The results speak volumes: our network outshines many of its predecessors, achieving an impressive 74.4% Mean Intersection over Union (IOU) on the Cityscapes test dataset, all while maintaining a blistering speed of 202.7 frames per second. This is not just a step forward; it is a leap into the future of image segmentation, where speed and accuracy coalesce in perfect harmony.", "introduction": "Semantic Segmentation is a fundamental problem in computer vision where the goal is to label each and every pixel in the image to its appropriate class.Since it is required to be deployed in real world settings like robots and autonomous vehicles, hence there is a need to balance the speed vs performance tradeoff.Fully Convolutional Network (FCN) (Long et al., 2015) composed of convolutional layers was one of the first works to get strong semantic representation.However, this method was not able to capture boundary information accurately.Atrous convolutions (Yu and Koltun, 2015) at the last several stages of their network was used to give feature maps with strong semantic representation, thus solving the problem with FCN based architectures.However, this comes at at the cost of increased computational complexity.For real world navigation tasks like autonomous driving, there is a need to improve the Frame Per Second(FPS).Chen et al. (2017) and (Yu and Koltun, 2015) used dilated convolutions to increase the recpetive field while maintaining the number of parameters.SegNet (Badrinarayanan et al., 2017) utilizes a small network structure and the skip-connected method to achieve improved FPS. (Mehta et al., 2018), (Paszke et al., 2016) and (Poudel et al., 2019) proposed unique approaches to tackle real time semantic segmentation problem.2 RELATED WORK (Fu et al., 2019) introduces spatial-wise and channelwise attention modules to enhance the recpetive field (Paszke et al., 2016) trims a a lot of convolution filters to reduce computation.ICNet (Zhao et al., 2018a) proposed an image cascade network using multiresolution branches.(Iandola et al., 2016) allows the neural network to find the critical channels of the feature map and select the most suitable channels by itself.ESPNet (Mehta et al., 2018) introduces an efficient spatial pyramid (ESP), which brings great improvement in both speed and performance.Bilateral Segmentation Network (BiSeNet) (Yu et al., 2018a) used two parts: Spatial Path (SP) is used to get with the loss of spatial information and Context Path (CP) for compressing the receptive field.(Yu et al., 2020) used multi-path framework to combine the low-level details and high-level semantics.(Li et al., 2019b) utilizes a light-weight backbone to speed up its network and a multi scale feature aggregation to improve accuracy.SwiftNet (Orsic et al., 2019) used lateral connections to restore the prediction resolution while maintaining the speed.(Lin et al., 2017) uses a multipath refinement network to refine the feature but ignores the global context feature.The speed-Accuracy performance comparison of state of the art methods on the Cityscapes test set is shown in Figure 1:Figure 1: Speed-Accuracy performance comparison on the Cityscapes test set.Our approach achieves higher mean IOU while still being faster than most existing methods..We summarize our main contributions as follows:\u2022 We propose a novel Attention Aware Network (AASeg) for real time semantic segmentation.\u2022 Our network is comprised of three parts: Spatial Attention(SA) module to capture the spatial dependencies of the feature maps, Channel Attention(CA) module to extract high level semantic information and Multi Scale Context(MSC) module to learn the information flow between feature maps of consecutive levels.\u2022 Detailed experiments and analysis indicate the efficacy of our proposed network in not only improving the performance but also FPS.We achieve results on par with previous state of the art using Cityscapes, Camvid and on ADE20K datasets."}
{"paper_id": 286, "abstract": "In the ever-evolving realm of computer vision, the attention mechanism has emerged as a beacon of innovation, captivating researchers with its promise of enhanced performance. Yet, as is often the case in our pursuit of excellence, this quest has frequently led to a burdensome increase in computational complexity. In this paper, we unveil a groundbreaking attention module that not only surpasses existing benchmarks but does so with a remarkable reduction in parameter count compared to most of its contemporaries.  Introducing the Dual Multi Scale Attention Network (DMSANet), a creation designed with both elegance and efficiency in mind. This network is divided into two distinct yet harmonious components: the first adeptly extracts features across multiple scales, weaving them together into a cohesive tapestry of information; the second employs parallel spatial and channel attention modules, skillfully adapting local features to their broader global contexts.  With DMSANet, we have crafted a lightweight solution that seamlessly integrates with existing convolutional neural networks, ensuring that performance does not come at the expense of practicality. We rigorously benchmark our network's capabilities, demonstrating its prowess in Image Classification on the ImageNet dataset, as well as in Object Detection and Instance Segmentation on the MS COCO dataset. In doing so, we invite the community to explore the potential of our approach and the future it heralds for the field of computer vision.", "introduction": "The local receptive field of the human eye has led to the construction of convolutional neural networks which has powered much of the recent advances in computer vision.Multi scale architecture used in the famous InceptionNet (Szegedy et al., 2016) aggregates multi-scale information from different size convolutional kernels.Attention Networks has attracted a lot of attention recently as it allows the network to focus on only then essential aspects while ignoring the ones which are not useful (Li et al., 2019), (Cao et al., 2019) and (Li et al., 2019).A lot of problems have been successfully tackled using attention mechanism in computer vision like image classification, image segmentation, object detection and image generation.Most of the attention mechanisms can be broadly classified into two types channel attention and spatial attention, both of which strengthens the original features by aggregating the same feature from all the positions with different aggregation strategies, transformations, and strengthening functions (Zhang et al., 2021).Some of the work combined both these mechanism together and achieved better results (Cao et al., 2019) and (Woo et al., 2018).The computational burden was reduced by (Wang et al., 2020) using efficient channel attention and 1 \u00d7 1 convolution.The most popular attention mechanism is the Squeeze-and Excitation module (Hu et al., 2018b), which can significantly improve the performance with a considerably low cost.The \"channel shuffle\" operator is used (Zhang and Yang, 2021) to enable information communication between the two branches.It uses a grouping strategy, which divides the input feature map into groups along the channel dimension."}
{"paper_id": 287, "abstract": "In the vast and intricate realm of genomic biology, the challenge of regulatory genome modeling stands as a pivotal quest, essential for unraveling the mysteries of downstream tasks like promoter classification and the prediction of transcription factor binding sites. At the heart of this endeavor lies the need to understand how regulatory elements engage with one another, while also accounting for their variability across diverse cell types. Yet, the current landscape of deep learning methods often confines itself to modeling genome sequences tied to a limited selection of cell types, neglecting the complex interplay between multiple regulatory elements. This oversight results in models that excel only within the confines of their training sets, lacking the adaptability necessary for broader biological applications.  In response to this challenge, we unveil a novel approach to pre-training genomic data, one that is both elegantly simple and profoundly effective. We introduce **GeneBERT**, a multi-modal and self-supervised framework designed to transcend the limitations of existing methods. Our model ingeniously integrates the one-dimensional sequences of genomic data with a two-dimensional matrix representing the interactions between transcription factors and genomic regions. To bolster the robustness and generalizability of our approach, we propose three innovative pre-training tasks.  Our GeneBERT model undergoes rigorous training on an expansive ATAC-seq dataset, encompassing a staggering 17 million genome sequences. We then rigorously evaluate its performance across a spectrum of regulatory downstream tasks spanning various cell types, including promoter classification, transcription factor binding site prediction, disease risk estimation, and splicing site identification. The results of our extensive experiments illuminate the remarkable efficacy of our multi-modal and self-supervised pre-training strategy, heralding a new era in the analysis of large-scale regulatory genomics data.", "introduction": "In the gene biology research, many important regulatory problems including promoter (Li et al., 2006) classification and transaction factor binding sites (Stewart et al., 2012) prediction requires a well-designed approach for modeling regulatory genome sequences.As a multi-cellular organism, the human body is formed by different cell types, with each has its own gene expression patterns (Schena et al., 1995;Ross et al., 2000).Therefore, modeling regulatory genome, especially across different cell types, is crucial for both the understanding of this fundamental biological process and the development of RNA-level therapeutic intervention of a vast among of diseases.The main challenge is how to model the interaction between regulatory elements and its variability across different cell types.In recent years, some works (Chen et al., 2016;Kelley et al., 2016;Romero et al., 2017;Qiu et al., 2018;Torada et al., 2019;Chen et al., 2020a;Avsec et al., 2021;Ji et al., 2021) have been proposed to apply deep learning architectures on modeling the genome data.For example, Enformer (Avsec et al., 2021) combines dilated CNN and transformer architecture as well as multi-head output to predict gene expression and epigenomic marks, and gain some performance improvements against traditional methods.Most of these models are based on a supervised learning paradigm, which limits their abilities to learn the general interaction representations between different regulatory elements.As a result, each model is only useful for some specific downstream tasks.More recently, DNABERT (Ji et al., 2021) is introduced to formulate the whole DNA sequence as a sentence of nucleotide k-mers and utilize BERT to model the sequence generatively.In this way, the interactions between different regulatory elements could be well captured, like the language modeling approach in natural language processing (NLP) to capture the co-occur information between different words, and could be employed for modeling several related downstream tasks.However, DNABERT only performs well on the cell types in the training set, and generalizes poorly to unseen cell types.This is mainly because the whole DNA sequence is common for different cells, and pre-training on such data cannot well reflect various interaction patterns across different cell types.Integration of genome data modalities across different cell types could help to build a more holistic model of gene expression regulation and benefit downstream applications such as mutation impact evaluation and disease risk prediction, as well as promote our understanding of the cell-type-specific regulatory programs and various development processes and disease etiology.Inspired by this idea, in this work, we present a simple yet effective method called GeneBERT, for pre-training large-scale genome data in a multi-modal and self-supervised manner.Specifically, we simultaneously take the 1D modality (i.e.sequence) and a 2D modality (i.e. regulatory region) of genome data as the input, where three pre-training tasks are proposed to improve the robustness and generalizability of our model.1) masked genome modeling: we randomly mask some parts of the input k-mers with a special token (i.e., [MASK]), and the model is trained to predict the masked k-mers.2) next genome segment prediction: we train the model using the embedding [CLS] to classify whether a pair of given sequences are two consecutive sequences in a cell.3) sequence-region matching: a sequenceregion matching mechanism is proposed to capture the multi-modal alignment between sequence and regulatory region of genome data.We pre-train the GeneBERT on the ATAC-seq dataset (Domcke et al., 2020) with 17 million gene sequences.Furthermore, we conduct extensive experiments to evaluate our GeneBERT on various downstream tasks, including promoter prediction, transaction factor binding sites prediction, gene mutation localization, and personalized diseases prediction.Comprehensive ablation studies demonstrate the effectiveness of multi-modal self-supervised pre-training for large-scale genome data across different cell types.The main contributions of this work lie in the proposal of a simple yet effective method named GeneBERT, for large-scale genome data pre-training in a multi-modal and self-supervised manner.To the best of our knowledge, we are the first to incorporate different genome data modalities across various cell types into the pre-training for large-scale genome data, to tackle the regulatory genome modeling problem.Except for meaningful biological improvements, our model makes an important contribution to the machine learning community, by introducing a novel multi-modality construction.Different from existing multi-modal learning tasks, such as visual question answering, image caption, and image-text retrieval, the 'visual' modality in our work is constructed based on the regulatory property of the 'language' sequential units.This idea brings some inspiration to building a new 'visual' modality, based on text matching, part-of-speech tagging, and named entity recognition etc., for the study of NLP."}
{"paper_id": 288, "abstract": "In this paper, we unveil two innovative neural network architectures designed to enhance interpretability while achieving universal approximation: the Triangularly-constructed Neural Network (TNN) and the Semi-Quantized Activation Neural Network (SQANN). These constructions possess remarkable characteristics that set them apart in the realm of machine learning. First, they demonstrate a remarkable resilience against the dreaded phenomenon of catastrophic forgetting, ensuring that knowledge is preserved even as new information is learned. Second, we provide a rigorous proof that guarantees the potential for achieving arbitrarily high accuracy on training datasets. Finally, for any given input \\( x \\), users can trace the \"fingerprints\" of activation patterns back to specific training samples, allowing them to pinpoint those that resonate closely with \\( x \\). Furthermore, our networks empower users to identify out-of-distribution samples, enhancing the interpretability and reliability of neural network predictions.", "introduction": "Artificial neural networks (NN) have recently seen successful applications in many fields.Modern deep neural network (DNN) architecture, usually trained through the backpropagation mechanism, has been called a black-box because of its lack of interpretability.To tackle this issue, various studies have been performed to understand how a NN works; see the following surveys Arrieta et al. (2020); Gilpin et al. (2018); Tjoa & Guan (2020); Wiegreffe & Marasovi\u0107 (2021).This paper primarily proposes two interpretable models, namely triangularly-constructed NN (TNN) and Semi-Quantized Activation NN (SQANN).Both possess the following three notable properties: (1) Resistance to catastrophic forgetting.(2) Mathematical proofs for arbitrarily high accuracy on training datasets; experimentally demonstrable with python code and simple common datasets (see supp.materials).(3) Detection of out-of-distribution samples through weak activations.Concept disambiguation.Several concepts have multiple possible definitions.We clarify the definitions used in this paper.1. Interpretability.We consider only fine-grained interpretation, i.e. we look at the meaning of each single neuron or its activation in our models.2. Universal approximation.Readers might be familiar with universal approximation of functions on certain conditions, e.g.compact sets.Our models can be more general, e.g.user can freely choose the interpolation function between 2 activations based on knowledge of the local manifold.The function can even be pathological.See appendix A.2.1.3. Catastrophic forgetting: the tendency for knowledge of previously learned dataset to be abruptly lost as information relevant to a new dataset is incorporated.This definition is a slightly nuanced version of Kirkpatrick et al. (2017).Hence, our models' resistance to catastrophic forgetting is the following.Given a training dataset D accurately modeled by an architecture M , a new dataset D \u2032 (especially new, out of distribution dataset) can be incorporated into M without losing accuracy on D. See appendix A.2.2 (analogy to biological system included).Related works and interpretability issues.Recent remarkable studies on universal approximators include the Deep Narrow Network by Kidger & Lyons (2020), DeepONet universal approximation for operators by Lu et al. (2021) and the Broad Learning System by Chen et al. (2019); Hanin (2019); Park et al. (2021); Johnson (2019).While insightful, they do not directly address the eXplainable Artificial Intelligence (XAI) issue, especially the blackbox property of the DNN.Similarly, a number of classical papers provide theoretical insights for NN as universal approximators, but interpretability, transparency and fairness issues are not their main focus.The universal approximation theorem by Cybenko (1989) asserts that a NN with a single hidden layer can approximate any function to arbitrarily small error under common conditions, proven by asserting the density of that set of NN in the function space using classic mathematical theorems.In particular, its theorem 1 uses an abstract proof by contradiction.From the proof, it is not easy to observe the internal mechanism of a NN in a straight-forward manner; consequently modern works that depend on it (e.g.Deep Narrow Network) might inherit the blackbox property.Bottom-up constructions for function approximation using NN then emerged, though they also lack the interpretability (see appendix A.3 for more related works).Also consider a demonstration in Nielsen (2015) that could help improve our understanding of universal approximation.Outline.This paper is arranged as the following.Section 2 shows explicit TNN construction, related results, including a pencil-and-paper example for pedagogical purpose.Likewise, section 3 shows SQANN construction, statements regarding SQANN, another pencil-and-paper example, then experimental results of its application, before we conclude the paper with limitations and future works.Python codes and clearer figures are fully available in supp.materials (also see appendix)."}
{"paper_id": 289, "abstract": "In the intricate realm of Deep Reinforcement Learning, the shadows of transparency and fairness loom large, often cast by the enigmatic black-box nature of deep neural networks. These sophisticated constructs, while powerful, can obscure the very policies and value functions they are designed to illuminate. In this paper, we embark on a quest to dismantle these barriers through a meticulously crafted, bottom-up approach to neural network design. Here, each neuron and layer is imbued with distinct meanings and functionalities, translating complex computations into concepts that resonate with human understanding.  Through our deliberate and thoughtful architecture, we demonstrate that even the challenging lavaland problems can be navigated with a neural network model that boasts a surprisingly lean parameter count. Additionally, we unveil the Self Reward Design (SRD), a novel framework inspired by the principles of Inverse Reward Design. This innovative mechanism not only allows our interpretable model to tackle challenges through intentional design\u2014albeit with some imperfections\u2014but also facilitates optimization through the SRD process. Furthermore, it empowers the model to evade the treacherous unknown states by discerning neuron inactivations, which are aggregated into a collective activation termed \\(w_{unknown}\\). In this way, we forge a path toward a more transparent and fair future in Deep Reinforcement Learning, where understanding and utility walk hand in hand.", "introduction": "Reinforcement Learning (RL) and Deep Neural Network (DNN) have recently been integrated to solve problems with remarkable performance.The deep reinforcement learning greatly improves the state-of-the-art of control and, in the words of Sutton & Barto (2018), learning from interaction.Among the well-known successes are (1) the Deep Q-Network (Mnih et al. (2015)) which enabled machine to play Atari Games with incredible performance, and (2) AlphaGo which is capable of playing notoriously complex game of Go (Silver et al. (2016)), which has also been further developed and popularized as being capable of defeating human at pro level.Although DNN has proven to possess great potentials, it is a blackbox that is difficult to interpret.To address this difficulty, various works have emerged, thus we have a host of different approaches to eXplainable Artificial Intelligence (XAI); see surveys Arrieta et al. (2020); Gilpin et al. (2018); Tjoa & Guan (2020).They have shed some lights into the inner working of a DNN, but there may still be large gaps to fill.Note that there is no guarantee that interpretability is even attainable, especially when context-dependent interpretability can be subjective.In this paper, we propose the Self Reward Design, a non-traditional RL solution that combines highly interpretable human-centric design and the power of DNN.Our robot (representing any artificial agent) rewards itself through purposeful design of DNN architecture, enabling it to solve problem without training.The solution might be sub-optimal, but the use of trainable DNN modules (we use pytorch, specifically) addresses the problem.We show that performance is improved while interpretability is retained.This paper is arranged as the following.We start with clarifications.Then we briefly go through related works that inspire this paper.Then our interpretable design and SRD training idea are demonstrated with a 1D toy example, RobotFish.Following that, we introduce SRD on the main robot navigation problem on the 2D lavaland with a large focus on interpretable design, followed by SRD training and unknown avoidance.Its experimental results will be discussed in the section after, and finally, we conclude with limitations and future works."}
{"paper_id": 290, "abstract": "In the realm of graph-related challenges, Graph Neural Networks (GNNs) have risen to prominence, wielding their power with remarkable efficacy. Yet, as we delve deeper into the complexities of real-world problems, we encounter vast graphs that demand an ever-increasing amount of computational resources to harness the full potential of GNNs. The sheer scale and inherent noise of these graphs often push GNNs toward overfitting, unless they are tempered with proper regularization.   Intriguingly, recent explorations have unveiled a compelling truth: many of these expansive graphs harbor redundant elements that can be excised with minimal impact on performance. This revelation opens the door to innovative strategies, allowing for the removal of nodes or edges during GNN inference or as a preprocessing step that prunes the input graph. Such insights pave the way for the creation of cutting-edge GNNs that strike a balance between efficiency and accuracy.  In this paper, we embark on a quest to further illuminate this phenomenon, introducing a systematic approach we call Locality-Sensitive Pruning (LSP). Grounded in the principles of Locality-Sensitive Hashing, our method aims to refine a graph by ensuring that similar local environments from the original structure are mirrored in the resulting sparsified graph\u2014an essential characteristic for tackling graph-centric tasks. We substantiate the merits of our locality-driven pruning method by showcasing its advantages over alternative strategies across diverse scenarios.  Through a series of extensive experiments on both synthetic and real-world datasets, we demonstrate the prowess of LSP. Our findings reveal that it can deftly remove a substantial number of edges from large graphs, all while maintaining performance and achieving significant acceleration. In the ever-evolving landscape of graph neural networks, our work stands as a testament to the power of strategic pruning, unlocking new possibilities for efficiency without sacrificing effectiveness.", "introduction": "Graph neural networks have become extremely popular for tasks involving graph-data.The majority of architectures employ varieties of message-passings, such as Graph Convolutional Networks (Kipf & Welling, 2016), Graph Isomorphism Networks (Xu et al., 2018) and more (Wu et al., 2020).These architectures propagate belief information from nodes of a graph through adjacencies in order to generate representations that depend on wide graph environments.This property makes them well suited for learning tasks such as node classification (Bhagat et al., 2011;Zhang et al., 2018), link prediction (Kumar et al., 2020) and graph classification (Kriege et al., 2020;Cai et al., 2018).Although the aforementioned architectures have demonstrated great success, recent research has shown considerable limitations of GNNs.First, more complex architectures tend to be computationally demanding.For instance, Graph Attention Networks (GATs) (Veli\u010dkovi\u0107 et al., 2017) whose neighborhood aggregation mechanism employ computations of self-attentions to assign weights for neighboring nodes.Second, this methodology of neighborhood aggregation leads to an exponentially growing amount of information originating from exponentially growing neighborhoods that needs to be encoded within fixed-size node representation vectors, a phenomenon referred as oversquashing (Alon & Yahav, 2020).Furthermore, the varying number of nodes participating in each node's neighborhood leads to a highly varying amount of information that needs to be encoded within a fixed-length code, a phenomenon that we call the varying neighborhoods.Note that many other architectures (e.g., CNNs, MLPs, and RNNs) do not encounter this since they only accept fixed-size inputs or produce outputs with varying sizes that correspond to the input size.GNNs break this correspondence due to their neighborhood aggregation methodology.These phenomena become very prominent as the depth of the neighborhood grows, which limits our ability to develop Figure 1: Illustration of consistent pruning versus non-consistent pruning.On the left, the input consists of a graph with two highlighted environments whose topology is similar because the central nodes have the same set of neighbors.In this case, consistent pruning results with a graph that preserves the similarity between these environments while non-consistent pruning, e.g., random removal of edges, results with dissimilar environments.On the right, the two highlighted environments have dissimilar topologies because the central nodes have different sets of neighbors.Consistent pruning is likely to preserve this dissimilarity, while random might bring the similarity between them closer.deep GNNs.We further discuss these phenomena and provide explanations on how pruning edges could help mitigating them in Section 4.4.2.A popular approach for tackling these problems is to apply some transformation or augmentation to the input graph.One prominent approach is Graph Sparsification (Hu & Lau, 2013;Spielman & Teng, 2008;Spielman & Srivastava, 2008;Calandriello et al., 2018) in order to accelerate GNNs by removing nodes and edges from a graph under the constraint of preserving the predictive performance (Rong et al., 2019;Srinivasa et al., 2020;Ye & Ji, 2021;Hamilton et al., 2017).In fact, Faber et al. (2021) showed that very often a significant amount of edges can be removed without degrading the performance of a model, which raises a concern regarding the usage of popularly used benchmarks.They claimed that those tasks can be virtually solved through node features only, implying that the graph topology has limited contribution and edges can be safely disregarded.Motivated by the above discussion, we argue that graph sparsification can greatly improve the performance of GNNs in tasks where the graph topology is significant.Moreover, we argue that sparsification that is based on local properties of a graph is more in-line with the prevailing methodology of GCNs, as opposed to approaches that rely on global topologies, such as spectral methods.Therefore, we introduce Locality Sensitive Pruning, a new algorithm for edges pruning based on locality sensitive hashing (LSH) (Shakhnarovich et al., 2008).As a result, pruned graphs are qualified with structure dynamics that alleviate the ability to distinguish between different graphs and mitigate the computation burden.More importantly, similar environments of the input graph result in similar environments in the sparsified graph while dissimilar environments result in dissimilar environments in the sparsified graph with high probability, as depicted in Figure 1.Consequently, we preserve the ability to distinguish between environments of the graph that were distinguishable prior to the sparsification process."}
{"paper_id": 291, "abstract": "In the realm of distributed optimization, a powerful yet often misunderstood mechanism known as error feedback (EF) emerged from the minds of Seide et al. in 2014. Initially conceived as a heuristic, EF has since become a cornerstone for ensuring convergence in gradient-based methods, especially when intertwined with the art of communication compression through contractive operators. Yet, the existing theoretical framework surrounding EF is burdened by stringent assumptions\u2014think bounded gradients\u2014and offers rather disheartening convergence rates. For instance, while the best-known rate for EF in the smooth nonconvex landscape hovers at $O(1/T^{2/3})$, the traditional gradient descent method achieves a more favorable $O(1/T)$.  Enter Richt\u00e1rik et al. in 2021, who unveiled a groundbreaking variant known as EF21. This new mechanism constructs a Markov compressor derived from a contractive compressor, effectively addressing the shortcomings of its predecessor while simultaneously enhancing practical performance. In this work, we venture further into the depths of EF21, proposing six innovative extensions: partial participation, stochastic approximation, variance reduction, proximal setting, momentum, and bidirectional compression. Each of these extensions is fortified by robust convergence theories applicable to both the smooth nonconvex and Polyak-\u0141ojasiewicz regimes. Notably, many of these techniques have remained uncharted territory in conjunction with EF, and where previous analyses did exist\u2014such as with bidirectional compression\u2014our findings reveal convergence rates that soar to new heights. Join us as we explore this uncharted landscape, illuminating the path toward more efficient and effective distributed optimization.", "introduction": "In this paper, we consider the nonconvex distributed/federated optimization problem of the form minwhere  denotes the number of clients/workers/devices/nodes connected with a server/master and client  has an access to the local loss function   only.The local loss of each client is allowed to have the online/expectation formor the finite-sum formProblems of this structure appear in federated learning (Kone\u010dn\u00fd et al., 2016;Kairouz, 2019), where training is performed directly on the clients' devices.In a quest for state-of-the-art performance, machine learning practitioners develop elaborate model architectures and train their models on enormous data sets.Naturally, for training at this scale to be possible, one needs to rely on distributed computing (Goyal et al., 2017;You et al., 2020).Since in recent years remarkable empirical successes were obtained with massively over-parameterized models (Arora et al., 2018), which puts an extra strain on the communication links during training, recent research activity and practice focuses on developing distributed optimization methods and systems capitalizing on (deterministic or randomized) lossy communication compression techniques to reduce the amount of communication traffic.A compression mechanism is typically formalized as an operator  : R  \u21a6 \u2192 R  mapping hard-tocommunicate (e.g., dense) input messages into easy-to-communicate (e.g., sparse) output messages.The operator is allowed to be randomized, and typically operates on models Khaled & Richt\u00e1rik (2019) or on gradients Alistarh et al. (2017); Beznosikov et al. (2020), both of which can be described as vectors in R  .Besides sparsification (Alistarh et al., 2018), typical examples of useful compression mechanisms include quantization (Alistarh et al., 2017;Horv\u00e1th et al., 2019a) and low-rank approximation (Vogels et al., 2019;Safaryan et al., 2021).There are two large classes of compression operators often studied in the literature: i) unbiased compression operators , meaning that there exists  \u2265 0 such thatand ii) biased compression operators , meaning that there exists 0 <  \u2264 1 such that(5)Note that the latter \"biased\" class contains the former one, i.e., if  satisfies (4) with , then a scaled version (1 + ) -1  satisfies (5) with  = 1 /(1+).While distributed optimization methods with unbiased compressors (4) are well understood (Alistarh et al., 2017;Khirirat et al., 2018;Mishchenko et al., 2019;Horv\u00e1th et al., 2019b;Li et al., 2020;Li & Richt\u00e1rik, 2021a;Li & Richt\u00e1rik, 2020;Islamov et al., 2021;Gorbunov et al., 2021), biased compressors ( 5) are significantly harder to analyze.One of the main reasons behind this is rooted in the observation that when deployed within distributed gradient descent in a naive way, biased compresors may lead to (even exponential) divergence (Karimireddy et al., 2019;Beznosikov et al., 2020).Error Feedback (EF) (or Error Compensation (EC))-a technique originally proposed by Seide et al. (2014)-emerged as an empirical fix of this problem.However, this technique remained poorly understood until very recently.Although several theoretical results were obtained supporting the EF framework in recent years (Stich et al., 2018;Alistarh et al., 2018;Beznosikov et al., 2020;Gorbunov et al., 2020;Qian et al., 2020;Tang et al., 2020;Koloskova et al., 2020), they use strong assumptions (e.g., convexity, bounded gradients, bounded dissimilarity), and do not get ( 1 / ) convergence rates in the smooth nonconvex regime.Very recently, Richt\u00e1rik et al. (2021) proposed a new EF mechanism called EF21, which uses standard smoothness assumptions only, and also enjoys the desirable ( 1 / ) convergence rate for the nonconvex case (in terms of number of communication rounds  this matches the best-known rate ( (1+  / \u221a ) / ) obtained by Gorbunov et al. (2021) using unbiased compressors), improving the previous ( 1 /( ) 2/3 ) rate of the standard EF mechanism (Koloskova et al., 2020)."}
{"paper_id": 292, "abstract": "In the realm of machine learning, where the quest for efficiency often collides with the need for precision, Quantized Neural Networks (QNNs) emerge as a beacon of hope. By substituting full-precision weights \\(\\boldsymbol{W}\\) with their quantized counterparts \\(\\boldsymbol{\\hat{W}}\\), we unlock the potential to deploy expansive models on mobile and compact devices. Yet, this journey is fraught with peril; the non-differentiable nature of quantization can yield gradients that are either infinite or vanish altogether, wreaking havoc on the training of these models.   To navigate this treacherous terrain, many training-based quantization methods lean on the Straight-Through Estimator (STE), which approximates the gradients \\(\\nabla_{\\boldsymbol{W}}\\) with respect to \\(\\boldsymbol{W}\\) using those \\(\\nabla_{\\boldsymbol{\\hat{W}}}\\) associated with \\(\\boldsymbol{\\hat{W}}\\), all under the assumption that \\(\\boldsymbol{W}\\) is constrained to the interval \\([-1, +1]\\). However, this straightforward application of STE introduces a gradient mismatch problem that can destabilize the training process, much like a hero facing unexpected trials on their journey.  In this paper, we chart a new course by revising the approximated gradient to better navigate the quantization function through the lens of manifold learning. By conceptualizing the parameter space as a Riemannian manifold with a metric tensor, we unveil the Manifold Quantization (ManiQuant) technique, enhanced by a refined STE, to mitigate the gradient mismatch dilemma. Our ablation studies and experimental results reveal that this innovative approach not only improves performance but also brings stability to the training of various deep neural networks across the CIFAR10/100 and ImageNet datasets, proving that even in the face of adversity, progress can be achieved.", "introduction": "Neural networks can handle many complex tasks due to their large number of trainable parameters and strong nonlinear capabilities (Krizhevsky et al., 2012).However, the massive amount of models and calculations hinder the application of neural networks on mobile and miniaturized devices, which naturally comes with constraints on computing power and resources.Neural network quantization is considered an efficient solution in the inference that alleviates the number of parameters and optimizes the computation by reducing the bit width of weights or activations (Courbariaux et al., 2016;Li et al., 2016;Zhu et al., 2016).Existing neural network quantization methods can be roughly divided into two categories: \"STE\" and \"Non-STE\" methods.Most of the quantization methods adopted by the QNNs belong to the former, i.e. there is always a non-differentiable quantization function during training.The role of STE is to penetrate this non-differentiable quantization function and pass the gradients in backpropogation (Hinton, 2012), e.g.DeepShift (Elhoushi et al., 2019), INT8 (Zhu et al., 2020), AQE (Chen et al., 2020), etc. \"Non-STE\" methods refer to maintain feasible quantization during training, which does not need to apply STE directly to all full-precision weights.For example, Zhou et al. (Zhou et al., 2017) divided the weights into two groups until all parameters are quantized, where the first group is directly quantized and fixed; the second group needs to be retrained to make up for the decrease of accuracy caused by quantization of the first group.Louizos et al. (Louizos et al., 2019) introduced a differentiable quantizer that can transform the continuous distributions of weights and activations to categorical distributions with gradient-based optimization.However, \"Non-STE\" methods face the setting and influence of heavy hyper-parameters in the training process.Relatively, \"STE\" methods are wider choices for quantized models from simplicity and versatility.To approximate \u2207 W w.r.t.W and complete stable quantization training, Courbariaux et al. (Courbariaux et al., 2016) binarized the neural networks by the approximated gradients using STE:Figure 1: The general gradient is intuitively defined in Euclidean space, which means that the direction of the general gradient will not be affected by the curvature.On the contrary, the natural gradient and weak curvature gradient intrinsically contain the curvature of manifolds, although the definitions of the manifolds constructing these two gradients are different.However, it will inevitably bring the gradient mismatch problem raised by just simply application of STE.To overcome this challenge, Zhou et al. (Zhou et al., 2016) proposed to transform W to W :and then quantize it using a quantization function Q(\u2022).During backpropogation, the gradients \u2207 W w.r.t.W can be further computed using the chain rule:.Based on this work, Chen et al. (Chen et al., 2019) proposed to learn \u2207 W by fully-connected neural networks or LSTM, which replaces this gradients via a meta quantizer M \u03c6 parameterized by \u03c6 across layers:However, such methods not only add many additional parameters, but also increase the difficulty of the training of quantized models.In this paper, we introduce the Manifold Quantization (ManiQuant) to train a quantized model via embedding Riemannian manifolds for STE.In Figure 1, we treat the parameter space in a quantized model as a Riemannian manifold, which will alleviate the gradient mismatch problem caused by non-differentiable quantization and assist quantized models in achieving a more stable convergence and better performance.The main contributions of this work are three-fold: First, we propose to use Fisher Information Matrix embedding to alleviate the gradient mismatch problem.Second, we define a novel Hyperbolic divergence by a convex function with geometric structure.With the constraint of Hyperbolic divergence, we introduce a weak curvature manifold that forms the background of ManiQuant.Third, based on the Second, we propose to use weak curvature metric embeddings for STE as the weak curvature gradient to approximate \u2207 W without extra parameters and training complexity."}
{"paper_id": 293, "abstract": "In the realm of differential geometry, the Ricci flow stands as a powerful tool for the meticulous art of manifold surgery, capable of refining the chaotic complexities of these intricate spaces into forms of greater regularity. Yet, like many grand endeavors, it is not without its perils; the Ricci flow often encounters singularities that threaten to unravel the very fabric of the solution, leading it down a path of divergence.   In this paper, we embark on a quest to forge a new path through this mathematical wilderness, introducing linearly nearly Euclidean metrics as a beacon of hope for manifold micro-surgery. We unveil the dynamical stability and convergence of these metrics under the guidance of the Ricci-DeTurck flow, illuminating the way forward. Through the lenses of information geometry and mirror descent, we construct a bridge to approximate the steepest descent gradient flow on our linearly nearly Euclidean manifold, ensuring that stability reigns supreme.  In practical terms, our exploration reveals that the regular oscillations\u2014whether shrinking or expanding\u2014of Ricci solitons, when woven with linearly nearly Euclidean metrics, will serve as a potent geometric optimization strategy. This methodology promises to enhance the solutions that dwell within the manifold, crafting a more harmonious existence for the geometries we seek to understand.", "introduction": "In general relativity (Wald, 2010), a complete Riemannian manifold (M, g) endowed with a linearly nearly flat spacetime metric g ij is considered for linearized gravity to solve the Newtonian limit.The form of this metric is g ij = \u03b7 ij + \u03b3 ij , where \u03b7 ij represents a flat Minkowski metric whose background is special relativity and \u03b3 ij is small from \u03b7 ij .An adequate definition of \"smallness\" in this context is that the components of \u03b3 ij are much smaller than 1 in some global inertial coordinate system of \u03b7 ij .Now, let us step out of the physical world and give a similar metric g ij = \u03b4 ij + \u03b3 ij in Riemannian n-manifold (M n , g), i.e. the linearly nearly Euclidean metric, where \u03b4 ij represents a flat Euclidean metric and \u03b3 ij is small from \u03b4 ij .A natural problem for such a linearly nearly Euclidean metric is: how does the metric evolve over time with respect to the Ricci flow while ensuring the constant topological structure?Let us review some stability analyses of different manifolds along with the Ricci flow.For the Riemannian n-dimensional manifold (M n , g) that is isometric to the Euclidean ndimensional space (R n , \u03b4), Schn\u00fcrer et al. (Schn\u00fcrer et al., 2007) have showed the stability of Euclidean space under the Ricci flow for a small C 0 perturbation.Koch et al. (Koch & Lamm, 2012) have given the stability of the Euclidean space along with the Ricci flow in the L \u221e -Norm.Moreover, for the decay of the L \u221e -Norm on Euclidean space, Appleton (Appleton, 2018) has given the proof of a different method.Considering the stability of integrable and closed Ricci-flat metrics, Sesum (Sesum, 2006) has proved that the convergence rate is exponential because the spectrum of the Lichnerowicz operator is discrete.Furthermore, Deruelle et al. (Deruelle & Kr\u00f6ncke, 2021) have proved that an asymptotically locally Euclidean Ricci-flat metric is dynamically stable under the Ricci flow, for an L 2 \u2229 L \u221e perturbation on non-flat and non-compact Ricci-flat manifolds.If we embed a Riemannian n-dimensional manifold in the neural network, then we are training the neural network on the dynamic manifold.The most famous method for training neural networks on manifolds is the natural gradient (Amari, 1998).However, for the Riemannian manifold corresponding to the KL divergence (representing the natural gradient), its stability and convergence are still unknown (Martens, 2020).As a way of manifold evolution, Ricci flow seems to be an excellent choice to ensure that neural networks are trained on dynamic and stable manifolds (Glass et al., 2020;Jejjala et al., 2020).But the research on the relationship between the two has not yet sprouted.In this paper, we consider a complete Riemannian n-dimensional manifold (M n , g) endowed with linearly nearly Euclidean metrics g(t) = \u03b4 + \u03b3(t).First of all, we prove the stability of linearly nearly Euclidean manifolds under the Ricci-DeTurck flow in the L 2 -Norm if initial metrics are integrable and linearly stable, i.e. has a manifold structure of finite dimension.We mean that any Ricci-DeTurck flow which starts from near g exists for all time and converges to a linearly nearly Euclidean metric near g.Note that we use the Einstein summation convention and denote generic constants by C or C 1 .Furthermore, we define and construct linearly nearly Euclidean manifolds based on information geometry and mirror descent algorithm.Based on a symmetrized convex function, we obtain the linearly nearly Euclidean divergence which is used to calculate the steepest descent gradient in linearly nearly Euclidean manifolds.Experimentally, when we use the approximated steepest descent gradient flow to learn several neural networks on classification tasks, we observe the evolution of its metric is consistent with the micro-surgery process under the Ricci-DeTurck flow."}
{"paper_id": 294, "abstract": "In the ever-evolving landscape of artificial intelligence, vision-language pre-training has emerged as a beacon of innovation, heralding a new era in representation learning. Gone are the days when we relied solely on static images and discrete labels to forge fixed sets of weights, which we deemed as visual concepts. Instead, we now harness the power of aligning images with raw text through two distinct encoders, paving the way for a richer and more versatile source of supervision. This paradigm shift not only enhances our understanding but also enables zero-shot transfer to downstream tasks, as visual concepts can be dynamically conjured from the vast expanse of natural language\u2014what we call prompts.  Yet, as we delve into the practical deployment of these models, we encounter a formidable obstacle: prompt engineering. Crafting the perfect prompt, particularly when it comes to the context words that encircle a class name, demands a level of domain expertise that can be both daunting and time-consuming. The subtleties of language are such that a mere tweak in wording can lead to dramatic shifts in performance. Furthermore, the unique requirements of different downstream tasks complicate the deployment process, creating a labyrinth of inefficiencies.  To navigate this challenge, we introduce a groundbreaking approach known as **context optimization (CoOp)**. At its core, CoOp reimagines the way we model context within prompts by utilizing continuous representations and enabling end-to-end learning from data\u2014all while keeping the pre-trained parameters intact. This innovative method allows for the complete automation of task-relevant prompt design, liberating us from the shackles of manual tuning.  Our experiments, spanning eleven diverse datasets, reveal that CoOp transforms pre-trained vision-language models into remarkably data-efficient visual learners. With as few as one or two shots, CoOp outperforms meticulously crafted prompts by a significant margin, and the advantages only amplify with additional shots\u2014averaging around a 17% gain at 16 shots, with some instances soaring over 50%. Moreover, CoOp demonstrates impressive robustness against distribution shifts, solidifying its place as a powerful tool in the arsenal of modern AI.", "introduction": "The traditional approach for visual representation learning is to train vision models to predict for a fixed set of object categories using discrete labels (He et al., 2016;Dosovitskiy et al., 2021).However, this approach limits visual recognition systems to closed-set visual concepts defined during training, making them unable to handle new categories once deployed in target environments, since additional data are required for learning a new classifier.Recently, vision-language pre-training such as CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) has emerged as a promising alternative.The main idea is to align images and raw text using two separate encoders-one for each modality.Through large-scale pre-training, vision-language models are allowed to learn open-set visual concepts and can readily be transferred to downstream tasks.In particular, for each new classification task, one can synthesize the classification weights by feeding natural language describing classes of interest to the text encoder, and compare them with image features produced by the image encoder.We observe that for pre-trained vision-language models, the text input, known as prompt, plays a key role in downstream datasets.However, identifying the right prompt is a non-trivial task, which often takes a significant amount of time for words tuning-since a slight change in wording could make a huge difference in performance.For instance, for Caltech101 (Figure 1(a), 2nd vs. 3rd prompt), adding \"a\" before the class token brings more than 5% increase in accuracy.Moreover, prompt engineering also requires expertise about the task and ideally the language model's underlying mechanism.This is exemplified in Figure 1(b-d) where adding task-relevant context can lead to significant improvements, i.e., \"flower\" for Flowers102, \"texture\" for DTD and \"satellite\" for EuroSAT.Tuning the sentence structure could bring further improvements, e.g., putting \"a type of flower\" after the class token for Flowers102, keeping only \"texture\" in the context for DTD, and [CLASS] texture.[ adding \"centered\" before \"satellite photo\" for EuroSAT.However, even with extensive tuning, the resulting prompts are by no means guaranteed to be optimal for these downstream tasks.Inspired by recent prompt learning research in NLP (Shin et al., 2020;Jiang et al., 2020;Zhong et al., 2021), we propose context optimization (CoOp)foot_0 to automate prompt engineering to allow more efficient and task-specific transfer for pre-trained vision-language models.Specifically, we model a prompt's context using continuous representations which are essentially initialized with random vectors with the same dimension as word embeddings (see Figure 2).The context could be shared among all classes or designed to be class-specific.During training, we simply minimize the prediction error using the cross-entropy loss with respect to the learnable context vectors, while keeping the pre-trained parameters fixed.The gradients can be back-propagated all the way through the text encoder, distilling the rich knowledge encoded in the parameters for learning task-relevant context.To demonstrate the effectiveness of CoOp, we benchmark on 11 datasets, which cover a diverse set of visual recognition tasks including classification on generic objects, scenes, actions and fine-grained categories, as well as specialized tasks like recognizing textures and satellite imagery.The results show that CoOp can effectively turn pre-trained vision-language models into data-efficient visual learners, requiring as few as one or two shots to beat hand-crafted prompts with a decent margin.The performance can also be further boosted by using more shots, e.g., at 16 shots the margin over hand-crafted prompts averages at around 17% and reaches over 50% for the highest.CoOp also outperforms the linear probe alternative known as a strong few-shot learning baseline (Tian et al., 2020), and crucially, demonstrates much stronger robustness to distribution shift.Extensive analysis is also conducted to offer a comprehensive picture on how to apply CoOp in practice.The source code for reproducing the experiments will be released to facilitate future research."}
{"paper_id": 295, "abstract": "In the ever-evolving realm of deep learning, researchers have uncovered a remarkable truth: deep neural networks possess the uncanny ability to uncover effective heuristics for a myriad of Combinatorial Optimization Problems (COPs). Yet, a daunting challenge looms on the horizon\u2014these sophisticated models often struggle to adapt when faced with distributions that diverge from their training data. To confront this formidable obstacle, we unveil a novel framework: Generative Adversarial Neural Combinatorial Optimization, or GANCO for short. This innovative approach integrates a secondary deep model tasked with generating training instances tailored for the optimization model, thereby enhancing its capacity to generalize across varied distributions.  In this adversarial dance, the two models engage in a dynamic interplay, training alternately; the generation model, empowered by reinforcement learning, seeks out instance distributions that pose a challenge for the optimization model. We put the GANCO framework to the test with two cutting-edge deep combinatorial optimization models: the Attention Model (AM) and Policy Optimization with Multiple Optima (POMO). Our extensive experiments, spanning a diverse array of problems\u2014including the Traveling Salesman Problem, the Capacitated Vehicle Routing Problem, and the 0-1 Knapsack Problem\u2014demonstrate that GANCO significantly bolsters the generalization capabilities of optimization models across various instance distributions, all while maintaining performance levels on the original training data with minimal compromise.", "introduction": "Combinatorial Optimization Problems (COPs) are a family of problems with the goal of finding the best one(s) from a finite set of solutions.Due to the considerably large solution space, many important COPs are hard to solve, such as the vehicle routing problems (Toth & Vigo, 2002).Exact algorithms based on branch-and-bound (Lawler & Wood, 1966) or its variants can provide elegant theoretical guarantees but the worst-case computational complexity is exponential, hence impractical for problems of medium and large sizes.In contrast, heuristic methods can usually attain good solutions in reasonable running time, which are often preferred in practical applications.Traditional heuristics are designed based on expert knowledge for specific problems, which usually requires a large amount of time and efforts to develop.These manually designed heuristics could suffer from relatively poor performance.Moreover, for less studied problems, sufficient expert knowledge may even be unavailable.Recent studies suggest that deep learning could greatly facilitate in automating the design of heuristics, and alleviating the heavy reliance on expert knowledge.With the prior that the instances may follow certain distribution (e.g., locations in a routing problem may uniformly scatter in an area), deep models can be trained to learn heuristics in an end-to-end way (Dai et al., 2017;Kool et al., 2019;Chen & Tian, 2019).It has been shown that these models perform well with relatively short running time on COP instances following the training distribution.However, after the trained model is deployed, it could encounter many instances following unknown distributions different from the training one, especially for real-life applications.As shown in many existing works and our experiments, when applied to infer the instances following a different distribution, the generalization performance of deep models gets much inferior, which severely hinders the practical use of the learned heuristics.Such mismatch between the training and testing distributions is always an important issue for most learning based methods.Especially, for neural combinatorial optimization (NCO) models, deep learning models are mostly trained on instances sampled from specific distributions and the solution quality intricately depends on the instance distributions.The generalization to instances with different distributions has been widely acknowledged for the importance (e.g., in Mazyavkina et al. (2021)) and remains a challenge.To tackle this issue, we propose the Generative Adversarial Neural Combinatorial Optimization (GANCO) framework which is model agnostic and generally applicable to various neural combinatorial optimization models for solving different COPs.Instead of training an optimization model only on instances following the predefined distribution, another deep neural network is deployed as a generation model to produce training instances following the distributions on which the optimization model performs poorly.The generation model and optimization model are trained alternatively in an adversarial way.Specifically, the generation model is trained by reinforcement learning to maximize the performance gap of the current optimization model on the generated instances with respect to a traditional non-learning baseline algorithm.The optimization model is trained in the original way but using the training dataset augmented with the newly generated instances to improve the generalization performance, i.e., to reduce the performance gap.As we will show in the experiments, the non-learning baseline algorithms do not need to be very strong or fast.To demonstrate the effectiveness of the proposed GANCO framework, we apply it to a representative neural combinatorial optimization model, the Attention Model (AM, Kool et al. (2019)) on various COPs including Traveling Salesman Problem (TSP), Capacitated Vehicle Routing Problem (CVRP), Orienteering Problem (OP), Prize Collecting TSP (PCTSP) and 0-1 Knapsack Problem (KP).In the extensive experiments, we show that the proposed GANCO framework improves the generalization performance of the original optimization model on various testing distributions with little sacrifice of performance on the original training distribution.Furthermore, we show that the proposed GANCO framework can be readily and effectively applied to other optimization models such as the Policy Optimization with Multiple Optima (POMO, Kwon et al. (2020))."}
{"paper_id": 296, "abstract": "In the realm of machine learning, a fundamental assumption reigns: that the training and test examples emerge from the same distribution. Yet, the world outside our algorithms is fraught with distribution shifts\u2014unexpected twists and turns that can send model performance plummeting when faced with real-world scenarios. In this paper, we delve into the intricate challenges posed by domain shifts and subpopulation shifts, where the quest for invariant representations often leads researchers to align domain-specific representations or balance risks across domains using regularizers. However, the true art lies in crafting regularizers that can gracefully adapt to the diverse tapestry of real-world datasets.  Here, we unveil a fresh perspective on tackling distribution shifts by directly vanquishing domain-related spurious correlations through augmentation. Enter LISA\u2014Learning Invariant Representations via Selective Augmentation\u2014a straightforward yet powerful technique inspired by the concept of mixup. LISA deftly interpolates samples, either pairing those with the same labels but differing domains or merging samples from the same domain but with contrasting labels.  Through rigorous empirical investigation across nine distinct benchmarks, ranging from subpopulation shifts to domain shifts, we reveal that LISA not only rises to the occasion but consistently eclipses other state-of-the-art methods, delivering superior invariant representations. Our findings are not mere happenstance; they are bolstered by a robust theoretical analysis, illuminating the path forward in the ever-evolving landscape of machine learning.", "introduction": "To deploy machine learning algorithms in real-world applications, we must pay attention to distribution shifts, i.e. when the test distribution is different from the training distribution, which substantially degrades model performance.In this paper, we refer this problem as out-of-distribution (OOD) generalization and specifically consider performance gaps caused by two kinds of distribution shifts: domain shifts and subpopulation shifts.In domain shifts, the test data is sampled from different domains than the training data, which requires the trained model to generalize well to test domains without seeing training data from those domains.Take health risk prediction as an example.We may want to train a model on patients from a few sampled hospitals and then deploy the model to a broader set of hospitals (Koh et al., 2021).In subpopulation shifts, the proportions of subpopulations in the test distribution differ from the proportions in the training distribution.When subpopulation shift occur, models perform poorly when they falsely rely on spurious correlations, which may occur when some subpopulations are under-represented in the training set.For example, in financial risk prediction, a machine learning model trained on the entire population may associate the labels with demographic features (e.g., religion and race), making the model fail on the test set when such an association does not hold in reality.To improve model robustness under these two kinds of distribution shifts, methods for learning invariant representations have shown effectiveness in various applications.These methods learn features or prediction mechanisms that are invariant to different domains while still containing sufficient information for the targeted task (Li et al., 2018;Arjovsky et al., 2019).Concretely, some prior works learn invariant representations by aligning and regularizing the domain-specific representations (Li et al., 2018;Sun & Saenko, 2016).Other works aim to find invariant representations by balancing the risk across domains using regularizers (Arjovsky et al., 2019;Krueger et al., 2021;Rosenfeld et al., 2021), which further increases the dependency between the invariant representations and labels.However, designing regularizers that are widely suitable to datasets from diverse domains is especially challenging and insuitable regularizers may adversely limit the model's expressive power, leading to inconsistent performance among various real-world datasets.For example, on the WILDS datasets (Koh et al., 2021), invariant risk minimization (IRM) (Arjovsky et al., 2019) outperforms empirical risk minimization (ERM) on CivilComments, but fails to improve robustness on a variety of other datasets like Camelyon17 and RxRx1.A similar phenomenon is also reflected in the performance of CORAL (Sun & Saenko, 2016).Instead of explicitly imposing regularization to learn invariant representations, we turn towards an implicit solution.Inspired by mixup (Zhang et al., 2018), we aim to alleviate the effects of domainrelated spurious information through data interpolation, leading to a simple algorithm called LISA (Learning Invariant Representations with Selective Augmentation).Concretely, LISA linearly interpolates the features for a pair of samples and applies the same interpolation strategy on the corresponding labels.Critically, the pairs are selectively chosen according to two sample selection strategies.In selection strategy I, LISA interpolates samples with the same label but from different domains, aiming to eliminate domain-related spurious correlations.In selection strategy II, LISA interpolates samples with the same domain but different labels, where the model should to ignore the domain information and generate different predicted values as the interpolation ratio changes.In this way, LISA encourages the model to learn domain-invariant predictors without explicitly constraining or regularizing the representation.The primary contributions of this paper are as follows: (1) We develop a method that tackles the problem of distribution shifts by canceling out the domain-related spurious correlations via data interpolation.(2) We conduct broad empirical experiments to evaluate the effectiveness of LISA on nine benchmark datasets from diverse domains.In these experiments, we make the following observations.First, we find that LISA consistently outperforms seven prior methods in addressing both domain shifts and subpopulation shifts.Second, we identify that the performance gains of LISA are indeed caused by canceling out domain-specific information and learning invariant representations, rather than simply involving more data via interpolation.Third, when the degree of distribution shift increases, LISA achieves more significant performance gains.(3) Finally, we provide theoretical analysis of the phenomena distilled from the empirical studies, where we provably demonstrate that LISA can achieve smaller worst-domain error compared with ERM and vanilla mixup.We also note that to the best of our knowledge, this is the first theoretical analysis of how mixup (with or without the selection strategies) affects mis-classification error.In domain shifts, we investigate the problem that the test domains are disjoint from the training domains, i.e., D tr \u2229 D ts = \u2205.In general, we assume the test domains share some common properties with the training domains.For example, in Camelyon17 (Koh et al., 2021) data, we train the model on some hospitals and test it in a new hospital.We evaluate the worst-domain or average performance of the classifier among all test domains."}
{"paper_id": 297, "abstract": "In the realm of machine learning, Graph Neural Networks (GNNs) rise as a powerful evolution of traditional Neural Networks (NNs), harnessing the intricate tapestry of graph structures through the lens of relational inductive bias, particularly the homophily assumption. While GNNs are heralded for their prowess in navigating the complexities of real-world tasks, their superiority over graph-agnostic NNs often falls short of expectations. This shortfall can be traced back to the elusive phenomenon of heterophily, which has spurred a multitude of research efforts aimed at mitigation. In this work, we embark on a journey to unravel the intricacies of heterophily, revealing that not all manifestations of this phenomenon spell doom for GNNs employing aggregation operations. We introduce innovative metrics grounded in a similarity matrix that deftly balance the influences of graph structure and input features, showcasing their superiority over traditional homophily metrics through rigorous testing on synthetic graphs. Our exploration leads to a pivotal discovery: certain detrimental cases of heterophily can be effectively countered through a diversification operation. Armed with this insight and inspired by the principles of filterbanks, we unveil the Adaptive Channel Mixing (ACM) framework. This framework adeptly navigates the interplay of aggregation, diversification, and identity channels within each GNN layer, providing a robust solution to the challenges posed by harmful heterophily. We put the ACM-enhanced baselines to the test across ten real-world node classification tasks, where they consistently outshine the competition, achieving remarkable performance gains and surpassing the state-of-the-art GNNs, all while maintaining a manageable computational footprint.", "introduction": "Deep Neural Networks (NNs) (LeCun et al., 2015) have revolutionized many machine learning areas, including image recognition (Krizhevsky et al., 2012), speech recognition (Graves et al., 2013) and natural language processing (Bahdanau et al., 2014), etc.One major strength is their capacity and effectiveness of learning latent representation from Euclidean data.Recently, the focus has been put on its applications on non-Euclidean data (Bronstein et al., 2016), e.g., relational data or graphs.Combining graph signal processing and convolutional neural networks (LeCun et al., 1998), numerous Graph Neural Networks (GNNs) (Scarselli et al., 2008;Defferrard et al., 2016;Hamilton et al., 2017;Velickovic et al., 2017;Kipf & Welling, 2016;Luan et al., 2019) have been proposed which empirically outperform traditional neural networks on graph-based machine learning tasks, e.g., node classification, graph classification, link prediction and graph generation, etc.GNNs are built on the homophily assumption (McPherson et al., 2001), i.e., connected nodes tend to share similar attributes with each other (Hamilton, 2020), which offers additional information besides node features.Such relational inductive bias (Battaglia et al., 2018) is believed to be a key factor leading to GNNs' superior performance over NNs' in many tasks.Nevertheless, growing evidence shows that GNNs do not always gain advantages over traditional NNs when dealing with relational data.In some cases, even simple Multi-Layer Perceptrons (MLPs) can outperform GNNs by a large margin (Zhu et al., 2020b;Liu et al., 2020;Luan et al., 2020b;Chien et al., 2021).An important reason for the performance degradation is believed to be the heterophily problem, i.e., connected nodes tend to have different labels which makes the homophily assumption fail.Heterophily challenge has received lots of attention recently and there are increasing number of models being put forward to address this problem (Zhu et al., 2020b;Liu et al., 2020;Chien et al., 2021;Zhu et al., 2020a;Yan et al., 2021).Contributions In this paper, we first demonstrate that not all heterophilous graphs are harmful for aggregation-based GNNs and the existing metrics of homophily are insufficient to decide whether the aggregation operation will make nodes less distinguishable or not.By constructing a similarity matrix from backpropagation analysis, we derive new homophily metrics to depict how much GNNs are influenced by the graph structure and node features.We show the advantage of our metrics over the existing metrics by comparing the ability of characterizing the performance of two baseline GNNs on synthetic graphs of different levels of homophily.According to the similarity matrix, we observe that diversification operation is able to address some harmful heterophily cases, and based on which we propose Adaptive Channel Mixing (ACM) GNN framework.The experiments on the synthetic datasets, ablation studies and real-world datasets consistently show that the baseline GNNs augmented by ACM framework are able to obtain significant performance boost on node classification tasks on heterophilous graphs.The rest of this paper is mainly organized as follows: In section 2, we introduce the notation and the background knowledge.In section 3, we conduct node-wise analysis on heterophily, derive new homophily metrics based on a similarity matrix and conduct experiments to show their advantages over the existing homophily metrics.In section 4, we demonstrate the capability of diversification operation on addressing some cases of harmful heterophily and propose the ACM-GNN framework to adaptively utilize the information from different filterbank channels for each node to address heterophily problem.In section 5, we discuss the related works and clarify the differences with our method.In section 6, we provide empirical evaluations on ACM framework, including ablation study and tests on real-world node classification tasks."}
{"paper_id": 298, "abstract": "In this endeavor, we set forth on a quest to transfer the formidable powers of a Generative Adversarial Network (GAN), previously honed on a singular image domain, to a new realm with the audacity of relying on as little as a single target image. The challenge before us is daunting: under the constraints of limited supervision, conjuring forth photo-realistic and richly diverse images while capturing the essence of the target proves to be a formidable task.   Unlike the conventional methods that merely fine-tune the existing model, our approach introduces two nimble modules into the heart of the generator and discriminator. First, we weave an **attribute adaptor** into the fabric of the generator, preserving its original parameters while allowing it to draw upon its prior knowledge. This clever design ensures that we retain the quality and diversity of our synthesis. Simultaneously, we enhance the well-trained backbone of the discriminator with an **attribute classifier**, a safeguard to guarantee that the generator faithfully captures the defining traits of the reference image.  Moreover, recognizing the inherent limitations posed by the scant diversity of our training data\u2014sometimes as meager as a single image\u2014we propose a strategy to impose diversity constraints within the generative domain during training. This innovative tactic mitigates the challenges of optimization, paving the way for richer outputs.   Our method yields compelling results across a multitude of scenarios, significantly eclipsing the performance of existing state-of-the-art alternatives, particularly in the realm of synthesis diversity. Remarkably, our approach thrives even in the presence of substantial domain gaps, demonstrating a robust convergence within mere minutes for each experiment. In this pursuit, we have not only expanded the horizons of GAN capabilities but also charted a new course in the synthesis of diverse and realistic imagery.", "introduction": "Generative Adversarial Network (GAN) (Goodfellow et al., 2014), consisting of a generator and a discriminator, has significantly advanced image synthesis yet relies on a large number of training samples (Karras et al., 2018;2019;2020b;Brock et al., 2018).Many attempts have been made to train GANs from scratch with limited data (Zhang & Khoreva, 2019;Zhao et al., 2020d;b;Karras et al., 2020a;Yang et al., 2021a), but it still requires hundreds or thousands of images to get a satisfying performance.Sometimes, however, we may have as few as only one single image as the reference, like the masterpiece Mona Lisa from Leonardo da Vinci.Under such a case, learning a generative model with both good quality and high diversity becomes extremely challenging.Domain adaptation is a commonly used technique that can apply an algorithm developed on one data domain to another (Csurka, 2017).Prior works (Wang et al., 2018;Noguchi & Harada, 2019;Wang et al., 2020;Mo et al., 2020;Zhao et al., 2020a;Li et al., 2020;Robb et al., 2020) have introduced this technique to GAN training to alleviate the tough requirement on the data scale.Typically, they first train a large-scale model in the source domain with adequate data, and then transfer it to the target domain with only a few samples.A common practice is to fine-tune both the generator and the discriminator on the target dataset until the generator produces samples conforming to the target domain.To stabilize the fine-tuning process and improve the generation quality and diversity, existing approaches propose to tune partial parameters (Noguchi & Harada, 2019;Mo et al., 2020;Robb et al., 2020) and introduce some regularizers (Li et al., 2020;Ojha et al., 2021), but the overall adaptation strategy remains.When there is only one image from the target domain, these methods would fell short of synthesis diversity, producing very similar images.Recall that the pre-trained model can already produce highly diverse images from the source domain.It makes us wonder what indeed causes the diversity drop in the adaptation process.We argue that directly tuning the model weights will result in the loss of the prior knowledge gained from the large-scale data.On the other hand, however, when adapting the model to the target domain, most variation factors (e.g., gender and pose of human faces) could be reused.These observations help raise a question: is it possible to simply focus on the most representative characters of the reference image while inheriting all the other knowledge from the source model?In this work, we develop a method, called GenDA, for one-shot Generative Domain Adaptation.In particular, we design a lightweight module connecting the latent space and the synthesis network.We call this module an attribute adaptor since it helps adapt the generator with the attributes of the target image.Unlike the conventional fine-tuning strategy, we freeze the parameters of the original generator and merely optimize the attribute adaptor during training.Thereby, we manage to reuse the prior knowledge learned by the source model and hence inherit the synthesis quality and, more importantly, diversity.Meanwhile, we employ the discriminator to compete with the generator via a domain-specific attribute classification.In this way, the generator is forced to capture the most representative attributes from the reference, or otherwise, the discriminator would spot the discrepancy.However, instead of directly tuning the original discriminator, we freeze its entire backbone and introduce a lightweight attribute classifier on top of that.Similar to the generator, the discriminator has also learned rich knowledge in its pre-training.Since the synthesized images before and after adaptation share most visual concepts (e.g., a face model would still produce faces after domain transfer), the discriminator can be reused as a well-learned feature extractor.Therefore, we simply train the attribute classifier to help guide the generator.Furthermore, since there is only one training sample (which means no diversity in the target domain), we propose to also constrain the diversity of the generative domain by truncating the latent distribution during training.Intuitively, learning a one-to-one mapping would be easier than learning a many-to-one mapping.Such a design mitigates the optimization difficulty and further improves the synthesis quality.We evaluate our approach through extensive experiments on human faces and outdoor churches.Given only one training image, GenDA can convincingly adapt the source model to the target domain with sufficiently high quality and diversity.Such an adaptation includes both attribute-level and style-level, as shown in Fig. 1.Our method outperforms the state-of-the-art competitors by a large margin both qualitatively and quantitatively.We also show that, when the number of the samples available in the target domain increases, GenDA can filter out the individual attributes and only preserve their common characters (see Fig. 4).Noticeably, GenDA works well for the extreme cases where there is a large domain gap, like transferring the characters of Mona Lisa to churches (see Fig. 5).Besides, thanks to the lightweight design of both the attribute adaptor and the attribute classifier, GenDA can finish each adaptation experiment within a few minutes."}
{"paper_id": 299, "abstract": "In the realm of machine learning, two forces vie for supremacy: optimization and generalization. In this paper, we unveil a novel framework that forges a bridge between these two vital concepts. By delving into the intricacies of generalization error, we scrutinize the optimization trajectory's length\u2014specifically under the gradient flow algorithm\u2014once convergence has been achieved.  Our exploration reveals a fascinating truth: with the right initialization, gradient flow meanders along a remarkably short path, one that we can explicitly quantify. This length estimate becomes a powerful tool, leading us to a length-based generalization bound. In essence, we demonstrate that when the optimization path is succinct post-convergence, it heralds a strong capacity for generalization.  This framework is not merely a theoretical construct; it holds practical implications across a diverse array of settings. We illustrate its versatility by applying it to derive generalization estimates for three distinct machine learning models: the underdetermined \\(\\ell_p\\) linear regression, kernel regression, and the overparameterized two-layer ReLU neural networks. Through our findings, we illuminate the interconnectedness of optimization paths and generalization, providing a deeper understanding of the landscape of machine learning.", "introduction": "From the perspective of statistical learning theory, the goal of machine learning is to find a predictive function that can give accurate predictions on new data.For supervised learning problems, empirical risk minimization (ERM) is a common practice to achieve this goal.The idea of ERM is to minimize a cost function on observed data by an optimization algorithm.Therefore, a fundamental question is whether an optimization algorithm produces a solution with good generalization.Recent works have shed light on providing theoretical explanations to this question from different angles.One line of works considered the case when gradient methods converge to minimal norm solutions (Bartlett et al., 2020;Tsigler & Bartlett, 2020;Liang & Rakhlin, 2020;Liang et al., 2020) on kernel regression, and then analyzed the generalization of those minimal norm solutions.However, the phenomenon of norm minimization has been known to happen only for the quadratic loss with an appropriate initialization.Another line of works focused on overparameterized models, e.g., neural networks under the Neural Tangent Kernel (NTK) regime (Allen-Zhu et al., 2019;Arora et al., 2019;Cao & Gu, 2020;Ji & Telgarsky, 2020;Chen et al., 2021), proving that overparameterized neural networks trained by (stochastic) gradient descent ((S)GD) have good generalization performance on certain target functions (for example, polynomial functions).Although existing works have made significant progress on the interplay of optimization and generalization, they focused on studying specific models, such as the NTK and models possessing minimal norm solutions.In this paper, instead, we study general loss function conditions that induce interesting connections between optimization and generalization.We start with a simple observation, as shown in Figure 1, that under a generic random initialization, the generalization performance for both linear regression model and random feature regression model is closely related to the length of the optimization path 1 after convergence.In particular, short optimization path are associated with good generalization.Here, by length we mean the trajectory or path length of the parameter evolution during training.This is not the \"length of time\" used for training, as is usually analyzed in early-stopping type of algorithms.Thus, the generalization error concerns the weights of model trained to completion by empirical risk minimization.This empirical investigation motivates us to use the trajectory length to connect optimization and generalization.Intuitively, the length of the optimization path can be viewed as a kind of capacity control, and a Figure 1: Illustrations of the relationship between optimization path lengths and generalization.(Left) Linear regression model.(Right) Random feature regression model with the feature extracted by a neural network.We train both models by gradient descent under Gaussian initialization N (\u00b5, \u03c3 2 ) with varied \u00b5 and \u03c3, and record the trajectory lengths len(w (0) , \u221e) from initialization until convergence.We observe that short optimization paths lead to good generalization.See experiment settings and more numerical results in Appendix A.1 & A.2. short path signifies low \"complexity\".In other words, a general condition that guarantees a short optimization path can be used to induce good generalization performance.Thus, inspired by the theory in (Bolte et al., 2007) that \u0141ojasiewicz gradient inequality (LGI) induces an explicit bound for the gradient flow path, we consider Uniform-LGI (Definition 1).This is a modified version of LGI and plays a critical role in obtaining a length estimate for the gradient flow trajectory.Once the length is estimated, we can use our length-based generalization bounds to show the generalization performance.Contributions.We summarize our contributions as follows:\u2022 We focus on the gradient flow algorithm and propose a framework for combining optimization and generalization.This framework is based on the length of the gradient flow trajectory, and its key component is the Uniform-LGI property.\u2022 We then prove that under appropriate conditions, gradient flow returns a global minimum and gives an explicit length-estimate for the gradient flow trajectory (Theorem 1).If this length-estimate has an uniform bound for a certain initialization method, then we can give a length-based generalization bound (Theorem 2).\u2022 We further show applications of Theorem 1 and Theorem 2 to obtain generalization bounds on underdetermined \u2113 p linear regression (Theorem 3), kernel regression (Theorem 4), and overparameterized two-layer ReLU neural networks (Theorem 5).These bounds match or expand the type of scenarios where we can rigorously establish the phenomenon of benign overfitting."}
{"paper_id": 300, "abstract": "In the realm of machine learning, where data flows like a river from countless sources, Federated Learning (FL) emerges as a beacon of hope\u2014an innovative approach that champions privacy while harnessing the power of decentralized data from countless end devices, or clients. Yet, lurking in the shadows of this promise lies a formidable challenge: many clients possess data of questionable quality\u2014tainted by bias, noise, or even irrelevance. This low-quality data threatens to impede the convergence of our global model, casting a pall over its potential and quality.  Enter FedProf, a groundbreaking algorithm crafted to navigate these treacherous waters without sacrificing the sanctity of data privacy. At the heart of our approach lies a sophisticated data representation profiling and matching scheme. With the global model as our guiding star, we dynamically assess and profile the data representations, enabling a low-cost, lightweight matching process. This allows us to assign adaptive scores to each client, fine-tuning their participation probability to shield the training process from the detrimental effects of low-value clients.  Through rigorous experimentation across diverse public datasets and various FL configurations, we unveil the prowess of FedProf. Our findings reveal a remarkable reduction in communication rounds and overall training time\u2014achieving an impressive speedup of up to 4.5 times\u2014while simultaneously enhancing the accuracy of the global model. In this journey through the landscape of decentralized learning, FedProf stands as a testament to the power of innovation, ensuring that even amidst the noise, clarity and progress can prevail.", "introduction": "With the advances in Artificial Intelligence (AI), we are seeing a rapid growth in the number of AI-driven applications as well as the volume of data required to train them.However, a large proportion of data used for machine learning are often generated outside the data centers by distributed resources such as mobile phones and IoT (Internet of Things) devices.It is predicted that the data generated by IoT devices will account for 75% of the total in 2025 (Meulen, 2018).Under this circumstance, it will be very costly to gather all the data for centralized training.More importantly, moving the data out of their local devices (e.g., mobile phones) is now restricted by law in many countries, such as the General Data Protection Regulation (GDPR)foot_0 enforced in EU.We face three main difficulties to learn from decentralized data: i) massive scale of end devices; ii) limited communication bandwidth at the network edge; and iii) uncertain data distribution and data quality.As an promising solution, Federated Learning (FL) (McMahan et al., 2017) is a framework for efficient distributed machine learning with privacy protection (i.e., no data exchange).A typical process of FL is organized in rounds where the devices (clients) download the global model from the server, perform local training on their data and then upload their updated local models to the server for aggregation.Compared to traditional distributed learning methods, FL is naturally more communication-efficient at scale (Kone\u010dn\u1ef3 et al., 2016;Wang et al., 2019).Nonetheless, several issues stand out."}
{"paper_id": 301, "abstract": "In the realm of deep learning, where the intricate tapestry of graph data weaves itself into the fabric of modern technology, Graph Neural Networks (GNNs) stand as formidable constructs. These networks, crafted with precision, rely on node features to breathe life into their first layers. Yet, when faced with graphs devoid of such features, a conundrum arises: should one extract rudimentary graph-based characteristics, like the simple count of degrees, or delve deeper into the arcane art of learning input node representations\u2014embeddings\u2014during the training of the network?  The latter path, while promising the potential for enhanced performance, carries with it a heavy burden. The number of parameters tied to these embeddings swells in tandem with the number of nodes, transforming the quest for knowledge into a daunting challenge when confined by the limits of GPU memory, particularly in the vast expanse of industrial-scale graph data.   Drawing inspiration from the innovative embedding compression techniques that have emerged in the realm of natural language processing, we present a novel approach to node embedding compression. In this method, each node is elegantly encapsulated within a compact bit vector, eschewing the traditional float-point representation. Remarkably, the parameters governing this compression can be trained in harmony with the GNNs themselves.   Our findings reveal that this pioneering node embedding compression method not only alleviates the burdens of parameter growth but also outshines existing alternatives, heralding a new era of efficiency and performance in the world of graph neural networks.", "introduction": "Graph neural networks (GNNs) are representation learning methods for graph data.When a GNN model is applied on node classification problems, the model typically learns the node representation from input node features X and its graph G where the node features X are used as the input node representation to the first layer of the model and the graph G dictates the propagation of information (Kipf & Welling, 2016;Hamilton et al., 2017;Zhou et al., 2020).However, the input node features X may not always be available for certain datasets.In order to apply such type of model on graph without node features X, we could either 1) extract simple graph based node features (e.g., number of degrees) from the graph G or 2) use embedding learning methods to learn the node embeddings as features X (Duong et al., 2019).While both approaches are valid, it has been shown that the second approach constantly outperforms the first one with a noticeable margin (Duong et al., 2019), and most recent methods learn the node embeddings jointly with the parameters of GNNs (He et al., 2017;2020;Wang et al., 2019).Learning node features (or embedding) X for graph with small number of nodes may not be much of a problem for common computer system.But, as the size of the embedding matrix X grows linearly with the number of nodes, scalability quickly becomes a problem, especially when attempting to apply such method on industrial grade graph data.For example, if a given graph has 1 billion nodes, we set the dimension of the learned embedding to 64, and store the embedding X using singleprecision floating-point format, the memory cost for the embedding layer alone is 238 gigabytes, which is beyond the capability of common graphics processing unit (GPU).To solve the scalability issue, we adopt the embedding compression idea originally developed for natural language processing (NLP) models (Suzuki & Nagata, 2016;Shu & Nakayama, 2017;Svenstrup et al., 2017;Takase & Kobayashi, 2020).Particularly, we study the ALONE method proposed by Takase & Kobayashi (2020) as it only requires single stage of training unlike other methods.ALONE represents each word using a randomly generated compositional code vector; then a decoder model, which can be trained end-to-end with the downstream modelfoot_0 , uncompresses the compositional code vector into a floating-point vector.The bit size of the compositional code vector is parametrization by c and m where c is the cardinality of each element in the code vector and m is the length of the code vector.For example, if we set c = 4 and m = 6, one valid code vector is [2, 0, 3, 1, 0, 1] where the length of the vector is 6 and each element in the vector is within the set {0, 1, 2, 3}.The code vector can be converted to a bit vector of length m log 2 c by representing each element in the code vector as a binary number and concatenating the resulting binary numbers 2 .Continuing the example, the code vector [2, 0, 3, 1, 0, 1] can be compactly stored as [10 00 11 01 00 01].GloVe, analogy random hashing learn raw 5 0 0 0 1 0 0 0 0 2 5 0 0 0 5 0 0 0 0 1 0 0 0 0 0 2 0 0 0 0 0 number of compressed entitiesrandom hashing (pre-trained) hashing (graph) learn raw 5 0 0 0 1 0 0 0 0 2 5 0 0 0 5 0 0 0 0 1 0 0 0 0 0 2 0 0 0 0 0 number of compressed entities 0.5 0.8 nmi.random hashing (pre-trained) hashing (graph) learn rawFigure 1: Three coding schemes are tested: 1) random coding/ALONE, 2) hashing-based coding/the proposed method, and 3) learning-based coding scheme with autoencoder.For GloVe embeddings, we apply the hashing-based coding method on the pre-trained embedding.For metapath2vec and metapath2vec++ embeddings, we apply the hashing-based coding method on either the pretrained embedding or the adjacency matrix from the graph.The horizontal line labeled with \"raw\" shows the performance of the original embeddings' performance without any compression.The yaxis of each sub-figure is the performance measurement (the higher the better).See Section 5.1 for more details.Using the same conversion trick, it only requires 48 bits to store each word with the parametrization (c = 64, m = 8, 8 log 2 64 = 48 bits) used by Takase & Kobayashi (2020) in their experiments.The coding scheme can uniquely represent up to 2 48 words, which is way beyond the number of words (or sub-words) used in conventional NLP models (Vaswani et al., 2017;Takase & Okazaki, 2019).However, generating the code vectors in a random fashion hinders the quality of the uncompressed embedding.One way to quickly benchmark an embedding compression method's capability is by evaluating the performance of reconstructed (or uncompressed) pre-trained embeddings.As shown in Figure 1, when the model compresses more and more embeddings, the performance of the reconstructed embeddings drops considerably when each entity is represented with a randomly generated code vector following Takase & Kobayashi (2020) (see lines labeled as \"random\").The phenomenon is observed in our experiments with GloVe word embeddings (Pennington et al., 2014b) on both word analogy/similarity task and metapath2vec/metapath2vec++ node embeddings (Dong et al., 2017a) on node clustering task.In order to solve this performance degradation problem, instead of using a randomly generated code vector to represent each entity, we adopt an efficient random projection hashing method to generate a code vector for each entity using auxiliary information such as the adjacency matrix associated with the graph G or the pre-trained embedding 3 .The adopted random projection hashing method is a locality-sensitive hashing (LSH) method (Charikar, 2002) because it hashes entities with similar auxiliary information into similar code vectors.By referring once again to Figure 1, the proposed method with hashing-based coding scheme (see lines labeled as \"hashing\") outperformed the random coding scheme in all scenarios.Similar to ALONE, the proposed method does not introduce additional training stage since it is based on random projection.On top of that, the memory footprint is identical to ALONE as the proposed method only replaces the coding scheme.In additional to the proxy tasks of pre-trained embedding reconstruction, we also compare the effectiveness of different coding schemes where the GNN model and the decoder model are trained together in an end-to-end fashion.Particularly, we trained the GraphSAGE model (Hamilton et al., 2017) on three different node classification datasets.We choose to use the GraphSAGE model (Hamilton et al., 2017) because it is one of the most scalable GNNs in the literature (Ying et al., 2018).To show that the proposed method also benefits other GNNs/tasks, we present additional experiment results in Appendix B.4 on more GNNs with link prediction task.The experimental results have confirmed the superb performance of the proposed hashing-based coding scheme comparing to the random coding scheme used in ALONE under the intended use scenario."}
{"paper_id": 302, "abstract": "In this exploration of meta-learning, we delve into the realm of deep networks that utilize stochastic local winner-takes-all (LWTA) activations. Picture, if you will, a network where each layer is a bustling marketplace of ideas, yet only one voice rises above the din to make its mark\u2014this is the essence of our LWTA units. These units are cleverly arranged into blocks, ensuring that amidst the chaos, only one unit shines forth with a non-zero output, creating a tapestry of sparse representations.  The heart of our design lies in the stochastic nature of these activations. Here, the network engages in a dance of posterior sampling, pitting competing units against one another in a bid to determine the ultimate victor. This approach is a stark departure from the traditional deterministic paradigms that have long dominated the field. By drawing inspiration from the principles of Bayesian statistics, we argue that our method equips the network to navigate the murky waters of uncertainty\u2014an all-too-common plight in meta-learning, where task-related training data often runs scarce.  During the training phase, we harness the power of the reparameterization trick for discrete distributions, allowing us to conduct reliable training through the art of Monte Carlo sampling. When it comes time for inference, we turn to the elegance of Bayesian Model Averaging, which deftly weaves together a multitude of sampled representations, creating a richer tapestry of understanding.  Our experimental results speak volumes: our approach not only achieves state-of-the-art predictive accuracy on established few-shot image classification benchmarks but does so without sacrificing computational efficiency. In a world where every decision counts, we believe our method provides a beacon of hope for the future of meta-learning.", "introduction": "When we train machine learning models on problems with limited amounts of training data, we cannot usually get good predictive performance (Lai, 2019;Sculley et al., 2015).This comes in contrast to the human ability to quickly derive information from a range of different tasks, and then adapt to a new task with limited available new examples (K\u00fchl et al., 2020;Castro et al., 2008).In essence, this capability of the mind of learning how to learn (Black et al., 2006) has inspired researchers to investigate the concept of Meta-Learning (ML) (Lake et al., 2017;Vilalta & Drissi, 2002;Wang et al., 2016;Zoph & Le, 2017;Flennerhag et al., 2020;Lake et al., 2015;Hutsebaut-Buysse et al., 2019).There is a large variety of deep learning methods for ML (Finn et al., 2019;Baik et al., 2020;Andrychowicz et al., 2016).Specifically, Finn et al. (2017) presented the Model-Agnostic Meta-Learning (MAML) algorithm that enables tuning the parameters of a trained network to quickly learn a new task with only a few gradient updates.To get away with the entailed second-order computations, which are expensive, several researchers proposed appropriate first-order approximations for MAML; such works are the First-Order MAML (FOMAML) of Finn et al. (2017) and the Reptile algorithm of Nichol et al. (2018).Recently, several researchers have also considered Bayesian inference-driven methods for deep learning ML, extending upon older works built for conventional machine learning approaches (Yoon et al., 2018;Finn et al., 2018;Ravi & Beatson, 2019;Patacchiola et al., 2020;Zou & Lu, 2020;Grant et al., 2018;Chen et al., 2020).This work proposes a different regard toward improving generalization capacity for deep networks in the context of ML.Specifically, our proposed approach relies on the following main concepts:\u2022 The concept of sparse learned representations.For the first time in the literature of deep network-driven ML, we employ a mechanism that inherently learns to extract sparse data representations.This consists in replacing standard unit nonlinearities (e.g., ReLU) with a unit competition mechanism.Specifically, (linear) units are organized into blocks.Presented with some input, the units within a block engage in a competition process with only one winner.The outputs of all units except for the winner are zeroed out; the output of the winner retains its computed value (local winner takes-all, LWTA, architecture).\u2022 The concept of stochastic representations.We establish a stochastic formulation for the previously described competition process.Specifically, we postulate that, within a block of competing units, winner is selected via sampling from an appropriate Categorical posterior.The corresponding winning probability of each unit is proportional to its linear computation (thus depending on the layer input).Via this competition process, we yield stochastic representations from network layers, that is representations that may change each time we present to the network layer exactly the same input.Based on the results from existing approaches, we posit that the proposed treatment of the ML problem, which combines learned representation sparsity and stochasticity, will be extremely beneficial to the deep learning community.We dub our approach Stochastic LWTA for ML (StochLWTA-ML).We perform a variational Bayes treatment of the proposed model.We opt for a full Bayesian treatment, by also handling network weights as latent variables.That is, we elect to impose an appropriate prior over the network weights and fit approximate (variational) posteriors.We evaluate our approach on a number of standard benchmarks in the field, namely Omniglot (Lake et al., 2017), Mini-Imagenet (Vinyals et al., 2016) and CIFAR-100 (Krizhevsky, 2009).We show that our approach offers a variety of advantages over the current state-of-the-art methods, namely: (i) incurring reduced predictive error rate compared to the currently state-of-the-art methods in the field; (ii) obtaining this performance with networks that comprise one order of magnitude less trainable parameters, and therefore give rise to better computational efficiency and imposed memory footprint.The remainder of this paper is organized as follows: In Section 2, we briefly review related work.Section 3 introduces our approach and provides the related training and prediction algorithms.In Section 4, we perform a thorough experimental evaluation of StochLWTA-ML, and compare our findings to the current state-of-the-art.In the final Section 5, we end up with the conclusions of our work, and suggest lines of further research."}
{"paper_id": 303, "abstract": "In the realm of imitation learning, we embark on a bold quest: harnessing the power of visual observations to master the control of dynamical systems characterized by continuous states and actions. This pursuit is particularly enticing, given the vast ocean of video data at our disposal, ripe for agents to draw upon. Yet, lurking in the shadows are formidable challenges: the absence of direct action observations and the daunting complexity of high-dimensional visual spaces.   In our exploration, we unveil a series of innovative strategies for imitation learning, intertwining the principles of adversarial learning and optimal transport. Central to our approach is a clever utilization of representations derived from reinforcement learning encoders, which we employ to compute imitation rewards. These methodologies allow us to scale our efforts, ultimately achieving expert-level performance in the visually intricate tasks presented by the DeepMind control suite.  As we delve into the intricate trade-offs of our techniques, we present a thorough evaluation of critical design choices that shape our findings. In the spirit of fostering collaboration and reproducibility within the research community, we offer an accessible implementation, complete with our methods, to serve as a benchmark for visual imitation learning. Together, let us unlock the potential of visual data and propel the boundaries of what agents can achieve.", "introduction": "Learning continuous control policies directly from pixel observations is an important problem due to its potential impact on fields like robotics, autonomous driving and video games.These domains have rich resources of data available of humans performing expert-level demonstrations that our software agents do not leverage as they are often trained from scratch without any knowledge of how humans think about these problems.However, using unlabeled video data is challenging as it i) requires distilling a representation of the world into the policy of an agent, and ii) we do not know the underlying actions and reasoning process of the expert.This renders common algorithms like canonical behavioral cloning (Pomerleau, 1988;1991) useless in the no-action setting.Recently the community has advanced our understanding in learning visual representations and learning to imitate demonstrations provided as proprioceptive states.Visual representation learning has been crucial in recent advancements for sample-efficient reinforcement learning (RL) directly from pixels in continuous spaces, e.g. with reconstruction (Finn et al., 2016;Yarats et al., 2019), contrastive learning (Srinivas et al., 2020;Stooke et al., 2020), unsupervised pre-training (Liu & Abbeel, 2021;Yarats et al., 2021b;Seo et al., 2021), world models (Hafner et al., 2018;2019;2020) and data augmentation (Yarats et al., 2021c;Raileanu et al., 2020;Laskin et al., 2020).These approaches require known reward signals from the environment, which are not always available or well-defined.When expert demonstrations are available, imitation learning (IL) and inverse RL (IRL) methods overcome the issue of not having a reward signal and seek to recover the expert agent (Ng & Russell, 2000).These methods are very effective when low-dimensional proprioceptive states and actions are available and typically consist in learning based on the mismatch between the expert and agent's state(-action) distributions.In this paper, we combine the budding areas of model-free image-based reinforcement learning and state-action imitation learning to control non-trivial continuous dynamical systems from pixel demonstrations.We extend two leading proprioceptive-state approaches to comparing the pixel trajectories of the learner and expert -see fig. 1 for an illustration.The first approach, pixel sinkhorn imitation learning, P-SIL, extends optimal transport approaches for imitation learning to the image --Figure 1: Summary of our proposed methods P-SIL and P-DAC.Top: P-SIL, i) encodes the agent o a and expert o e trajectories into a latent space, ii) computes a cost matrix C and iii) transport map \u03c0 between these to iv) produce imitation rewards r 1:T .Bottom: P-DAC, i) encodes the (data-augmented) agent o a and expert o e trajectories into a latent space, ii) passes them through the discriminator, and iii) evaluates the DAC loss.iv) Rewards r 1:T can then be produced with the discriminator.setting (Papagiannis & Li, 2020;Dadashi et al., 2021).A key component here is how we learn and compare latent representations using the cosine distance on a target encoder that is updated with the RL encoder's weights.Our second approach, pixel discriminator actor critic, P-DAC, is a GAIL-based method for pixels (Ho & Ermon, 2016;Kostrikov et al., 2019;Torabi et al., 2018).We propose several key modifications, including data augmentation and using the RL encoder for representations, that enable P-DAC to scale to non-trivial control tasks without needing access to expert actions.The imitation rewards from these methods are optimized with DrQ-v2 as an underlying image-based RL backbone (Yarats et al., 2021a).We demonstrate the versatility and strong performance of both approaches on DeepMind control suite tasks, and find that both P-DAC and P-SIL are able to recover expert performance while outperforming canonical extensions of corresponding state-based IL methods, such as SIL (Papagiannis & Li, 2020) and DAC (Kostrikov et al., 2019;Ho & Ermon, 2016) in terms of performance and sample efficiency.Our extensive ablation study reveals insights into the key design decisions for pixel imitation learning.Finally, we also provide an easy-to-use implementation of these methods with unified DrQ-v2 backbone to make it simple for practitioners to build upon in this under-explored area."}
{"paper_id": 304, "abstract": "In the realm of deep learning, where the quest for efficiency often clashes with the demands of complexity, pruning emerges as a beacon of hope\u2014an artful technique designed to compress intricate models for deployment on edge devices that tread lightly on resources. Yet, many of the prevailing methods in this domain lean heavily on unstructured pruning, yielding models that struggle to find their footing on standard hardware. Moreover, the burden of manual exploration and fine-tuning of the pruning process can be a daunting, time-consuming endeavor, often leading to results that fall short of their potential.  To shatter these constraints, we unveil a novel approach\u2014an adaptive, activation-based, structured pruning method that promises to forge compact, accurate, and hardware-efficient models tailored to user specifications. At the heart of our strategy lies an iterative structured pruning technique, harnessing activation-based attention feature maps to deftly pinpoint and eliminate filters that lack significance. Complementing this, we introduce adaptive pruning policies that instinctively align with the objectives of accuracy-sensitive, memory-limited, and latency-critical tasks.  Our extensive evaluations reveal that this pioneering method does not merely compete; it excels, significantly eclipsing the state-of-the-art structured pruning techniques across the CIFAR-10 and ImageNet datasets. For instance, when applied to ResNet-56 on CIFAR-10, our approach achieves a remarkable parameter reduction of 79.11% without sacrificing accuracy, outstripping related works by an impressive margin of 22.81% to 66.07%. Furthermore, we attain a substantial reduction in FLOPs of 70.13%, surpassing comparable methods by 14.13% to 26.53%. In this way, we pave a new path forward in the landscape of model pruning, balancing the scales of performance and efficiency in a world that demands both.", "introduction": "Deep neural networks (DNNs) have substantial compute and memory requirements.As deep learning becomes pervasive and moves towards edge devices, DNN deployment becomes harder because of the mistmatch between resource-hungry DNNs and resource-constrained edge devices.DNN pruning is a promising approach (Li et al. (2016); Han et al. (2015); Molchanov et al. (2016); Theis et al. (2018); Renda et al. (2020)), which identifies the parameters (or weight elements) that do not contribute significantly to the accuracy and prunes them from the network.Recently, works based on the Lottery Ticket Hypothesis (LTH) have achieved great successes in creating smaller and more accurate models through iterative pruning with rewinding (Frankle & Carbin (2018)).However, LTH has only been shown to work successfully with unstructured pruning which, unfortunately leads to models with low sparsity and difficult to accelerate on commodity hardware such as CPUs and GPUs (e.g., Hill et al. (2017) shows directly applying NVDIA cuSPARSE on unstructured pruned models can lead to 60\u02c6slowdown on GPU compared to dense kernels.)Moreover, most pruning methods require users to explore and adjust multiple hyper-parameters, e.g., with LTH-based iterative pruning, users need to determine how many parameters to prune in each round.Tuning the pruning process is time consuming and often leads to sub-optimal results.We propose activation-based, adaptive, iterative structured pruning to find the \"winning ticket\" models that are at the same time hardware efficient and to automatically meet the users' model accuracy, size, and speed requirements.First, we propose an activation-based structured pruning method to identify and remove unimportant filters in an LTH-based iterative pruning (with rewinding) process.Specifically, we properly define an attention mapping function that takes a 2D activation feature maps of a filter as input, and outputs a 1D value used to indicate the importance of the filter.This approach is more effective than weight-value based filter pruning because activationbased attention values not only capture the features of inputs but also contain the information of convolution layers that act as feature detectors for prediction tasks.We then integrate this attention-based method into the LTH-based iterative pruning framework to prune the filters in each round and find the winning ticket that is small, accurate, and hardware-efficient.Second, we propose adaptive pruning that automatically optimizes the pruning process according to different user objectives.For latency-sensitive scenarios like interactive virtual assistants, we propose FLOPs-guaranteed pruning to achieve the best accuracy given the maximum amount of compute FLOPs; For memory-limited environments like embedded systems, we propose modelsize-guaranteed pruning to achieve the best accuracy given the maximum amount of memory footprint; For accuracy-critical applications such as those on self-driving cars, we propose accuracyguaranteed pruning to create the most resource-efficient model given the acceptable accuracy loss.Aiming for different targets, our method adaptively controls the pruning aggressiveness by adjusting the global threshold used to prune filters.Moreover, it considers the difference in each layer's contributions to the model's size and computational complexity and uses a per-layer threshold, calculated by dividing each layer's remaining parameters or FLOPs by the entire model's remaining parameters or FLOPs, to prune each layer with differentiated level of aggressiveness.Our results outperform the related works significantly in all cases targeting accuracy loss, parameters reduction, and FLOPs reduction.For example, on ResNet-56 with CIFAR-10 dataset, without accuracy drop, our method achieves the largest parameter reduction (79.11%), outperforming the related works by 22.81% to 66.07%, and the largest FLOPs reduction (70.13%), outperforming the related works by 14.13% to 26.53%.In addition, our method enables a pruned model the reach 0.6% or 1.08% higher accuracy than the original model but with only 30% or 50%of the original's parameters.On ResNet-50 on ImageNet, for the same level of parameters and FLOPs reduction, our method achieves the smallest accuracy loss, lower than the related works by 0.08% to 3.21%; and for the same level of accuracy loss, our method reduces significantly more parameters (6.45% to 29.61% higher than related works) and more FLOPs (0.82% to 17.2% higher than related works)."}
{"paper_id": 305, "abstract": "In the realm of few-shot learning (FSL), the power of semi-supervised learning has emerged as a beacon of hope, deftly harnessing the strengths of limited labeled data alongside vast oceans of unlabeled information. Yet, as we venture into the unpredictable landscapes of real-world applications, we encounter a treacherous challenge: the query and support sets may be tainted with noise\u2014corrupted labels and outliers lurking in the shadows, ready to undermine our efforts.  To combat this peril, we unveil a groundbreaking approach: Robust Cross-Modal Semi-Supervised Few-Shot Learning (RCFSL), forged in the fires of Bayesian deep learning. By enveloping the parameters of an infinite Gaussian mixture model with an uncertainty prior, we create a fortress against noisy inputs. Here, the multifaceted wisdom of both image and text data converges within a resilient heterogeneous variational autoencoder, weaving a tapestry of understanding that transcends modalities.  But we do not stop there. To fortify our defenses further, we introduce a robust divergence measure, a powerful tool that enhances our model\u2019s resilience. From this, we derive and optimize a novel variational lower bound, allowing us to deftly infer the parameters of our network. As the final stroke of our masterpiece, we incorporate a robust semi-supervised generative adversarial network, a cunning ally that generates resilient features to bridge the chasm of data sparsity in few-shot learning. Through a harmonious joint optimization process, we refine our training and inference strategies.  Our approach stands as a testament to efficiency, scalability, and adaptability, outshining previous methods in the face of complexity. With compelling evidence of superior performance across multiple benchmark multi-modal datasets, we demonstrate that even amid the chaos of noise, our RCFSL strategy emerges victorious in the realm of semi-supervised few-shot learning.", "introduction": "Despite the impressive success of deep learning models, frequently it requires massive amount of training data to fully demonstrate the potential of the model.In contrast, human is capable of learning new concepts given limited data.Consequently, few-shot learning gathers extensive research interest due to the capabilities of learning new concepts from limited training data.Nevertheless, the success of few-shot learning requires careful handling to robustness and generalization as it is extremely susceptible to noisy labels, outliers as well as adversarial attack Lu et al. (2020a).For instance, in order to automatically recognize several kinds of uncommon animals, only a few annotated images for them are available due to their rarity.Moreover, the images could potentially be corrupted due to an uncontrollable shooting environment or an instrumental malfunction.To mitigate this, one common approach to few shot learning is meta-learning Ren et al. (2018), where the goal is to learn a classifier to distinguish between previously unseen classes, given labeled classes and a larger pool of unseen examples, some of which may belong to the classes of interest, namely semi-supervised few shot learning (SFSL).Despite the impressive capabilities equipped the ability to leverage unlabeled examples for SFSL, the challenge of lacking novel samples remains to be a bottleneck.Besides visual information, textual data frequently contains rich information and more descriptive concepts for learning.Incorporating image-text multi-modal learning into the framework by training on image-text pairs provides an efficient tool to inject the diversity to the generation process Pahde1 et al. (2021) Pahde1 et al. (2018).The work in Pahde1 et al. (2018) provides a benchmark for multimodal few-shot learning relying on a class-discriminative text conditional generative adversarial network.Later on, Pahde1 et al. (2021) tackles the multimodal few shot learning problem by employing a cross-modal feature generation network to infer the class membership of unseen samples with a simple nearest neighbor approach.Despite of the success of these methods with clean features and perfect labels, the important case that features and labels are contaminated due to out-of-distribution samples, adversarial attack and human fatigue is rarely studied.In parallel, Bayesian deep learning (BDL) has served as a powerful Figure 1: The overall blockdiagram of the proposed robust cross-modal semi-supervised few shot learning (RCFSL) framework to leverage both the advantage of robust GAN as a high quality generative model and a robust heterogenous semi-supervised VAE as a posterior distribution learner on multi-modality data.Robust multimodal prototypes are then calculated from the robust embedding using the last hidden layer in the discriminator of RCFSL, where three novel components are highlighted in the orange bounding boxes.tool in terms of transforming the problem of posterior inference of a BDL model into the optimization of an objective function based on latent variables.Now the question then is: how to design a Bayesian deep learning which counters the noisy labels and outliers jointly in the multimodal semisupervised few-shot learning.Accordingly, this paper tackles this challenging problem in robust cross-modal few-shot learning by integrating a deep generative heterogenous model that generalizes well to multi-modality (e.g.image-text modeling) in order to counter noisy labels and outliers.Specifically, a robust heterogeneous variational auto-encoder is first proposed to encode the noisy visual features and labels in order to jointly learn the information from both modalities by placing the uncertainty prior on the top of the infinite Gaussian mixture models.Subsequently, a robust variational lower bound based on \u03b2-divergence is derived to infer the network parameters.Finally, a robust semi-supervised GAN is integrated with the heterogenous variational auto-encoder by collapsing the generator and the decoder into one to further boost the learning capabilities.RCFSL is built on top of Bayesian deep learning by fusing cross-modal information via the approximation of the joint posterior distributions.In contrast to modality-alignment methods Xing et al. (2019a)Tsai et al. (2017) for robust few-shot learning, the robustness of RCFSL is achieved by accurate modeling of the complicated joint distribution of multi-modality data and robust variational inference by the derived lower bound.Distinct from the work in Xing et al. (2019b) calculating linear combinations in the prototypical representation space, our fusion of multimodality features in the probability distribution is completely data-driven, yielding more robust classification performance in few-shot learning.Major contributions of this paper are: (1) RCFSL harnesses three levels of denoising to ensure the robustness of cross-modal semi-supervised few-shot learning: Firstly, motivated by Hou & Galata (2008), it places the uncertainty prior of the parameters of an infinite Gaussian mixture distribution of image data to avoid mixture components collapsing onto a point or a hyperplane due to outliers.(2) Subsequently, the robust \u03b2-divergence is employed to replace Kullback-Leibler divergence used for data fitting to infer the network parameters and a novel evidence lower bound for semi-supervised few shot learning is derived.(3) Noise-transition layers are applied to both the heterogenous variational encoder and the robust discriminator in semi-supervised learning with an end-to-end training.The performance of RCFSL is further boosted with robust feature generation yielding 7% to 10% absolute accuracy improvement over STOA approaches.Related Work Previous work in multimodal few shot learning frequently tends to first learn text to image mapping to generate additional visual features and then calculate the joint prototype using a weighted average from two representations.Two recent approaches have attracted significant attention in the few-shot learning domain: Matching Networks Vinyals et al. (2016), andPrototypical Networks Snell et al. (2017) where the sample set and the query set are embedded with a neural network, and nearest neighbor classification is exploited relying on a metric in the embedded space.In Oreshkin et al. (2018), metric scaling and metric task conditioning are utilized to improve the performance of few-shot learning algorithms.Kim et al. (2018) andFinn et al. (2018) employ a probabilistic extension to model-agnostic meta-learning (MAML) framework trained with variational approximation so that the model can generalize well to a new task with a few fine-tuning updates.In Zhang et al. (2020), a bidirectional joint image-text modeling was proposed and VHE-raster-scan-GAN was applied.RCFSL advances the work from Zhang et al. (2020) by improving the robustness of the multi-modal heterogenous encoder and extend the solution to semi-supervised few shot learning.In Tseng et al. (2020), feature-wise transformation layers are utilized for augmenting the image features relying on affine transforms to simulate various feature distributions under different domains for few-shot learning.Different from Tseng et al. (2020), RCFSL augments the robust feature generation from BDL perspective relying on robust semi-supervised GAN.Moreover, our method advances from other robust few shot learning such as Rapnets Lu et al. (2020b) and Adversarial Query Goldblum et al. (2020) by providing mathmatically rigoriously denoising schemes via uncertainty priors and robust divergence in variational inference.Figure 2: The probabilistic graph model of the proposed RCFSL framework, where x denotes the observations of image data and t represents the observations of embedded feature vectors for text, \u1ef9 and \u0177 refer to noisy labels and corrected labels respectively, z and a stand for the latent variable and auxiliary variable for VAE respectively, serving as a bridge to fuse two modalities.T k and C k represent the mean and the covariance for the k Gaussian component of image data.\u03bb l represents the parameter in the Poisson distribution for the lth feature.Each observation pair (x i , t i ) depends on their cluster assignment and distribution parameters and each cluster assignment depends on the stick-breaking procedure Beta((K -1)\u03b1, \u03c7)."}
{"paper_id": 306, "abstract": "In the realm of imaging, where light dances across dimensions, the inverse art of snapshot compressive imaging beckons us to unravel the mysteries of hyperspectral image (HSI) reconstruction. Here, we embark on a journey from the confines of 2D measurements, reaching deep into the vibrant, intricate tapestry of 3D spatial-spectral signals. Amidst a landscape dotted with intricate neural networks, each built upon a foundation of assumptions, we find ourselves at a crossroads. While these networks have flourished in controlled experimental realms, they struggle to deliver the high-fidelity reconstructions demanded by practical applications\u2014facing the dual challenges of efficiency in computation and power consumption.  In this exploration, we first reflect upon the assumptions that underpin these networks, illuminating the urgent need for a truly practical solution within the reconstruction community. Through a critical analysis of the widely-adopted U-Net architecture, we unveil both its strengths and its shortcomings. From this understanding, we introduce the Simple Reconstruction Network, or SRN\u2014a design forged from familiar techniques such as scale and spectral-invariant learning, alongside the elegant simplicity of identity connections.   The results are striking: under the current constraints, SRN not only eclipses existing methods in terms of reconstruction quality but does so with a newfound efficiency that paves the way for real-world applications. We envision SRN as a beacon for future advancements in reconstruction techniques, serving as a robust backbone for challenges such as real-time, high-resolution HSI reconstruction. In this endeavor, we aspire to lay a foundation that not only meets the demands of the present but also inspires the innovations of tomorrow.", "introduction": "Hyperspectral imaging (HSI) refers to multi-channel imaging where each channel stores information at a specific spectral wavelength for a fixed real-world scene (Plaza et al., 2009).By capturing spatial intensity in a spectral-wise manner, hyperspectral images empower richer information than traditional RGB image cubes and they have been applied in a wide range of scenarios, e.g., object detection (Kim et al., 2012;Xu et al., 2015), remote sensing (Borengasser et al., 2007;Melgani & Bruzzone, 2004;Yuan et al., 2017), medical image processing (Lu & Fei, 2014;Meng et al., 2020c) etc. HSI can be captured and measured by snapshot compressive imaging (SCI) systems, which tend to compress information of snapshots along the spectral axis into one single 2D measurement (Yuan et al., 2021).The coded aperture snapshot spectral imaging (CASSI) system (Wagadarikar et al., 2008;Meng et al., 2020b) forms one mainstream research direction among existing SCI systems due to its passive modulation property (Llull et al., 2013;Wagadarikar et al., 2008;2009;Yuan et al., 2015).The goal of HSI reconstruction is to transform the measurements into desired cubic hyperspectral images.As a result, a dimensional-expansion mapping function (2D to 3D) is required, for which reason such a mapping relationship approximation is deemed to be much harder than general image regression tasks.By introducing domain expertise, previous research efforts have proposed a quite number of reconstruction algorithms (Bioucas-Dias & Figueiredo, 2007;Liu et al., 2019;Miao et al., 2019;Meng et al., 2020b;Wang et al., 2020;2017;2019;Yuan, 2016), among which deep neural networks (Meng et al., 2020b;c;Miao et al., 2019;Wang et al., 2019;Wang et al., 2019;2020;Zheng et al., 2021) enable an effective way to faithfully bridge between input and output compositional hierarchies (LeCun et al., 2015).  1While neural networks become increasingly prevalent in solving the HSI reconstruction problem, their success is reliant on several idealistic hypotheses, which hardly holds in practical scenarios.This prohibits the further application and exploration of HSI.Proper Dataset Volume.The feasibility of learning in a universal sense can be defined by satisfying a VC generalization bound Abu-Mostafa et al. (2012).From this perspective, deep neural networks are considered to be high potential due to their high VC bound (which can be described as the amount of free variables).Notably, the corresponding feasibility of neural networks is generally determined by their model complexity and the volume of datasets.In the recent HSI reconstruction methods, more increasingly complicated reconstructive networks have been proposed by assuming the underlying dataset is sufficiently large so that the learning is feasible.This neglects the fact that there's limited data accessible.Under such circumstance, simpler model might be more promising.Ideal reconstructive rate.Practically, any reconstructive algorithm appears with a substantial forward model.For temporal-insensitive situations, the inverse model processes the measurements at any reconstructive rate r recon > 0, without considering cooperating with the forward model.However, the reconstructive rate should be higher than (or at least equal to) capture rate of the forward model, i.e., r recon \u2265 r cap > 0, for temporal-sensitive cases, i.e., real-time/high-speed HSI reconstruction.For example, \u03bb-net proposed by Miao et al. (2019) can be applied in reality with r recon > 30f ps.More generally, reconstructive methods are always hypothetically deemed to be efficient enough, i.e., compatible with any forward systems, which however probably suffers from a deviation from current technical support.Considering the arbitrariness of the lower bound r cap , we cannot determine an \"ideal\" reconstructive rate but only to maximize it, i.e., minimize the reconstructive time.For neural network-based methods, eliminating the number of parameters and computational burdens is a sound solution.Unconstrained computing power.The success of the deep neural networks can largely attribute to the overparameterization (Soltanolkotabi et al., 2018)-the parameters in neural networks are a lot more than the training samples for a good representation learning.Actually, just by investing more computing power P , the cornerstone which overparametrization is reliant upon, researchers have made great progress in diverse applications (Thompson et al., 2020).Latent performance boost can be further expected if more computing power is available, until one reaches the computing power bottleneck P * .It turns out it's difficult to solve such a bottleneck in practical.For example, to conduct TSA-Net (Meng et al., 2020b) for high resolution (HR)-HSI restoration (1024\u00d71024), the required GPU memory would be unrestrictedly over 18Gb, which indicates dilemma encountered by all high-performanced reconstructive methods with large model size."}
{"paper_id": 307, "abstract": "In the realm of private multi-winner voting, we embark on a quest to unveil k-hot binary vectors that adhere to a strict differential privacy guarantee\u2014a challenge that, despite its significance in fields such as healthcare, has remained largely overlooked in the machine learning landscape. In response to this gap, we introduce three innovative mechanisms designed to preserve privacy in multi-label voting: Binary, $\\tau$, and Powerset voting.  The Binary voting mechanism operates independently for each label, employing a strategy of composition that ensures privacy without sacrificing utility. Meanwhile, $\\tau$ voting cleverly constrains the votes within an optimal $\\ell_2$ norm, striking a balance between privacy and accuracy. Powerset voting takes a more holistic approach, treating the entire binary vector as a power set of potential outcomes, allowing for a richer exploration of the voting landscape.  Through rigorous theoretical analysis, we unveil the intricate tradeoffs at play, revealing that Powerset voting necessitates strong correlations between labels to eclipse the performance of the simpler Binary voting method. Building upon these mechanisms, we extend the well-established PATE framework to facilitate privacy-preserving multi-label learning.  In our empirical evaluations, we pit our techniques against DPSGD on extensive real-world healthcare datasets and established multi-label benchmarks. The results are striking: our methods consistently outshine their competitors in centralized settings. Furthermore, we expand the horizons of multi-label collaborative learning by demonstrating how our mechanisms can foster model improvement across multiple sites in a distributed environment. Thus, we chart a new course in the intersection of privacy, voting, and machine learning, illuminating pathways for future exploration and application.", "introduction": "Differential privacy techniques for machine learning have predominantly focused on two types of mechanisms: for a given query, release either a noisy estimate using the Gaussian mechanism (Abadi et al., 2016b) or the noisy arg max (Papernot et al., 2017).These two mechanisms appeal well to single-label classification per input (a.k.a.multi-class classification), which is a widely studied setting of supervised learning where a given input has only one class present.The Gaussian mechanism can enable arbitrary queries of data, for instance, differentially private stochastic gradient descent (DPSGD), which returns noisy gradients (Abadi et al., 2016b).Instead, the Gaussian noisy arg max mechanism can only reveal the max count, which is used in the canonical Private Aggregation of Teacher Ensembles (PATE) (Papernot et al., 2017).In contrast, more real world tasks, such as multi-label classification (Tsoumakas & Katakis, 2007), can be modeled using multi-winner elections (Elkind et al., 2017;Faliszewski et al., 2017), where more than one candidates may win, simultaneously.Outside of the obvious situation of elections, other multi-winner election scenarios include canonical computer visions tasks like object recognition, which require models to recognize all the objects present in an image (Boutell et al., 2004).Further, multi-winner problems arise when inferring the topics written about in a corpus of text (document): e.g., a news article may discuss one or more topics like politics, finance, or education.Another principal setting is healthcare, in which patient data (e.g., symptom reports or X-rays) may be indicative of multiple conditions (Irvin et al., 2019;Johnson et al., 2019).We thus focus on creating private mechanisms for releasing the outcome of a multi-winner election.We first formalize the multi-winner election and then provide a solution, termed Binary voting, where we apply a single-winner election mechanism independently to each of the k candidates.Though simple, we prove that it is optimal when there is a lack of correlation among the outcomes of particular candidates.We then derive tighter data-independent privacy bounds for multi-winner elections by considering the situation where each voter is limited in their ( 2 ) votes for candidates, which we call \u03c4 voting.By casting multi-winner elections to an analogous task of single-winner elections, we create a third mechanism termed Powerset voting, which reveals the result for all k candidates jointly."}
{"paper_id": 308, "abstract": "In the realm of machine learning, data augmentation has become a trusted ally for enhancing image and linguistic datasets. Yet, when it comes to the intricate world of graph-structured data, this powerful technique remains largely uncharted territory. Current methodologies have primarily approached graph augmentation from a sweeping, global perspective, often falling into two distinct camps: structural manipulation and adversarial training that employs the injection of feature noise. However, these strategies are not without their flaws. Structural manipulation can lead to the perilous loss of crucial information, while adversarial training risks degrading feature quality through the very noise it introduces.  In this paper, we embark on a journey into the heart of graph augmentation, unveiling a novel approach we call local augmentation. This method operates on the principle of enhancing node features through the intricate web of their local subgraph structures. We reframe data augmentation as a process of feature generation: given the attributes of a central node, our local augmentation technique learns the conditional distribution of its neighboring nodes' features, allowing it to generate optimal features that elevate the performance of downstream tasks.  To bring our vision to life, we introduce a groundbreaking framework: LA-GNN. This versatile architecture seamlessly integrates with any Graph Neural Network (GNN) model, functioning as a plug-and-play solution. Through rigorous experimentation and in-depth analysis, we demonstrate that local augmentation consistently drives performance improvements across a diverse array of GNN architectures and benchmarks, paving the way for new possibilities in the exploration of graph-structured data.", "introduction": "Graph Neural Networks (GNNs) and their variants (Abu-El-Haija et al., 2019;Kipf & Welling, 2017;Veli\u010dkovi\u0107 et al., 2018) have achieved state-of-the-art performance for many tasks on graphs such as recommendation system (Ying et al., 2018) and traffic prediction (Guo et al., 2019).However, most of the GNN models, such as GCN (Kipf & Welling, 2017) and GAT (Veli\u010dkovi\u0107 et al., 2018), learn the node representations by aggregating information over only the 2-hop neighborhood.Such shallow architectures limit their ability to extract information from higher-layer neighborhoods (Wang & Derr, 2021).But deep GNNs are prone to over-smoothing (Li et al., 2018), which suggests the node representations tend to converge to a certain vector and thus become indistinguishable.One solution to address this problem is to preserve the locality of node representations when increasing the number of layers.For example, JKNet (Xu et al., 2018) densely connects (Huang et al., 2017) each hidden layer to the final layer.GCNII (Chen et al., 2020) employs an initial residual to construct a skip connection from the input layer.Besides, Zeng et al. (2021) pointed out that the key for GNN is to smooth the local neighborhood into informative representation, no matter how deep it is.And they decouple the depth and scope of GNNs to help capture local graph structure.Prior works have emphasized the importance of local information, but one property of the graph is that the number of nodes in the local neighborhood is far fewer than higher-order neighbors.And this property limits the expressive power of GNNs due to the limited neighbors in the local structure.A very intuitive idea is to use data augmentation to increase the number of nodes in the local substructure.However, existing graph data augmentation methods ignore the importance of local information and only perturb at the topology-level and feature-level from a global perspective, which can be divided into two categories: topology-level augmentation (Rong et al., 2020;Wang et al., 2020b;Zhao et al., 2021) and feature-level augmentation (Deng et al., 2019;Feng et al., 2019;Kong et al., 2020).Topology-level augmentation perturbs the adjacency matrix, yielding different graph structures.On the other hand, existing feature-level augmentation mainly exploits perturbation of node attributes guided by adversarial training (Deng et al., 2019;Feng et al., 2019;Kong et al., 2020).These augmentation techniques have two drawbacks.1) Some of they employ full-batch training for augmentation, which is computationally expensive, and introduce some additional side effects such as over-smoothing.2) The type of feature-level augmentation is coarse-grained, which focuses on global augmentation and overlooks the local information of the neighborhood.Moreover, to our best knowledge, none of the existing approaches combines both the feature representations and the graph topology, especially the local subgraph structures, for graph-level data augmentation.In this work, we propose a framework: Local Augmentation for Graph Neural Networks (LA-GNNs), to further enhance the locality of node representations based on both the topology-level and featurelevel information in the substructure.The term \"local augmentation\" refers to the generation of neighborhood features via a generative model conditioned on local structures and node features.Specifically, our proposed framework learns the conditional distribution of the connected neighbors' representations given the representation of the central node, bearing some similarities with the Skipgram (Mikolov et al., 2013) and Deepwalk Perozzi et al. (2014), with the difference that our method does not base on word or graph embedding.The motivation behind this work concludes three-fold. 1) Existing feature-level augmentation works primarily pay attention to global augmentation without considering the informative neighborhood.2) The distributions of the representations of the neighbors are closely connected to the central node, making ample room for feature augmentation.3) Preserving the locality of node representations is key to avoiding over-smoothing (Xu et al., 2018;Klicpera et al., 2019;Chen et al., 2020).And there are several benefits in applying local augmentation for the GNN training.First, local augmentation is essentially a data augmentation technique that can improve the generalization of the GNN models and prevent over-fitting.Second, we can recover some missing contextual information of the local neighborhood in an attributed graph via the generative model (Jia & Benson, 2020).Third, our proposed framework is flexible and can be applied to various popular backbone networks such as GCN (Kipf & Welling, 2017), GAT (Veli\u010dkovi\u0107 et al., 2018), GCNII (Chen et al., 2020), and GRAND (Feng et al., 2020) to enhance their performance.Extensive experimental results demonstrate that our proposed framework could improve the performance of GNN variants on 7 benchmark datasets."}
{"paper_id": 309, "abstract": "In the ever-evolving realm of cellular imaging, recent strides in Focused Ion Beam-Scanning Electron Microscopy (FIB-SEM) have opened the door to unprecedented insights into cellular ultrastructure at the nanoscale. Yet, this journey is fraught with challenges\u2014challenges that often seem insurmountable. From the painstaking manual annotations that demand the expertise of highly trained specialists to the troublesome artifacts introduced by prolonged scanning, the path to pristine, label-free data is riddled with obstacles.  In response to these trials, we unveil a groundbreaking solution: the fully unsupervised Noise Reconstruction and Removal Network. This innovative architecture, inspired by the elegant mechanics of gated recurrent units, embarks on a quest to denoise scanning electron microscopy images. By synthesizing sequential data, our network adeptly reconstructs and eradicates noise, all while navigating the complexities of unsupervised training. This unique approach empowers the network to discern the true signal from mere noise, yielding results that rival\u2014and in some cases surpass\u2014those achieved by traditional supervised methods on 3D electron microscopy datasets.  To substantiate our claims, we present a thorough performance analysis, employing both numerical and empirical metrics. In doing so, we illuminate the path toward clearer, more accurate imaging, heralding a new era in the study of cellular architecture.", "introduction": "Recent advances in Focused Ion Beam-Scanning Electron Microscopy (FIB-SEM) have led to unprecedented biological tissue visualization and analysis, as well as understanding of cellular ultrastructure and cell-to-cell interactions (Xu et al., 2017).High-resolution FIB-SEM data sets often consist of volumes sliced into thousands of 6K\u00d74K images with 4nm resolution per voxel, allowing a 3D reconstruction of a fraction of tissue volume.Depending on the tissue type, sample preparation, acquisition settings, detector used, etc., the images may contain a significant quantity of noise making any further analysis tedious or even impossible (Kubota et al., 2018;Liu et al., 2018).By definition, image denoising is the process of taking a noisy image x and separating the noise n from the true signal s: x = s + n.Following the typical assumption for the noise (Foi et al., 2008;Wu et al., 2019) and taking the microscope's characteristics into account, we can assume that the noise is: (i) a zero-mean random noise, that is for any pixel the noise is a discrete random number added to the pixel 'true value'; (ii) each pixel noise is independent, so the noise value at any pixel does not depend on the noise at any other pixel, but it is signal dependent.While the noise is random and independent, the signal is not and this is what, typically, denoising methods rely on.In recent years, deep learning methods, and particularly the Convolutional Neural Networks (CNNs), have established themselves as powerful analytical tools in machine learning -to name just a few Redmon & Farhadi (2018); Tan et al. (2020); Chen et al. (2017); Ronneberger et al. (2015); Tao et al. (2020); Sun & Chen (2020).In the field of denoising, CNNs have been very useful (Kim et al., 2019;Liu et al., 2019;Yu et al., 2019) especially when the noise characteristics are unknown, making any mathematical modeling difficult.In this paper we apply a CNN technique to FIB-SEM acquired images, taking into account the relevant specifics.FIB-SEM allows 3D imaging of biological fine structure at nanoscale resolution: a thin slice of the sample is removed with the ion beam and the newly exposed surface is imaged with the electron beam.That results in a sequence of images containing isotropic voxels down to 4nm.Traditionally, training neural networks for denoising demands pairs of noisy and clean images (ground truth images).Theoretically, obtaining denoised images is possible by averaging multiple (up to hundreds) acquisitions of the same sample.As mentioned, this is not feasible with FIB-SEM.The challenges with obtaining ground truth images, as in many biological use cases, is motivation for developing and utilizing unsupervised techniques, such as a Noise2Noise approach (Wu et al., 2019;Lehtinen et al., 2018).Our network uses a triplet of images as input and is trained to map one noise realization to the other, using our modified Noise2Noise loss function.We refer to the proposed architecture as Noise Reconstruction and Removal Network (NRRN).Fig. 1 shows a visual comparison between NRRN and other state-of-the-art networks trained using supervised techniques.The NRRN is applicable to the case of improving the image quality based on two or three scans of the same slice, or denoising based on the two adjacent slices in the volume stack.Our major three contributions discussed in this paper are:\u2022 a novel noise reconstruction module with soft attention and signal boosting, that upon deployment on large images (more than 24M pixels) homogeneously removes the noise, \u2022 a neural network architecture design using our noise reconstruction module with detailed performance analysis, and \u2022 updated Noise2Noise loss function specifically designed for denoising FIB-SEM data."}
{"paper_id": 310, "abstract": "In the intricate tapestry of human intelligence, symbolic reasoning stands as a luminous thread, a testament to our capacity for rule-based manipulation of symbols. Yet, despite its potential, traditional rule-based systems have often faltered in the face of their learning-based counterparts, particularly beyond the confines of structured realms like automated theorem proving. We propose a different path, one that seeks to transcend the limitations of manually crafted rules.   In this exploration, we unveil MetaQNL\u2014a \"Quasi-Natural\" language that elegantly bridges the gap between formal logic and the fluidity of natural language. Alongside it, we introduce MetaInduce, a pioneering learning algorithm designed to extract MetaQNL rules from training data comprising questions and answers, whether or not they include intermediate reasoning steps. Our innovative approach not only achieves state-of-the-art accuracy across a variety of reasoning benchmarks, but also learns compact models with significantly less data. Moreover, it empowers users by generating not just answers, but verifiable proofs, illuminating the path toward a new era of intelligent reasoning.", "introduction": "Symbolic reasoning-rule-based symbol manipulation-is a core component of human intelligence (Mercier & Sperber, 2017).It has been a core part of computer science research, and has achieved significant success in domains such as software verification (Darvas et al., 2005) and theorem proving (Kov\u00e1cs & Voronkov, 2013;McCune, 1997).However, such success has been restricted to domains amenable to rigid, precise formalization.It remains a challenge how to translate such success into \"informal\" domains such as reasoning with common sense knowledge and natural language input.Prior attempts to build rule-based systems, which rely on manually constructed rules, have achieved limited success and tended to produce brittle systems.Deep learning provides an attractive alternative that can easily sidestep the question of representation.A deep network can be trained to perform a reasoning task by directly predicting the answer without explicit symbol manipulation (Clark et al., 2020).However, deep networks can require a large amount of training data and can suffer from poor generalization.More importantly, unlike symbolic systems, a deep network is a black box that is hard to interpret, inspect, and verify.Such lack of interpretability can be undesirable in certain applications, especially those critical to safety and security.In this work, we ask how to build a rule-based system that reasons symbolically but can work with natural language and handle domains difficult to formalize.Such a system would perform reasoning by explicit symbol manipulation based on known rules, therefore more interpretable and verifiable, but at the same time flexible enough to handle natural language input.At a glance, this may appear a large departure from the conventional wisdom that learning-based systems, particularly deep networks, are far superior to rule-based systems, as history has demonstrated repeatedly.However, we hypothesize that this conventional wisdom is incorrect because it assumes a false dichotomy between using learning and using rules; rule-based systems underperformed not because they were rule-based, but because it is difficult to construct rules manually.Further, we hypothesize learning rules from data is key to building effective rule-based systems, but it may require a different kind of learning than gradient descent.The goal of this work is thus to develop a method that automatically learns symbolic rules from data to enable rules-based reasoning with natural language.This poses two main questions.First, what is the system of rules-the basic structures that define what symbols and manipulations allowed-suchThe elephant is bigThe elephant is tall Big, tall things are strongIf something is strong, then it likes cats The elephant likes cats Figure 1: An example proof with 4 assumptions, 1 goal, and 2 rule applications.Each rule have multiple premises and one conclusion.Both the premises and the conclusion can have variables that bind to concrete sentences when the rule is applied.that it is compatible with not only formal logic but also natural language?Second, what is the learning algorithm that induces a set of rules from training data?In this work, we take initial steps toward answering both questions.We propose MetaQNL, a formal symbolic system we call a \"Quasi-Natural Language\", which is compatible with not only rigorous logical inference but also natural language expressions.We also propose MetaInduce, a learning algorithm that induces MetaQNL rules from training data that consists of questions and answers, with or without intermediate reasoning steps.MetaQNL: a Symbolic System in Quasi-Natural Language.In MetaQNL, a sentence is a sequence of words and variables (\"The elephant is [X]\").They also include ordinary English sentences without variables.A rule consists of multiple sentences as its premises (\"The elephant is [X]\", \"If something is [X] then it [Y]\") and one sentence as its conclusion (\"The elephant [Y]\").When applying the rule in reasoning, variables are substituted with concrete sentences ([X] \u2192 strong, [Y] \u2192 likes cats).Therefore, rules capture abstract knowledge that is independent of specific instances-the above rule holds whether [Y] is \"likes cats\" or \"is sleepy\".Such abstraction is essential for reasoning in both humans and machines (Marcus & Davis, 2020).Fig. 1 illustrates how sentences and rules are used in reasoning.Starting from known sentences (assumptions), we apply rules to derive new sentences until the goal is reached.At each step, we substitute the variables in a rule with concrete sentences.This process resembles Metamath (Megill & Wheeler, 2019), a formal language developed for formalizing mathematical proofs, where each step also consists of selecting a theorem and instantiating it with a suitable substitution.So we refer to the reasoning process as theorem proving and the result in Fig. 1 as a proof.It is worth noting that reasoning in MetaQNL is interpretable by design: it is transparent about what rules are assumed; it produces not only an answer, but also a proof that can be mechanically checked against the rules.Assumptions and the goal are usually given when applying our system to a specific task.To solve the task, two issues remain: (1) Rule induction: What is the set of rules?(2) Theorem proving: How to apply the rules to find a proof?Theorem proving has been studied extensively in classical AI (Robinson & Voronkov, 2001) and more recently with deep learning (Alemi et al., 2016;Yang & Deng, 2019;Bansal et al., 2019), and we can adapt existing algorithms such as forward chaining and backward chaining (Russell & Norvig, 2002).In this work, we simply use existing provers and focus instead on the more challenging problem of rule induction.MetaInduce: an Algorithm to Learn MetaQNL Rules.Rule induction can be formulated as a discrete optimization problem that seeks a minimum set of rules that are consistent with training examples.Note that it is important to seek a small number of rules because we always have a trivial solution that consists of one rule per example but is unlikely to generalize.This optimization is challenging due to the discrete, combinatorial search space.We introduce MetaInduce, a general method for learning MetaQNL rules.It encodes the problem as a maximum satisfiability (MAX-SAT) problem, which can be solved efficiently by off-the-shelf solvers (De Moura & Bj\u00f8rner, 2008).Our method consists of 3 steps.First, given a training example, a rule proposer proposes a set of concrete rules (rules without variables) as candidates.This set can be overcomplete and inaccurate.These rules are used to prove the example using existing provers such as forward/backward chaining.Second, we generate abstract rules from concrete rules via a symbolic procedure called anti-unification (Plotkin, 1970;Kutsia et al., 2014).Third, we encode the proof paths in MAX-SAT and solve for a subset of all rules using a MAX-SAT solver.. We benchmark our method on 2 tasks: learning compositional instructions and logical reasoning.For learning compositional instructions, our method not only achieves 100% accuracy on two standard benchmarks: MiniSCAN (Lake et al., 2019) and SCAN (Lake & Baroni, 2018), but also recovers precisely the ground truth rules.For logical reasoning, our method achieves state of the art on the RuleTaker dataset (Clark et al., 2020).Compared to existing methods, our approach learns compact models with much less data, and produces not only answers but also checkable proofs.On RuleTaker, our approach learns a model that has only 2869 symbols but is competitive with a prior approach that uses a neural network with 11 billion parameters."}
{"paper_id": 311, "abstract": "In the realm of visual recognition, where clarity reigns supreme, the concept of face obfuscation\u2014be it through blurring, mosaicing, or other means\u2014stands as a guardian of privacy. Yet, the prevailing narrative in object recognition research often overlooks the implications of such obfuscation, favoring pristine, unobscured images. In this study, we delve into the intricate interplay between face obfuscation and the esteemed ImageNet challenge, a cornerstone of visual recognition benchmarks. While the vast majority of categories within ImageNet do not feature people, incidental figures frequently populate these scenes, raising valid concerns about their privacy. To address this, we embark on a meticulous journey, first annotating the faces embedded within the dataset. Through our exploration, we unveil that common obfuscation techniques, such as face blurring and overlaying, wield surprisingly little influence over the accuracy of recognition models. In fact, our rigorous benchmarking of various deep neural networks on face-obfuscated images reveals a mere whisper of a decline in overall recognition accuracy\u2014an insignificant drop of less than 1.0%. Moreover, we extend our investigation into the realm of transfer learning, applying our findings to four downstream tasks: object recognition, scene recognition, face attribute classification, and object detection. Here, we discover that features gleaned from face-obfuscated images retain their remarkable transferability. Our findings illuminate a promising path toward privacy-aware visual recognition, enhance the widely-utilized ImageNet challenge benchmark, and pave the way for future datasets that honor both privacy and performance.", "introduction": "Visual data is being generated at an unprecedented scale.People share billions of photos daily on social media (Meeker, 2014).There is one security camera for every 4 people in China and the United States (Lin & Purnell, 2019).Even your home can be watched by smart devices taking photos (Butler et al., 2015;Dai et al., 2015).Learning from the visual data has led to computer vision applications that promote the common good, e.g., better traffic management (Malhi et al., 2011) and law enforcement (Sajjad et al., 2020).However, it also raises privacy concerns, as images may capture sensitive information such as faces, addresses, and credit cards (Orekondy et al., 2018).Extensive prior research has focused on preventing unauthorized access to sensitive information in private datasets (Fredrikson et al., 2015;Shokri et al., 2017).However, are publicly available datasets free of privacy concerns?Taking the popular ImageNet dataset (Deng et al., 2009) as an example, there are only 3 people categoriesfoot_0 in the 1000 categories of the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) (Russakovsky et al., 2015); nevertheless, the dataset exposes many people co-occurring with other objects in images (Prabhu & Birhane, 2021), e.g., people sitting on chairs, walking dogs, or drinking beer (Fig. 1).It is concerning since ILSVRC is freely available for academic usefoot_1 and widely used by the research community.In this paper, we attempt to mitigate ILSVRC's privacy issues.Specifically, we construct a privacyenhanced version of ILSVRC and gauge its utility as a benchmark for image classification and as a dataset for transfer learning.Face annotation.As an initial step, we focus on a prominent type of private information-faces.To examine and mitigate their privacy issues, we first annotate faces in ImageNet using face detectors and crowdsourcing.We use Amazon Rekognition to detect faces automatically, and then refine the results through crowdsourcing on Amazon Mechanical Turk to obtain accurate annotations.We have annotated 1,431,093 images in ILSVRC, resulting in 562,626 faces from 243,198 images (17% of all images have at least one face).Many categories have more than 90% images with faces, Figure 1: Most categories in ImageNet Challenge (Russakovsky et al., 2015) are not people categories.However, the images contain many people co-occurring with the object of interest, posing a potential privacy threat.These are example images (with faces blurred or overlaid) of barber chair, husky, beer bottle, volleyball and military uniform.even though they are not people categories, e.g., volleyball and military uniform.Our annotations confirm that faces are ubiquitous in ILSVRC and pose a privacy issue.We release the face annotations to facilitate subsequent research in privacy-aware visual recognition on ILSVRC.Effects of face obfuscation on classification accuracy.Obfuscating sensitive image areas is widely used for preserving privacy (McPherson et al., 2016).We focus on two simple obfuscation methods: blurring and overlaying (Fig. 1), whose privacy effects have been analyzed in prior work (Oh et al., 2016;Li et al., 2017;Hasan et al., 2018).Using our face annotations, we construct face-obfuscated versions of ILSVRC.What are the effects of using them for image classification?At first glance, it seems inconsequential-one should still recognize a car even when the people inside have their faces blurred.Indeed, we verify that validation accuracy drops only slightly (0.1%-0.7% for blurring, 0.3%-1.0%for overlaying) when using face-obfuscated images to train and evaluate.We analyze this drop in detail (identifying categories which are particularly affected), but this key result demonstrates that we can train privacy-aware visual classifiers on ILSVRC which remain highly competitive, with less than a 1% accuracy drop.Effects on feature transferability.Besides a classification benchmark, ILSVRC also serves as pretraining data for transferring to domains where labeled images are scarce (Girshick, 2015;Liu et al., 2015a).So a further question is: Does face obfuscation hurt the transferability of visual features learned from ILSVRC?We investigate by pretraining models on the original/obfuscated images and finetuning on 4 downstream tasks: object recognition on CIFAR-10 ( Krizhevsky et al., 2009), scene recognition on SUN (Xiao et al., 2010), object detection on PASCAL VOC (Everingham et al., 2010), and face attribute classification on CelebA (Liu et al., 2015b).They include both classification and spatial localization, as well as both face-centric and face-agnostic recognition.In all of the 4 tasks, models pretrained on face-obfuscated images perform closely with models pretrained on original images.We do not see a statistically significant difference between them, suggesting that visual features learned from face-obfuscated pretraining are equally transferable.Again, this encourages us to adopt face obfuscation as an additional protection on visual recognition datasets without worrying about detrimental effects on the dataset's utility.Contributions.Our contributions are twofold.First, we obtain accurate face annotations in ILSVRC, facilitating subsequent research on privacy protection.We will release the code and the annotations.Second, to the best of our knowledge, we are the first to investigate the effects of privacy-aware face obfuscation on large-scale visual recognition.Through extensive experiments, we demonstrate that training on face-obfuscated images does not significantly compromise accuracy on both image classification and downstream tasks, while providing some privacy protection.Therefore, we advocate for face obfuscation to be included in ImageNet and to become a standard step in future dataset creation efforts."}
{"paper_id": 312, "abstract": "In the ever-evolving landscape of point-of-interest (POI) recommendation, the foundation lies in the intricate art of vector representation, or embedding. Yet, many existing methods stumble, relying on rudimentary discretization and mere interval analyses, ultimately neglecting the rich tapestry of spatiotemporal attributes that define each POI. Drawing inspiration from the remarkable capabilities of the mammalian brain, particularly the entorhinal-hippocampal system, we find a parallel that speaks to the heart of effective knowledge representation. Neuroscience reveals that this system crafts efficient graph representations, with entorhinal grid cells providing a succinct spatial framework and hippocampal place cells weaving together perceptions with remarkable clarity.  Emboldened by these insights, we introduce the SpatioTemporal aware Embedding framework (STE), a transformative approach tailored for POIs, aptly named STEP. This innovative framework harnesses two distinct yet complementary types of POI-specific representations: the sequential representation and the spatiotemporal conjunctive representation. These representations are deftly learned from sparse unlabeled data, guided by our novel graph-building policies. The spatiotemporal conjunctive representation, in particular, elegantly intertwines spatial and temporal dimensions, crafting a holistic view of each POI.  Moreover, we unveil a user privacy-secure method for successive POI recommendations, leveraging the power of STEP. Our experiments, conducted across two diverse datasets, reveal that STEP not only captures the nuanced spatiotemporal information of POIs with remarkable precision but also sets a new benchmark in the realm of successive POI recommendation performance. In doing so, this work not only offers a fresh perspective on spatiotemporal representation but also lays the groundwork for future advancements in related modeling tasks, illuminating a path forward in this dynamic field.", "introduction": "With the rapid growth of location-based web services like Instagram and Yelp, there has been a seismic shift in how people interact with locations around them.Through exploitation of Points-of-Interest (POIs) and their contexts, successive POI recommendation can benefit users and businesses greatly.As a core of POI information utilization, encoding POIs into vector representation space is of great significance for advanced POI analysis and downstream applications.Existing studies attempt to represent POI from different perspectives and collaborate with user preference modeling to achieve recommendation.Since consecutive check-ins are usually highly correlated, naturally, sequence modeling approaches like the Markov chain model were used to capture the check-in sequential characteristics of POIs (Ye et al., 2011;Liu et al., 2013;Zhang, 2014;Feng et al., 2015).Employing tensor factorization technique, the works (Yang et al., 2017;Wang et al., 2018) modeled target users and POIs separately by interacted features for POI recommendation.More recently, enlightened by neural networks' success, recurrent neural nets were remolded to represent POIs and user preferences implicitly (Liu et al., 2016a;Zhao et al., 2019;Zhu et al., 2017).Considering the geographical attributes of POIs, researchers have used power-law distribution, Gaussian distribution, or hierarchical tiling methods to depict the geographical influence over POI distributional features (Ye et al., 2011;Lian et al., 2014;Feng et al., 2017;Chang & Kim, 2020;Luo et al., 2020).However, geographical modeling methods above only provide single-scale or coarse-grained manually designed representations of POI geographical influences, which is deficient in capturing the POI-specific spatial features.Also, the arbitrary modeling might even lead to over-parameterization.While temporal dimension offers indeterminate auxiliary information for POI modeling, to utilize the POI temporal information within the check-ins, some works using time interval, time state variables or temporal transition vectors to promote the POI representing (Zhao et al., 2019;2016;2017;Li et al., 2018;Manotumruksa et al., 2018;Zhao et al., 2020).However, these methods focused on utilizing general temporal patterns among all POIs and failed to exploit the POI-specific visiting time patterns sufficiently.Still, the POI-specific spatiotemporal characteristics were not adequately mined and utilized.Inspirations.The entorhinal-hippocampal system plays a central role in the mammal cognition architecture.The Nobel Prize-winning neuroscience research (O'keefe & Nadel, 1978) demonstrated that entorhinal grid cells provide an effective multi-scale periodic spatial representation (Yuan et al., 2015;Banino et al., 2018;Mai et al., 2020;Dang et al., 2021).Moreover, the entorhinalhippocampal system is also critical for the non-spatial inference that relies on understanding the associations between perceptions from various perspectives (Whittington et al., 2018;Stachenfeld et al., 2018;Whittington et al., 2020).Some promising researches cast spatial and non-spatial problems as connected graphs and point out the cells inside entorhinal-hippocampal structure provide efficient conjunctive representation for those graphs (Stachenfeld et al., 2018;Gustafson & Daw, 2011).As the representation mechanism in the entorhinal-hippocampal system was extensively studied, it is widely accepted that conjunctions of representations from different aspects form the hippocampal representation for relational memory (Whittington et al., 2018;2020;Eichenbaum, 2017;MacDonald et al., 2011;Sargolini et al., 2006).For the general spatiotemporal aware embedding, various contexts can be constructed into affinity graphs for latent representation learning.Furthermore, strategies like conjunctive-representing in entorhinal-hippocampal structure can be translated to improve the quality of the representations (see Fig. 1In this paper, borrowing inspirations from the entorhinal-hippocampal system, we propose the Spa-tioTemporal aware Embedding framework, namely STE and apply to POIs (STEP) for successive POI recommendation, the model architectures are shown in Figure 1.Firstly, we build context graphs to enable unsupervised embedding learning on sparse check-ins.Secondly, we employ a sequential model to represent POIs from the check-in sequence perspective.Most importantly, we elaborate a spatiotemporal model consists of a grid-cell spatial encoder and a visiting time encoder to capture the POI-specific spatiotemporal characteristics.The spatiotemporal model learns to get the POI spatiotemporal latent representations using the spatiotemporal context graph.Finally, we implement successive POI recommendation systems based on the STEP and achieve high-performance using simple recurrent neural networks as recommenders.This work's main contributions are summarized as follows:(1) Motivated by the graph-representing strategy of structural knowledge in the entorhinalhippocampal system, we solve the spatiotemporal aware embedding learning in a graph-based unsupervised learning manner through specific context graph building policies, especially the spatiotemporal context graph, to fully exploit rich unlabeled data.(2) Inspired by the conjunctive representation mechanism in the entorhinal-hippocampal complex, we present a spatiotemporal model with a grid-cell spatial encoder and a time pattern encoder to utilize the spatiotemporal information.The conjunctive representing approach based on a unique spatiotemporal context graph addresses the problem of previous spatiotemporal modeling methods in which spatial and temporal information are isolated and represented separately.(3) We introduce a successive POI recommendation system by incorporating STEP and simple sequence predictors to show the feasibility of implementing specific applications based on the proposed STE framework.We perform experiments on large real-world datasets to demonstrate the effectiveness of STEP, and our method outperforms baselines according to experimental results.(4) Moreover, compared with classical recommendation systems, our POI-centered solution can avoid the ethical risks of artificial intelligence, like personal data leakage, as it does not need access to private information such as user preferences.Furthermore, our framework can be applied to more valuable applications like wildlife preservation and urban traffic scheduling as a general spatiotemporal aware modeling method.Figure 1: Representing mechanisms in entorhinal-hippocampal system (E-H system for short) and the framework of spatiotemporal aware embedding model.The proposed STE framework consists of three essential parts: the context graph building strategies to construct simplified affinity graphs, the spatiotemporal model to extract rich item-specific spatiotemporal features, and the sequential model to extract sequential feature embeddings.The uniqueness about our spatiotemporal information usage is that we represent item from spatiotemporal perspective (not isolated) using observations and contexts conjunctively.Unlike previous works, we do not regard all check-in records of a user as one sequence since check-ins with relatively long intervals are not very relevant.Although we assign notation to users for generality, the user information is not used in the training phase except to split sequences.We define context graphs as graphs that encode context information as affinity among POIs.Various contexts in the check-in records can be easily built into graphs G p = {V p , E p }, where V p is the set of POIs and E p is the set of edges between adjacent POIs.The edges in context graphs represent the correlation between neighboring POIs defined by geographical distance, relative position in check-in sequences, or spatiotemporal adjacent criterions.We summarize notations in this paper using Table 1.Data description.The Instagram Check-in dataset (Chang et al., 2018) was collected from Instagram in New York and the data was preprocessed in the same manner as previous works (Zhao et al., 2016;2017).The Instagram Check-in dataset has been pre-processed when it is made public, it includes 2,216,631 check-in records at 13,187 POIs of 78,233 users.Check-in sequences are sorted by timestamps, the first 70% are used as a training set and the remaining 30% for validation and testing.The Gowalla dataset is a globally-collected large-scale social media dataset (Cho et al., 2011).We eliminate users with fewer than ten check-ins and POIs accessed by fewer than ten users.Then the check-in records are sorted according to timestamps and first 70% check-ins are used for training and the remaining latest records for testing.We perform vivid data analyses in the Appendix due to the space constraints."}
{"paper_id": 313, "abstract": "In the ever-evolving landscape of machine learning, contrastive learning has emerged as a powerful ally in the realms of image classification and generation. While recent explorations have ventured into integrating contrastive learning within the discriminators of Generative Adversarial Networks (GANs), the potential of applying this technique to encoders for the purpose of learning disentangled representations remains largely uncharted. In this work, we unveil a novel and straightforward approach, which we have dubbed **ContraLORD**\u2014an acronym that hints at its purpose: Contrastive Learning for Optimizing Representational Disentanglement.  Our method begins with a generator, meticulously crafted to extract discriminative and disentangled embeddings through a process we refer to as latent optimization. From there, we introduce an encoder alongside two momentum encoders, each designed to dynamically capture disentangled information across a vast array of samples. By employing both content-level and residual-level contrastive losses, we ensure that our model learns robust representations. At the same time, we fine-tune the encoder in an amortized manner, allowing it to adapt seamlessly to the learned embeddings.  To validate the prowess of ContraLORD, we conduct rigorous evaluations across ten distinct benchmarks, focusing on representation disentanglement and linear classification performance. The results of our extensive experiments reveal that ContraLORD excels not only in learning discriminative representations but also in enhancing generative capabilities. In a world where clarity and precision are paramount, our findings illuminate a promising path forward in the quest for better representation learning.", "introduction": "In recent years, disentanglement of factors in images has attracted many researchers' attention, which mainly includes two folds: adversarial and non-adversarial methods.Adversarial methods (Mathieu et al., 2016;Denton & Birodkar, 2017;Hadad et al., 2018;Razavi et al., 2019;Gabbay & Hoshen, 2021) often apply a min-max optimization framework (Goodfellow et al., 2014) for disentanglement of images, which costs much time on hyper-parameters tuning.In terms of non-adversarial models, several variational autoencoders (Higgins et al., 2017;Kim & Mnih, 2018) variants have been proposed to disentangle the generative factors in an unsupervised manner without inductive biases, which did not achieve satisfactory results as proven in an empirical study (Locatello et al., 2019).With the extra class supervision, semi-supervised methods achieve promising performance in disentanglement.Typically, comprehensive experiments in (Locatello et al., 2020) validate the effectiveness of a limited amount of supervision in state-of-the-art unsupervised disentanglement models.LORD (Gabbay & Hoshen, 2020) applies a latent optimization framework with a noise regularizer on content embeddings to achieve superior performance over amortized inference.Based on LORD, OverLORD (Gabbay & Hoshen, 2021) is proposed to disentangle class, correlated and uncorrelated attributes for image translation.A more recent work (Gabbay et al., 2021) adopts a pre-trained CLIP (Radford et al., 2021) model to generate partial annotations for image manipulation.However, there exist two main drawbacks among these methods: 1) using different separate encoders for different factors is resource-wasteful for real-world applications and requires expensive human design.2) just learning the content embeddings inside each sample is not sufficient to learn the diversity existing in the dataset.Driven by the shortcomings discussed above, we propose a simple yet effective method named ContraLORD, where we incorporate contrastive learning into latent optimization for representation disentanglement.Recent works (Deng et al., 2020;Ojha et al., 2020) apply the contrastive learning on the discriminator of the GAN (Goodfellow et al., 2014) for disentangling representations.Typically, the 3D imitative-contrastive learning in (Deng et al., 2020) is used for controllable face image generation by comparing pairs of generated images.However, in this work, we focus on applying contrastive learning on the encoders to learn the discriminative and generative embeddings with disentangled information.Specifically, we first apply a generator to learn discriminative and generative embeddings via latent optimization.Then we apply an encoder and a momentum encoder to dynamically learn disentangled information across a large number of samples with content-level and residual-level contrastive loss.In the meanwhile, we use the learned discriminative and generative embeddings to tune the encoder in an amortized manner.We evaluate our ContraLORD on two main tasks: linear classification and disentanglement.Extensive experiments show the effectiveness of the learned discriminative embeddings on linear classification and generative embeddings on the disentanglement of factors.We conduct comprehensive studies on three benchmarks on linear classification and seven benchmarks on disentanglement to investigate if contrastive self-supervised models can learn disentangled features.In the meantime, we achieve superior performance on linear classification compared to baselines.Our ContraLORD also achieves promising results over state-of-the-art methods in terms of disentanglement.The main contributions of this work can be summarized as follows:\u2022 We present a simple yet effective method called ContraLORD by incorporating contrastive learning into latent optimization for representation disentanglement and linear classification.\u2022 We formally explore the disentangled features across a large number of samples with content-level and residual-level contrastive losses.\u2022 Extensive experiments on ten benchmarks show the effectiveness of our approach on learning disentangled representations."}
{"paper_id": 314, "abstract": "In the realm of neural networks, the concept of implicit regularization emerges as a crucial element in unraveling the mysteries of their learning processes. Recent empirical observations reveal a fascinating phenomenon: the input weights of hidden neurons, which encompass both the weight connecting the input layer to the hidden neuron and the bias term, tend to cluster along distinct orientations when initialized with small values. This clustering, or condensation, suggests that the training process inherently nudges the network toward a form with a significantly reduced effective size.  In this study, we delve into the intricacies of multilayer networks to unveil a compelling insight: during the initial stages of training, the maximum number of these condensed orientations is precisely double the multiplicity of the activation function. Here, \u201cmultiplicity\u201d refers to the number of roots the activation function possesses at the origin. Our theoretical framework aligns seamlessly with empirical findings across two pivotal scenarios: one where the activation function exhibits a multiplicity of one\u2014encompassing a variety of widely-used activation functions\u2014and another involving layers with one-dimensional inputs.  This exploration not only sheds light on the mechanisms by which small initializations drive neural networks toward condensation in their formative training phases, but it also lays a foundational stone for future inquiries into the nonlinear dynamics of neural networks and their implicit regularization effects as training progresses. In essence, we are charting a course through the uncharted territories of neural network behavior, revealing the hidden patterns that govern their evolution.", "introduction": "Over-parameterized neural networks often show good generalization performance on real-world problems by minimizing loss functions without explicit regularization (Breiman, 1995;Zhang et al., 2017).For over-parameterized NNs, there are infinite possible sets of training parameters that can reach a satisfying training loss.However, their generalization performances can be very different.It is important to study what implicit regularization is imposed aside to the loss function during the training that leads the NN to a specific type of solutions.Empirical works suggest that NNs may learn the data from simple to complex patterns (Arpit et al., 2017;Xu et al., 2019;Rahaman et al., 2019;Xu et al., 2020;Jin et al., 2020;Kalimeris et al., 2019).For example, an implicit bias of frequency principle is widely observed that NNs often learn the target function from low to high frequency (Xu et al., 2019;Rahaman et al., 2019;Xu et al., 2020), which has been utilized to understand various phenomena (Ma et al., 2020;Xu & Zhou, 2021) and inspiring algorithm design Liu et al. (2020).The NN output, either simple or complex, is a collective result of all neurons.The study of how neuron weights evolve during the training is central to understanding the collective behavior, including the complexity, of the NN output.Luo et al. (2021) establish a phase diagram to study the effect of initialization on weight evolution for two-layer ReLU NNs at the infinite-width limit and find three distinct regimes in the phase diagram, i.e., linear regime, critical regime and condensed regime.The non-linear regime, a largely unexplored non-linear regime, is named as condensed regime because the input weights of hidden neurons (the input weight or the feature of a hidden neuron consists of the weight from its input layer to the hidden neuron and its bias term) condense on isolated orientations during the training (Luo et al., 2021).The three regimes are identified based on the relative change of input weights as the width approaches infinity, which tends to 0, O(1) and +\u221e, respectively.The condensation is a feature learning process, which is important to the learning of DNNs.Note that in the following, condensation is accompanied by a default assumption of small initialization or large relative change of input weights during training.For practical networks, such as resnet18-like (He et al., 2016) in learning CIFAR10, as shown in Fig. 1(a) and Table 1, we find that Table 1: Comparison of common (Glorot & Bengio, 2010) and condensed Gaussian initializations on resnet18.m = (m in + m out )/2. m in : in-layer width.m out : out-layer width.common condensed Glorot uniform Glorot normal N (0, 1 m ) N (0, 1m ) 2 ) Test 1 0.8807 0.8777 0.8816 0.8847 0.8824 0.8826 Test 2 0.8857 0.8849 0.8806 0.8785 0.8813 0.8807 Test 3 0.8809 0.8860 0.8761 0.8824 0.8861 0.8800the performance of networks with initialization in the condensed regime is very similar to the common initialization methods.However, the condensation phenomenon provides an intuitive explanation of the good performance as follows, which may lead to a quantitative theoretical explanation in future work.The condensation transforms a large network to a network of only a few effective neurons, leading to an output function with low complexity.Since the complexity bounds the generalization error (Bartlett & Mendelson, 2002), the study of condensation could provide insight to how NNs are implicitly regularized to achieve good generalization performance in practice.For two-layer ReLU NN, Maennel et al. (2018) prove that, as the initialization of parameters goes to zero, the features of hidden neurons condense at finite number of orientations depending on the input data; when performing a linearly separable classification task with infinite data, Pellegrini & Biroli (2020) show that at mean-field limit, a two-layer infinite-width ReLU NN is effectively equal to a NN of one hidden neuron, i.e., condensation on a single orientation.Both works (Maennel et al., 2018;Pellegrini & Biroli, 2020)   In this work, we show that the condensation at the initial stage is closely related to the multiplicity p at x = 0, which means the derivative of activation at x = 0 is zero up to the (p -1)th-order and is non-zero for the p-th order.To verify their relation, we use the common activation function sigmoid(x), softplus(x), tanh(x), which are multiplicity one, and variants of tanh(x),i.e.x tanh(x) and x 2 tanh(x) with multiplicity two and three, for our experiments.For comparison, we also show the initial condensation of ReLU(x), which is studied previously (Maennel et al., 2018) and has totally different properties at origin compared with tanh(x).Our experiments suggest that the maximal number of condensed orientations is twice the multiplicity of the activation function used in general NNs.For finite-width two-layer NNs with small initialization at the initial training stage, each hidden neuron's output in a finite domain around 0 can be approximated by a p-th order polynomial and so is the NN output function.Based on the p-th order approximation, we show a preliminary theoretical support for condensation by a theoretical analysis for two cases, one is for the activation function of multiplicity one with arbitrary dimension input, which contains many common activation functions, and the other is for the layer with one-dimensional input and arbitrary multiplicity.Therefore, small initialization imposes an implicit regularization that restricts the NN to be effectively much narrower neural network at the initial training stage.As commonly used activation functions, such as tanh(x), sigmoid(x), softplus(x), etc., are all multiplicity one, our study of initial training behavior lays an important basis for the further study of implicit regularization throughout the training."}
{"paper_id": 315, "abstract": "In the realm of deep reinforcement learning, agents often find themselves ensnared by the shackles of sample inefficiency, a challenge that stifles their potential to tackle the complexities of the real world. Yet, a beacon of hope has emerged in the form of model-based methods, with the concept of learning within the imaginative confines of a world model standing out as a particularly promising strategy. However, the allure of limitless interaction with a simulated environment is tempered by a critical requirement: the world model must maintain its fidelity over extended periods.  Enter IRIS\u2014a groundbreaking agent inspired by the triumph of Transformers in sequence modeling. This innovative design incorporates a world model built from a discrete autoencoder and an autoregressive Transformer, allowing IRIS to navigate the vast expanse of its simulated universe with remarkable efficiency. In a mere two hours of gameplay within the Atari 100k benchmark, IRIS achieves a mean human normalized score of 1.046, surpassing human performance in 10 out of 26 games. This achievement not only sets a new benchmark for methods devoid of lookahead search but also illuminates a path forward for the integration of Transformers and world models in the quest for sample-efficient reinforcement learning.  To empower the next generation of researchers eager to explore this frontier, we have made our code and models publicly available at https://github.com/eloialonso/iris. Join us as we embark on this exciting journey into the future of intelligent agents.", "introduction": "Deep Reinforcement Learning (RL) has become the dominant paradigm for developing competent agents in challenging environments.Most notably, deep RL algorithms have achieved impressive performance in a multitude of arcade (Mnih et al., 2015;Schrittwieser et al., 2020;Hafner et al., 2021), real-time strategy (Vinyals et al., 2019;Berner et al., 2019), board (Silver et al., 2016;2018;Schrittwieser et al., 2020) and imperfect information (Schmid et al., 2021;Brown et al., 2020a) games.However, a common drawback of these methods is their extremely low sample efficiency.Indeed, experience requirements range from months of gameplay for DreamerV2 (Hafner et al., 2021) in Atari 2600 games (Bellemare et al., 2013b) to thousands of years for OpenAI Five in Dota2 (Berner et al., 2019).While some environments can be sped up for training agents, real-world applications often cannot.Besides, additional cost or safety considerations related to the number of environmental interactions may arise (Yampolskiy, 2018).Hence, sample efficiency is a necessary condition to bridge the gap between research and the deployment of deep RL agents in the wild.Model-based methods (Sutton & Barto, 2018) constitute a promising direction towards data efficiency.Recently, world models were leveraged in several ways: pure representation learning (Schwarzer et al., 2021), lookahead search (Schrittwieser et al., 2020;Ye et al., 2021), and learning in imagination (Ha & Schmidhuber, 2018;Kaiser et al., 2020;Hafner et al., 2020;2021).The latter approach is particularly appealing because training an agent inside a world model frees it from sample efficiency constraints.Nevertheless, this framework relies heavily on accurate world models since the policy is purely trained in imagination.In a pioneering work, Ha & Schmidhuber (2018) successfully built imagination-based agents in toy environments.SimPLe recently showed promise in the more challenging Atari 100k benchmark (Kaiser et al., 2020).Currently, the best Atari agent learning in imagination is DreamerV2 (Hafner et al., 2021), although it was developed and evaluated with two hundred million frames available, far from the sample-efficient regime.Therefore, designing new world model architectures, capable of handling visually complex and partially observable environments with few samples, is key to realize their potential as surrogate training grounds.The Transformer architecture (Vaswani et al., 2017) is now ubiquitous in Natural Language Processing (Devlin et al., 2019;Radford et al., 2019;Brown et al., 2020b;Raffel et al., 2020), and is also gaining traction in Computer Vision (Dosovitskiy et al., 2021;He et al., 2022), as well as in OfflineThe decoder D reconstructs an image x0 = D(z 0 ), from which the policy \u03c0 predicts the action a 0 .From z 0 and a 0 , G predicts the reward r0 , episode termination d0 \u2208 {0, 1}, and in an autoregressive manner \u1e911 = (\u1e91 1 1 , . . ., \u1e91K 1 ), the tokens for the next frame.A dashed box indicates image tokens for a given time step, whereas a solid box represents the input sequence of G, i.e. (z 0 , a 0 ) at t = 0, (z 0 , a 0 , \u1e911 , a 1 ) at t = 1, etc.The policy \u03c0 is purely trained with imagined trajectories, and is only deployed in the real environment to improve the world model (E, D, G).Reinforcement Learning (Janner et al., 2021;Chen et al., 2021).In particular, the GPT (Radford et al., 2018;2019;Brown et al., 2020b) family of models delivered impressive results in language understanding tasks.Similarly to world models, these attention-based models are trained with highdimensional signals and a self-supervised learning objective, thus constituting ideal candidates to simulate an environment.Transformers particularly shine when they operate over sequences of discrete tokens (Devlin et al., 2019;Brown et al., 2020b).For textual data, there are simple ways (Schuster & Nakajima, 2012;Kudo & Richardson, 2018) to build a vocabulary, but this conversion is not straightforward with images.A naive approach would consist in treating pixels as image tokens, but standard Transformer architectures scale quadratically with sequence length, making this idea computationally intractable.To address this issue, VQGAN (Esser et al., 2021) and DALL-E (Ramesh et al., 2021) employ a discrete autoencoder (Van Den Oord et al., 2017) as a mapping from raw pixels to a much smaller amount of image tokens.Combined with an autoregressive Transformer, these methods demonstrate strong unconditional and conditional image generation capabilities.Such results suggest a new approach to design world models.In the present work, we introduce IRIS (Imagination with auto-Regression over an Inner Speech), an agent trained in the imagination of a world model composed of a discrete autoencoder and an autoregressive Transformer.IRIS learns behaviors by accurately simulating millions of trajectories.Our approach casts dynamics learning as a sequence modeling problem, where an autoencoder builds a language of image tokens and a Transformer composes that language over time.With minimal tuning, IRIS outperforms a line of recent methods (Kaiser et al., 2020;Hessel et al., 2018;Laskin et al., 2020;Yarats et al., 2021;Schwarzer et al., 2021) for sample-efficient RL in the Atari 100k benchmark (Kaiser et al., 2020).After only two hours of real-time experience, it achieves a mean human normalized score of 1.046, and reaches superhuman performance on 10 out of 26 games.We describe IRIS in Section 2 and present our results in Section 3.Figure 2: Four imagined trajectories in KungFuMaster.We use the same conditioning frame across the four rows, in green, and let the world model imagine the rest.As the initial frame only contains the player, there is no information about the enemies that will come next.Consequently, the world model generates different types and numbers of opponents in each simulation.It is also able to reflect an essential game mechanic, highlighted in the blue box, where the first enemy disappears after getting hit by the player."}
{"paper_id": 316, "abstract": "In the ever-evolving landscape of neural language generation, the prevailing wisdom has long relied on maximum likelihood estimation (MLE) as the cornerstone of optimization. Yet, beneath this veneer of statistical rigor lies a deeper truth: MLE seeks to minimize the Kullback-Leibler divergence (KLD) between the model's predictions and the actual data distribution. This, however, leads to a troubling consequence\u2014the model is compelled to allocate non-zero (and often substantial) probability mass to every training sample, regardless of its inherent quality. In its quest to cover the vast, low-probability regions of the data distribution, the model inadvertently overestimates the likelihood of corrupted text sequences. We posit that this overestimation is a significant contributor to the text degeneration that plagues autoregressive decoding.  To counter this challenge, we turn to the total variation distance (TVD), a robust metric that offers resilience against outliers. By establishing practical bounds for its application in language generation, we introduce the TaiLr objective. This innovative approach artfully navigates the delicate balance of estimating TVD, employing a mechanism that downweights real data samples with low model probabilities through a tunable penalization intensity. Our experimental results reveal a promising outcome: our method mitigates the overestimation of degenerated sequences while preserving diversity, ultimately enhancing the quality of generated text across a spectrum of generation tasks. In this way, we aim to reshape the future of language generation, forging a path toward clearer and more coherent narratives.", "introduction": "The dominant approach to train language generation models is to maximize the likelihood of text samples in training data.With the development of pre-training techniques, the quality of texts generated by current models has been improved by a large margin (Radford et al., 2019;Brown et al., 2020).However, the text degeneration phenomena, e.g., repetitions (Holtzman et al., 2020;Welleck et al., 2020), incoherence (Guan et al., 2021;Ji & Huang, 2021), and other ill-formed generation results sampled from the noisy long tail (Dou et al., 2022;LeBrun et al., 2022), are still widely observed in large pre-trained models.These results indicate that using MLE as the optimizing method has theoretical limitations that are hard to be compensated by increasing the model size.Given the real data distribution p(x) and the model distribution q(x) defined by a learned generation model, we can view MLE as minimizing the KLD between p(x) and q(x).However, minimizing D KL (p, q) will lead to a zero-avoiding solution of q(x) that spreads itself to cover all the modes in the real data (Minka, 2005;Malinin & Gales, 2019).As the model is forced to take into account all the modes regardless of their quality and saliency, this behavior could deteriorate the overall generation quality when (i) the data inherently exhibits too many variations, e.g., in open-ended generation, the model often over-presents unrelated words in the unreliable long tail of its distribution (Holtzman et al., 2020).(ii) the data contains flawed or noisy references, e.g., hallucination and missing contents in text summarization (Zhao et al., 2020) degrade the generation quality of the model.In language generation, the attempt to cover all the non-zero probability regions in the data distribution would lead to a problem directly related to text degeneration, which we term as data void overestimation.Concretely, the model assigns considerably more probability mass than it should to the void of the real data distribution, where degenerated text sequences lie.An intuitive illustration is shown in Figure 1 where KLD pushes the model to place large mass on the zero-probability region of the target distribution to cover the minor mass portion on the right.These degenerated texts include random word sequences and partially corrupted texts that have high lexical overlap with the real texts.Therefore, during free-run generation, the model is likely to trap into the void regions and produce \"over-generalized\" text samples that are unlike the training data (Huszar, 2015).In this work, we start with a robust alternative to KL divergence, i.e., the total variation distance (TVD).TVD is known to be robust to outliers in the data (Beran, 1977;Knoblauch & Vomfell, 2020), as it measures the absolute difference between two probability distributions averaging at each point.In \u00a72.2, we show that TVD allows the model to place zero probability to low-quality training samples and prevent overestimation of the data void region through gradient analysis.Though appealing, TVD cannot be directly applied to text generation because (i) TVD measures the distance at the sequence level while we desire a token-level criterion for autoregressive generation models, (ii) we only have samples from the data distribution, whereas calculating TVD demands the real data probability p(x) of the training sample x.We overcome these two issues by (i) developing an upper bound on the sequence-level TVD with its token-level factorization ( \u00a73.1), and (ii) introducing a proxy distribution ( \u00a73.2) that handles the bias-variance tradeoff during estimating TVD ( \u00a73.3).Finally, we derive the Total Variation Guided Language Generation (TaiLr) objective by leveraging access to the non-zero gradient of TVD to guide the model.Intuitively, TaiLr weights the loglikelihood of a text sequence at each position according to the model probability and uses a tunable hyperparameter to control the penalization intensity.We first conduct experiments on synthetic data to show that TaiLr achieves better generation quality without sacrificing diversity and reduces the overestimation of degenerated texts compared to MLE.Further experiments on real data demonstrate that the proposed method outperforms existing methods that modify MLE at different aspects on a wide range of language generation tasks, including machine translation, text summarization, and long text generation."}
{"paper_id": 317, "abstract": "In the realm of deep reinforcement learning (RL), the quest for effective exploration stands as one of the most formidable challenges. Recent endeavors have sought to tackle this issue through the lens of population-based methods, drawing upon a diverse array of behaviors generated by a collective of exploratory policies. While adaptive policy selection has emerged as a strategy for maneuvering through this complex landscape, the scope of behavior selection remains constrained by the limitations of a predefined policy population, thereby stifling the potential for true behavioral diversity.  In this paper, we unveil a transformative framework known as Learnable Behavioral Control (LBC), designed to shatter these constraints. Our approach accomplishes two pivotal advancements: first, it expands the behavior selection space significantly by crafting a hybrid behavior mapping that synthesizes insights from all available policies; second, it establishes a cohesive, learnable process for behavior selection that adapts to the dynamic needs of the environment.  By integrating LBC into distributed off-policy actor-critic methods, we empower our agents to optimize behavior selection through the use of bandit-based meta-controllers. The results are nothing short of extraordinary: our agents have achieved an astonishing mean human normalized score of 10,077.52%, shattering 24 world records held by human players\u2014all within the span of 1 billion training frames in the Arcade Learning Environment. This remarkable performance not only sets a new standard in the field but does so without compromising sample efficiency, heralding a new era of exploration in deep reinforcement learning.", "introduction": "Reinforcement learning (RL) has led to tremendous progress in a variety of domains ranging from video games (Mnih et al., 2015) to robotics (Schulman et al., 2015;2017).However, efficient exploration remains one of the significant challenges.Recent prominent works tried to address the problem with population-based training (Jaderberg et al., 2017, PBT) wherein a population of policies with different degrees of exploration is jointly trained to keep both the long-term and shortterm exploration capabilities throughout the learning process.A set of actors is created to acquire diverse behaviors derived from the policy population (Badia et al., 2020b;a).Despite the significant improvement in the performance, these methods suffer from the aggravated high sample complexity due to the joint training on the whole population while keeping the diversity property.To acquire diverse behaviors, NGU (Badia et al., 2020b) uniformly selects policies in the population regardless of their contribution to the learning progress (Badia et al., 2020b).As an improvement, Agent57 adopts an adaptive policy selection mechanism that each behavior used for sampling is periodically selected from the population according to a meta-controller (Badia et al., 2020a).Although Agent57 achieved significantly better results on the Arcade Learning Environment (ALE) benchmark, it costs tens of billions of environment interactions as much as NGU.To handle this drawback, GDI (Fan & Xiao, 2022) adaptively combines multiple advantage functions learned from a single policy to obtain an enlarged behavior space without increasing policy population size.However, the population-based scenarios with more than one learned policy has not been widely explored yet.Taking a further step from GDI, we try to enable a larger and non-degenerate behavior space by learning different combinations across a population of different learned policies.In this paper, we attempt to further improve the sample efficiency of population-based reinforcement learning methods by taking a step towards a more challenging setting to control behaviors with significantly enlarged behavior space with a population of different learned policies.Differing from all of the existing works where each behavior is derived from a single selected learned policy, we formulate the process of getting behaviors from all learned policies as hybrid behavior mapping, and the behavior control problem is directly transformed into selecting appropriate mapping functions.By combining all policies, the behavior selection space increases exponentially along with the population size.As a special case that population size degrades to one, diverse behaviors can also be obtained by choosing different behavior mappings.This two-fold mechanism enables tremendous larger space for behavior selection.By properly parameterizing the mapping functions, our method enables a unified learnable process, and we call this general framework Learnable Behavior Control.We use the Arcade Learning Environment (ALE) to evaluate the performance of the proposed methods, which is an important testing ground that requires a broad set of skills such as perception, exploration, and control (Badia et al., 2020a).Previous works use the normalized human score to summarize the performance on ALE and claim superhuman performance (Bellemare et al., 2013).However, the human baseline is far from representative of the best human player, which greatly underestimates the ability of humanity.In this paper, we introduce a more challenging baseline, i.e., the human world records baseline (see Toromanoff et al. (2019); Hafner et al. (2021) for more information on Atari human world records).We summarize the number of games that agents can outperform the human world records (i.e., HWRB, see Figs. 1) to claim a real superhuman performance in these games, inducing a more challenging and fair comparison with human intelligence.Experimental results show that the sample efficiency of our method also outperforms the concurrent work MEME Kapturowski et al. (2022), which is 200x faster than Agent57.In summary, our contributions are as follows:1.A data-efficient RL framework named LBC.We propose a general framework called Learnable Behavior Control (LBC), which enables a significantly enlarged behavior selection space without increasing the policy population size via formulating a hybrid behavior mapping from all policies, and constructs a unified learnable process for behavior selection.A family of LBC-based RL algorithms.We provide a family of LBC-based algorithms by combining LBC with existing distributed off-policy RL algorithms, which shows the generality and scalability of the proposed method.The state-of-the-art performance with superior sample efficiency.From Figs. 1, our method has achieved 10077.52%mean human normalized score (HNS) and surpassed 24 human world records within 1B training frames in the Arcade Learning Environment (ALE), which demonstrates our state-of-the-art (SOTA) sample efficiency. 2 BACKGROUND 2.1 REINFORCEMENT LEARNING RL can be formulated as a Markov Decision Process (Howard, 1960, MDP) defined by (S, A, p, r, \u03b3, \u03c1 0 ).Considering a discounted episodic MDP, the initial state s 0 is sampled from the initial distribution \u03c1 0 (s) : S \u2192 P(S), where we use P to represent the probability distribution.At each time t, the agent chooses an action a t \u2208 A according to the policy \u03c0(a t |s t ) : S \u2192 P(A) at state s t \u2208 S. The environment receives a t , produces the reward r t \u223c r(s, a) : S \u00d7 A \u2192 R and transfers to the next state s t+1 according to the transition distribution p (s \u2032 | s, a) : S \u00d7 A \u2192 P(S).The process continues until the agent reaches a terminal state or a maximum time step.Define the discounted state visitation distribution as d \u03c0 \u03c10is the discount factor.The goal of reinforcement learning is to find the optimal policy \u03c0 * that maximizes the expected sum of discounted rewards G t :"}
{"paper_id": 318, "abstract": "In the ever-evolving realm of machine learning, we present ADCLR: **A**ccurate and **D**ense **C**ontrastive **R**epresentation **L**earning\u2014a groundbreaking self-supervised framework designed to forge precise and intricate visual representations. At its core, ADCLR harnesses the power of query patches, enabling a nuanced contrast that captures spatially-sensitive information alongside the broader strokes of global contrast.  Unlike its predecessors in the dense contrastive landscape, ADCLR boasts three significant advantages: first, it deftly balances global-discriminative and spatially-sensitive representations; second, it achieves remarkable model efficiency, requiring no additional parameters beyond those of the established global contrast baseline; and third, it operates without the need for correspondence, simplifying implementation without sacrificing performance.  Our innovative approach has set a new benchmark in the realm of contrastive methods. On classification tasks, the ViT-S model, when enhanced by ADCLR, achieves an impressive 78.1% top-1 accuracy on ImageNet through linear probing\u2014surpassing our baseline, DINO, by a noteworthy 1.1%. The ViT-B model follows suit, reaching 79.8% and 84.0% accuracy on ImageNet through linear probing and fine-tuning, respectively, again outpacing DINO by 0.6% and 0.4%.  In the domain of dense tasks, ADCLR shines on MS-COCO, delivering substantial improvements with 44.3% AP in object detection and 39.7% AP in instance segmentation, eclipsing the previous state-of-the-art method, SelfPatch, by 2.2% and 1.2%, respectively. On the ADE20K dataset, ADCLR further solidifies its superiority, outperforming SelfPatch by 1.0% mIoU and 1.2% mAcc in segmentation tasks.  In a world where every detail matters, ADCLR stands as a testament to the power of innovation and precision in the quest for superior visual representation.", "introduction": "Self-supervised representation learning (SSL) has been attracting increasing attention for deep learning, whereby a prediction problem is often formulated by a pretext task for pre-training with unlabeled data.SSL methods can mainly be divided into three categories: 1) Generative approaches (Goodfellow et al., 2014) learn to generate samples in the input space.However, generation can be computationally expensive and may not be necessary for representation learning.2) Contextual methods (Gidaris et al., 2018) design pretext tasks (denoising auto-encoders (Vincent et al., 2008), context auto encoders (Zhang et al., 2016), etc).3) Contrastive methods (Jin et al., 2022;Zhang et al., 2022;Chen et al., 2021;Caron et al., 2021) take augmented views of the same image as positive pairs and others as negative pairs.Contrastive-based methods have shown great promise e.g. in image classification/detection, video classification (Caron et al., 2021), and others (Chen et al., 2021).It has been recently shown (Chen et al., 2020a;Wang et al., 2021) that existing contrastive learning in general aims to learn global-discriminative features, which may lack spatial sensitivity (Yi et al., 2022), and it limits their ability on downstream fine-tuning tasks, especially for dense vision tasks like detection and segmentation.Consequently, object-level (Wei et al., 2021;H\u00e9naff et al., 2022) and pixel-level (Xie et al., 2021c;Wang et al., 2021) contrastive objectives and frameworks are proposed.Meanwhile, with the success of recent ViT-based visual backbones (Dosovitskiy et al., 2020;Liu et al., 2021), patch-level contrastive approaches (Yun et al., 2022) are devised, which achieve state-of-the-art performance on downstream dense tasks.However, there are mainly three disadvantages in these dense contrasting methods.i) It is hard to balance the global and patchlevel losses in the dense contrasting methods (Xie et al., 2021c;Wang et al., 2021), causing their less competitive linear/fine-tune accuracy on the global classification task.ii) Establishing the correspondence among pixels/patches usually requires bilinear interpolation, which is complex and heavily sensitive to random crop augmentation (in an extreme case, if two views have no intersection parts, there's no correspondence relation).iii) each corresponding pixel/patch need be involved in the final contrastive loss, which is time-consuming.In this paper, we propose Accurate and Dense Contrastive Representation Learning (ADCLR), which is more global-discriminative, spatial-sensitive, correspondence-free and efficient.The main contributions of ADCLR are:1) Cross-view Query-based Patch-level Contrasting Paradigm: For patch-level contrasting as recently used for dense tasks in negative-free Transformer-based SSL methods, we propose to augment two views, and perform contrasting crops from different views, for more effective learning with increased contrasting difficulty.The motivation for our cross-view design instead of the commonlyused single-view in existing patch-level contrasting is: it is non-triv and even impossible to establish patch correspondence within a single view (especially adding random resized crop augmentation).While introducing two views and replacing the correspondence establishing with more feasible query could meanwhile increase the patch appearance variance for more difficult contrasting.The above module can be introduced to existing global contrasting only SSL baselines.As shown in Fig. 1, the [CLS] tokens are used to extract global information, and the designed query patches are used to extract local information, making ADCLR both global-discriminative and spatial-sensitive.2) Robust Unidirectional Cross-Attention Scheme under the above Paradigm: The above patchlevel contrasting paradigm can technically be prone to collapse to a trivial solutionfoot_0 (see more explanation in our theoretical analysis in Sec.3.3) if we directly resort to the unidirectional selfattention scheme as used in the vanilla vision Transformers.Instead, we design unidirectional cross-attention, which takes both query patches and raw patches from raw images as input.For attention block, the data flow are: RP \u2192 RP , {RP, QP i } \u2192 QP i , QP i \u219b QP j (i \u0338 = j) where RP and QP i means raw patches (including [CLS] token) and the i-th query patch, respectively.3) Boosting baselines to new SOTA accuracy on classification and dense tasks: The proposed ADCLR can serve as a plugin based on existing Transformer-based and negative-free SSL baselines e.g.DINO (Caron et al., 2021) and iBOT (Zhou et al., 2022).Our experimental results on both linear probing, finetune classification as well as other downstream tasks show the effectiveness of ADCLR."}
{"paper_id": 319, "abstract": "In the ever-evolving realms of aerial imagery and autonomous navigation, the challenge of oriented object detection looms large. Traditional benchmarks, predominantly reliant on horizontal bounding boxes, have created a dissonance between the wealth of available training data and the burgeoning need for more nuanced detection methods. Enter H2RBox\u2014a pioneering solution that harnesses the simplicity of horizontal box annotations to empower weakly-supervised training. By bridging this critical gap, H2RBox not only rises to the occasion but also competes fiercely with models trained on more intricate rotated boxes.  At the heart of our approach lies the innovative fusion of weakly- and self-supervised learning, which enables the model to predict object angles by discerning the consistency across two distinct perspectives. To our knowledge, H2RBox stands as the first of its kind\u2014an oriented object detector forged from horizontal box annotations. Unlike conventional methods that rely heavily on instance segmentation, our strategy exhibits resilience against the pitfalls of mask prediction quality, thriving in complex environments teeming with dense objects and outliers.  Our experimental findings reveal that H2RBox not only outshines horizontal box-supervised instance segmentation techniques in both performance and speed but also boasts reduced memory demands. When placed alongside rotated box-supervised detectors, H2RBox holds its ground, delivering comparable results while maintaining swift execution. For those eager to explore this groundbreaking work, the source code is readily accessible through PyTorch-based [MMRotate](https://github.com/yangxue0827/h2rbox-mmrotate) and Jittor-based [JDet](https://github.com/yangxue0827/h2rbox-jittor).", "introduction": "In addition to the relatively matured area of horizontal object detection (Liu et al., 2020), oriented object detection has received extensive attention, especially for complex scenes, whereby fine-grained bounding box (e.g.rotated/quadrilateral bounding box) is needed, e.g.aerial images (Ding et al., 2019;Yang et al., 2019a), scene text (Zhou et al., 2017), retail scenes (Pan et al., 2020) etc.Despite the increasing popularity of oriented object detection, many existing datasets are annotated with horizontal boxes (HBox) which may not be compatible (at least on the surface) for training an oriented detector.Hence labor-intensive re-annotation 1 have been performed on existing horizontalannotated datasets.For example, DIOR-R (Cheng et al., 2022) and SKU110K-R (Pan et al., 2020) are rotated box (RBox) annotations of the aerial image dataset DIOR (192K instances) (Li et al., 2020) and the retail scene SKU110K (1,733K instances) (Goldman et al., 2019), respectively.One attractive question arises that if one can achieve weakly supervised learning for oriented object detection by only using (the more readily available) HBox annotations than RBox ones.One poten- Figure 1: Visual comparison of three HBox-supervised rotated detectors on aircraft detection (Wei et al., 2020), ship detection (Yang et al., 2018), vehicle detection (Azimi et al., 2021), etc.The HBox-Mask-RBox style methods, i.e.BoxInst-RBox (Tian et al., 2021) and BoxLevelSet-RBox (Li et al., 2022b), perform not well in complex and object-cluttered scenes.tial and verified technique in our experiments is HBox-supervised instance segmentation, concerning with BoxInst (Tian et al., 2021), BoxLevelSet (Li et al., 2022b), etc.Based on the segmentation mask by these methods, one can readily obtain the final RBox by finding its minimum circumscribed rectangle, and we term the above procedure as HBox-Mask-RBox style methods i.e.BoxInst-RBox and BoxLevelSet-RBox in this paper.Yet it in fact involves a potentially more challenging task i.e. instance segmentation whose quality can be sensitive to the background noise, and it can influence heavily on the subsequent RBox detection step, especially given complex scenes (in Fig. 1(a)) and the objects are crowded (in Fig. 1(b)).Also, involving segmentation is often more computational costive and the whole procedure can be time consuming (see Tab. 1-2).In this paper, we propose a simple yet effective approach, dubbed as HBox-to-RBox (H2RBox), which achieves close performance to those RBox annotation supervised methods e.g.(Han et al., 2021b;Yang et al., 2023a) by only using HBox annotations, and even outperforms in considerable amount of cases as shown in our experiments.The cores of our method are weakly-and selfsupervised learning, which predicts the angle of the object by learning the enforced consistency between two different views.Specifically, we predict five offsets in the regression sub-network based on FCOS (Tian et al., 2019) in the WS branch (see Fig. 2 left) so that the final decoded outputs are RBoxes.Since we only have horizontal box annotations, we use the horizontal circumscribed rectangle of the predicted RBox when computing the regression loss.Ideally, predicted RBoxes and corresponding ground truth (GT) RBoxes (unlabeled) have highly overlapping horizontal circumscribed rectangles.In the SS branch (see Fig. 2 right), we rotate the input image by a randomly angle and predict the corresponding RBox through a regression sub-network.Then, the consistency of RBoxes between the two branches, including scale consistency and spatial location consistency, are learned to eliminate the undesired cases to ensure the reliability of the WS branch.Our main contributions are as follows:1) To our best knowledge, we propose the first HBox annotation-based oriented object detector.Specifically, a weakly-and self-supervised angle learning paradigm is devised which closes the gap between HBox training and RBox testing, and it can serve as a plugin for existing detectors.2) We prove through geometric equations that the predicted RBox is the correct GT RBox under our designed pipeline and consistency loss, and does not rely on not-fully-verified/ad-hoc assumptions, e.g.color-pairwise affinity in BoxInst or additional intermediate results whose quality cannot be ensured, e.g.feature map used by many weakly supervised methods (Wang et al., 2022).3) Compared with the potential alternatives e.g.HBox-Mask-RBox whose instance segmentation part is fulfilled by the state-of-the-art BoxInst, our H2RBox outperforms by about 14% mAP (67.90% vs. 53.59%) on DOTA-v1.0 dataset, requiring only one third of its computational resources (6.25 GB vs. 19.93GB), and being around 12\u00d7 faster in inference (31.6 fps vs. 2.7 fps).4) Compared with the fully RBox annotation-supervised rotation detector FCOS, H2RBox is only 0.91% (74.40% vs. 75.31%)and 1.01% (33.15% vs. 34.16%)behind on DOTA-v1.0 and DIOR-R, respectively.Furthermore, we do not add extra computation in the inference stage, thus maintaining a comparable detection speed, about 29.1 FPS vs. 29.5 FPS on DOTA-v1.0."}
{"paper_id": 320, "abstract": "In the ever-evolving landscape of visual pretraining, we unveil a novel approach to contextual masking image modeling (MIM), which we aptly name contrasting-aided contextual MIM (ccMIM). This innovative method diverges from the traditional path of random sampling, instead employing the art of importance sampling to meticulously select masked patches that harbor richer semantic information for reconstruction. By doing so, we elevate the challenge of reconstructing the remaining less informative patches, crafting a task that not only tests the limits of our models but also enhances their learning capabilities.  To counterbalance the potential slowdown in convergence that arises from this heightened difficulty, we introduce a groundbreaking contrastive loss. This loss aligns the tokens extracted from the selected masked patches with those from the unmasked ones, acting as a guiding force that fosters patch feature learning. The goal is clear: to capture the overarching global information of the image across both masked and unmasked patches. Notably, our approach to single-view contrasting circumvents the cumbersome image augmentation steps that have become commonplace in recent adaptations of contrastive learning within the MIM framework, thus streamlining the process of enhancing convergence speed and discriminative prowess.  Moreover, the attention scores derived from the contrastive global features serve as powerful semantic guides, further refining our masking patch selection strategy. In this intricate dance of contextual MIM and contrastive learning, we create a synergistic loop\u2014where semantic patch selection and token alignment through contrast work hand in hand\u2014unlocking the dual benefits of rapid convergence and robust performance on downstream tasks. Our empirical results on the ImageNet-1K dataset for both classification and dense vision tasks validate the effectiveness of this approach, showcasing a harmonious blend of innovation and practicality in the realm of visual pretraining.", "introduction": "Self-supervised learning (SSL) (Zbontar et al., 2021;Jin et al., 2022;Chen et al., 2020b) has been attracting increasing attention recently in deep learning, due to its label-free property and capability of learning rich holistic representations.Recent SSL methods mainly fall into two classes.Contrastive methods (He et al., 2020;Chen et al., 2020b;Zhang et al., 2021) construct multiple views of the given image to increase the variance and align the representations of these views in latent space.The pair-wise learning paradigm endows strong linear evaluation accuracy of contrastive methods.However, these methods tend to extract global information but ignore local information, which limits their performance on downstream dense vision tasks, e.g.detection, segmentation, etc. Besides, the two-view learning paradigm is usually sensitive to the choice of augmentation function (Chen et al., 2020c), batch size (Chen et al., 2020b;Zhang et al., 2021) and output dimension (Zbontar et al., 2021;Zhang et al., 2022) etc.With the success of recent ViT-based vision backbones, which divide images into several patches, Mask-Image-Modeling (MIM) methods (He et al., 2021;Fang et al., 2022;Chen et al., 2022) randomly mask some patches and use the self-attention mechanism to recover pixel-wise information from the remaining un-masked patches.These MIM-based methods are shown can obtain a better pre-trained encoder even than contrastive approaches, and especially show strong performance on transfer learning and finetuning tasks thanks to the ability to capture local information.However, these methods can suffer from the limited discriminability for global representation with less competitive linear accuracy (Gao et al., 2022), slow convergence speed (1,600 epochs for MAE), and GPU-consuming (e.g. with batch size 4096).Therefore, recent attempts (Wang et al., 2022;Chen et al., 2022;Yi et al., 2022) are devoted to combining contrastive learning and MIM.The hope is that both (global) discriminative information (by contrasting) and spatial-sensitive information (by MIM) can be captured.Although these approaches show improved convergence speed (in the sense of fewer epochs needed for convergence) compared with using MIM alone, they can hardly further boost the performance of MIM on downstream tasks given even more (e.g.800, 1600) pretraining epochs.Meanwhile, such direct combing contrasting with MIM can also bring about side effects e.g. the increased sensitivity to data augmentation and more overhead at each training epoch.In this paper, we aim to improve both convergence speed and accuracy by devising a new contextual MIM scheme, which is synergistically aided by contrastive learning.The highlights of our proposed ccMIM are:1) Novel framework for synergizing MIM and contrastive learning in a close-loop: We propose a novel contextual MIM framework by actively selecting the rich semantic patches as masking patches for reconstruction to improve the MIM learning difficulty, whereby the semantic clue is learned and measured by our devised vision transformer token alignment contrasting loss.As such, the synergizing of the two components can be fulfilled in the first place.In contrast, existing efforts on the combination of MIM and contrasting often perform these two components independently until their weighted loss is finally computed and show limited accuracy improvement (perhaps also partly due to their random patch selection strategy as observed in our ablation study in Sec.4.3).2) Cost-effective technical design for contextual MIM: Under the above framework, we propose to use importance sampling to contextually select the patches with richer semantic information for masking and reconstruction, to increase the learning difficulty and effectiveness.The selection is guided by the attentional semantic score derived from the [CLS] token alignment as contrastive learning between the selected and un-selected patches.Compared to the widely adopted two-view augmentation for contrasting in recent MIM works, our contrasting is efficiently performed within a single view (for speedup MIM training).Moreover, our way of using contrasting also directly guides the MIM learning instead of being two independent components as done in the existing literature.3) Improvement over MAE: Using ViT-B (Dosovitskiy et al., 2020) as a standard protocol in MIM, ccMIM achieves 83.6%, 84.2% top-1 accuracy with 300 and 800 epochs pre-training, outperforming MAE (He et al., 2021) 0.8% (82.8% for 300 epochs) and 0.6% (83.6% for 1600 epochs) accuracy on ImageNet-1K, respectively.Source code will be made publicly available."}
{"paper_id": 321, "abstract": "In the realm of Few-shot Semantic Segmentation (FSS), a challenge looms over our understanding of feature extraction\u2014a critical aspect that has been largely overshadowed by the prevailing trend of relying on deep models pretrained on ImageNet without any subsequent fine-tuning. It is within this context that we delve into the intricacies of the FSS feature extractor, recognizing its untapped potential. Our exploration reveals a formidable adversary: heterogeneity, manifesting as the intra-class diversity found within raw images, which significantly obstructs the compactness of intra-class features.  This heterogeneity unfolds across three distinct levels, each presenting its own set of challenges:   1. **Sample-level**: The unavoidable distribution gap between support and query images creates a rift of heterogeneity between them.  2. **Region-level**: The backgrounds in FSS are not uniform; they encompass multiple regions, each imbued with its own semantics.  3. **Patch-level**: Even among neighboring patches belonging to the same class, notable differences can arise, complicating the extraction process.  In light of these revelations, we introduce a novel feature extractor, dubbed Multi-level Heterogeneity Suppressing (MuHS). This innovative approach harnesses the power of attention mechanisms within a transformer backbone to adeptly mitigate the three tiers of heterogeneity we\u2019ve identified. Specifically, MuHS enhances attention and interaction across different samples (both query and support), orchestrates cross-region interactions, and employs a groundbreaking masked image segmentation technique\u2014drawing inspiration from the latest advancements in masked image modeling.  Through rigorous empirical analysis, we demonstrate that MuHS consistently elevates performance across a variety of FSS heads. Moreover, when paired with a straightforward linear classification head, MuHS achieves unprecedented results on multiple FSS datasets, underscoring the vital role of feature learning in the Few-shot Semantic Segmentation landscape. In this way, we chart a new course, illuminating the path forward in the intricate dance between feature extraction and segmentation.", "introduction": "Few-shot semantic segmentation (FSS) aims to generalize the semantic segmentation model from base classes to novel classes, using very few support samples.FSS depicts a potential to reduce the notoriously expensive pixel-wise annotation and has thus drawn great research interest.However, we observe that the current research has been biased towards partial component of the FSS framework.Concretely, an FSS framework typically consists of a feature extractor and a matching head, while the recent state-of-the-art methods (Zhang et al. (2019); Tian et al. (2020b); Li et al. (2021a); Xie et al. (2021b); Wu et al. (2021); Zhang et al. (2021a); Li et al. (2020)) all focus on the matching head.They pay NO effort on learning the feature extractor and adopt a ImageNet-pretrained model without any fine-tuning.Under this background, we think the FSS feature extractor deserves exploration and take a rethink on the corresponding challenge.Some prior literature (Tian et al. (2020b); Zhang et al. (2021b)) argue that the challenge is mainly because the limited support samples are insufficient for finetuning a large feature extractor (e.g., ResNet-50 (He et al. (2016))), therefore leading to the overfitting problem.We hold a different perspective and observe the heterogeneity (i.e., the intra-class diversity in the raw images) as a critical challenge hindering the intra-class compactness of FSS features.Although the heterogeneity is not a unique problem in FSS (e.g., it does exist in the shows the sample-level heterogeneity between the support and the query.The \"cow\" in the support is adopted to segment \"cattle\" in the query, in spite of their different appearance.(b) shows the region-level heterogeneity in the background.When the foreground object is the \"rider\", the \"horse\" should share the same class (BG:background) with \"grass\".(c) shows the patch-level heterogeneity among neighboring patches.The color of upper and lower part of body is different.canonical segmentation as well), its challenge is significantly amplified by the few-shot setting.In our viewpoint, the heterogeneity has three levels from coarse to fine:\u2022 Sample-level heterogeneity exists between the query and support images due to their distribution gap.For example, in Fig. 1 (a), the foreground objects (\"cow\" and \"cattle\") in the support and query images look quite different, although they both belong to a same semantic class \"cow\".\u2022 Region-level heterogeneity exists (mostly) in the background, which actually contains multiple regions with different semantics.In Fig. 1 (b), \"horse\" in the image is a foreground region when the support object is another horse.However, when the support object shifts to a \"rider\", the horse in the image should be merged into the background, resulting in the region-level heterogeneity.\u2022 Patch-level heterogeneity exists among neighboring patches which belong to a same semantic class but have significant appearance variations.For example, in Fig. 1 (c), the upper and lower body of a single person are in different colors, therefore introducing patch-level heterogeneity.Motivated by these observations, we propose an FSS feature extractor with Multi-level Heterogeneity Suppressing (MuHS).MuHS adopts the transformer backbone and leverages the attention mechanism to suppress all these three-level heterogeneity.Our choice of using the transformer backbone is natural: the attention mechanism provides strong potential for constructing long-range dependencies across samples, regions and patches.Concretely, MuHS reinforces the attention / interaction between different samples (query and support), different regions and neighboring patches by constructing cross-sample attention, cross-region interaction and a novel masked image segmentation, respectively.To be more specific, these attention / interaction are as below:(i) Cross-Sample Attention.In popular transformers, the attention is within each single sample and does not cross multiple samples.In contrast, MuHS constructs cross-sample attention with a novel design of \"linking\" tokens.In each transformer layer, we use some linking tokens to connect all the patch tokens from the query and support samples simultaneously, therefore efficiently propagating information across different samples.(ii) Cross-Region Interaction.In popular transformers, the attention usually encourages feature interaction (absorption) between similar patch tokens.In contrary to this common practice, MuHS enforces additional feature absorption between patch tokens from dissimilar regions in the background.Such a cross-region interaction smooths the background and suppresses the region-level heterogeneity.(iii) Masked Image Segmentation.Inspired by the recent masked image modeling (MIM), MuHS randomly masks some patch tokens and makes partial prediction for the existing patches.Afterwards, MuHS fills trainable mask tokens and encourages the deep model to make the holistic prediction for complete patches, yielding a novel masked image segmentation.The learned capacity of inferring the semantics of the masked patches from neighboring patches suppresses the patch-level heterogeneity.In MuHS, the above three components respectively mitigate a corresponding type of heterogeneity and achieve complementary benefits for few-shot semantic segmentation.Empirically, we show that using MuHS to replace the frozen feature extractor (pretrained on ImageNet) brings consistent improvement for multiple popular FSS heads.Importantly, since the MuHS feature has relatively good intra-class compactness, we simply cooperate it with a linear classification head and achieve new state of the art on multiple FSS datasets.For example, on PASCAL-5 i , MuHS achieves 69.1% mIoU under 1-shot setting.Our main contributions are summarized as follows: First, we shift the FSS research focus from the matching head to the feature extractor and reveal the heterogeneity as an important challenge.Second.we propose Multi-level Heterogeneity Suppressing (MuHS).MuHS utilizes novel crosssample attention, cross-region interaction and masked image segmentation to suppress the heterogeneity from three levels.Third, we conduct extensive experiments to validate the effectiveness of the proposed MuHS.Experimental results confirm that MuHS is compatible to multiple FSS heads and achieves new state of the art using a simple linear classification head.Under FSS scenario, we observe three-level heterogeneity (i.e., sample-level, region-level, patchlevel), which hinders intra-class compactness of FSS features.We think the attention mechanism in the transformer provides strong potential for constructing long-range dependencies across samples, regions and patches.Therefore, the proposed MuHS adopts the transformer network as its backbone and utilizes the characteristics of transformer to suppress all these three-level heterogeneity in a unified framework."}
{"paper_id": 322, "abstract": "In the realm of deep learning, where layer-stacked architectures reign supreme, a persistent challenge looms: the struggle for interpretability. While symbolic probabilistic models shine with their clarity and transparency, the quest to fuse their strengths with the prowess of neural networks remains an uncharted frontier. In this paper, we embark on a journey to unite these seemingly disparate worlds for the purpose of text classification, employing a structured language model as our bridge.   We introduce the Symbolic-Neural model, a groundbreaking approach that learns to predict class labels for text spans directly from a constituency tree\u2014without the need for span-level gold labels. This innovative model harnesses the power of self-supervision, training solely on raw text and sentence-level labels, thus emerging as a robust, constituent-level self-interpretable classification system. Our experiments reveal that this methodology not only achieves commendable prediction accuracy in various downstream tasks but also aligns the predicted span labels with human rationales to a notable extent. In this synthesis of neural and symbolic realms, we uncover a new path forward, one that promises both clarity and performance in the intricate landscape of text classification.", "introduction": "Lack of interpretability is an intrinsic problem in deep neural networks based on layer-stacking for text classification.Many methods have been proposed to provide posthoc explanations for neural networks (Lipton, 2018;Lundberg & Lee, 2017;Sundararajan et al., 2017).However, these methods have multiple drawbacks.First, there is only word-level attribution but no high-level attribution such as those over phrases and clauses.Take sentiment analysis as an example, in addition to the ability to recognize the sentiment of sentences, an ideal interpretable model should be able to identify the sentiment and polarity reversal at the levels of words, phrases, and clauses.Secondly, as argued by Rudin (2019), models should be inherently interpretable rather than explained by a posthoc model.A widely accepted property of natural languages is that \"the meaning of a whole is a function of the meanings of the parts and of the way they are syntactically combined\" (Partee, 1995).Compared with the sequential outputs of layer-stacked model architectures, syntactic tree structures naturally capture features of various levels because each node in a tree represents a constituent span.Such a characteristic motivates us to think about whether the representations of these internal nodes could be leveraged to design an inherently constituent-level interpretable model.One challenge faced by this idea is that traditional syntactic parsers require supervised training and have degraded performance on out-of-domain data.Fortunately, with the development of structured language models (Tu et al., 2013;Maillard et al., 2017;Choi et al., 2018;Kim et al., 2019), we are now able to learn hierarchical syntactic structures in an unsupervised manner from any raw text.In this paper, we propose a general selfinterpretable text classification model that can learn to predict span-level labels unsupervisedly as shown in Figure 1.Specifically, we propose a novel label extraction framework based on a simple inductive bias for inference.During training, we maximize the probability summation of all potential trees whose extracted labels are consistent with a gold label set via dynamic programming with linear complexity.By using a structured language model as the backbone, we are able to leverage the internal representations of constituent spans as symbolic interfaces, based on which we build transition functions for the dynamic programming algorithm.The main contribution of this work is that we propose a Symbolic-Neural model, a simple but general model architecture for text classification, which has three advantages:1. Our model has both competitive prediction accuracy and self-interpretability, whose rationales are explicitly reflected on the label probabilities of each constituent.2. Our model can learn to predict span-level labels without requiring any access to span-level gold labels.3. It handles both single-label and multi-label text classification tasks in a unified way instead of transferring the latter ones into binary classification problems (Read et al., 2011) in conventional methods.To the best of our knowledge, we are the first to propose a general constituent-level self-interpretable classification model with good performance on downstream task performance.Our experiment shows that the span-level attribution is consistent with human rationales to a certain extent.We argue such characteristics of our model could be valuable in various application scenarios like data mining, NLU systems, prediction explanation, etc, and we discuss some of them in our experiments."}
{"paper_id": 323, "abstract": "In the ever-evolving landscape of visual scene understanding, the quest for 3D object detection from multiple image perspectives stands as both a fundamental challenge and a beacon of opportunity. The allure of this task lies in its promise of low-cost, high-efficiency solutions, heralding a future where multi-view 3D object detection could reshape our interaction with the world. Yet, the path is fraught with obstacles; the absence of depth information renders the accurate identification of objects through mere perspective views a formidable endeavor.   Current methodologies often lean heavily on bulky image encoders, which, while powerful, render them ill-suited for practical deployment in real-world scenarios. In contrast, LiDAR points emerge as champions of spatial awareness, offering a clarity of localization that images alone cannot provide.   In this paper, we embark on a journey to weave LiDAR-based detectors into the fabric of multi-view 3D object detection. Rather than merely constructing a depth prediction network, we take a bold step: unifying the disparate realms of image and LiDAR features within the Bird-Eye-View (BEV) space. Here, we harness the power of knowledge transfer across these non-homogeneous representations through a teacher-student paradigm.   Introducing BEVDistill, our innovative framework for cross-modal BEV knowledge distillation (KD) in the realm of multi-view 3D object detection. Through rigorous experimentation, we reveal that our approach not only surpasses existing KD methods on the formidable BEVFormer baseline but does so without incurring additional costs during the inference phase. Remarkably, our finest model achieves a groundbreaking 59.4 NDS on the nuScenes test leaderboard, setting a new benchmark against a variety of image-based detectors.   For those eager to delve deeper, our code will soon be available at https://github.com/zehuichen123/BEVDistill. Join us as we push the boundaries of what is possible in the realm of 3D object detection, forging pathways toward a future rich with possibility.", "introduction": "3D object detection, aiming at localizing objects in the 3D space, is a crucial ingredient for 3D scene understanding.It has been widely adopted in various applications, such as autonomous driving (Chen et al., 2022a;Shi et al., 2020;Wang et al., 2021b), robotic navigation (Antonello et al., 2017), and virtual reality (Schuemie et al., 2001).Recently, multi-view 3D object detection has drawn great attention thanks to its low cost and high efficiency.As images offer a discriminative appearance and rich texture with dense pixels, detectors can easily discover and categorize the objects, even in a far distance.Despite the promising deployment advantage, accurately localizing instances from camera view only is extremely difficult, mainly due to the ill-posed nature of monocular imagery.Therefore, recent approaches adopt heavy backbones (e.g., ResNet-101-DCN (He et al., 2016), VoVNetV2 (Lee & Park, 2020)) for image feature extraction, making it inapplicable for real-world applications.LiDAR points, which capture precise 3D spatial information, provide natural guidance for camerabased object detection.In light of this, recent works (Guo et al., 2021b;Peng et al., 2022) start to explore the incorporation of point clouds in 3D object detection for performance improvement.One line of work (Wang et al., 2019b) projects each points to the image to form depth map labels, and subsequently trains a depth estimator to explicitly extract the spatial information.Such a paradigm generates intermediate products, i.e., depth prediction maps, therefore introducing extra computational cost.Another line of work (Chong et al., 2021) is to leverage the teacher-student paradigm for knowledge transfer.(Chong et al., 2021) projects the LiDAR points to the image plane, constructing a 2D input for the teacher model.Since the student and teacher models are exactly structurally identical, feature mimicking can be naturally conducted under the framework.Although it solves the alignment issue across different modalities, it misses the opportunity of pursuing a strong LiDARbased teacher, which is indeed important in the knowledge distillation (KD) paradigm.Recently, UVTR (Li et al., 2022a) propose to distill the cross-modal knowledge in the voxel space, while maintaining the structure of respective detectors.However, it directly forces the 2D branch to imitate the 3D features, ignoring the divergence between different modalities.In this work, by carefully examining the non-homogenous features represented in the 2D and 3D spaces, we explore the incorporation of knowledge distillation for multi-view 3D object detection.Yet, there are two technical challenges.First, the views of images and LiDAR points are different, i.e., camera features are in the perspective view, while LiDAR features are in the 3D/bird's-eye view.Such a view discrepancy indicates that a natural one-to-one imitation (Romero et al., 2014) may not be suitable.Second, RGB images and point clouds hold respective representations in their own modalities.Therefore, it can be suboptimal to mimic features directly, which is commonly adopted in the 2D detection paradigm (Yang et al., 2022a;Zhang & Ma, 2020).We address the above challenges by designing a cross-modal BEV knowledge distillation framework, namely BEVDistill.Instead of constructing a separate depth estimation network or explicitly projecting one view into the other one, we convert all features to the BEV space, maintaining both geometric structure and semantic information.Through the shared BEV representation, features from different modalities are naturally aligned without much information loss.After that, we adaptively transfer the spatial knowledge through both dense and sparse supervision: (i) we introduce soft foreground-guided distillation for non-homogenous dense feature imitation, and (ii) a sparse style instance-wise distillation paradigm is proposed to selectively supervise the student by maximizing the mutual information.Experimental results on the competitive nuScenes dataset demonstrate the superiority and generalization of our BEVDistill.For instance, we achieve 3.4 NDS and 2.7 NDS improvements on a competitive multi-view 3D detector, BEVFormer (Li et al., 2022c), under the single-frame and multi-frame settings, respectively.Besides, we also present extensive experiments on lightweight backbones, as well as detailed ablation studies to validate the effectiveness of our method.Notably, our best model reaches 59.4 NDS on nuScenes test leaderboard, achieving new state-of-the-art results among all the published multi-view 3D detectors."}
{"paper_id": 324, "abstract": "In the realm of knowledge graph completion, where the unseen edges of a vast web of entities await illumination, probabilistic logical rule learning has emerged as a formidable ally. This approach excels in mining logical rules, deftly predicting those elusive connections by weaving together the threads of existing relationships. Yet, a significant limitation has persisted in this landscape: the focus on chain-like Horn clauses, such as R1(x; z) ^ R2(z; y) ) H(x; y), which fail to capture the rich tapestry of contextual information nestled within the neighboring sub-graphs of our entities\u2014x, y, and z. This oversight creates a chasm, for it is well-established that these local sub-graphs harbor critical insights essential for the art of knowledge graph completion.  Driven by this realization, we introduce the concept of Logical Entity RePresentation (LERP)\u2014a novel framework designed to encapsulate the contextual essence of entities within the knowledge graph. Each LERP is crafted as a vector of probabilistic logical functions that draw upon the entity\u2019s surrounding sub-graph, offering an interpretable representation that facilitates differentiable optimization. By integrating LERP into the fabric of probabilistic logical rule learning, we empower our model to forge more expressive rules, surpassing the limitations of previous methodologies.  Our empirical findings reveal a compelling narrative: with the incorporation of LERP, our model not only eclipses traditional rule learning techniques in the quest for knowledge graph completion but also stands toe-to-toe\u2014or even outshines\u2014the state-of-the-art black-box methods. Furthermore, we uncover the model\u2019s ability to unveil a richer family of logical rules, enhancing its interpretability. LERP\u2019s potential does not end here; it can be seamlessly merged with embedding learning techniques, such as TransE, to further elevate its clarity and insight. In this way, we embark on a journey to illuminate the hidden connections of knowledge graphs, transforming the abstract into the tangible, and the unknown into the understood.", "introduction": "In recent years, the use of logical formulation has become prominent in knowledge graph (KG) reasoning and completion (Teru et al., 2020;Campero et al., 2018;Payani & Fekri, 2019), mainly because a logical formulation can be used to enforce strong prior knowledge on the reasoning process.In particular, probabilistic logical rule learning methods (Sadeghian et al., 2019;Yang et al., 2017) have shown further desirable properties including efficient differentiable optimization and explainable logical reasoning process.These properties are particularly beneficial for KGs since KGs are often large in size, and modifying KGs has social impacts so rationales are preferred by human readers.Due to the large search space of logical rules, recent efforts Sadeghian et al. (2019); Yang et al. (2017); Payani & Fekri (2019) focus on learning chain-like Horn clauses of the following form:where r k represents relations and x, y z k represent entities in the graph.Even though this formulation is computationally efficient (see Section 3), it overlooks potential contextual information coming from local sub-graphs neighboring the entities (variables x, y, and all z i ).However, this kind of contextual information can be important for reasoning on knowledge graphs.Figure 1(b) shows an example.If we only know that z is mother of x and y, we are not able to infer if y is a brother or sister of x.However, in Figure 1(c), with the contextual logical information that \u2203z is son of(y, z ) we can infer that y is a male and that y should be the brother rather than the sister of x.Recent deep neural network models based on Graph Neural Networks (GNNs) (Teru et al., 2020;Mai et al., 2021) have utilized local sub-graphs as an important inductive bias in knowledge graph completion.Although GNNs can efficiently incorporate neighboring information via messagepassing mechanisms to improve prediction performance (Zhang et al., 2019;Lin et al., 2022), they are not capable of discovering explicit logical rules, and the reasoning process of GNNs is largely unexplainable.In this paper, we propose Logical Entity RePresentation (LERP) to incorporate information from local sub-graphs into probabilistic logic rule learning.LERP is a logical contextual representation for entities in knowledge graph.For an entity e, a LERP L(e) is designed as a vector of logical functions L i (e) on e's neighboring sub-graph and the enclosed relations.We then incorporate LERP in probabilistic logical rule learning methods to provide contextual information for entities.Different from other embedding learning methods, LERP encodes contextual information rather than identity information of entities.In the example discussed above in Figure 1, an ideal LERP for y might contain logical functions like L i (y) = \u2203z is son of(y, z ).Therefore, when predicting the relation is brother of, the rule learning model can select L i from LERP and get the rule written in Figure 1(c).In our model, LERP can be jointly optimized with probabilistic logical rules.We empirically show that our model outperforms previous logical rule learning methods on knowledge graph completion benchmarks.We also find that LERP allows our model to compete, and sometimes even exceed, strong black-box baselines.Moreover, LERP is itself an interpretable representation, so our model is able to discover more complex logical rules from data.In Section 5.4 we demonstrate that LERP can also be combinedd with embedding-learning models like TransE (Bordes et al., 2013) to construct a hybrid model that learns interpretable embeddings."}
{"paper_id": 325, "abstract": "In the realm of clinical diagnostics, the quest for image-based solutions that maintain their integrity across diverse centers is a formidable challenge. Our aim is clear: to isolate and capture features that are intrinsically linked to disease while navigating the complexities introduced by varying center effects. These center effects often intertwine with the very features we wish to analyze, complicating our ability to generalize findings to new, unseen domains.  To tackle this issue, we turn to the power of structural causal modeling, a method that allows us to distinctly identify and separate disease-related features from center-specific influences. Building on this foundation, we introduce an innovative framework known as the Domain Agnostic Representation Model (DarMo), which harnesses the capabilities of a variational Auto-Encoder.   At the heart of DarMo lies a dual-encoder architecture: a domain-agnostic encoder dedicated to extracting the essential disease-related features, and a domain-aware encoder designed to capture the nuances of center effects. This dual approach is further enhanced by a domain-aware batch normalization layer, ensuring that our model remains robust across various settings.  Moreover, we impose a critical constraint on the disease-related features, demanding that they not only predict disease labels but also align with clinical attributes. To achieve this, we incorporate a Graph Convolutional Network (GCN) into our decoder, enriching our model's predictive power.  The efficacy of our approach is underscored by its impressive performance on both publicly available datasets and our proprietary collections, establishing DarMo as a pioneering solution in the pursuit of reliable, image-based diagnosis across clinical landscapes.", "introduction": "A major barrier to the deployment of current deep learning systems to medical imaging diagnosis lies in their non-robustness to distributional shift between internal and external cohorts (Castro et al., 2020;Ma et al., 2022;Lu et al., 2022), which commonly exists among multiple healthcare centers (e.g., hospitals) due to differences in image acquisition protocols.For example, the image appearance can vary a lot among scanner models, parameters setting, and data preprocessing, as shown in Fig. 1  (a,b,c).Such a shift can deteriorate the performance of trained models, as manifested by a nearly 6.7% AUC drop of empirical risk minimization (ERM) method from internal cohorts (source domain, in distribution) to external cohorts (unseen domain, out of distribution), as shown in Fig. 1 (bar graph).To resolve this problem, existing studies have been proposed to learn task-related features (Castro et al., 2020;Kather et al., 2022;Wang et al., 2021b) from multiple environments of data.Although the learned representation can capture lesion-related information, it is not guaranteed that such features can be disentangled from the center effect, i.e., to variations in image distributions due to domain differences in acquisition protocols (Fang et al., 2020;Du et al., 2019;Garg et al., 2021).The mixtures of such variations lead to biases in learned features and final predictions.Therefore, a key question in robustness is: in which way can the disease-related features be disentangled from center-effect?Recently, (Sun et al., 2021) showed that the task-related features can be disentangled from others, but requires that the input X and the output Y are generated simultaneously.However, this requirement often does not satisfy disease prediction scenarios, e.g., Y can refer to ground-truth disease labels acquired from pathological examination, which can affect lesion patterns in image X.To achieve this disentanglement, we build our model in Fig. 2 (b), via structural causal modeling (SCM) that can effectively encode prior knowledge beyond data with hidden variables and causal relations.As shown, we introduce v ma and v mi to respectively denote macroscopic and microscopic parts of disease-related features that often employed in clinical diagnosis.Specifically, the macroscopic features encode morphology-related attributes (Surendiran & Vadivel, 2012) of lesion areas, as summarized in American College of Radiology (ACR) (Sickles et al., 2013); while the microscopic features are hard to observe but reflect subtle patterns of lesions.Taking the mammogram in Fig. 2 (a) as an illustration, the macroscopic features refer to the margins, shapes, and speculations of the masses; while the microscopic features refer to the textures, and the curvatures of contours (Ding et al., 2020a).As these disease-related patterns vary between malignancy and benign, they are determined by the disease status Y and we have y \u2192 (v ma , v mi ) in Fig. 2 (b) correspondingly.Besides, the v ma differs from v mi , as it is related to clinical attributes A that are easy to observe from the image.In addition to disease-related features, we also introduce v d to account for domain gaps from the center effect in the image.Note that given the image X (i.e., condition on X), the v d is correlated to (v ma , v mi ), making them entangled with each other.This entanglement can cause bias and thus unstable prediction behaviors when transferred to unseen centers/domains.Equipped with this causal modeling, we can observe that the distributional shift of data is mainly accounted for by the variation of v d across domains.Moreover, we can theoretically prove that when this variation is diverse enough, the disease-related features can be disentangled from the center effect.To the best of our knowledge, we are the first to prove that this disentanglement is possible, in the literature on imaging diagnosis.Inspired by this result, we propose a disentangling learning framework, dubbed as Domain Agnostic Representation Model (DarMo), to disentangle diseaserelated features for prediction.Specifically, we adopt a variational auto-encoder framework and decompose the encoder into domain-agnostic and domain-aware branches, which respectively encode disease-related information (v ma , v mi ) and domain effect v d .To account for the variation of v d across domains, we propose to incorporate a domain-aware batch normalization (BN) layer into the domainaware encoder, to well capture the effect in each domain.To capture disease-related information, we use disease labels to supervise (v ma , v mi ) and additionally constrain v ma to reconstruct clinical attributes with Graph Convolutional Network (GCN) to model relations among attributes.To verify the utility and effectiveness of our method, we perform our method on mammogram benign/malignant classification.Here the clinical attributes are those related to the masses, which are summarized in ACR (Sickles et al., 2013) and are easy to obtain.We consider four datasets (one public and three in-house) that are collected from different sources.The results on unseen domains show that our method can outperform others by 6.2%.Besides, our learned disease-related features can successfully encode the information on the lesion areas.In summary, our contributions are mainly three-fold: a) We leverage SCM to encode medical prior knowledge, equipped with which we theoretically show that the disease-related features can be disentangled from the domain effect; b) We propose a novel DarMO framework with domain-agnostic and domain-aware encoders, which facilitates the disentanglement of disease-related features from center effect to achieve robust prediction; c) Our model can achieve state-of-the-art performance in terms of robustness to distributional shifts across domains in breast cancer diagnosis.2018).It considers multiple domains (centers) and aims to improve the diagnosis performance in unseen domains.However, under unseen domains, previous methods will lead to a dramatic performance decrease when testing on data from a different domain with a different bias (Ilse et al., 2020;Sathitratanacheewin et al., 2020;Zhang et al., 2022a).Thus such previous models are not robust enough to the actual task (Azulay & Weiss, 2020;Cheng et al., 2022).An intuitive idea to solve domain gaps among multi-centers is learning domain-agnostic representation.Progress has been made can be roughly divided into three classes: (i) Learning the domain-specific constraints, e.g., (Chattopadhyay et al., 2020) aim to learn domain-specific masks but fails in medical images for not suitable to distinguish different domains based on masks.(ii) Disentangle-based, e.g., (Ilse et al., 2020) model three independent latent subspaces for the domain, the class, and the residual variations respectively.They do not make use of the medical attribute knowledge which is important in our mammogram classification.(iii) Design invariant constraints, e.g., (Arjovsky et al., 2019;Zhang et al., 2022b) aim to learn invariant representation across environments by minimizing the Invariant Risk Minimization term.(Ganin et al., 2016) and (Li et al., 2018) use an adversarial way with the former performing domain-adversarial training to ensure a closer match between the source and the target distributions.Lack of disentanglement and the guidance of medical prior knowledge limits their performance on unseen domains.In the following, we first introduce our causal model that incorporates medical priors regarding heterogeneous data from multiple domains in Sec.3.1.With this modeling, we show that the domain-agnostic causal features can be disentangled from domain-aware features if we can fit distributions of each domain well.Guided by this result, we in Sec.3.2 propose a variational auto-encoder (VAE)-based method as a generative model to fit these distributions, so as to learn causal features for disease prediction.decompose latent factors v of the input image x into domain-agnostic causal features (v ma , v mi ) that are determined by the disease status y, and other domain-aware features v d affected by the domain variable d.For domain-agnostic casual features, we further denote v ma as macroscopic features that generate clinical attributes v ma (such as shapes, margins (Sickles et al., 2013;Wang et al., 2021b;Zhao et al., 2022)) that are normally utilized by clinicians for disease prediction, and v mi as microscopic features (such as textures, curvatures of contours (Ding et al., 2020a)) that may be difficult to observe but can encode the high-frequency patterns of lesions.For v d , it can encode biases introduced during the imaging acquisition process from different centers/medical devices.If we directly train a predictor p(y|x) using a neural network, the extracted representation from x can entangle the causal features (v ma , v mi ) and center effects v d because conditioning on x can induce the spurious path from v d to (v ma , v mi ), making v d and (v ma , v mi ) correlated with each other.Such an entanglement makes it hard to generalize well on new centers' data.Specifically, if we denote S as the learned representation from training domains' data, then S's distribution of the diseased group can be affected by v d , which is domain-aware.Therefore, this distribution can change a lot on another domain's data, which may cause difficulty in discriminating the diseased group from the normal one in terms of S's distribution.To remove this domain dependency, it is desired to disentangle causal features from domain-aware features.Indeed, this disentanglement can be achieved via acquisition from multiple domains with diverse distributions.Specifically, the difference between (v ma , v mi ) and v d mainly lies in whether this feature is domain-invariant.The diversity among domains can thus provide a clue to identify invariant information, i.e., (v ma , v mi ) in our scenario, as shown in the following theorem: Theorem 3.1 (Informal).Suppose that multiple domains are diverse enough.Then as long as we can fit each domain's distribution well, then for each imageRemark 3.1.The diversity condition means the extent of dependency v d on d and (v ma , v mi ) on y are large enough, which can be shown to hold generically in the appendix.This theorem informs that as long as we can fit data well, we can identify each factor, particularly domain-agnostic causal features (v ma , v mi ) up to the transformation that does not depend on v d .In this regard, the learned domain-agnostic causal features are disentangled from domain effects.Guided by this analysis, we propose a variational auto-encoder (VAE)-based method, as a generative model to fit data from each center.with q \u03c8 d (v|x) learned to approximate p d \u03b8 (v|x).To optimize the loss, we need to respectively parameterize the prior models, inference models q \u03c8 d (v|x) (i.e., encoder) and generative models p \u03b8 (x|v ma , v mi , v d ), p \u03b8 (A|v ma ), p \u03b8 (y|v ma , v mi ) (i.e., decoder).In the following, we will introduce our implementation for these models.As illustrated in Fig. 3, we propose a two-branch encoder: Domain-Agnostic Encoder to extract (v ma , v mi ) and Domain-Aware Encoder to extract v d .For the latter, we incorporate a domain-aware BN to capture the variation of multiple domains.With learned causal features, we implement graph convolution network to capture relations among clinical attributes.Domain-Aware Prior Models.Following the causal graph in Fig. 2, we factorize, where the p(v ma , v mi ) can be modeled as isotropic Gaussian while p \u03b8 d (v d |d) is domain-aware, and is parameterized as a Multilayer Perceptron (MLP) with one-hot encoded vector d \u2208 R m as input.Domain-Aware/Agnostic Inference Models.To disentangle causal features (v ma , v mi ) from domain effects, we adopt a mean-field approximation to factorize, with q \u03c8 (v ma , v mi |x) and q(v d |x, d) respectively implemented via a domain-agnostic disease-relevant encoder (DADR) and a domain-aware disease-irrelevant encoder (DADI).This parameterization is inspired by the domain-invariant/-variant properties of (v ma , v mi ) and v d .By attributing the domain-aware effects to feature v d while sharing parameters of the domain-agnostic encoder \u03c8 1 for all centers, the domain-aware effects can be removed in learned macroscopic and microscopic information, leading to robust generalization ability across domains.With shared parameters of the domain-agnostic encoder, we havewhich hence does not depend on the domain index d.To reflect the variety of different domain effects, the domain-aware encoder contains a Domain-Aware Layer (DAL), which is composed of m batch-normalization (BN) layers with (\u03b3 d , \u03b2 d ) for each center:denoting the normalized features by the mini-batch mean \u00b5 B and variance \u03b4 B .Disease-Attribute Generative Models.To learn v ma , v mi , v d , we constrain them to well recover x and predict A, y, respectively via p \u03b8x (x|v), p \u03b8y (y|v ma , v mi ) and p \u03b8 A (A|v ma ).Specifically, to capture macroscopic patterns in v ma , we constrain it to estimate the clinical attributes A that include macroscopic information such as shape, margins, lobulation, etc.As correlations among clinical attributes can be helpful for disease diagnosis, we propose to reconstruct A via Graph ConvolutionalTable 1: AUC evaluation of public/in-house datasets on external cohorts (unseen domains), i.e., training and testing data are from different domains.\u20dd: domains for testing, \u2022: domains for training).(He et al., 2016) 0.822 0.758 0.735 0.779 (Chen et al., 2019) 0.877 0.827 0.804 0.830 Guided-VAE (Ding et al., 2020b) 0.872 0.811 0.779 0.811 IAIA-BL (Barnett et al., 2021) 0.861 0.803 0.767 0.782 ICADx (Kim et al., 2018) 0.882 0.802 0.777 0.826 (Li et al., 2019) 0.848 0.794 0.769 0.815 DANN (Ganin et al., 2016) 0.857 0.811 0.781 0.813 MMD-AAE (Li et al., 2018) 0.860 0.783 0.770 0.786 DIVA (Ilse et al., 2020) 0.865 0.809 0.784 0.813 IRM (Arjovsky et al., 2019) 0.889 0.830 0.795 0.829 (Chattopadhyay et al., 2020) 0.851 0.796 0.772 0.797 DDG (Zhang et al., 2022b) 0.867 0.811 0.778 0.802 EFDM (Zhang et al., 2022c) 0.864 0.812 0.765 0.796 Ours 0.948 0.874 0.858 0.892 Ours (2/3 DAL) 0.946 0.874 0.853 0.889 Ours (1/2 DAL) 0.942 0.871 0.847 0.883 Ours (1/3 DAL) 0.930 0.863 0.842 0.871 Ours (one layer DAL) 0.926 0.857 0.835 0.864 Ours (DAL -> ME) 0.946 0.873 0.855 0.891 Ours (DAL -> GL) 0.947 0.872 0.854 0.887 Network (GCN) Kipf & Welling (2016) that is flexible to capture the topological structure in the label space.More details are left in the appendix."}
{"paper_id": 326, "abstract": "In the ever-evolving realm of machine learning, self-supervised learning has emerged as a beacon of innovation, capturing the interest of researchers far and wide. This approach, which deftly sidesteps the need for labeled data, has paved the way for methods like contrastive learning\u2014an intriguing contender that has demonstrated remarkable empirical success. Yet, beneath its impressive surface lies a murky depth of theoretical understanding regarding its generalization prowess.  To illuminate this obscured territory, we introduce a novel mathematical construct: the $(\\sigma,\\delta)$-measure. This measure serves as a lens through which we can scrutinize data augmentation, allowing us to derive an upper bound on the classification error rate in downstream tasks. Our findings reveal that the generalization capability of contrastive self-supervised learning hinges on three pivotal elements: the alignment of positive samples, the divergence of class centers, and the concentration of augmented data. The first two elements are intrinsic properties of the representations we learn, while the third is dictated by the augmentation techniques we employ.  Delving deeper, we explore two foundational contrastive loss functions\u2014InfoNCE and cross-correlation\u2014demonstrating how each one adeptly addresses the first two elements. Furthermore, our empirical investigations into the third element reveal a compelling relationship: the concentration of augmented data is strongly correlated with downstream performance. Through this work, we hope to not only clarify the intricate dance of these factors but also to lay the groundwork for future explorations in the vibrant landscape of self-supervised learning.", "introduction": "Contrastive Self-Supervised Learning (SSL) has attracted great attention for its fantastic data efficiency and generalization ability in computer vision (He et al., 2020;Chen et al., 2020a;b;Grill et al., 2020;Chen & He, 2021;Zbontar et al., 2021) and natural language processing (Fang et al., 2020;Wu et al., 2020;Giorgi et al., 2020;Gao et al., 2021;Yan et al., 2021).It learns the representation through a large number of unlabeled data and manually designed supervision signals (i.e., regarding the augmented views of a data sample as positive samples).The model is updated by encouraging the features of positive samples close to each other.To overcome the feature collapse issue, various losses (e.g., InfoNCE (Chen et al., 2020a;He et al., 2020) and cross-correlation (Zbontar et al., 2021)) and training strategies (e.g., stop gradient (Grill et al., 2020;Chen & He, 2021)) are proposed.In spite of the empirical success of contrastive SSL in terms of their generalization ability on downstream tasks, the theoretical understanding is still limited.Arora et al. (2019) propose a theoretical framework to show the provable downstream performance of contrastive SSL based on the InfoNCE loss.However, their results rely on the assumption that positive samples are drawn from the same latent class, instead of the augmented views of a data point as in practice.Wang & Isola (2020) propose alignment and uniformity to explain the downstream performance, but they are empirical indicators and lack of theoretical generalization guarantees.Both of the above works avoid characterizing the important role of data augmentation, which is the key to the success of contrastive SSL, since the only human knowledge is injected via data augmentation.Recently, HaoChen et al. (2021) propose to model the augmented data as a graph and study contrastive SSL from a matrix decomposition perspective, but it is only applicable to their own spectral contrastive loss.Besides the limitations of existing contrastive SSL theories, there are also some interesting empirical observations that have not been unraveled theoretically yet.For example, why does the richer data augmentation lead to the more clustered structure in the embedding space (Figure 1) as well as the better downstream performance (also observed by Chen et al. (2020a))?Why is aligning positive samples (augmented from the \"same data point\") able to gather the samples from the \"same latent class\" into a cluster (Figure 1c)?More interestingly, decorrelating components of representation like Barlow Twins (Zbontar et al., 2021) does not directly optimize the geometry of embedding space, but it still results in the clustered structure.Why is this?In this paper, we focus on exploring the generalization ability of contrastive SSL provably, which can explain the above interesting observations.We start with understanding the role of data augmentation in contrastive SSL.Intuitively, samples from the same latent class are likely to have similar augmented views, which are mapped to the close locations in the embedding space.Since the augmented views of each sample are encouraged to be clustered in the embedding space by contrastive learning, different samples from the same latent class tend to be pulled closer.As an example, let's consider two images of dogs with different backgrounds (Figure 2).If we augment them with transformation \"crop\", we may get two similar views (dog heads), whose representations (gray points in the embedding space) are close.As the augmented views of each dog image are enforced to be close in the embedding space due to the objective of contrastive learning, the representations of two dog images (green and blue points) will be pulled closer to their augmented views (gray points).In this way, aligning positive samples is able to gather samples from the same class, and thus results in the clustered embedding space.Following the above intuition, we define the augmented distance between two samples as the minimum distance between their augmented views, and further introduce the (\u03c3, \u03b4)-augmentation to measure the concentration of augmented data, i.e., for each latent class, the proportion of samples located in a ball with diameter \u03b4 (w.r.t. the augmented distance) is larger than \u03c3.With the mathematical description of data augmentation settled, we then prove an upper bound of downstream classification error rate in Section 3. It reveals that the generalization of contrastive SSL is related to three key factors.The first one is alignment of positive samples, which is a common objective that contrastive learning algorithms aim to optimize.The second one is divergence of class centers, which prevents the collapse of representation.The third factor is concentration of augmented data, i.e., a sharper concentration of augmented data indicates a better generalization error bound.We remark that the first two factors are properties of representations that can be optimized during the learning process.However, the third factor is determined by pre-defined data augmentation and is independent of the learning process.Thus, data augmentation plays a crucial role in contrastive SSL.We then study the above three factors in more depth.In Section 4, we rigorously prove that not only the InfoNCE loss but also the cross-correlation loss (which does not directly optimize the geometry of embedding space) can satisfy the first two factors.For the third factor, we conduct various experiments on the real-world datasets and observe that the downstream performance of contrastive SSL is highly correlated to the concentration of augmented data in Section 5.In summary, our contributions include: 1) proposing a novel (\u03c3, \u03b4)-measure to quantify data augmentation; 2) presenting a theoretical framework for contrastive SSL that highlights alignment, divergence, and concentration as key factors for generalization ability; 3) provably verifying that not only the InfoNCE loss but also the cross-correlation loss satisfy alignment and divergence; 4) showing a strong correlation between downstream performance and concentration of augmented data.Algorithms of Contrastive SSL.Early works such as MoCo (He et al., 2020) and SimCLR (Chen et al., 2020a), use the InfoNCE loss to pull the positive samples close while enforcing them away from the negative samples in the embedding space.These methods require large batch sizes (Chen et al., 2020a), memory banks (He et al., 2020), or carefully designed negative sampling strategies (Hu et al., 2021).To obviate these, some recent works get rid of negative samples and prevent representation collapse by cross-correlation loss (Zbontar et al., 2021;Bardes et al., 2021) or training strategies (Grill et al., 2020;Chen & He, 2021).In this paper, we mainly study the effectiveness of the InfoNCE loss and the cross-correlation loss, and do not enter the discussion of training strategies.Theoretical Understandings of Contrastive SSL.Most theoretical analysis is based on the InfoNCE loss, and lack of understanding of recently proposed cross-correlation loss (Zbontar et al., 2021).Early works understand the InfoNCE loss based on maximizing the mutual information (MI) between positive samples (Oord et al., 2018;Bachman et al., 2019;Hjelm et al., 2018;Tian et al., 2019;2020;Tschannen et al., 2019).However, a rigorous relationship between mutual information and downstream performance has not been established.Besides, Arora et al. (2019) directly analyze the generalization of InfoNCE loss based on the assumption that positive samples are drawn from the same latent classes, which is different from practical algorithms.Ash et al. ( 2021) study the role of negative samples and show an interesting collision-coverage trade-off theoretically.HaoChen et al. ( 2021) study contrastive SSL from a matrix decomposition perspective, but it is only applicable to their spectral contrastive loss.The behavior of InfoNCE is also studied from the perspective of alignment and uniformity (Wang & Isola, 2020), sparse coding model (Wen & Li, 2021), the expansion assumption (Wei et al., 2020), stochastic neighbor embedding (Hu et al., 2022), and augmentation robustness (Zhao et al., 2023)."}
{"paper_id": 327, "abstract": "In the realm of logistics, transportation, and robotics, the Large-scale Vehicle Routing Problem (VRP) stands as a formidable challenge, akin to a vast, intricate tapestry woven from the threads of complexity and efficiency. Recent advancements have introduced data-driven heuristics capable of crafting real-time solutions for VRPs boasting up to 100 nodes. Yet, even amidst this progress, the path forward is fraught with obstacles. Three major trials loom over the horizon: the struggle to adapt heuristics learned from small-scale VRPs to their larger counterparts without the burden of retraining, the quest for real-time solutions amidst the chaos of scale, and the intricate dance of embedding global constraints into the fabric of learned heuristics.  In this paper, we rise to meet these trials head-on. We unveil the Two-stage Divide Method (TAM), a strategic approach that shifts the focus from mere node sequences to the creation of sub-route sequences, thereby enhancing our ability to generalize heuristics from smaller to larger scales in real-time. Our journey also introduces a novel two-step reinforcement learning technique, replete with innovative reward structures and padding methods, to train our TAM effectively. Furthermore, we propose a global mask function, a clever mechanism designed to ensure that global constraints remain intact as we deconstruct a large-scale VRP into manageable small-scale Traveling Salesman Problems (TSPs). This allows us to tackle these smaller challenges in parallel, accelerating our quest for solutions.  Our experiments, conducted on both synthetic and real-world large-scale VRPs, reveal a remarkable truth: our method can successfully generalize heuristics trained on datasets of VRP 100 to adeptly navigate VRPs with over 5000 nodes in real-time. Not only does this approach maintain superior solution quality compared to traditional data-driven heuristics, but it also stands toe-to-toe with established traditional methods. In the grand narrative of vehicle routing, we have forged a new path, one that promises efficiency and adaptability in the face of complexity.", "introduction": "Vehicle Routing Problems (VRPs) are widely used in logistics, supply chain, transportation, and robotic systems (Toth & Vigo, 2002b;Golden et al., 2008;Bullo et al., 2011).For instance, on e-commerce platforms, hundreds and thousands of goods are sold in real-time and then transported to customers with maximum efficiencies, minimum number of vehicles, and shortest distance.Therefore, more large-scale VRPs need to be solved in real-time to improve logistics or transportation efficiency (Dong et al., 2021;Duan et al., 2020).Although VRP is one of the most well-studied combinatorial optimization problems, the large-scale VRP is still challenging due to its NP-hard characteristic (Golden et al., 2008).Exact methods or solvers (such as branch and bound (Toth & Vigo, 2002a), branch and cut (Naddef & Rinaldi, 2002), column generation (Chabrier, 2006), Gurobi, and Cplex) could obtain global optimal solutions on small-scale VRPs with theory guarantee.However, these methods are time-consuming and hard to be extended to large-scale VRPs because permutation number is growing exponentially.Traditional heuristics or solvers could solve small-scale VRPs quickly with near-optimal solutions.Some heuristics could be extended to solve large-scale VRPs (Ortools (Perron & Furnon), LKH3 (Helsgaun, 2017), HGS (Vidal, 2022;Vidal et al., 2012), and SISRs (Christiaens & Vanden Berghe, 2020)).However, massive iterations are needed to obtain good solutions.The algorithm for solving large-scale VRP in real-time (seconds) still lags behind.which is called Decoder.Reinforcement learning techniques are also applied to train the encoderdecoder model (sequence-to-sequence model) to improve its accuracy (Nazari et al., 2018).These learn-to-construct heuristics can outperform or be comparable to traditional VRP heuristics with up to 100 nodes.However, when it comes to large-scale VRPs (over 1000 nodes), the learned heuristics still face three challenges: 1) the training of data-driven large-scale VRP model is time-consuming and computationally expensive.For instance, the computation complexity and memory space of training the Transformer are quadratic to the lengths of the input sequence (nodes number of VRP) (Kool et al., 2019;Kitaev et al., 2019); 2) the model trained on small-scale VRPs is difficult to be generalized to large-scale VRPs because the nodes distribution of large-scale VRPs in test dataset is different from that of the small-scale VRPs in the training dataset; 3) the global constraints like maximum vehicle number are hard to be encoded in the encoder-decoder model because global constraints become active only at the end of the sequence.Although the limitations of traditional and data-driven methods, we ask: Could we generalize the learned heuristics to solve large-scale VRPs in real-time by taking advantages of both data-driven and traditional methods?We try to answer this question from the following perspectives:1) Although the traditional heuristic methods are time-consuming when solving large-scale VRPs, they can quickly obtain optimal or near-optimal solutions with some theory guarantees when solving small-scale VRPs.We observe that vehicle capacity for real-world large-scale VRPs is limited, and each vehicle serves a few customers.If we know the customers that each vehicle needs to serve, then the original large-scale VRP could be divided into several small-scale TSPs, which could be solved by traditional heuristics quickly and parallelly.2) The generalization of data-driven heuristics to large-scale VRPs is difficult because the sequenceto-sequence model needs to learn the distribution of each node in the long sequence.We observe that if we just model the distribution of sub-routes and ignore the order of nodes inside a sub-route, then we could possibly better generalize the model trained on small-scale VRPs to solve large-scale VRPs.3) Although the global constraints are only active at the end of the sequence, we could design a global mask function with theory guarantee to prevent the infeasible solution beforehand.In addition, the global constraints could include some prior information, which helps improve the generalization of the learned heuristics.For instance, we observe that the predefined maximum vehicle number could provide some global information about the possible range of the optimal vehicle number in the testing dataset, which could help identify the minimum travel length.In the second stage, all small-scale sub-routes could be solved in parallel using traditional heuristics or learned heuristics.To better understand our method, we illustrate the process by splitting a VRP 13 into three smaller TSP-5s and obtaining the final route by combining the three optimized TSPs in Figure 2.Technically, we summarize our key contributions as follows:(1) We propose a new formulation and Two-stage Dividing Method for generalizing the learned heuristics trained on small-scale VRPs to solve large-scale VRPs in a real-time and zero-shot way.(2) We propose three techniques to improve the zero-shot generalization ability of our TAM: 1) Propose generating sub-route sequence to take advantages of both learned and traditional heuristics;2) Propose a two-step RL training method with new reward and padding method to accelerate training process and make the TAM invariant to node sequence of sub-route; 3) Propose a global mask function to encode maximum vehicle number constraint with theory guarantee.A proof of the mask function and a generalization analysis of TAM are provided in Section A.4 of Appendix.(3) We validate our TAM on synthetic and real-world large-scale VRPs.The results show our TAM could generalize the learned heuristics trained on VRP 100 to solve VRPs with over 1000 nodes in real-time while keeping the solution quality competitive with traditional heuristics.For VRP 2000, the solution quality of our TAM is over 50% better than the data-driven Attention Model.Example results of generalizing to VRP 2000 and real-world VRP 1040 are shown in Figure 5 of Appendix.(4) Our TAM can scale to VRPs with over 5000 nodes.For VRP 7000, the solution quality of our TAM is about 20% better than LKH3 with only 6.6% solution time.The scalability performance of our TAM (TAM-LKH3 and TAM-AM) against the size of VRP is shown in Figure 3. Example results of generalizing to VRP 7000 and 5000 are shown in Figure 7 and 8 of Appendix.As above mentioned, VRPs are usually solved by exact methods, traditional heuristics, and datadriven methods.The exact methods are time-consuming and incapable of solving large-scale VRPs in a reasonable time.Therefore, we mainly focus on the traditional heuristics and learned methods.In the following part, we summarize the related work about the learned methods.More comprehensive reviews about the traditional heuristics and learned methods could be found in Appendix A.5.Learned methods include learn-to-construct method and learn-to search method.The learn-toconstruct method constructs good VRP solutions in real-time directly without iterations.Vinyals et al. first proposed using pointer network (PN) to generate TSP solution in real-time (Vinyals et al., 2015).From there, several improvements are witnessed (Nazari et al., 2018;Kool et al., 2019;2021).The learn-to-construct ideas are also applied in other variant VRPs and get promising results (Delarue et al., 2020;Peng et al., 2020;Falkner & Schmidt-Thieme, 2020;Xin et al., 2020).However, due to the difficulty of training the model on large-scale VRPs, generating solutions for VRPs with over 400 nodes is still challenging (Fu et al., 2021;Joshi et al., 2020;Ma et al., 2019).Different from previous works, our TAM generalizes the learned heuristics to solve VRPs with over 1000 nodes in real-time by taking the advantages of traditional heuristics, data-driven heuristics, and prior information.To the best of our knowledge, our TAM is the first learn-to-construct method that could solve VRPs with over 5000 nodes in seconds with zero-shot generalization.In contrast, the learn-to-search method is mainly used in exact methods (Gasse et al., 2019) and traditional heuristics (Lu et al., 2019;Khalil et al., 2017;Chen & Tian, 2019;Hottung & Tierney, 2019;Xin et al., 2021;Chen et al., 2020), such as learning better large neighborhood search heuristics (Hottung & Tierney, 2019;Chen et al., 2020), designing better constructors, destructors, and improvement operators (Lu et al., 2019;Khalil et al., 2017) to accelerate the search process.However, the iterative search is still necessary.Decomposition technique has been used in traditional heuristics (Queiroga et al., 2021;Zhang et al., 2021;Bosman & Poutr\u00e9, 2006;Ventresca et al., 2013;Alvim & Taillard, 2013;Lalla-Ruiz & Voss, 2016;Taillard & Helsgaun, 2019).Recently, the decomposition ideas are introduced to data-driven heuristics for solving TSP and VRP (Fu et al., 2021), such as divide and conquer networks for TSP (Nowak et al., 2018) and learn-to-delegate method for VRP (Li et al., 2021).They both belong to learn-to-search methods.Different from previous works, our TAM contributes to generalize learn-to-construct heuristics in real-time, which decomposes VRP as independent sub-routes and just call sub-problem solver once in parallel.Besides, our work could easily encode other VRP constraints by changing mask functions, which is more difficult for the learn-to-search method.In the following sections, we will focus on the learn-to-construct method."}
{"paper_id": 328, "abstract": "In the ever-evolving landscape of natural language processing, the art of fine-tuning large pre-trained language models has emerged as a pivotal practice. Yet, the conventional approach\u2014adjusting every parameter of these vast models\u2014can quickly become an insurmountable challenge when faced with a multitude of downstream tasks. As a result, researchers have sought more efficient methods to incrementally update pre-trained weights, with techniques like low-rank increments gaining traction. However, these methods often apply a uniform distribution of updates across all weight matrices, neglecting the nuanced significance of individual parameters. This oversight can lead to subpar fine-tuning performance.  To address this critical shortcoming, we introduce AdaLoRA\u2014a method that intelligently allocates the parameter budget among weight matrices based on their importance scores. By parameterizing incremental updates through singular value decomposition, AdaLoRA elegantly prunes the singular values of less critical updates. This not only reduces the parameter budget but also sidesteps the computational intensity of exact SVD calculations.   We conducted comprehensive experiments across various pre-trained models in natural language processing, question answering, and natural language generation, demonstrating the robust efficacy of AdaLoRA. The results reveal a significant enhancement over existing baselines, particularly in scenarios constrained by limited budgets. For those eager to explore this innovative approach, our code is freely available at https://github.com/QingruZhang/AdaLoRA.", "introduction": "Pre-trained language models (PLMs) have manifested superior performance in various natural language processing tasks (Devlin et al., 2019;Liu et al., 2019;He et al., 2021b;Radford et al., 2019;Brown et al., 2020).The most common way to adapt pre-trained models to down-stream tasks is to fine-tune all the parameters (full fine-tuning, Qiu et al. (2020); Raffel et al. (2020)).However, pre-trained models typically incurs large memory footprint.For example, BERT model (Devlin et al., 2019) consists up to 300 million parameters; T5 (Raffel et al., 2020) comprises up to 11 billion parameters and GPT-3 (Brown et al., 2020) contains up to 175 billion parameters.When building a NLP system upon these pre-trained models, we usually handle multiple tasks that arrive simultaneously (Radford et al., 2019).Given a large number of down-stream tasks, full fine-tuning requires that each task maintains a separated copy of large models.The resulting memory consumption is prohibitively expensive.To address this issue, researchers have proposed two main lines of research to reduce the fine-tuning parameters, while maintaining or even improving the performance of PLMs.Specifically, one line of research focuses on adding small neural modules to PLMs and fine-tune only these modules for each task -the base model is kept frozen and shared across tasks.In this way, only a small number of task-specific parameters are introduced and updated, greatly enhancing the practicality of large models.For example, adapter tuning (Houlsby et al., 2019;Rebuffi et al., 2017;Pfeiffer et al., 2020; Wq W k Wv Wo Wf 1 Wf 2 88.50 88.75 89.00 89.25 89.50 89.75 90.00 MNLI Matched Acc 88.58 88.98 89.36 89.28 89.91 89.99 (a) Selected weight matrix 1,2,3 4,5,6 7,8,9 10,11,12 78 80 82 84 86 88 MNLI Matched Acc 77.87 85.82 88.15 88.6 (b) Selected layersFigure 1: Given the total trainable parameters as 0.28M, we apply LoRA only to selected weight matrices (left) or selected layers (right) of DeBERTaV3-base and compare the fine-tuning performance on MNLI-m.Figure 1a: we only fine-tune a selected type of weight matrix of every transformer layer, including query/key/value projection (Wq, W k , Wv), output projection (Wo) in the self-attention, and two weight matrices (W f 1 , W f 2 ) in two-layer FFNs.In Figure 1b, we apply LoRA to every weight matrix of the selected layers.He et al., 2022) inserts small neural modules called adapters between the layers of the base model.Prefix tuning (Li & Liang, 2021) and prompt tuning (Lester et al., 2021) attach additional trainable prefix tokens to the input or hidden layers of the base model.These methods have shown to achieve comparable performance to full fine-tuning, while only updating less than 1% of the original model parameters, significantly releasing the memory consumption.Another line of research proposes to model the incremental update of the pre-trained weights in a parameter-efficient way, without modifying the model architecture (Zaken et al., 2021;Guo et al., 2020;Hu et al., 2022).Given a pre-trained weight matrixfoot_0 W (0) , for example, diff pruning (Guo et al., 2020) models its incremental update \u2206 as a sparse matrix.Diff pruning initializes \u2206 as the same dimension as W (0) and then prunes \u2206 element-wise based on the magnitude of the entries.As such, diff pruning can increase the parameter efficiency substantially by adaptively retaining important updates and pruning unimportant ones.Nonetheless, diff pruning has several limitations.First, it relies on low-level implementation to speed up the computation of unstructured sparse matrices, which is not well supported by existing deep learning frameworks.Therefore, we have to store \u2206 as a dense matrix during training.Second, it needs to update every entry of \u2206 with their gradients and then prune them.This results in similar computational cost as full fine-tuning (Guo et al., 2020).To overcome these drawbacks, Hu et al. (2022) propose a method named LoRA, which parameterizes \u2206 as a low-rank matrix by the product of two much smaller matrices:whereDuring fine-tuning, only A and B are updated.The rank r is chosen to be much smaller than the dimension of W (e.g., r = 8 when d 1 = d 2 = 1024).With less than 0.5% additional trainable parameters, the training overhead can be reduced up to 70%, compared to full fine-tuning.However, LoRA achieves comparable or even better performance than full fine-tuning (Hu et al., 2022).Meanwhile, the product of two samll matrices is more friendly to implement and deploy than unstructured sparse matrices in diff pruning.LoRA still has limitations as it prespecifies the rank r of each incremental matrix \u2206 identical.This ignores the fact that the importance of weight matrices varies significantly across modules and layers when fine-tuning pre-trained models.To illustrate this point, we present an concrete example in Figure 1.We compare the performance of LoRA when fine-tuning specific modules or layers with the same number of trainable parameters.Figure 1a shows that fine-tuning feed-forward networks (FFN) achieves better performance than self-attention modules.In addition, Figure 1b demonstrates that weight matrices in top layers are more important than those in bottom layers.Adding more trainable parameters to the critical weight matrices can lead to better model performance.In contrast, adding more parameters to those less important weight matrices yields very marginal gains or even hurt model performance.Given the parameter budget, i.e., the number of total trainable parameters, we always prefer to allocate more parameters to those important modules.Distributing the budget evenly to all weight matrices/layers, like LoRA and other methods (e.g., adapter and prefix tuning), often gives suboptimal performance.To this end, a natural question is:How can we allocate the parameter budget adaptively according to importance of modules to improve the performance of parameter-efficient fine-tuning?To answer this question, we propose a new method -AdaLoRA (Adaptive Low-Rank Adaptation), which dynamically allocates the parameter budget among weight matrices during LoRA-alike finetuning.Specifically, AdaLoRA adjusts the rank of incremental matrices to control their budget.Critical incremental matrices are assigned with high rank such that they can capture more fine-grained and task-specific information.Less importance ones are pruned to have lower rank to prevent overfitting and save the computational budget.There are some methods to control the rank of matrices in the existing literature of matrix approximation (Cai et al., 2010;Koltchinskii et al., 2011;Toh & Yun, 2010).Most of them directly compute singular value decomposition (SVD) of a matrix and then truncate the smallest singular values.Such an operation can manipulate the rank explicitly and, more importantly, minimize the difference between the resulting matrix and the original matrix.However, for fine-tuning large models, it becomes prohibitively expensive to iteratively apply SVD for a large number of high-dimensional weight matrices.Therefore, instead of computing SVD exactly, we parameterize \u2206 as \u2206 = P \u039bQ to mimic SVD.The diagonal matrix \u039b contains singular values while the orthogonal matrices P and Q represent left/right singular vectors of \u2206.To regularize the orthogonality of P and Q, an additional penalty is added to training loss.Such a parameterization avoids the intensive computations of SVD.Besides, another advantage is that we only need to drop the unimportant singular values while the singular vectors are maintained.This preserves the possibility of future recovery and stabilizes the training.See a detailed comparison to LoRA in Section 3.Based on our SVD parameterization, AdaLoRA dynamically adjusts the rank of \u2206 = P V Q by importance scoring.Specifically, we divide the incremental matrix P \u039bQ into triplets, where each triplet G i contains the i-th singular value and the corresponding singular vectors.To quantify the importance of triplets, we propose a novel importance metric, which takes account of the contribution of every entry in G i to the model performance (Sanh et al., 2020;Liang et al., 2021;Zhang et al., 2022).Triplets with low importance scores are granted low priority and hence the singular values are zeroed out.Triplets with high importance are retained for fine-tuning.Moreover, we also propose a global budget scheduler to facilitate the training.In particular, we start from an initial parameter budget, which is slightly higher than the final budget, and then gradually reduce it until matching the target.Such a scheduler can improve the training stability and model performance.Please see Section 3 for a detailed description of our importance metric and budget scheduler.We conduct extensive experiments on a wide range of tasks and models to demonstrate the effectiveness of AdaLoRA.Specifically, we evaluate the performance using DeBERTaV3-base (He et al., 2021a) on natural language understanding (GLUE, Wang et al. (2019)) and question answering (SQuADv1, Rajpurkar et al. (2016) and SQuADv2, Rajpurkar et al. (2018)) datasets.We also apply our methods to BART-large (Lewis et al., 2019) and evaluate the performance on natural language generation (XSum, Narayan et al. (2018) and CNN/DailyMail, Hermann et al. (2015)) tasks.We show AdaLoRA consistently outperforms the baseline, especially under low budget settings.For example, with less than 0.1% trainable parameters of full fine-tuning, AdaLoRA achieves a 1.2% F1 improvement on the SQuAD2.0dataset compared with state-of-the-art approaches."}
{"paper_id": 329, "abstract": "In the realm of speech recognition, a formidable adversary lurks: the chaos of noise. When faced with such tumult, the performance of acoustic models falters, leaving automatic speech recognition (ASR) systems vulnerable. Enter speech enhancement (SE), a powerful ally that can serve as a front-line defense. Yet, the current training objectives of SE methods often fall short, failing to weave together the intricate tapestry of speech-text and noise-clean paired data necessary for training against the unpredictable nature of unseen ASR systems.  In this endeavor, we unveil a novel denoising framework, D4AM, crafted to bolster a variety of downstream acoustic models. Our approach fine-tunes the SE model through a backward gradient, aligning it with the specific requirements of a given acoustic model and its classification objective. But we do not stop there; we introduce an auxiliary regression objective, designed to empower the SE model with the versatility to adapt to uncharted acoustic territories.  To achieve this synergy of SE with both regression and classification objectives, D4AM employs a unique adjustment scheme. This innovative mechanism directly estimates optimal weighting coefficients, sparing us the burden of exhaustive grid searches that often inflate training costs. Our scheme comprises two pivotal components: gradient calibration and regression objective weighting, working in concert to refine our model's performance.  The results speak volumes. D4AM consistently delivers enhancements across a spectrum of unseen acoustic models, outshining other combination strategies. Notably, when tested against the Google ASR API using real-world noisy data that remained untouched during SE training, D4AM achieved a remarkable 24.65% reduction in word error rate (WER) compared to the unrefined input. To our knowledge, this represents the first effective fusion of regression (denoising) and classification (ASR) objectives, paving the way for a truly universal pre-processor adaptable to various unseen ASR systems. For those eager to explore this groundbreaking work, our code awaits at https://github.com/ChangLee0903/D4AM.", "introduction": "Speech enhancement (SE) aims to extract speech components from distorted speech signals to obtain enhanced signals with better properties (Loizou, 2013).Recently, various deep learning models (Wang et al., 2020;Lu et al., 2013;Xu et al., 2015;Zheng et al., 2021;Nikzad et al., 2020) have been used to formulate mapping functions for SE, which treat SE as a regression task trained with noisy-clean paired speech data.Typically, the objective function is formulated using a signal-level distance measure (e.g., L1 norm (Pandey & Wang, 2018;Yue et al., 2022), L2 norm (Ephraim & Malah, 1984;Yin et al., 2020;Xu et al., 2020), SI-SDR (Le Roux et al., 2019;Wisdom et al., 2020;Lee et al., 2020), or multiple-resolution loss (D\u00e9fossez et al., 2020)).In speech-related applications, SE units are generally used as key pre-processors to improve the performance of the main task in noisy environments.To facilitate better performance on the main task, certain studies focus on deriving suitable objective functions for SE training.For human-human oral communication tasks, SE aims to improve speech quality and intelligibility, and enhancement performance is usually assessed by subjective listening tests.Because largescale listening tests are generally prohibitive, objective evaluation metrics have been developed to objectively assess human perception of a given speech signal (Rix et al., 2001;Taal et al., 2010;Jensen & Taal, 2016;Reddy et al., 2021).Perceptual evaluation of speech quality (PESQ) (Rix et al., 2001) and short-time objective intelligibility (STOI) (Taal et al., 2010;Jensen & Taal, 2016) are popular objective metrics designed to measure speech quality and intelligibility, respectively.Recently, DNSMOS (Reddy et al., 2021) has been developed as a non-instructive assessment tool that predicts human ratings (MOS scores) of speech signals.In order to obtain speech signals with improved speech quality and intelligibility, many SE approaches attempt to formulate objective functions for SE training directly according to speech assessment metrics (Fu et al., 2018;Fu et al., 2019;2021).Another group of approaches, such as deep feature loss (Germain et al., 2019) and HiFi-GAN (Su et al., 2020), propose to perform SE by mapping learned noisy latent features to clean ones.The experimental results show that the deep feature loss can enable the enhanced speech signal to attain higher human perception scores compared with the conventional L1 and L2 distances.Another prominent application of SE is to improve automatic speech recognition (ASR) in noise (Seltzer et al., 2013;Weninger et al., 2015b;Li et al., 2014;Cui et al., 2021).ASR systems perform sequential classification, mapping speech utterances to sequences of tokens.Therefore, the predictions of ASR systems highly depend on the overall structure of the input utterance.When regard to noisy signals, ASR performance will degrade significantly because noise interference corrupts the content information of the structure.Without modifying the ASR model, SE models can be trained separately and \"universally\" used as a pre-processor for ASR to improve recognition accuracy.Several studies have investigated the effectiveness of SE's model architecture and objective function in improving the performance of ASR in noise (Geiger et al., 2014a;Wang et al., 2020;Zhang et al., 2020;Chao et al., 2021;Meng et al., 2017;Weninger et al., 2015a;Du et al., 2019;Kinoshita et al., 2020;Meng et al., 2018).The results show that certain specific designs, including model architecture and input format, are favorable for improving ASR performance.However, it has also been reported that improved recognition accuracy in noise is not always guaranteed when the ASR objective is not considered in SE training (Geiger et al., 2014b).A feasible approach to tune the SE model parameters toward the main ASR task is to prepare the data of (noisy) speech-text pairs and backpropagate gradients on the SE model according to the classification objective provided by the ASR model.That is, SE models can be trained on a regression objective (using noisy-clean paired speech data) or/and a classification objective (using speech-text paired data).Ochiai et al. (2017a;b) proposed a multichannel end-to-end (E2E) ASR framework, where a mask-based MVDR (minimum variance distortionless response) neural beamformer is estimated based on the classification objective.Experimental results on CHiME-4 (Jon et al., 2017) confirm that the estimated neural beamformer can achieve significant ASR improvements under noisy conditions.Meanwhile, Chen et al. (2015) and Ma et al. (2021) proposed to train SE units by considering both regression and classification objectives, and certain works (Chen et al., 2015;Ochiai et al., 2017a;b) proposed to train SE models with E2E-ASR classification objectives.A common way to combine regression and classification objectives is to use weighting coefficients to combine them into a joint objective for SE model training.Notwithstanding promising results, the use of combined objectives in SE training has two limitations.First, how to effectively combine regression and classification objectives remains an issue.A largescale grid search is often employed to determine optimal weights for regression and classification objectives, which requires exhaustive computational costs.Second, ASR models are often provided by third parties and may not be accessible when training SE models.Moreover, due to various training settings in the acoustic model, such as label encoding schemes (e.g., word-piece (Schuster & Nakajima, 2012), byte-pair-encoding (BPE) (Gage, 1994;Sennrich et al., 2016), and character), model architectures (e.g., RNN (Chiu et al., 2018;Rao et al., 2017;He et al., 2019;Sainath et al., 2020), transformer (Vaswani et al., 2017;Zhang et al., 2020), and conformer (Gulati et al., 2020)), and objectives (e.g., Connectionist Temporal Classification (CTC) (Graves et al., 2006), Attention (NLL) (Chan et al., 2016), and their hybrid version (Watanabe et al., 2017)), SE units trained according to a specific acoustic model may not generalize well to other ASR systems.Based on the above limitations, we raise the question: Can we effectively integrate speech-text and noisy-clean paired data to develop a denoising pre-processor that generalizes well to unseen ASR systems?In this work, we derive a novel denoising framework, called D4AM, to be used as a \"universal\" pre-processor to improve the performance of various downstream acoustic models in noise.To achieve this goal, the proposed framework focuses on preserving the integrity of clean speech signals and trains SE models with regression and classification objectives jointly.By using the regression objective as an auxiliary loss, we circumvent the need to require additional training costs to grid search the appropriate weighting coefficients for the regression and classification objectives.Instead, D4AM applies an adjustment scheme to determine the appropriate weighting coefficients automatically and efficiently.The adjustment scheme is inspired by the following concepts: (1) we attempt to adjust the gradient yielded by a proxy ASR model so that the SE unit can be trained to improve the general recognition capability; (2) we consider the weighted regression objective as a regularizer and, thereby, prevent over-fitting while training the SE unit.For (1), we derive a coefficient \u03b1 gclb (abbreviation for gradient calibration) according to whether the classification gradient conflicts with the regression gradient.When the inner product of the two gradient sets is negative, \u03b1 gclb is the projection of the classification gradient on the regression gradient; otherwise, it is set to 0. For (2), we derive a coefficient \u03b1 srpr (abbreviation for surrogate prior) based on an auxiliary learning method called ARML (auxiliary task reweighting for minimum-data learning) (Shi et al., 2020) and formulate the parameter distribution induced by the weighted regression objective as the surrogate prior when training the SE model.From the experimental results on two standard speech datasets, we first notice that by properly combining regression and classification objectives, D4AM can effectively improve the recognition accuracy of various unseen ASR systems and outperform the SE models trained only with the classification objective.Next, by considering the regression objective as an auxiliary loss, D4AM can be trained efficiently to prevent over-fitting even with limited speech-text paired data.Finally, D4AM mostly outperforms grid search.The main contribution of this study is two-fold: (1) to the best of our knowledge, this is the first work that derives a general denoising pre-processor applicable to various unseen ASR systems; (2) we deploy a rational coefficient adjustment scheme for the combination strategy and link it to the motivation for better generalization ability."}
{"paper_id": 330, "abstract": "In the realm of high-dimensional data visualization, two titans stand tall: $t$-SNE and UMAP. Though they arise from disparate origins and wield seemingly unrelated loss functions, their impact on the interpretation of data is profound and, at times, conflicting. This divergence in embeddings has left many puzzled, as the underlying reasons for their differences remain shrouded in mystery.   In this exploration, we embark on a journey to illuminate the conceptual bridge that connects these two contrasting methods through the lens of contrastive learning. By employing noise-contrastive estimation, we reveal how it can be harnessed to optimize $t$-SNE, while UMAP draws strength from the principles of negative sampling\u2014another facet of contrastive methodology. Our investigation uncovers the intricate relationship between these techniques, culminating in a mathematical characterization of the distortion introduced by negative sampling.   Visually, this distortion manifests in UMAP's ability to forge more compact embeddings with tighter clusters, a stark contrast to the broader strokes painted by $t$-SNE. Yet, we do not merely stop at understanding; we leverage this newfound connection to propose a novel generalization of negative sampling. This innovation allows us to traverse a spectrum between $t$-SNE and UMAP, enabling interpolation and even extrapolation of their respective embeddings. As we navigate this continuum, we strike a delicate balance between discrete local structures and continuous global patterns, thus mitigating the temptation to over-interpret the nuances of any single embedding.  To aid fellow explorers in this endeavor, we provide a comprehensive PyTorch implementation, empowering researchers to delve into the rich tapestry of high-dimensional data with renewed clarity and insight.", "introduction": "Low-dimensional visualization of high-dimensional data is a ubiquitous step in exploratory data analysis, and the toolbox of visualization methods has been rapidly growing in the last years (McInnes et al., 2018;Amid & Warmuth, 2019;Szubert et al., 2019;Wang et al., 2021).Since all of these methods necessarily distort the true data layout (Chari et al., 2021), it is beneficial to have various tools at one's disposal.But only equipped with a theoretic understanding of the aims of and relationships between different methods, can practitioners make informed decisions about which visualization to use for which purpose and how to interpret the results.The state of the art for non-parametric, non-linear dimensionality reduction relies on the neighbor embedding framework (Hinton & Roweis, 2002).Its two most popular examples are t-SNE (van der Maaten & Hinton, 2008;van der Maaten, 2014) and UMAP (McInnes et al., 2018).Both can produce insightful, but qualitatively distinct embeddings.However, why their embeddings are different and what exactly is the conceptual relation between their loss functions, has remained elusive.Here, we answer this question and thus explain the mathematical underpinnings of the relationship between t-SNE and UMAP.Our conceptual insight naturally suggests a spectrum of embedding methods complementary to that of B\u00f6hm et al. (2022), along which the focus of the visualization shifts from local to global structure (Fig. 1).On this spectrum, UMAP and t-SNE are simply two instances and inspecting various embeddings helps to guard against over-interpretation of apparent structure.As a practical corollary, our analysis identifies and remedies an instability in UMAP.As Z increases, the scale of the embedding decreases, clusters become more compact and separated before eventually starting to merge.The Neg-t-SNE spectrum produces embeddings very similar to those of (f) t-SNE, (g) NCVis, and (h) UMAP, when Z equals the partition function of t-SNE, the learned normalization parameter Z of NCVis, or |X|/m = n 2 /m used by UMAP, as predicted in Sec.4-6.(i) The partition function ij (1+d 2 ij ) -1 tries to match Z and grows with it.Here, we initialized all Neg-t-SNE runs using Z = |X|/m; without this 'early exaggeration', low values of Z yield fragmented clusters (Fig. S11).We provide the new connection between t-SNE and UMAP via a deeper understanding of contrastive learning methods.Noise-contrastive estimation (NCE) (Gutmann & Hyv\u00e4rinen, 2010;2012) can be used to optimize t-SNE (Artemenkov & Panov, 2020), while UMAP relies on another contrastive method, negative sampling (NEG) (Mikolov et al., 2013).We investigate the discrepancy between NCE and NEG, show that NEG introduces a distortion, and this distortion explains how UMAP and t-SNE embeddings differ.Finally, we discuss the relationship between neighbor embeddings and self-supervised learning (Wu et al., 2018;He et al., 2020;Chen et al., 2020;Le-Khac et al., 2020).In summary, our contributions are 1. a new connection between the contrastive methods NCE and NEG (Sec.4), 2. the exact relation of t-SNE and UMAP and a remedy for an instability in UMAP (Sec.6), 3. a spectrum of 'contrastive' neighbor embeddings encompassing UMAP and t-SNE (Sec.5), 4. a connection between neighbor embeddings and self-supervised learning (Sec.7), 5. a unified PyTorch framework for contrastive (non-)parametric neighbor embedding methods.Our code is available at https://github.com/berenslab/contrastive-neand https://github.com/hci-unihd/cl-tsne-umap."}
{"paper_id": 331, "abstract": "In the realm of dynamical systems, where agents interact in a delicate dance of relationships, the underlying structure is often represented as a graph\u2014a tapestry woven from the threads of connection between its constituents. Yet, in the ever-complex world we inhabit, these interactions are seldom as simple as binary choices. Recent endeavors have sought to unveil the hidden relationships among these agents through the lens of deep neural networks, but they often fall short, relying on discrete interactions that fail to capture the nuanced strengths of real-world connections.  Enter the Relational Attentive Inference Network, or RAIN, a breakthrough that dares to tread where others have hesitated. Our model transcends simplistic assumptions, inferring continuously weighted interaction graphs without the need for predetermined interaction strengths. At its heart lies a groundbreaking pairwise attention mechanism, which meticulously refines the representations of trajectories, while a graph transformer deftly extracts the heterogeneous interaction weights that define each pair of agents.  Through rigorous experimentation, we reveal that RAIN, empowered by its innovative PA mechanism, can accurately discern the continuous interaction strengths within simulated physical systems\u2014all achieved in an unsupervised manner. Moreover, RAIN showcases its prowess in predicting trajectories derived from motion capture data, unveiling an interpretable interaction graph that illuminates the intricate dynamics at play. In doing so, we demonstrate the profound potential of modeling unknown dynamics with continuous weights, paving the way for a deeper understanding of the interconnected systems that govern our universe.", "introduction": "Dynamical systems with interactions provide a fundamental model for a myriad of academic fields, yet finding out the form and strength of interactions remains an open problem due to its inherent degeneracy and complexity.Although it is crucial to identify the interaction graph of a complex system for understanding its dynamics, disentangling individual interactions from trajectory data without any ground-truth labels is a notoriously hard inverse problem.Further, if the interactions are heterogeneous and coupled with continuous strength constants, the interaction graph is called weighted and the inference became much harder with increased level of degeneracies.In this work, we assume the dynamical system with N objects (or agents), and their (discretized) trajectories x 1 , x 2 , . . ., x N from timestep t = 0 to T are given.If the system has an interaction kernel Q(x i , x j ) and the dynamics are governed by a form of \u1e8bi = j =i k ij Q(x i , x j ) with some variable k ij , which is prevalent in nature and physical system, we call k ij as an interaction strength between the object i and j.With proper normalization, we can always regard 0 \u2264 k ij \u2264 1.In general, k ij may have continuous values and forms a weighted interaction graph, which can be expressed in the form of a connectivity matrix K; a conventional adjacency matrix with continuous-valued entries of k ij .Hence, the problem is inferring continuous adjacency matrix K from trajectories x alone.In the current work, we propose a neural network called Relational Attentive Inference Network (RAIN) to address the problem of inferring weighted interaction graphs from multivariate trajectory data in an unsupervised manner.RAIN infers the interaction strength between two agents from previous trajectories by learning the attentive weight while simultaneously learning the unknown dynamics of the system and thus is able to precisely predict the future trajectories.Our model employs the attention mechanism twice: once for the construction of pairwise trajectory embedding The decoder module finally predicts the future trajectories of each agent with an LSTM decoder, but here, each prediction can only employ the weighted information from other agents.This restriction on information induces the attention weights in the learning process to properly reflect the strengths of the connections.and once for the actual graph weight extraction.Differing from previous approaches such as the graph attention network (GAT) Veli\u010dkovi\u0107 et al. (2017), RAIN aims to infer the absolute interaction strength that governs the system dynamics by employing attention module with multilayer perceptron (MLP) and sigmoid activation.By comparing the inferred interaction strengths of simulated physical systems with ground-truth values that are not provided at the training stage, we verify that RAIN is capable of inferring both system dynamics and weighted interaction graphs solely from multivariate data.We further show that RAIN outperforms discrete baselines on real-world motion capture data, representing a system in which we cannot be certain whether a continuous form of interaction strengths even exists.In this way, we demonstrate that the rich flexibility and expressibility of the continuous modeling of interaction strengths are crucial for the accurate prediction of the future dynamics of an unknown empirical system."}
{"paper_id": 332, "abstract": "In the intricate tapestry of monocular scene reconstruction, the challenge of rendering complex environments from posed images looms large. Recent advancements in volumetric methods have made strides, learning to predict the Truncated Signed Distance Function (TSDF) volume with remarkable efficacy. Yet, a significant gap remains: while many methods excel at extracting and fusing 2D features into a 3D feature volume, few have ventured to enhance the aggregation of that 3D volume itself.   In this pursuit, we introduce the SDF Transformer Network, a novel architecture that supplants traditional 3D CNNs to achieve superior 3D feature aggregation. To address the daunting computational demands of 3D multi-head attention, we unveil a sparse window attention module, which confines attention calculations to non-empty voxels within localized windows, thus streamlining the process. Building upon this foundation, we construct a top-down-bottom-up 3D attention network, incorporating a dilate-attention structure designed to combat geometry degeneration, alongside two global modules that expand our receptive fields.  Our experiments across multiple datasets reveal that this 3D transformer network not only produces reconstructions that are more accurate and complete but also significantly outperforms prior methods. Notably, on the ScanNet dataset, we achieve a remarkable 41.8% improvement in mesh accuracy and a 25.3% enhancement in mesh completeness. We are committed to advancing the field and will publicly release the code underpinning our method.", "introduction": "Monocular 3D reconstruction is a classical task in computer vision and is essential for numerous applications like autonomous navigation, robotics, and augmented/virtual reality.Such a vision task aims to reconstruct an accurate and complete dense 3D shape of an unstructured scene from only a sequence of monocular RGB images.While the camera poses can be estimated accurately with the state-of-the-art SLAM (Campos et al., 2021) or SfM systems (Schonberger & Frahm, 2016), a dense 3D scene reconstruction from these posed images is still a challenging problem due to the complex geometry of a large-scale environment, such as the various objects, flexible lighting, reflective surfaces, and diverse cameras of different focus, distortion, and sensor noise.Many previous methods reconstruct the scenario in a multi-view depth manner (Yao et al., 2018;Chen et al., 2019;Duzceker et al., 2021).They predict the dense depth map of each target frame, which can estimate accurate local geometry but need additional efforts in fusing these depth maps (Murez et al., 2020;Sun et al., 2021), e.g., solving the inconsistencies between different views.Recently, some methods have tried to directly regress the complete 3D surface of the entire scene (Murez et al., 2020;Sun et al., 2021) from a truncated signed distance function (TSDF) representation.They first extract the 2D features with 2D convolutional neural networks (CNN), and then back-project the features to 3D space.Afterward, the 3D feature volume is processed by a 3D CNN network to output a TSDF volume prediction, which is extracted to a surface mesh by marching cubes (Lorensen & Cline, 1987).This way of reconstruction is end-to-end trainable, and is demonstrated to output accurate, coherent, and complete meshes.In this paper, we follow this volume-based 3D reconstruction path and directly regress the TSDF volume.Inspired by recent successes of vision transformer (Vaswani et al., 2017;Dosovitskiy et al., 2020), some approaches (Bozic et al., 2021;Stier et al., 2021) have adopted this structure in 3D reconstruction, but their usages are all limited to fusing the 2D features from different views while the aggregation of the 3D feature volumes is still performed by the 3D CNN.In this paper, we claim that the aggregation of 3D feature volume is also critical, and the evolution from 3D CNN to 3D multi-head attention could further improve both the accuracy and completeness of the reconstruction.Obviously, the limited usage of 3D multi-head attention in 3D feature volume aggregation is mainly due to its explosive computation.Specifically, the attention between each voxel and any other voxel needs to be calculated, which is hard to be realized in a general computing platform.This is also the reason why there are only a few applications of 3D transformers in solving 3D tasks.In this work, to address the above challenges and make the 3D transformer practical for 3D scene reconstruction, we propose a sparse window multi-head attention structure.Inspired by the sparse CNN (Yan et al., 2018), we first sparsify the 3D feature volume with predicted occupancy, in which way the number of the voxels is reduced to only the occupied ones.Then, to compute the attention score of a target voxel, we define a local window centered on this voxel, within which the non-empty voxels are considered for attention computing.In this way, the computation complexity of the 3D multi-head attention can be reduced by orders of magnitude, and this module can be embedded into a network for 3D feature aggregation.Therefore, with this module, we build the first 3D transformer based top-down-bottom-up network, where a dilate-attention module and its inverse are used to downsample and upsample the 3D feature volume.In addition, to make up for the local receptive field of the sparse window attention, we add a global attention module and a global context module at the bottom of this network since the size of the volume is very small at the bottom level.With this network, the 3D shape is estimated in a coarse-to-fine manner of three levels, as is displayed in Figure 1.To the best of our knowledge, this is the first paper employing the 3D transformer for 3D scene reconstruction from a TSDF representation.In the experiments, our method is demonstrated to outperform previous methods by a significant margin on multiple datasets.Specifically, the accuracy metric of the mesh on the ScanNet dataset is reduced by 41.8%, from 0.055 to 0.032, and the completeness metric is reduced by 25.3%, from 0.083 to 0.062.In the qualitative results, the meshes reconstructed by our method are dense, accurate, and complete.The main contributions of this work are then summarized as follows:\u2022 We propose a sparse window multi-head attention module, with which the computation complexity of the 3D transformer is reduced significantly and becomes feasible.\u2022 We propose a dilate-attention structure to avoid geometry degeneration in downsampling, with which we build the first top-down-bottom-up 3D transformer network for 3D feature aggregation.This network is further improved with bottom-level global attention and global context encoding.\u2022 This 3D transformer is employed to aggregate the 3D features back-projected from the 2D features of an image sequence in a coarse-to-fine manner, and predict TSDF values for accurate and complete 3D reconstruction.This framework shows a significant improvement in multiple datasets."}
{"paper_id": 333, "abstract": "In the ever-evolving landscape of robotics and computer vision, a remarkable shift is taking place\u2014one that harnesses the power of neural implicit functions for the intricate task of map representation in Simultaneous Localization and Mapping (SLAM). Early pioneers in this field have laid a promising foundation with their work on RGB-D SLAM, but we stand at the brink of something even more ambitious.   In this paper, we unveil a groundbreaking approach: a dense RGB SLAM method that boldly forgoes traditional depth input in favor of a neural implicit map representation. To tackle this formidable challenge, we introduce a hierarchical feature volume, a clever construct designed to empower the implicit map decoder. This innovative design not only captures the nuances of shape across various scales but also enhances the fidelity of map reconstruction.  Our method is a dual force, simultaneously unraveling the mysteries of camera motion while sculpting the neural implicit map itself. By matching rendered frames with those captured in real-time, we create a seamless integration of perception and representation. To further refine our optimization process, we propose a novel photometric warping loss, inspired by the principles of multi-view stereo. This addition serves to anchor the camera pose and scene geometry with unprecedented precision.  We rigorously evaluate our approach against established benchmarks, pitting it against contemporary RGB and RGB-D SLAM systems. The results are not just promising; they are a testament to our method's superiority, outshining previous techniques and even eclipsing some of the latest RGB-D SLAM advancements. For those eager to delve into the intricacies of our work, the code can be found at poptree.github.io/DIM-SLAM/.", "introduction": "Visual SLAM is a fundamental task in 3D computer vision with many applications in AR/VR and robotics.The goal of visual SLAM is to estimate the camera poses and build a 3D map of the environment simultaneously from visual inputs.Visual SLAM methods can be primarily divided into sparse or dense according to their reconstructed 3D maps.Sparse methods (Mur-Artal & Tard\u00f3s, 2017;Engel et al., 2017) focus on recovering camera motion with a set of sparse or semi-dense 3D points.Dense works (Newcombe et al., 2011b) seek to recover the depth of every observed pixel and are often more desirable for many downstream applications such as occlusion in AR/VR or obstacle detection in robotics.Earlier methods (Newcombe et al., 2011a;Whelan et al., 2012) often resort to RGB-D cameras for dense map reconstruction.However, RGB-D cameras are more suitable to indoor scenes and more expensive because of the specialized sensors.Another important problem in visual SLAM is map representation.Sparse SLAM methods (Mur-Artal & Tard\u00f3s, 2017;Engel et al., 2017) typically use point clouds for map representation, while dense methods (Newcombe et al., 2011b;a) usually adopt triangle meshes.As observed in many recent geometry processing works (Mescheder et al., 2019;Park et al., 2019;Chen & Zhang, 2019), neural implicit function offers a promising presentation for 3D data processing.The pioneer work, iMAP (Sucar et al., 2021), introduces an implicit map representation for dense visual SLAM.This map representation is more compact, continuous, and allowing for prediction of unobserved areas, which could potentially benefit applications like path planning (Shrestha et al., 2019) and object manipulation (Sucar et al., 2020).However, as observed in NICE-SLAM (Zhu et al., 2022), iMAP (Sucar et al., 2021) is limited to room-scale scenes due to the restricted representation power of MLPs.NICE-SLAM (Zhu et al., 2022) introduces a hierarchical feature volume to facilitate the map reconstruction and generalize the implicit map to larger scenes.However, both iMAP (Sucar et al., 2021) and NICE-SLAM (Zhu et al., 2022) are limited to RGB-D cameras.This paper presents a novel dense visual SLAM method with regular RGB cameras based on the implicit map representation.We also adopt a hierarchical feature volume like NICE-SLAM to deal with larger scenes.But our formulation is more suitable for visual SLAM.Firstly, the decoders in NICE-SLAM (Zhu et al., 2022) are pretrained, which might cause problems when generalizing to different scenes (Yang et al., 2021), while our method learns the scene features and decoders together on the fly to avoid generalization problem Secondly, NICE-SLAM (Zhu et al., 2022) computes the occupancy at each point from features at different scales respectively and then sums these occupancies together, while we fuse features from all scales to compute the occupancy at once.In this way, our optimization becomes much faster and thus can afford to use more pixels and iterations, enabling our framework to work on the RGB setting.In experiments, we find the number of feature hierarchy is important to enhance the system accuracy and robustness.Intuitively, features from fine volumes capture geometry details, while features from coarse volumes enforce geometry regularity like smoothness or planarity.While NICE-SLAM (Zhu et al., 2022) only optimizes two feature volumes with voxel sizes of 32cm and 16cm, our method solves six feature volumes from 8cm to 64cm.Our fusion of features across many different scales leads to more robust and accurate tracking and mapping as demonstrated in experiments.Another challenge in our setting is that there are no input depth observations.Therefore, we design a sophisticated warping loss to further constrain the camera motion and scene map in the same spirit of multi-view stereo (Zheng et al., 2014;Newcombe et al., 2011b;Wang et al., 2021b;Yu et al., 2021).Specifically, we warp one frame to other nearby frames according to the estimated scene map and camera poses and optimize the solution to minimize the warping loss.However, this warping loss is subject to view-dependent intensity changes such as specular reflections.To address this problem, we carefully sample image pixels visible in multiple video frames and evaluate the structural similarity of their surrounding patches to build a robust system.We perform extensive evaluations on three different datasets and achieve state-of-the-art performance on both mapping and camera tracking.Our method even surpasses recent RGB-D based methods like iMAP (Sucar et al., 2021) and NICE-SLAM (Zhu et al., 2022) on camera tracking.Our contributions can be summarized as the following:\u2022 We design the first dense RGB SLAM with neural implicit map representation,\u2022 We introduce a hierarchical feature volume for better occupancy evaluation and a multiscale patchbased warping loss to boost system performance with only RGB inputs,\u2022 We achieve strong results on benchmark datasets and even surpass some recent RGB-D methods."}
{"paper_id": 334, "abstract": "In the realm of multi-task dense visual scene understanding, the quest for crafting effective representations from a multitude of tasks within a singular, cohesive network framework stands as a pivotal challenge. This endeavor hinges on the delicate balance of joint modeling: (i) the creation of task-generic representations, (ii) the development of task-specific representations, and (iii) the intricate dance of cross-task representation interactions. Traditionally, the literature has approached these three facets through distinctly designed structures\u2014utilizing shared modules for task-generic learning, separate constructs for task-specific endeavors, and establishing bridges for cross-task interactions. However, the notion of weaving these perspectives seamlessly into each layer of a network in an end-to-end fashion remains largely uncharted territory. By doing so, we can not only alleviate the burden of meticulously crafting empirical structures for each of these multi-task representation objectives but also significantly enhance the overall representation learning prowess of the network, as all model capacity can be harnessed to optimize these objectives in unison.  In this paper, we unveil TaskPrompter, a groundbreaking spatial-channel multi-task prompting transformer framework designed to meet this ambitious goal. We introduce a suite of spatial-channel task prompts, each meticulously crafted to learn their spatial and channel interactions with the shared image tokens within every transformer layer through an attention mechanism\u2014an approach that proves essential for the nuanced demands of dense prediction tasks. Each task prompt is dedicated to capturing task-specific representations, while collectively, they synergize to refine the shared image token representations. Moreover, the interplay between different task prompts elegantly encapsulates the cross-task relationships. To translate the insights gleaned from these spatial-channel task prompts into dense predictions across multiple tasks, we have devised a novel dense task prompt decoding mechanism. This mechanism queries the shared image tokens through the task prompts, yielding spatial- and channel-wise task-specific representations.  Our extensive experiments on two formidable benchmarks for multi-task dense scene understanding\u2014NYUD-V2 and PASCAL-Context\u2014demonstrate the remarkable efficacy of our proposed framework. TaskPrompter not only showcases significant advancements in performance but also sets a new state-of-the-art in multi-task dense predictions. For those eager to explore further, our codes and models are freely accessible at https://github.com/prismformore/Multi-Task-Transformer.", "introduction": "Dense visual scene understanding is a fundamental research topic in computer vision that involves many dense prediction tasks, including semantic segmentation, depth estimation, surface normal estimation, boundary detection, etc.These distinct tasks share a fundamental understanding of the scene, which motivates researchers to design learning systems that model and predict multiple tasks in a unified framework, which is called \"multi-task learning\" (MTL).MTL mainly has two strengths: on one hand, learning a unified multi-task model for multiple tasks is typically more parameterefficient than training several single-task models; on the other hand, different tasks can facilitate each other with a good design in MTL (Vandenhende et al., 2021).With the powerful boost of deep learning, researchers have successfully designed highly promising multi-task learning models by exploiting the commonality and individuality of the tasks (Mani- TaskPrompter unifies the learning of task-specific and task-generic representations as well as crosstask interactions in each layer throughout the whole transformer architecture, with the embedding of task prompts and and patch tokens.The task prompts are projected to spatial task prompts and channel task prompts to learn spatial-and channel-wise interactions, which are critical for dense predictions.The spatial and channel task prompts as well as patch tokens are further used in the proposed Dense Spatial-Channel Task Prompt Decoding module to prompt dense task-specific features and the final multi-task predictions.nis et al., 2019;Xu et al., 2018;Kendall et al., 2018;Kokkinos, 2017).Traditionally, researchers manually design different types of modules in the multi-task network architecture to learn useful information for multi-task predictions on three aspects: task-generic representations, task-specific representations, and cross-task interactions.For instance, earlier works (Liu et al., 2019;Gao et al., 2019;Misra et al., 2016) design dedicated modules to learn task-specific representations and embed cross-task information interactions through hand-crafted structures deployed in the encoder, while several recent works (Ye & Xu, 2022;Li et al., 2022b;Vandenhende et al., 2020) choose to develop task-specific and cross-task modules in the decoder, and share encoder among different tasks.However, all of these methods decouple the learning of task-generic representations, task-specific representations, and cross-task interactions, into different network modules, which not only makes the architecture design more challenging as each module needs to be configured with a specific structure and capacity, but also suboptimal as learning effective communication among these three important perspectives of information is critical for multi-task dense prediction.To tackle the above-mentioned issue, we believe a better MTL framework should be capable of learning task-generic and task-specific representations as well as their interactions jointly in each layer across the whole network architecture.In this paper, we achieve this goal by proposing a novel Spatial-Channel Multi-task Prompting framework, coined as TaskPrompter.The core idea of TaskPrompter is to design \"spatial-channel task prompts\" which are task-specific learnable tokens to learn spatial-and channel-wise task-specific information for each task.More specifically, the task prompts are embedded together with the task-generic patch tokens computed from the input image as input of a transformer with a specially designed Spatial-Channel Task Prompt Learning module.The task prompts and patch tokens interact with each other and refine themselves by means of attention mechanism in each transformer layer.In this way, TaskPrompter manages to learn taskgeneric and task-specific representation as well as cross-task interaction simultaneously and does not require the design of different types of network modules.With the learned spatial-channel task prompts and image patch tokens, it is a non-trivial problem how to effectively decode multi-task dense features and predictions from them.To meet this challenge, we further propose a novel Dense Spatial-Channel Task Prompt Decoding method, which leverages both the spatial-wise and channel-wise affinities calculated between the task prompts and the patch tokens in attention modules to extract dense task features.The features are further refined by the cross-task affinity obtained from the self-attention weights among task prompts.The final multi-task dense predictions are produced based on the dense task features.In summary, the contribution of this work consists of three parts:\u2022 We propose a novel Spatial-Channel Multi-task Prompting framework (TaskPrompter) for multitask dense scene understanding.Our method essentially combines the learning of task-generic and task-specific representations, as well as cross-task interactions in each layer across the whole network architecture by introducing task prompts in our transformer.\u2022 A Spatial-Channel Task Prompt Learning module is designed.It can be flexibly deployed in each transformer layer for learning and refining task prompts and patch tokens along both spatial and channel dimensions.\u2022 We further design a novel Dense Spatial-Channel Task Prompt Decoding method based on the learned task-specific task prompts and task-generic patch tokens to generate pixel-wise predictions for multiple tasks simultaneously.Extensive experiments on two challenging multi-task dense prediction benchmarks (i.e.PASCAL-Context and NYUD-v2) clearly verify the effectiveness of the proposed method, which demonstrates superior performance compared with the previous state-of-the-art methods."}
{"paper_id": 335, "abstract": "In the intricate tapestry of decision-making, there exists a realm where choices bind the chooser, leading them down a path fraught with uncertainty and cost. Picture a scenario where a decision-maker, much like a hero on a quest, must commit to long-term actions, enduring the weight of continual costs, all while the outcome remains shrouded in the mists of the future. Take, for instance, the realm of healthcare\u2014where a newly discovered treatment awaits its moment in the sun, but only after the arduous journey of a clinical trial, a venture that demands both time and treasure. Yet, not every commitment bears fruit; the path may twist unexpectedly, leading to a trial that reveals no efficacy, leaving the decision-maker at a crossroads.  Faced with the relentless ticking of the clock and the burden of ongoing costs, we pose a crucial question: When should our intrepid decision-maker sever ties with a commitment that seems destined for failure? Should they pivot to a new venture, or perhaps choose the path of inaction? To unravel this conundrum, we formulate our inquiry into a novel challenge\u2014the Optimal Commitment Problem (OCP)\u2014a unique variant of the optimal stopping and switching problem. Through rigorous theoretical analysis, we uncover insights that illuminate the shadows of uncertainty, leading us to devise a practical algorithm capable of navigating this treacherous landscape.  Finally, we put our creation to the test, empirically evaluating its prowess in the realm of clinical trials, particularly in the nuanced art of subpopulation selection. In this journey, we strive not only to illuminate the path for decision-makers but also to forge a tool that empowers them to traverse the complexities of commitment with confidence and clarity.", "introduction": "In many real-world settings, decision-makers must commit to long-term actions and wait until their resolution before receiving the payoff of said actions.Meanwhile, staying committed to such actions incurs continual costs.For instance, in portfolio management, it might take time for an asset to develop additional value after an initial investment, and keeping capital tied up in an asset comes with an opportunity cost for the investor (Markowitz, 1959;Merton, 1969;Karatzas and Wang, 2020).In an energy network, turning power stations on and off is not an immediate action, hence a sudden increase in energy demand can only be met with a delay after putting more stations into operation, and keeping stations operational obviously consumes resources (Rafique and Jianhua, 2018;Olofsson et al., 2022).In healthcare, a newly-discovered treatment can only be marketed to patients once a successful clinical trial that targets the said treatment is conducted, which both requires time and is also costly (Kaitin, 2010;Umscheid et al., 2011).Of course, not all commitments eventually pay off: An asset might end up losing value despite investments, energy demands might shift faster than a network can react to, and a clinical trial might fail to show efficacy for the targeted treatment.Given the time pressure created by the continual cost of keeping a commitment, our goal in this paper is to answer the question: When should a decision-maker break a commitment-thereby avoiding future costs but also forfeiting any potential returns-either to make an alternative commitment instead or to make no further commitments at all? Solving this problem optimally requires a careful balance between exploration and exploitation:The earlier a commitment that is bound to fail is broken, the more resources would be saved (cf.exploitation); but the longer one is kept, the more information is revealed regarding whether the commitment is actually failing or might still succeed (cf.exploration)-and in certain cases, also regarding the prospects of similar commitments one could make instead.Related problems are mostly studied within the context of adaptive experimentation and sequential hypothesis testing (see Section 5).As such, we focus on adaptive experimentation as our main application as well.More specifically, we consider the problem of selecting the target population of an adaptive experiment.Suppose an experimenter, who is interested in proving the efficacy of a new treatment, starts running an initial experiment that targets a certain population of patients.Incidentally, the treatment being tested is effective only for a relatively narrow subpopulation of patients but not for the wider population as a whole.Hence, an experiment targeting the overall population, but not the subpopulation specifically, will most probably fail to prove efficacy and prevent the deployment of the treatment for the patients who would have actually benefited from it, not to mention waste time and resources (Moineddin et al., 2008;Lipkovich et al., 2017;Chiu et al., 2018).Of course, the experimenter has no knowledge of this in advance but the initial experiment they have set up would slowly reveal more information regarding the effects of the treatment and the fact that the ongoing experiment is bound to fail.In that case, we want to be able to determine at what point the experimenter has enough information to justify breaking their commitment to the initial experiment that targets too wide of a population to be successful, in favor of making a new commitment to a follow-up experiment that focuses on a narrower subpopulation instead?Contributions Our contributions are threefold: First, we formulate the problem of making and breaking commitments in a timely manner as a new type of optimal stopping/switching problem called the optimal commitment problem (OCP) (Section 2).The defining feature of OCP is that rewards are received only when a known time point is reached but costs are incurred continually, requiring commitment to actions but with incentive to abandon those commitments.As we will show later, OCP cannot be easily solved via conventional reinforcement learning techniques due to its non-convex nature.Second, we theoretically analyze a simplified case of OCP to identify the characteristics of the optimal solution (Section 3), and based on the insights we gain, propose a practical algorithm for the more general case (Section 4).Third, we empirically evaluate the performance of our algorithm in running experiments with subpopulation selection (Section 6).Before we move on, it should be emphasized that, although we predominantly consider adaptive experimentation as our main application, our contributions remain generally applicable to portfolio management, energy systems, and any other decision-making scenarios that require commitments to long-term actions."}
{"paper_id": 336, "abstract": "In the realm of data retrieval, where the quest for efficiency and accuracy reigns supreme, we find ourselves at the intersection of locality-sensitive hashing (LSH) and the art of approximate nearest-neighbor search (ANNS). Picture a function that deftly maps points from a sprawling dataset into a hidden realm, a latent space where the essence of their pairwise distances is preserved. This is the heart of our classic hash clustering (CHC) approach: we take the dataset, hash it into a low-dimensional binary expanse, and then cluster these points based on their hashed identities. When a query arrives, we delve into its designated hash-cluster and the neighboring clusters, embarking on a multi-probe search for its nearest companions.  Yet, herein lies a challenge. The CHC method, bound to a low-dimensional space, distorts the true distances from the high-dimensional reality of the original dataset, leading to a frustratingly low recall. To combat this, practitioners often resort to multiple hash tables, a solution that incurs additional burdens in storage and memory.  In this paper, we unveil a transformative approach to harnessing LSH for ANNS\u2014a method we have dubbed the Polar Code Nearest-Neighbor (PCNN) algorithm. By leveraging the power of modern error-correcting codes, specifically polar codes, we navigate the complexities of maintaining a streamlined number of clusters within a high-dimensional latent space. This innovative embedding allows the LSH function to capture the true distances of the original space with remarkable fidelity, resulting in a significant boost in recall.  At the core of PCNN lies a novel probing strategy that utilizes polar codes in a multi-probe scheme, employing efficient list-decoding techniques that operate independently of the dataset size. Our experimental results reveal a striking performance advantage for PCNN over the traditional CHC method. Remarkably, PCNN, with its singular table, eclipses the performance of CHC, which relies on multiple tables, thus alleviating the burdens of excessive memory and storage. In this way, we pave a new path in the landscape of data retrieval, one that promises greater efficiency without sacrificing accuracy.", "introduction": "In similarity search, one is first given a dataset D of points, then a set of query points from the same space.For each query, the goal is to find the closest point (or points) in D to that query, according to some given metric.The simplest way to find these nearest neighbors of a query is to calculate the distance of the query from each point in the dataset D; however, when the dataset D is large, this linear cost in the dataset's size is prohibitive.Thus, upon query, one would like to consider only a small subset of D. Since these non-exhaustive algorithms do not consider all points in the dataset, we are interested in approximate similarity search, with the following relaxations: \u2022 We allow an approximation ratio, i.e., the algorithm is allowed to return neighbors whose distance to the query is at most some factor \u03b1 \u2265 1 times the distance of the nearest neighbor to the queryfoot_0 .\u2022 Since the algorithm does not explore all points, it can sometimes return a result which is not within the desired distance; the fraction of results which are within the desired range is called the recall of the algorithm.Clustering Methods.A common technique for approximate similarity search is to divide the dataset D into clusters.Then, upon receiving a query, the algorithm would only search the points of D that appear in the clusters which are closest to the query.(If the clusters are represented by points in the original space, the distance to the query is well defined.Otherwise, a different metric is needed.)Algorithms based on such clustering are often used in practice, since they allow storing different clusters on different machines (i.e., sharding) for efficient distributed processing of queries.In light of these benefits of clustering methods, we would like to study them in our approximate setting.However, existing clustering methods have some drawbacks in this setting, which motivate the algorithm presented in this paper.We now explore two popular clustering methods and describe their drawbacks.Clustering by Training Cluster Centers.In this method, introduced by Sivic & Zisserman (2003), cluster centers are trained on the dataset (or some sample of the dataset) using a clustering algorithm (namely k-means), and each dataset point is mapped to a cluster (e.g., by the nearest cluster center).Upon query, the clusters corresponding to the closest cluster centers are searched.This common algorithm is part of the popular Faiss library (Johnson et al., 2021) as the default inverted-file (IVF) method; we henceforth refer to this method as IVF.Since the cluster centers are unstructured, finding these closest centers requires a number of distance computations which is linear in the number of centers.The total number of distance computations (for both centers and dataset points) is therefore always at least the square root of the dataset's size.Thus, while popular in general, this method is less appropriate for an approximate regime in a large dataset, as we would like to get high recall using a much smaller computational cost, independent of the dataset size.Clustering by Locality-Sensitive Hashing.Another such clustering method uses Locality-Sensitive Hashing, or LSH (Indyk & Motwani, 1998).In this method, a locality-sensitive hash h : R d \u2192 {0, 1}nbit is used to map the dataset to hash codes (nbit-bit strings).Each such hash-code identifies a cluster which contains the dataset points in the hash-code's preimage.Upon receiving a query q, the hashcode h(q) is calculated, and closest points are search only within the clusters identified with the closest hashcodes to h(q) (Lv et al., 2007).We refer to this simple method, which is used in most classic LSH papers, as Classic Hash Clustering, or CHC for short.Note that the choice of the LSH function h is independent from the operation of CHC, and should be chosen such that distances in the embedding space approximate distances in the original space; two possible choices (which we also consider in this paper) are hyperplane LSH (Charikar, 2002) and the data-dependent autoencoder LSH (Tissier et al., 2019).The fact that CHC uses LSH functions provides some advantages.First, the closest clusters to a query can be found without calculating its distance to every cluster; this makes CHC more suitable for the approximate regime than IVF.Second, the index in CHC can be easily augmented with additional dataset points in an online fashion, as the clustering is not trained on the dataset.In addition, CHC usually has a low memory/storage footprint.However, using CHC does not usually achieve high recall; this is usually alleviated by using multiple tables (i.e., multiple clusterings), at significant memory and storage costs.Why does CHC achieve low recall?A possible explanation could be that the distances between the dataset/query points in high-dimensional space R d are not faithfully captured by the hashing to the space {0, 1} nbit .This is since the hash-code space must be low-dimensional, as memory and running time restrictions make it infeasible to use large nbit.For example, if one chooses nbit = 40 (and thus 2 40 clusters) for a dataset of a billion points, 99.9% of the clusters would be empty; Thus, nbit must remain small, and usually does not exceed 32.However, this severely limits the granularity of distances, as Hamming distances in this low-dimensional binary space only take on one of 32 nonzero values.This lack of granularity is a property of every low-dimensional embedding, and thus appears in all LSH functions (including data-dependent functions).In addition, using a low number of embedding bits could yield a high variance in the distance of any embedded pair of points.For example, consider hyperplane LSH: in this method, the expected relative Hamming distance of the hash-codes is equal to the relative angular distance between the original points (Charikar, 2002), but the bits of the hashcode are generated independently.Thus, the deviations from the expectation are very significant when the number of bits is low, as mandated by CHC.These drawbacks of CHC thus call for a different technique, which is able to simultaneously utilize distance information from a high-dimensional binary embedding, as well as preserve a reasonable number of clusters.Our Contributions.In this paper, we present a generalization of CHC which uses modern errorcorrecting codes (ECCs); we call this method the Polar Code Nearest Neighbor algorithm (or PCNN).PCNN encapsulates any LSH method H, similar to CHC, but yields superior performance.CHC uses H to embed into a nbit-dimensional binary space for some low nbit (e.g., nbit = 30); PCNN instead uses H to embed into a cdim-dimensional binary space, for some cdim \u226b nbit (e.g., cdim = 512).Then, the embedded dataset in PCNN is clustered, such that the set of clusters forms a nbit-dimensional subspace inside the larger cdim-dimensional embedding space.Upon query, the probing procedure is performed in the high-dimensional embedding space.As we later discuss, CHC is a special case of PCNN in which nbit = cdim.By separating the dimension of the embedding space cdim from the dimension of clusters nbit, PCNN addresses the previously-discussed shortcoming of CHC, i.e., the low dimensionality of the resulting embedding which leads to a distortion of distances.PCNN performs probing on a large, cdim-dimensional space, in which distances between embedded points better approximate the distances in the original space.At the same time, PCNN maintains the same number of clusters as CHC (i.e., 2 nbit ).In addition, PCNN enjoys the benefits of CHC: it has an index which is small and easily extensible, as well as an efficient probing method whose running time does not depend on the number of clusters.Polar codes and List Decoding.The crux of our algorithm is the choice of cluster centers in this high-dimensional binary space: these centers are chosen to allow efficient mapping from a binary point to the closest centers, for the sake of multi-probe.This is where we use recent advances in error-correcting codes, namely the modern polar codes: choosing the centers to be the codewords of a polar code allows us to use list-decoding, an ECC technique which efficiently maps from a binary word to the closest nprb codewords, for any parameter nprb.Specifically, list decoding to the nprb closest codewords (i.e., multi-probe in PCNN to find the nprb closest clusters) runs in time O(nprb \u2022 cdim log nbit); this is nearly optimal, as nprb \u2022 cdim is the representation size in bits of the nprb closest cluster centers themselves.foot_1(See Appendix D for detailed complexity comparison.)Evaluation.We evaluate PCNN empirically on real-valued real-world datasets, and establish that it performs significantly better than standard (multi-probe) CHC.As PCNN can be used to encapsulate any LSH method, we chose to evaluate PCNN against CHC on two very different LSH methods: the first is the classic hyperplane method (Charikar, 2002), and the second is a data-dependent method based on the output of an autoencoder (Tissier et al., 2019).Moreover, we also show that PCNN outperforms CHC with multiple tables, while having a memory and storage footprint identical to that of single-table CHC.This implies that PCNN is a strong alternative to using CHC with multiple tables.We also evaluate PCNN on binary datasets, where both PCNN and CHC run directly on the dataset points (and thus a real-to-binary LSH is not needed).The baseline here is provided by the IndexBi-naryMultiHash class from Faiss (Johnson et al., 2021).The results mirror those for real datasets, showing a clear advantage to PCNN.In summary, in this paper, we give the following contributions: \u2022 We present the PCNN algorithm as a new clustering method for approximate nearest-neighbor search.The PCNN algorithm uses error-correcting codes -specifically polar codes -to index according to a high-dimensional binary embedding while keeping the number of clusters low.\u2022 We provide a multi-probe scheme for PCNN, which is based on efficient list-decoding algorithms for polar codes.\u2022 We evaluate PCNN vs. multi-probe CHC as a baseline, and show robust performance gains.Source code of the PCNN algorithm and the evaluations presented in the paper can be found on https://github.com/amzn/amazon-nearest-neighbor-through-ecc."}
{"paper_id": 337, "abstract": "In a world where humans effortlessly navigate the intricate tapestry of visual scenes, modern AI systems often falter, struggling to match our innate prowess. Enter the Guided Attention Model for Reasoning, or GAMR\u2014a groundbreaking module designed to bridge this gap. Drawing inspiration from the dynamic theory of active vision, GAMR operates on the premise that our brains tackle complex visual reasoning through a series of deliberate attention shifts, deftly selecting and channeling essential visual cues into our memory.  In a series of rigorous experiments across diverse visual reasoning tasks and datasets, GAMR has proven itself not just robust, but remarkably efficient in learning visual routines. Perhaps most impressively, it showcases an extraordinary ability for zero-shot generalization, tackling entirely new reasoning challenges with finesse. This research not only enhances our understanding of computational models but also aligns with cognitive theories emphasizing the vital interplay between attention and memory. Together, these elements enable the dynamic maintenance and manipulation of task-relevant visual information, paving the way for solving even the most complex visual reasoning dilemmas.", "introduction": "Abstract reasoning refers to our ability to analyze information and discover rules to solve arbitrary tasks, and it is fundamental to general intelligence in human and non-human animals (Gentner & Markman, 1997;Lovett & Forbus, 2017).It is considered a critical component for the development of artificial intelligence (AI) systems and has rapidly started to gain attention.A growing body of literature suggests that current neural architectures exhibit significant limitations in their ability to solve relatively simple visual cognitive tasks in comparison to humans (see Ricci et al. (2021) for review).Given the vast superiority of animals over state-of-the-art AI systems, it makes sense to turn to brain sciences to find inspiration to leverage brain-like mechanisms to improve the ability of modern deep neural networks to solve complex visual reasoning tasks.Indeed, a recent human EEG study has shown that attention and memory processes are needed to solve same-different visual reasoning tasks (Alamia et al., 2021).This interplay between attention and memory is previously discussed in Buehner et al. (2006); Fougnie (2008); Cochrane et al. (2019) emphasizing that a model must learn to perform attention over the memory for reasoning.It is thus not surprising that deep neural networks which lack attention and/or memory system fail to robustly solve visual reasoning problems that involve such same-different judgments (Kim et al., 2018).Recent computer vision works (Messina et al., 2021a;Vaishnav et al., 2022) have provided further computational evidence for the benefits of attention mechanisms in solving a variety of visual reasoning tasks.Interestingly, in both aforementioned studies, a Transformer module was used to implement a form of attention known as self-attention (Cheng et al., 2016;Parikh et al., 2016).In such a static module, attention mechanisms are deployed in parallel across an entire visual scene.By contrast, modern cognitive theories of active vision postulate that the visual system explores the environment dynamically via sequences of attention shifts to select and route task-relevant information to memory.Psychophysics experiments (Hayhoe, 2000) on overt visual attention have shown that eye movement patterns are driven according to task-dependent routines.Inspired by active vision theory, we describe a dynamic attention mechanism, which we call guided attention.Our proposed Guided Attention Module for (visual) Reasoning (GAMR) learns to shift attention dynamically, in a task-dependent manner, based on queries internally generated by an LSTM executive controller.Through extensive experiments on the two visual reasoning challenges, the Synthetic Visual Reasoning Test (SVRT) by Fleuret et al. (2011) and the Abstract Reasoning Task (ART) by Webb et al. (2021), we demonstrate that our neural architecture is capable of learning complex compositions of relational rules in a data-efficient manner and performs better than other state-of-the-art neural architectures for visual reasoning.Using explainability methods, we further characterize the visual strategies leveraged by the model in order to solve representative reasoning tasks.We demonstrate that our model is compositional -in that it is able to generalize to novel tasks efficiently and learn novel visual routines by re-composing previously learned elementary operations.It also exhibit zero shot generalization ability by translating knowledge across the tasks sharing similar abstract rules without the need of re-training.Contributions Our contributions are as follows:\u2022 We present a novel end-to-end trainable guided-attention module to learn to solve visual reasoning challenges in a data-efficient manner.\u2022 We show that our guided-attention module learns to shift attention to task-relevant locations and gate relevant visual elements into a memory bank; \u2022 We show that our architecture demonstrate zero-shot generalization ability and learns compositionally.GAMR is capable of learning efficiently by re-arranging previously-learned elementary operations stored within a reasoning module.\u2022 Our architecture sets new benchmarks on two visual reasoning challenges, SVRT (Fleuret et al., 2011) and ART (Webb et al., 2021)."}
{"paper_id": 338, "abstract": "In the ever-evolving landscape of self-supervised learning, a curious trend has emerged: the practice of aligning diverse augmentations of a single image to converge upon a unified feature representation. The choice of data augmentations plays a pivotal role in sculpting the quality of these learned features. In this exploration, we delve into the realm of color jitter\u2014a staple of data augmentation\u2014and unearth its detrimental effects on the integrity of color features in the resulting representations.  To counter this challenge, we unveil a novel approach, one steeped in the principles of physics: we call it Planckian Jitter. This innovative method introduces realistic variations in chromaticity, fostering a model that not only withstands the capricious changes in illumination encountered in the wild but also excels at discerning the subtleties of image content through color.  Our experiments reveal a fascinating synergy; the representation crafted through Planckian Jitter complements the traditional color jitter augmentations. When combined, these methodologies yield remarkable performance enhancements across a diverse array of downstream datasets. Furthermore, we present a comprehensive color sensitivity analysis that elucidates the effects of various training techniques on model neurons, demonstrating that the robustness of our learned features remains steadfast against variations in illumination.  For those eager to explore this groundbreaking approach, the official code can be found at: https://github.com/TheZino/PlanckianJitter.", "introduction": "Self-supervised learning enables the learning of representations without the need for labeled data (Doersch et al., 2015;Dosovitskiy et al., 2014).Several recent works learn representations that are invariant with respect to a set of data augmentations and have obtained spectacular results (Grill et al., 2020;Chen & He, 2021;Caron et al., 2020), significantly narrowing the gap with supervised learned representations.These works vary in their architectures, learning objectives, and optimization strategies, however they are similar in applying a common set of data augmentations to generate different image views.These algorithms, while learning to map these different views to the same latent representation, learn rich semantic representations for visual data.The set of transformations (data augmentations) used induces invariances that characterize the learned visual representation.Before deep learning revolutionized the way visual representations are learned, features were handcrafted to represent various properties, leading to research on shape (Lowe, 2004), texture (Manjunath & Ma, 1996), and color features (Finlayson & Schaefer, 2001;Geusebroek et al., 2001).Color features were typically designed to be invariant to a set of scene-accidental events such as shadows, shading, and illuminant and viewpoint changes.With the rise of deep learning, feature Figure 1: Default color jitter (left) and Planckian Jitter (right).Augmentations based on default color jitter lead to unrealistic images, while Planckian Jitter leads to a set of realistic ones.The ARC chromaticity diagrams for each type of jitter are computed by sampling initial RGB values and mapping them into the range of possible outputs given by each augmentation.These diagrams show that Planckian Jitter transforms colors along chromaticity lines occurring in nature when changing the illuminant, whereas default color jitter transfers colors throughout the whole chromaticity plane.representations that simultaneously exploit color, shape, and texture are learned implicitly and the invariances are a byproduct of end-to-end training (Krizhevsky et al., 2009).Current approaches to self-supervision learn a set of invariances implicitly related to the applied data augmentations.In this work, we focus on the currently de facto choice for color augmentations.We argue that they seriously cripple the color quality of learned representations and we propose an alternative, physics-based color augmentation.Figure 1 (left) illustrates the currently used color augmentation on a sample image.It is clear that the applied color transformation significantly alters the colors of the original image, both in terms of hue and saturation.This augmentation results in a representation that is invariant with respect to surface reflectance -an invariance beneficial for recognizing classes whose surface reflectance varies significantly, for example many man-made objects such as cars and chairs.However, such invariance is expected to hurt performance on downstream tasks for which color is an important feature, like natural classes such as birds or food.One of the justifications is that without large color changes, mapping images to the same latent representation can be purely done based on color and no complex shape or texture features are learned.However, as a result the quality of the color representation learned with such algorithms is inferior and important information on surface reflectance might be absent.Additionally, some traditional supervised learning methods propose domain-specific variations of color augmentation Galdran et al. (2017); Xiao et al. (2019).In this paper we propose an alternative color augmentation (Figure 1, right) and we assess its impact on self-supervised learning.We draw on the existing color imaging literature on designing features invariant to illuminant changes commonly encountered in the real world (Finlayson & Schaefer, 2001).Our augmentation, called Planckian Jitter, applies physically-realistic illuminant variations.We consider the illuminants described by Planck's Law for black-body radiation, that are known to be similar to illuminants encountered in real-life (Tominaga et al., 1999).The aim of our color augmentation is to allow the representation to contain valuable information about the surface reflectance of objects -a feature that is expected to be important for a wide range of downstream tasks.Combining such a representation with the already high-quality shape and texture representation learned with standard data augmentation leads to a more complete visual descriptor that also describes color.Our experiments show that self-supervised representations learned with Planckian Jitter are robust to illuminant changes.In addition, depending on the importance of color in the dataset, the proposed Planckian jitter outperforms the default color jitter.Moreover, for all evaluated datasets the combination of features of our new data augmentation with standard color jitter leads to significant performance gains of over 5% on several downstream classification tasks.Finally, we show that Planckian Jitter can be applied to several state-of-the-art self-supervised learning methods."}
{"paper_id": 339, "abstract": "In the ever-evolving landscape of artificial intelligence, the quest for adaptable and efficient Convolutional Neural Network (CNN) architectures has become paramount. Each task presents its own unique challenges, demanding a careful consideration of input length, resolution, and dimensionality. In this endeavor, we unveil the Continuous Convolutional Neural Network (CCNN), a groundbreaking architecture that transcends the limitations of traditional CNNs.   Imagine a single, elegant design capable of processing data across any resolution, dimensionality, and length\u2014no structural modifications required. At the heart of the CCNN lies its innovative continuous convolutional kernels, which deftly model long-range dependencies at every layer. This revolutionary approach eliminates the cumbersome need for task-specific downsampling and varying depths that plague existing architectures.  We demonstrate the versatility of our CCNN by applying it seamlessly to a diverse array of tasks\u2014whether they be sequential (1D), visual (2D), or point-cloud (3D) data. The results speak for themselves: our architecture not only matches but often surpasses the current state-of-the-art across all evaluated tasks. In a world where adaptability is key, the CCNN stands as a beacon of innovation, ready to tackle the complexities of modern data.", "introduction": "The vast popularity of Convolutional Neural Networks (LeCun et al., 1998) (CNNs) is a result of their high performance and efficiency, which has led them to achieve state-of-the-art in applications across sequential (Abdel-Hamid et al., 2014;Van Den Oord et al., 2016), visual (Krizhevsky et al., 2012;Simonyan & Zisserman, 2014) and high-dimensional data (Sch\u00fctt et al., 2017;Wu et al., 2019).Nevertheless, a major limitation of CNNs -and other neural networks-is that their architectures must be tailored to particular applications in order to consider the length, resolution and dimensionality of the input data.This has led to a plethora of task-specific architectures (Oord et al., 2016;Bai et al., 2018;Simonyan & Zisserman, 2014;Szegedy et al., 2015;Ronneberger et al., 2015;He et al., 2016;Qi et al., 2017;Wu et al., 2019) which (i) hampers the selection of the most appropriate architecture for a particular task, and (ii) obscures the transfer and generalization of insights across applications.In this work, we tackle the need for problem-specific CNN architectures and propose a generic CNN architecture that can be used independent of the length, resolution and dimensionality of the data.CNN architectures are data dependent.Current CNN architectures are task-specific because they are tied to the length, resolution, and dimensionality of the input.The length of the data varies from task to task, e.g.audio fragments may span milliseconds to minutes.This requires carefully chosen  Figure 2: Continuous convolutional kernels: the key to a unified CNN architecture.The continuous parameterization of convolutional kernels used in this work consists of a small kernel network \u03c6 kernel that receives coordinates as input and outputs the value of the convolutional kernel at that position (2a).By changing the dimensionality of the coordinates x i , the same kernel network can render convolutional kernels for sequential (2b), visual (2c), and higher dimensional data (2d).strides and pooling to capture relevant dependencies across the entire input (Van Den Oord et al., 2016;Lee et al., 2017).In addition, physical signals, e.g., audio, images, are continuous in nature.As such, their semantic meaning is independent of the resolution at which they are sampled, e.g., the same audio may be expressed at different resolutions.Nevertheless, current CNN architectures are resolution-bound, and thus different resolutions require different CNNs.These limitations aggravate when considering multi-dimensional data.Each input dimension can be defined at different lengths and resolutions, e.g., video, rectangular images, and each data modality brings its own conventions for each of these properties, e.g., the resolution of a second of audio (16kHz) (Warden, 2018) strongly contrasts with that of images (32 \u00d7 32) (Krizhevsky et al., 2009).Towards a unified CNN architecture.As discussed in Sec. 3, the core component that makes CNNs data-dependent are their discrete convolutional kernels.Convolutional kernels are implemented via a one-to-one mapping between kernel values and model parameters (Fig. 1 left), which (i) binds them to the input resolution and length, and (ii) makes them ill suited to model long-range dependencies.The latter results from the large number of parameters needed to construct large convolutional kernels.This is why standard CNNs favour using local kernels in combination with task-dependent depths and pooling layers to model long-range dependencies, at the cost of making them task-dependent.The need for a continuous parameterization.To overcome task-dependent architectures, it is crucial to define a kernel parameterization that decouples parameter count from kernel size.Following Sch\u00fctt et al. (2017); Romero et al. (2022b), we use a small neural network to define a continuous mapping from positions to the value of the kernel at those positions.The resulting Continuous Convolutional Kernels (Fig. 2), allow for the construction of convolutional kernels of arbitrary size in a parameter efficient manner.Consequently, the same convolutional layers -and thus the same CNN-can be used regardless of the input length, resolution and dimensionality.We leverage this formulation to construct the Continuous Convolutional Neural Network (CCNN): a single CNN architecture that can be applied regardless of the input length, resolution and dimensionality.Empirical results.To showcase the proposed CCNN, we deploy the same CCNN for several tasks on sequential (1D), visual (2D) and point-cloud (3D) data.Our CCNN matches and often outperforms the current state-of-the-art across all tasks considered.Importantly, the continuous parameterization of our CCNN allows it to handle irregularly sampled data natively.As a result, the CCNN is not restricted to grid data, e.g., 3D voxels, and can be used on point-clouds directly.\u2022 We propose the Continuous Convolutional Neural Network: a general purpose CNN architecture able to process data of arbitrary resolution, dimensionality and length without structural changes.\u2022 We study the layers of CNNs, and demonstrate that the ability to model long-term dependencies on N D without the need of input dependent downsampling and depth values is necessary and sufficient for the construction of a general purpose CNN architecture.\u2022 In order to model long-term dependencies on N D without input dependent downsampling and depth values, we utilize and improve the Continuous Kernel Convolutions of Romero et al. (2022b).Our proposed improvements allow the proposed Continuous CNN to achieve good empirical results on the tasks considered in 1D, 2D and 3D without structural changes."}
{"paper_id": 340, "abstract": "In the ever-evolving realm of multi-agent reinforcement learning (MARL), a novel approach has emerged, one that seeks to bridge the chasm between artificial agents and human collaborators, all without relying on human data. Traditionally, the journey begins with a relentless cycle of self-play (SP), crafting a diverse pool of policies, only to culminate in the training of a final adaptive policy against this carefully curated collection. Yet, therein lies a significant flaw: each policy in this pool is honed solely against the environment's reward function, presupposing that the adaptive policy's human counterparts will share the same goals. However, the reality is often far more complex; human motivations are frequently shaped by personal biases that diverge sharply from the cold calculus of the environment's rewards.  Enter Hidden-Utility Self-Play (HSP), our innovative framework designed to confront this challenge head-on. HSP reimagines the self-play paradigm by explicitly incorporating human biases as hidden reward functions, weaving them into the very fabric of the self-play objective. By approximating the reward landscape as linear functions, HSP deftly constructs an augmented policy pool rich with biased policies, tailored to better align with human objectives.  We put HSP to the test within the Overcooked benchmark, and the results speak volumes. Our method not only outperforms traditional baselines in terms of rewards when collaborating with learned human models, scripted policies, and actual human players, but it also garners the highest ratings for assistiveness based on human feedback. In this endeavor, we have forged a path toward a more harmonious coexistence between human and machine, one that acknowledges and embraces the intricacies of human motivation.", "introduction": "Building intelligent agents that can interact with, cooperate and assist humans remains a longstanding AI challenge with decades of research efforts (Klien et al., 2004;Ajoudani et al., 2018;Dafoe et al., 2021).Classical approaches are typically model-based, which (repeatedly) build an effective behavior model over human data and plan with the human model (Sheridan, 2016;Carroll et al., 2019;Bobu et al., 2020).Despite great successes, this model-based paradigm requires an expensive and time-consuming data collection process, which can be particularly problematic for complex problems tackled by today's AI techniques (Kidd & Breazeal, 2008;Biondi et al., 2019) and may also suffer from privacy issues (Pan et al., 2019).Recently, multi-agent reinforcement learning (MARL) has become a promising approach for many challenging decision-making problems.Particularly in competitive settings, AIs developed by MARL algorithms based on self-play (SP) defeated human professionals in a variety of domains (Silver et al., 2018;Vinyals et al., 2019;Berner et al., 2019).This empirical evidence suggests a new direction of developing strong AIs that can directly cooperate with humans in a similar \"model-free\" fashion, i.e., via self-play.Different from zero-sum games, where simply adopting a Nash equilibrium strategy is sufficient, an obvious issue when training cooperative agents by self-play is convention overfitting.Due to the existence of a large number of possible optimal strategies in a cooperative game, SP-trained agents can easily converge to a particular optimum and make decisions solely based on a specific behavior pattern, i.e., convention (Lowe et al., 2019;Hu et al., 2020), of its co-trainers, leading to poor generalization ability to unseen partners.To tackle this problem, recent works proposed a two-staged framework by first developing a diverse policy pool consisting of multiple SP-trained policies, which possibly cover different conventions, and then further training an adaptive policy against this policy pool (Lupu et al., 2021;Strouse et al., 2021;Zhao et al., 2021).Despite the empirical success of this two-staged framework, a fundamental drawback exists.Even though the policy pool prevents convention overfitting, each SP-trained policy in the pool remains a solution, which is either optimal or sub-optimal, to a fixed reward function specified by the underlying cooperative game.This implies a crucial generalization assumption that any test-time partner will be precisely optimizing the specified game reward.Such an assumption results in a pitfall in the case of cooperation with humans.Human behavior has been widely studied in cognitive science (Griffiths, 2015), economics (Wilkinson & Klaes, 2017) and game theory (Fang et al., 2021).Systematic research has shown that humans' utility functions can be substantially biased even when a clear objective is given (Pratt, 1978;Selten, 1990;Camerer, 2011;Barberis, 2013), suggesting that human behaviors may be subject to an unknown reward function that is very different from the game reward (Nguyen et al., 2013).This fact reveals an algorithmic limitation of the existing SP-based methods.In this work, we propose Hidden-Utility Self-Play (HSP), which extends the SP-based two-staged framework to the assumption of biased humans.HSP explicitly models the human bias via an additional hidden reward function in the self-play training objective.Solutions to such a generalized formulation are capable of representing any non-adaptive human strategies.We further present a tractable approximation of the hidden reward function space and perform a random search over this approximated space when building the policy pool in the first stage.Hence, the enhanced pool can capture a wide range of possible human biases beyond conventions (Hu et al., 2020;Zhao et al., 2021) and skill-levels (Dafoe et al., 2021) w.r.t. the game reward.Accordingly, the final adaptive policy derived in the second phase can have a much stronger adaptation capability to unseen humans.We evaluate HSP in a popular human-AI cooperation benchmark, Overcooked (Carroll et al., 2019), which is a fully observable two-player cooperative game.We conduct comprehensive ablation studies and comparisons with baselines that do not explicitly model human biases.Empirical results show that HSP achieves superior performances when cooperating with behavior models learned from human data.In addition, we also consider a collection of manually scripted biased strategies, which are ensured to be sufficiently distinct from the policy pool, and HSP produces an even larger performance improvement over the baselines.Finally, we conduct real human studies.Collected feedbacks show that the human participants consistently feel that the agent trained by HSP is much more assistive than the baselines.We emphasize that, in addition to algorithmic contributions, our empirical analysis, which considers learned models, script policies and real humans as diverse testing partners, also provides a more thorough evaluation standard for learning human-assistive AIs."}
{"paper_id": 341, "abstract": "In this paper, we embark on a journey to unveil the hidden connections between gradient boosting with symmetric decision trees and the elegant realm of kernel methods. We reveal that these two seemingly disparate approaches can be harmoniously intertwined, reformulating gradient boosting as a pathway that converges towards the solution of a specific Kernel Ridge Regression problem. This convergence leads us into the rich landscape of Gaussian Processes, where we can extract the posterior mean with newfound clarity.  But the tale does not end there. Armed with this understanding, we transform gradient boosting into a powerful sampler from the posterior distribution, enabling us to delve deeper into the uncertainties of our knowledge. Through the art of Monte-Carlo estimation, we gain access to the posterior variance, allowing us to refine our uncertainty estimates with precision. Our findings demonstrate that this novel sampler not only enhances our understanding of uncertainty but also significantly improves our ability to detect out-of-domain instances. Thus, we forge a new path in the realm of machine learning, where our tools become more adept at navigating the complexities of the unknown.", "introduction": "Gradient boosting (Friedman, 2001) is a classic machine learning algorithm successfully used for web search, recommendation systems, weather forecasting, and other problems ("}
{"paper_id": 342, "abstract": "In the realm of decision-making under uncertainty, we delve into the intricate world of Contextual Bandits with Concave Rewards (CBCR). This multi-objective bandit scenario presents a compelling challenge, where the delicate balance of rewards is dictated by a known concave objective function, and the reward vector emerges from an observed stochastic context. In a groundbreaking development, we unveil the first algorithm that achieves provably vanishing regret for CBCR, free from the confines of a restricted policy space\u2014an advancement that surpasses previous efforts limited to finite policy frameworks or tabular forms.  Our approach is rooted in a geometric perspective, viewing CBCR algorithms as optimization tools navigating the convex terrain of expected rewards shaped by all stochastic policies. By leveraging insights from Frank-Wolfe analyses in constrained convex optimization, we craft a novel reduction that connects the regret of CBCR to that of a more straightforward scalar-reward bandit problem. This reduction serves as a powerful tool, allowing us to seamlessly adapt existing algorithms for CBCR, whether the reward functions are linear or more complex, particularly in scenarios involving non-combinatorial actions.  Driven by a commitment to fairness in recommendations, we explore a distinct variant of CBCR that incorporates rankings and fairness-aware objectives. This exploration leads us to pioneer the first algorithm that guarantees regret bounds for contextual combinatorial bandits while ensuring fairness of exposure. In this way, we not only advance the theory of bandit algorithms but also contribute to the ethical considerations in their application, paving the way for more equitable decision-making processes.", "introduction": "Contextual bandits are a popular paradigm for online recommender systems that learn to generate personalized recommendations from user feedback.These algorithms have been mostly developed to maximize a single scalar reward which measures recommendation performance for users.Recent fairness concerns have shifted the focus towards item producers whom are also impacted by the exposure they receive (Biega et al., 2018;Geyik et al., 2019), leading to optimize trade-offs between recommendation performance for users and fairness of exposure for items (Singh & Joachims, 2019;Zehlike & Castillo, 2020).More generally, there is an increasing pressure to insist on the multiobjective nature of recommender systems (Vamplew et al., 2018;Stray et al., 2021), which need to optimize for several engagement metrics and account for multiple stakeholders' interests (Mehrotra et al., 2020;Abdollahpouri et al., 2019).In this paper, we focus on the problem of contextual bandits with multiple rewards, where the desired trade-off between the rewards is defined by a known concave objective function, which we refer to as Contextual Bandits with Concave Rewards (CBCR).Concave rewards are particularly relevant to fair recommendation, where several objectives can be expressed as (known) concave functions of the (unknown) utilities of users and items (Do et al., 2021).Our CBCR problem is an extension of Bandits with Concave Rewards (BCR) (Agrawal & Devanur, 2014) where the vector of multiple rewards depends on an observed stochastic context.We address this extension because contexts are necessary to model the user/item features required for personalized recommendation.Compared to BCR, the main challenge of CBCR is that optimal policies depend on the entire distribution of contexts and rewards.In BCR, optimal policies are distributions over actions, and are found by direct optimization in policy space (Agrawal & Devanur, 2014;Berthet & Perchet, 2017).In CBCR, stationary policies are mappings from a continuous context space to distributions over actions.This makes existing BCR approaches inapplicable to CBCR because the policy space is not amenable to tractable optimization without further assumptions or restrictions.As a matter of fact, the only prior theoretical work on CBCR is restricted to a finite policy set (Agrawal et al., 2016).We present the first algorithms with provably vanishing regret for CBCR without restriction on the policy space.Our main theoretical result is a reduction where the CBCR regret of an algorithm is bounded by its regret on a proxy bandit task with single (scalar) reward.This reduction shows that it is straightforward to turn any contextual (scalar reward) bandits into algorithms for CBCR.We prove this reduction by first re-parameterizing CBCR as an optimization problem in the space of feasible rewards, and then revealing connections between Frank-Wolfe (FW) optimization in reward space and a decision problem in action space.This bypasses the challenges of optimization in policy space.To illustrate how to apply the reduction, we provide two example algorithms for CBCR with noncombinatorial actions, one for linear rewards based on LinUCB (Abbasi-Yadkori et al., 2011), and one for general reward functions based on the SquareCB algorithm (Foster & Rakhlin, 2020) which uses online regression oracles.In particular, we highlight that our reduction can be used together with any exploration/exploitation principle, while previous FW approaches to BCR relied exclusively on upper confidence bounds (Agrawal & Devanur, 2014;Berthet & Perchet, 2017;Cheung, 2019).Since fairness of exposure is our main motivation for CBCR, we show how our reduction also applies to the combinatorial task of fair ranking with contextual bandits, leading to the first algorithm with regret guarantees for this problem, and we show it is computationally efficient.We compare the empirical performance of our algorithm to relevant baselines on a music recommendation task.Related work.Agrawal et al. (2016) address a restriction of CBCR to a finite set of policies, where explicit search is possible.Cheung (2019) use FW for reinforcement learning with concave rewards, a similar problem to CBCR.However, they rely on a tabular setting where there are few enough policies to compute them explicitly.Our approach is the only one to apply to CBCR without restriction on the policy space, by removing the need for explicit representation and search of optimal policies.Our work is also related to fairness of exposure in bandits.Most previous works on this topic either do not consider rankings (Celis et al., 2018;Wang et al., 2021;Patil et al., 2020;Chen et al., 2020), or apply to combinatorial bandits without contexts (Xu et al., 2021).Both these restrictions are impractical for recommender systems.Mansoury et al. (2021); Jeunen & Goethals (2021) propose heuristics with experimental support that apply to both ranking and contexts in this space, but they lack theoretical guarantees.We present the first algorithm with regret guarantees for fair ranking with contextual bandits.We provide a more detailed discussion of the related work in Appendix A."}
{"paper_id": 343, "abstract": "In a world where the boundaries of perception and imagination intertwine, humans possess an extraordinary ability: to recognize entirely new categories of objects solely through the lens of language, drawing vivid images from mere descriptions of their visual traits. What if we could harness this innate skill and empower deep learning systems to perform complex vision tasks, such as object detection, without the cumbersome need for real images or painstaking annotations? Enter the realm of Imaginary-Supervised Object Detection (ISOD)\u2014a groundbreaking learning paradigm where traditional constraints are cast aside.   In this paper, we unveil ImaginaryNet, an innovative framework that conjures images from the ether by blending the prowess of a pretrained language model with the artistry of a text-to-image synthesis model. Given a class label, our language model crafts a rich narrative of a scene featuring a target object, while the text-to-image model breathes life into this narrative, generating photo-realistic images. Armed with these synthesized visuals and their corresponding class labels, we navigate the challenges of weakly supervised object detection to realize the vision of ISOD.   But the journey doesn\u2019t end there. By gradually integrating real images and manual annotations, ImaginaryNet evolves, collaborating with various supervision settings to amplify detection performance. Our experiments reveal that ImaginaryNet achieves approximately 75% of the performance of its weakly supervised counterpart trained on real data, while also significantly enhancing baseline results. In fact, when paired with other supervision settings, ImaginaryNet not only meets but often surpasses state-of-the-art performance benchmarks.   Join us as we explore this uncharted territory, where imagination meets innovation. Our code will be freely accessible at https://github.com/kodenii/ImaginaryNet.", "introduction": "Without the demand of training in reality, humans are able of detecting a new category of object simply based on the language description on its visual characteristics.Equipping this ability to deep learning may allow the neural network to handle complex vision tasks, e.g., object detection, without real images and annotations.Recently, we witness the rise of Contrastive Language-Image Pre-training (CLIP) (Radford et al., 2021), where general knowledge can be learned by pre-training and then be applied to various downstream tasks via zero-shot learning or task-specific fine-tuning.Unlike image classification, object detection is more challenging and has a larger gap than pretraining tasks.Several methods, such as RegionCLIP (Zhong et al., 2022a), ViLD (Gu et al., 2021), and Detic (Zhou et al., 2022), have been suggested to transfer knowledge from pre-trained CLIP (Radford et al., 2021) to some modules of detection.However, real images and annotations are still required for some key modules of the object detectors, such as Region Proposal Network (RPN) or Region of Interest (RoI) heads.In this work, we aim to raise and answer a question: given suitable pre-trained models, can we learn object detectors without real images and manual annotations?To this end, we introduce a novel learning paradigm, i.e., Imaginary-Supervised Object Detection (ISOD), where no real images and manual annotations can be used for training object detection.Fortunately, benefited from the progress in vision-language pre-training, ISOD is practically feasible.Here we propose IMAG-INARYNET, a framework to learn object detectors by combining pretrained language model as well as text-to-image synthesis models.In particular, the text-to-image synthesis model is adopted to generate photo-realistic images, and the language model can be used to improve the diversity and provide class labels for the synthesized images.Then, ISOD can be conducted by leveraging weakly supervised object detection (WSOD) algorithms on the synthesized images with class labels to learn object detectors.We set up a strong CLIP-based model as the baseline to verify the effectiveness of IMAGINARYNET.Experiments show that IMAGINARYNET can outperform the CLIP-based model with a large margin.Moreover, IMAGINARYNET obtains about 75% performance in ISOD compared with the weakly supervised model of the same backbone trained on real data, clearly showing the feasibility of learning object detection without any real images and manual annotations.By gradually introducing real images and manual annotations, IMAGINARYNET can collaborate with other supervision settings to further boost detection performance.It is worthy noting that the performance of existing object detection models may be constrained by the limited amount of training data.As a result, we use IMAGINARYNET as a dataset expansion approach to incorporate with real images and manual annotations.Further experiments show that IMAGINARYNET significantly improves the performance of the baselines while achieving state-of-the-art or comparable performance in the supervision setting.To sum up, the contributions of this work are as follows:\u2022 We propose IMAGINARYNET, a framework to generate synthesized images as well as supervision information for training object detector.To the best of our knowledge, we are among the first work to train deep object detectors solely based on synthesized images.\u2022 We propose a novel paradigm of object detection, Imaginary-Supervised Object Detection (ISOD), where no real images and annotations can be used for training object detectors.We set up the benchmark of ISOD and obtain about 75% performance in ISOD when compared with the WSOD model of the same backbone trained on real data.\u2022 By incorporating with real images and manual annotations, ImaginaryNet significantly improves the baseline model while achieving state-of-the-art or comparable performance."}
{"paper_id": 344, "abstract": "In recent years, a remarkable innovation has taken root in the realm of technology: Implicit Neural Representations (INRs). These entities have proven themselves to be formidable tools for encoding a vast array of signals\u2014images, videos, audio, and even the intricate forms of 3D shapes. When harnessed for 3D shapes, INRs rise to the challenge of overcoming the fragmentation and limitations that have long plagued traditional discrete representations. However, a crucial question looms: given that INRs are fundamentally neural networks, how can we seamlessly integrate them into deep learning pipelines designed for downstream tasks?  In this paper, we boldly tackle this research conundrum and introduce inr2vec, a pioneering framework capable of distilling a compact latent representation from an input INR in a single, efficient inference pass. Our findings reveal that inr2vec adeptly captures the essence of 3D shapes encoded by the input INRs. Furthermore, we demonstrate how these embeddings can be seamlessly incorporated into deep learning pipelines, enabling the resolution of various tasks while exclusively utilizing the power of INRs. Through this work, we aim to illuminate new pathways for leveraging the potential of INRs in the ever-evolving landscape of machine learning.", "introduction": "Since the early days of computer vision, researchers have been processing images stored as twodimensional grids of pixels carrying intensity or color measurements.But the world that surrounds us is three dimensional, motivating researchers to try to process also 3D data sensed from surfaces.Unfortunately, representation of 3D surfaces in computers does not enjoy the same uniformity as digital images, with a variety of discrete representations, such as voxel grids, point clouds and meshes, coexisting today.Besides, when it comes to processing by deep neural networks, all these kinds of representations are affected by peculiar shortcomings, requiring complex ad-hoc machinery (Qi et al., 2017b;Wang et al., 2019b;Hu et al., 2022) and/or large memory resources (Maturana & Scherer, 2015).Hence, no standard way to store and process 3D surfaces has yet emerged.Recently, a new kind of representation has been proposed, which leverages on the possibility of deploying a Multi-Layer Perceptron (MLP) to fit a continuous function that represents implicitly a signal of interest (Xie et al., 2021).These representations, usually referred to as Implicit Neural Representations (INRs), have been proven capable of encoding effectively 3D shapes by fitting signed distance functions (sdf) (Park et al., 2019;Takikawa et al., 2021;Gropp et al., 2020), unsigned distance functions (udf) (Chibane et al., 2020) and occupancy fields (occ) (Mescheder et al., 2019;Peng et al., 2020).Encoding a 3D shape with a continuous function parameterized as an MLP decouples the memory cost of the representation from the actual spatial resolution, i.e., a surface with arbitrarily fine resolution can be reconstructed from a fixed number of parameters.Moreover, the same neural network architecture can be used to fit different implicit functions, holding the potential to provide a unified framework to represent 3D shapes.Due to their effectiveness and potential advantages over traditional representations, INRs are gathering ever-increasing attention from the scientific community, with novel and striking results published more and more frequently (M\u00fcller et al., 2022;Martel et al., 2021;Takikawa et al., 2021;Liu et al., 2022).This lead us to conjecture that, in the forthcoming future, INRs might emerge as a standard representation to store and communicate 3D shapes, with repositories hosting digital twins of 3D objects realized only as MLPs becoming commonly available.An intriguing research question does arise from the above scenario: beyond storage and communication, would it be possible to process directly INRs of 3D shapes with deep learning pipelines to solve downstream tasks as it is routinely done today with discrete representations like point clouds or meshes?In other words, would it be possible to process an INR of a 3D shape to solve a downstream task, e.g., shape classification, without reconstructing a discrete representation of the surface?Since INRs are neural networks, there is no straightforward way to process them.Earlier work in the field, namely OccupancyNetworks (Mescheder et al., 2019) and DeepSDF (Park et al., 2019), fit the whole dataset with a shared network conditioned on a different embedding for each shape.In such formulation, the natural solution to the above mentioned research problem could be to use such embeddings as representations of the shapes in downstream tasks.This is indeed the approach followed by contemporary work (Dupont et al., 2022), which addresses such research problem by using as embedding a latent modulation vector applied to a shared base network.However, representing a whole dataset by a shared network sets forth a difficult learning task, with the network struggling in fitting accurately the totality of the samples (as we show in Appendix A).Conversely, several recent works, like SIREN (Sitzmann et al., 2020b) and others (Sitzmann et al., 2020a;Dupont et al., 2021a;Str\u00fcmpler et al., 2021;Zhang et al., 2021;Tancik et al., 2020) have shown that, by fitting an individual network to each input sample, one can get high quality reconstructions even when dealing with very complex 3D shapes or images.Moreover, constructing an individual INR for each shape is easier to deploy in the wild, as availability of the whole dataset is not required to fit an individual shape.Such works are gaining ever-increasing popularity and we are led to believe that fitting an individual network is likely to become the common practice in learning INRs.Thus, in this paper, we investigate how to perform downstream tasks with deep learning pipelines on shapes represented as individual INRs.However, a single INR can easily count hundreds of thousands of parameters, though it is well known that the weights of a deep model provide a vastly redundant parametrization of the underlying function (Frankle & Carbin, 2018;Choudhary et al., 2020).Hence, we settle on investigating whether and how an answer to the above research question may be provided by a representation learning framework that learns to squeeze individual INRs into compact and meaningful embeddings amenable to pursuing a variety of downstream tasks.Our framework, dubbed inr2vec and shown in Fig. 1, has at its core an encoder designed to produce a task-agnostic embedding representing the input INR by processing only the INR weights.These embeddings can be seamlessly used in downstream deep learning pipelines, as we validate experimentally for a variety of tasks, like classification, retrieval, part segmentation, unconditioned generation, surface reconstruction and completion.Interestingly, since embeddings obtained from INRs live in low-dimensional vector spaces regardless of the underlying implicit function, the last two tasks can be solved by learning a simple mapping between the embeddings produced with our framework, e.g., by transforming the INR of a udf into the INR of an sdf .Moreover, inr2vec can learn a smooth latent space conducive to interpolating INRs representing unseen 3D objects.Additional details and code can be found at https://cvlab-unibo.github.io/inr2vec.Our contributions can be summarised as follows:\u2022 we propose and investigate the novel research problem of applying deep learning directly on individual INRs representing 3D shapes; \u2022 to address the above problem, we introduce inr2vec, a framework that can be used to obtain a meaningful compact representation of an input INR by processing only its weights, without sampling the underlying implicit function; \u2022 we show that a variety of tasks, usually addressed with representation-specific and complex frameworks, can indeed be performed by deploying simple deep learning machinery on INRs embedded by inr2vec, the same machinery regardless of the INRs underlying signal.(Choy et al., 2016;Girdhar et al., 2016;Jimenez Rezende et al., 2016;Stutz & Geiger, 2018;Wu et al., 2016;2015).The huge memory requirements of voxel-based representations, though, led researchers to look for less demanding alternatives, such as point clouds.Processing point clouds, however, is far from straightforward because of their unorganized nature.As a possible solution, some works projected the original point clouds to intermediate regular grid structures such as voxels (Shi et al., 2020) or images (You et al., 2018;Li et al., 2020a).Alternatively, PointNet (Qi et al., 2017a) proposed to operate directly on raw coordinates by means of shared multi-layer perceptrons followed by max pooling to aggregate point features.PointNet++ (Qi et al., 2017b) extended PointNet with a hierarchical feature learning paradigm to capture the local geometric structures.Following PointNet++, many works focused on designing new local aggregation operators (Liu et al., 2020), resulting in a wide spectrum of specialized methods based on convolution (Hua et al., 2018;Xu et al., 2018;Atzmon et al., 2018;Wu et al., 2019;Fan et al., 2021;Xu et al., 2021;Thomas et al., 2019), graph (Li et al., 2019;Wang et al., 2019a;b), and attention (Guo et al., 2021;Zhao et al., 2021) operators.Yet another completely unrelated set of deep learning methods have been developed to process surfaces represented as meshes, which differ in the way they exploit vertices, edges and faces as input data (Hu et al., 2022).Vertex-based approaches leverage the availability of a regular domain to encode the knowledge about points neighborhoods through convolution or kernel functions (Masci et al., 2015;Boscaini et al., 2016;Huang et al., 2019;Yang et al., 2020;Haim et al., 2019;Schult et al., 2020;Monti et al., 2017;Smirnov & Solomon, 2021).Edge-based methods take advantages of these connections to define an ordering invariant convolution (Hanocka et al., 2019), to construct a graph on the input meshes (Milano et al., 2020) or to navigate the shape structure (Lahav & Tal, 2020).Finally, Face-based works extract information from neighboring faces (Lian et al., 2019;Feng et al., 2019;Li et al., 2020b;Hertz et al., 2020).In this work, we explore INRs as a unified representation for 3D shapes and propose a framework that enables the use of the same standard deep learning machinery to process them, independently of the INRs underlying signal.Recent approaches have shown the ability of MLPs to represent implicitly diverse kinds of data such as objects (Genova et al., 2019;2020;Park et al., 2019;Atzmon & Lipman, 2020;Michalkiewicz et al., 2019;Gropp et al., 2020) and scenes (Sitzmann et al., 2019;Jiang et al., 2020;Peng et al., 2020;Chabra et al., 2020).The works focused on representing 3D data by MLPs rely on fitting implicit functions such as unsigned distance (for point clouds), signed distance (for meshes) (Atzmon & Lipman, 2020;Park et al., 2019;Gropp et al., 2020;Sitzmann et al., 2019;Jiang et al., 2020;Peng et al., 2020) or occupancy (for voxel grids) (Mescheder et al., 2019;Chen & Zhang, 2019).In addition to representing shapes, some of these models have been extended to encode also object appearance (Saito et al., 2019;Mildenhall et al., 2020;Sitzmann et al., 2019;Oechsle et al., 2019;Niemeyer et al., 2020), or to include temporal information (Niemeyer et al., 2019).Among these approaches, sinusoidal representation networks (SIREN) (Sitzmann et al., 2020b) use periodical activation functions to capture the high frequency details of the input data.In our work, we focus on implicit neural representations of 3D data represented as voxel grids, meshes and point clouds and we use SIREN as our main architecture.Deep learning on neural networks.Few works attempted to process neural networks by means of other neural networks.For instance, (Unterthiner et al., 2020) takes as input the weights of a network and predicts its classification accuracy.(Sch\u00fcrholt et al., 2021) learns a network representation with a self-supervised learning strategy applied on the N -dimensional weight array, and then uses the learned representations to predict various characteristics of the input classifier.(Knyazev et al., 2021;Jaeckle & Kumar, 2021;Lu & Kumar, 2020) represent neural networks as computational graphs, which are processed by a GNN to predict optimal parameters, adversarial examples, or branching strategies for neural network verification.All these works see neural networks as algorithms and focus on predicting properties such as their accuracy.On the contrary, we process networks that represent implicitly 3D shapes to perform a variety of tasks directly from their weights, i.e., we treat neural networks as input/output data.To the best of our knowledge, processing 3D shapes represented as INRs has been attempted only in contemporary work (Dupont et al., 2022).However, they rely on a shared network conditioned on shape-specific embeddings while we process the weights of individual INRs, that better capture the underlying signal and are easier to deploy in the wild."}
{"paper_id": 345, "abstract": "In the realm of self-supervised learning (SSL), most methodologies are honed on meticulously curated datasets, like the illustrious ImageNet. Yet, the natural world often reveals a more chaotic truth, showcasing data distributions that resemble a long, winding tail. In this exploration, we delve into the intricacies of one of the most esteemed forms of SSL\u2014contrastive methods\u2014specifically within the context of imbalanced data.  Our journey begins with a keen examination of the temperature parameter, denoted as $\\tau$, within the contrastive loss framework. Through the lens of average distance maximization, we uncover a fascinating duality: a larger $\\tau$ amplifies group-wise discrimination, while a smaller $\\tau$ sharpens the focus on instance discrimination. Traditionally, $\\tau$ has been relegated to the status of a mere constant hyperparameter, but we propose a bold new approach: a dynamic $\\tau$. By employing a simple cosine schedule, we reveal the potential for significant enhancements in the quality of learned representations.  This innovative schedule orchestrates a constant 'task switching' dance between the realms of instance and group-wise discrimination. It ensures that our model becomes adept at capturing both the broader strokes of group characteristics and the finer details of individual instances. This balance is crucial; while frequent classes thrive on the former, the rarer classes demand the latter. Our findings demonstrate that this method consistently elevates class separation in long-tail distributions, all without incurring additional computational costs. In essence, we unveil a path forward, harmonizing the strengths of SSL with the complexities of real-world data.", "introduction": "Deep Neural Networks have shown remarkable capabilities at learning representations of their inputs that are useful for a variety of tasks.Especially since the advent of recent self-supervised learning (SSL) techniques, rapid progress towards learning universally useful representations has been made.Currently, however, SSL on images is mainly carried out on benchmark datasets that have been constructed and curated for supervised learning (e.g.ImageNet (Deng et al., 2009), CIFAR (Krizhevsky et al., 2009), etc.).Although the labels of curated datasets are not explicitly used in SSL, the structure of the data still follows the predefined set of classes.In particular, the class-balanced nature of curated datasets could result in a learning signal for unsupervised methods.As such, these methods are often not evaluated in the settings they were designed for, i.e. learning from truly unlabelled data.Moreover, some methods (e.g.(Asano et al., 2019;Caron et al., 2020)) even explicitly enforce a uniform prior over the embedding or label space, which cannot be expected to hold for uncurated datasets.In particular, uncurated, real-world data tends to follow long-tail distributions (Reed, 2001), in this paper, we analyse SSL methods on long-tailed data.Specifically, we analyse the behaviour of contrastive learning (CL) methods, which are among the most popular learning paradigms for SSL.In CL, the models are trained such that embeddings of different samples are repelled, while embeddings of different 'views' (i.e.augmentations) of the same sample are attracted.The strength of those attractive and repelling forces between samples is controlled by a temperature parameter \u03c4 , which has been shown to play a crucial role in learning good representations (Chen et al., 2020c;a).To the best of our knowledge, \u03c4 has thus far almost exclusively been treated as a constant hyper-parameter.In contrast, we employ a dynamic \u03c4 during training and show that this has a strong effect on the learned embedding space for long-tail distributions.In particular, by introducing a simple schedule for \u03c4 we consistently improve the representation quality across a wide range of settings.Crucially, these gains are obtained without additional costs and only require oscillating \u03c4 with a cosine schedule.This mechanism is grounded in our novel understanding of the effect of temperature on the contrastive loss.In particular, we analyse the contrastive loss from an average distance maximisation perspective, which gives intuitive insights as to why a large temperature emphasises group-wise discrimination, whereas a small temperature leads to a higher degree of instance discrimination and more uniform distributions over the embedding space.Varying \u03c4 during training ensures that the model learns both group-wise and instance-specific features, resulting in better separation between head and tail classes.Overall, our contributions are summarised as follows: \u2022 we carry out an extensive analysis of the effect of \u03c4 on imbalanced data; \u2022 we analyse the contrastive loss from an average distance perspective to understand the emergence of semantic structure; \u2022 we propose a simple yet effective temperature schedule that improves the performance across different settings; \u2022 we show that the proposed \u03c4 scheduling is robust and consistently improves the performance for different hyperparameter choices."}
{"paper_id": 346, "abstract": "In the ever-evolving landscape of neural network optimization, soft threshold pruning emerges as a beacon of innovation, showcasing remarkable performance that rivals the best in the field. Yet, despite its prowess, previous approaches have often wandered aimlessly through the labyrinth of threshold scheduling or opted for a simplistic trainable threshold, leaving a void in theoretical understanding. In this study, we embark on a quest to redefine soft threshold pruning, recasting it as an implicit optimization conundrum addressed through the venerable Iterative Shrinkage-Thresholding Algorithm (ISTA), a stalwart in the realms of sparse recovery and compressed sensing.  Within the sanctum of this theoretical framework, we unveil a unifying narrative: all prior threshold tuning strategies can be interpreted as distinct manifestations of tuning the $L_1$-regularization term. Our exploration leads us to the discovery of an optimal threshold scheduler, meticulously crafted through a thorough investigation of threshold dynamics. This scheduler maintains a steadfast $L_1$-regularization coefficient, which in turn suggests the existence of a time-invariant objective function, enriching our understanding of optimization.  In essence, the pruning algorithm we derive possesses the potential to sparsify any mathematical model trained via Stochastic Gradient Descent (SGD). Our extensive experiments validate its superiority, demonstrating state-of-the-art performance across a spectrum of architectures, including Artificial Neural Networks such as ResNet-50 and MobileNet-V1, as well as Spiking Neural Networks like SEW ResNet-18, all tested on the formidable ImageNet datasets. Building upon this foundational framework, we unveil a suite of pruning methodologies\u2014ranging from sparsification during training to early pruning and initialization pruning. For those eager to delve deeper, our code is readily accessible at https://github.com/Yanqi-Chen/LATS.", "introduction": "Pruning has been a thriving area of network compression.Since the day deep neural networks stretch their tentacles to every corner of machine learning applications, the demand for shrinking the size of network parameters has never stopped growing.Fewer parameters usually imply less computing burden on resource-constrained hardware such as embedded devices or neuromorphic chips.Some pioneering studies have revealed considerable redundancies in both Artificial Neural Networks (ANNs) (Han et al., 2015;2016;Wen et al., 2016;Liu et al., 2017) and Spiking Neural Networks (SNNs) (Qi et al., 2018;Chen et al., 2021;Yin et al., 2021;Deng et al., 2021;Kundu et al., 2021;Kim et al., 2022b).In essence, pruning can be formulated as an optimization problem under constraint on L 0 norm, the number of nonzero components in network parameters.Assuming L is the loss function of vectorized network weight w, we expect lower L 0 norm \u2225w\u2225 0 along with lower loss L(w).Despite different formulations like hard constraints min L(w)\u2264c \u2225w\u2225 0 ;(1)or soft constraints (penalized) min w {L(w) + \u00b5\u2225w\u2225 0 },all these forms are without exception NP-Hard (Natarajan, 1995;Davis et al., 1997;Nguyen et al., 2019).Relaxing L 0 norm to L p (0 < p < 1) norm will not make it more tractable for it is still strongly NP-Hard (Ge et al., 2011).Nowadays, research on pruning and sparse optimization is mainly focused on the L 1 -regularized problem, the tightest convex relaxation of L 0 norm, which dates back to a series of groundbreaking studies on compressed sensing (Donoho, 2006;Cand\u00e8s et al., 2006).These researches technically allows us to solve L 1 -regularized problem as an alternative or, sometimes even an equivalent option (Cand\u00e8s, 2008) to confront L 0 norm constraint.A variety of modern methods such as magnitude-based pruning are still firmly rooted in solving the L 1 regularized optimization problem.Be that as it may, L 1 regularization is mostly employed for shrinking the magnitude of weight before the hard thresholding step, which has started to be replaced by other sorts of novel regularization (Zhuang et al., 2020).In the past few years, a new range of pruning methods based on soft threshold reparameterization of weights has been developing gradually.The term \"reparameterization\" here refers to a specific mapping to network weights w from a latent space of hidden parameters \u03b8.The \"geometry\" of latent space could be designed for guiding actual weights w towards sparsity.In soft threshold pruning, the mapping is an element-wise soft threshold function with time-variant threshold.Among these studies, two representative ones are Soft Threshold weight Reparameterization (STR) (Kusupati et al., 2020) and State Transition of Dendritic Spines (STDS) (Chen et al., 2022).They both achieve the best performance of that time.STDS further demonstrates the analogy between soft threshold mapping and a structure in biological neural systems, i.e., dendritic filopodia and mature dendritic spines.However, few researchers notice that soft threshold mapping also appear as the shrinkage operator in the solution of LASSO (Tibshirani, 1996) when the design matrix is orthonormal.The studies on LASSO further derives the Iterative Shrinkage-Thresholding Algorithm (ISTA) (Daubechies et al., 2004;Elad, 2006), which used to be popularized in sparse recovery and compressed sensing.ISTA has many variants (Bioucas-Dias & Figueiredo, 2007;Beck & Teboulle, 2009b;Bayram & Selesnick, 2010) and has long been certified as an effective sparsification methods in all sorts of fields like deep learning (He et al., 2017;Zhang et al., 2018;Bai et al., 2020), computer vision (Beck & Teboulle, 2009a;Dong et al., 2013), medical imageology (Lustig et al., 2007;Otazo et al., 2015) and geophysics (Herrmann & Hennenfent, 2008).Despite an abecedarian analysis on the similarity between STDS and ISTA, many issues remains to be addressed, such as 1) the exact equivalence between ISTA and the growing threshold in soft threshold pruning, 2) the necessity of setting threshold trainable in STR, and 3) the way to improve existing methods without exhaustively trying different tricks for scheduling threshold.In this work, we proposed a theoretical framework serving as a bridge between the underlying L 1regularized optimization problem and threshold scheduling.The bridge is built upon the key finding that soft threshold pruning is an implicit ISTA for nonzero weights.Specifically, we prove that the L 1 coefficient in the underlying optimization problem is determined by both threshold and learning rate.In this way, any threshold tuning strategy can now be interpreted as a scheme for tuning L 1 penalty.We find that a time-invariant L 1 coefficient lead to performance towering over previous pruning studies.Moreover, we bring a strategy of tuning L 1 penalty called continuation strategy (Xiao & Zhang, 2012), which was once all the rage in the field of sparse optimization, to the field of pruning.It derives broad categories of algorithms covering several tracks in the present taxonomy of pruning.In brief, our contributions are summarized as follows:\u2022 Theoretical cornerstone of threshold tuning strategy.To the best of our knowledge, this is the first work interpreting increasing threshold as an ever-changing regularized term.Under theoretical analysis, we present a unified framework for the local equivalence of ISTA and soft threshold pruning.It enables us to make a comprehensive study on threshold tuning using the classic method in sparse optimization.\u2022 Learning rate adapted threshold scheduler.Through our proposed framework, we reveal the strong relation between the learning rate scheduler and the threshold scheduler.Then we show that an time-invariant L 1 coefficient requires the changing of threshold being proportional to the learning rate.The Learning rate Adapted Threshold Scheduler (LATS) built upon L 1 coefficient achieves a state-of-the-art performance-sparsity tradeoff on both deep ANNs and SNNs.\u2022 Sibling schedulers cover multiple tracks of pruning.We propose an early pruning algorithm by translating the homotopy continuation algorithm into a pruning algorithm with our framework.It achieves indistinguishable performance to LATS as a conventional early pruning method.Moreover, the algorithm in the pruning-at-initialization setting erases some subsequent layers in ResNet and maintains the identity mapping, shrinking deep ResNet to a shallow one."}
{"paper_id": 347, "abstract": "In the ever-evolving landscape of machine learning, Self-Supervised Learning (SSL) emerges as a powerful ally, harnessing the vast ocean of unlabeled data to forge robust models. Recent empirical investigations have illuminated SSL\u2019s potential to thrive even amidst the tumult of distribution shifts, where the realms of training and downstream data diverge. Yet, the theoretical underpinnings of its transferability remain shrouded in mystery.   In this paper, we embark on a quest to unravel these complexities, crafting a theoretical framework that scrutinizes the interplay between self-supervised contrastive learning and the art of data augmentation. Our findings unveil a crucial truth: the effectiveness of contrastive learning in downstream tasks hinges significantly on the chosen data augmentations. Furthermore, we uncover a fundamental limitation\u2014contrastive learning often falters in its quest to capture domain-invariant features, a shortcoming that hinders its transferability across different contexts.  Armed with these insights, we introduce a groundbreaking approach: Augmentation-robust Contrastive Learning (ArCL). This method not only ensures the acquisition of domain-invariant features but also seamlessly integrates with existing contrastive learning frameworks. Through rigorous experimentation across diverse datasets, we demonstrate that ArCL markedly enhances the transferability of contrastive learning, paving the way for more resilient models in the face of shifting data landscapes.", "introduction": "A common assumption in designing machine learning algorithms is that training and test samples are drawn from the same distribution.However, this assumption may not hold in real-world applications, and algorithms may suffer from distribution shifts, where the training and test distributions differ.This issue has motivated a plethora of research in various settings, such as transfer learning, domain adaptation and domain generalization (Blanchard et al., 2011;Muandet et al., 2013;Wang et al., 2021a;Shen et al., 2021).Different ways of characterizing the relationship between test and training distributions lead to different algorithms.Most literature studies this in the supervised learning scenario.It aims to find features that capture some invariance across different distributions, and assume that such invariance also applies to test distributions (Peters et al., 2016;Rojas-Carulla et al., 2018;Arjovsky et al., 2019;Mahajan et al., 2021;Jin et al., 2020;Ye et al., 2021).Self-Supervised Learning (SSL) has attracted great attention in many fields (He et al., 2020;Chen et al., 2020;Grill et al., 2020;Chen & He, 2021;Zbontar et al., 2021).It first learns a representation from a large amount of unlabeled training data, and then fine-tunes the learned encoder to obtain a final model on the downstream task.Due to its two-step nature, SSL is more likely to encounter the distribution shift issue.Exploring its transferability under distribution shifts has become an important topic.Some recent works study this issue empirically (Liu et al., 2021;Goyal et al., 2021;von K\u00fcgelgen et al., 2021;Wang et al., 2021b;Shi et al., 2022).However, the theoretical understanding is still limited, which also hinders the development of algorithms.In this paper, we study the transferability of self-supervised contrastive learning in distribution shift scenarios from a theoretical perspective.In particular, we investigate which downstream distribu-tions will result in good performance for the representation obtained by contrastive learning.We study this problem by deriving a connection between the contrastive loss and the downstream risk.Our main finding is that data augmentation is essential: contrastive learning provably performs well on downstream tasks whose distributions are close to the augmented training distribution.Moreover, the idea behind contrastive learning is to find representations that are invariant under data augmentation.This is similar to the domain-invariance based supervised learning methods, since applying each kind of augmentation to the training data can be viewed as inducing a specific domain.Unfortunately, from this perspective, we discover that contrastive learning fails to produce a domain-invariant representation, limiting its transferability.To address this issue, we propose a new method called Augmentation-robust Contrastive Learning (ArCL), which can be integrated with various widely used contrastive learning algorithms, such as SimCLR (Chen et al., 2020) and MoCo (He et al., 2020).In contrast to the standard contrastive learning, ArCL forces the representation to align the two farthest positive samples, and thus provably learns domain-invariant representations.We conducted experiments on various downstream tasks to test the transferability of representations learned by ArCL on CIFAR10 and ImageNet.Our experiments demonstrate that ArCL significantly improves the standard contrastive learning algorithms.Distribution shift in supervised learning.Distribution shift problem has been studied in many literature (Blanchard et al., 2011;Muandet et al., 2013;Wang et al., 2021a;Shen et al., 2021).Most works aim to learn a representation that performs well on different source domains simultaneously (Rojas-Carulla et al., 2018;Mahajan et al., 2021;Jin et al., 2020), following the idea of causal invariance (Peters et al., 2016;Arjovsky et al., 2019).Structural equation models are often assumed for theoretical analysis (von K\u00fcgelgen et al., 2021;Liu et al., 2020;Mahajan et al., 2021).Distributionally robust optimization optimizes a model's worst-case performance over some uncertainty set directly (Krueger et al., 2021;Sagawa et al., 2019;Duchi & Namkoong, 2021;Duchi et al., 2021).Stable learning (Shen et al., 2020;Kuang et al., 2020) learns a set of global sample weights that could remove the confounding bias for all the potential treatments from data distribution.Disentangled representation learning (Bengio et al., 2013;Tr\u00e4uble et al., 2021;Kim & Mnih, 2018) aims to learn representations where distinct and informative factors of variations in data are separated.Theoretical understanding of contrastive learning.A number of recent works also aim to theoretically explain the success of contrastive learning in IID settings.One way to explain it is through the mutual information between positive samples (Tian et al., 2020;Hjelm et al., 2018;Tschannen et al., 2019).Arora et al. (2019) directly analyze the generalization of InfoNCE loss based on the assumption that positive samples are drawn from the same latent classes.In the same setting, Bao et al. (2022) establish equivalence between InfoNCE and supervised loss and give sharper upper and lower bounds.Huang et al. (2021) take data augmentation into account and provide generalization bounds based on the nearest neighbor classifier.Contrastive learning in distribution shift.Shen et al. (2022) andHaoChen et al. (2022) study contrastive learning in unsupervised domain adaptation, where unlabeled target data are obtained.Shi et al. (2022) show that SSL is the most robust on distribution shift datasets compared to autoencoders and supervised learning.Hu et al. (2022) improve the out-of-distribution performance of SSL from an SNE perspective.Other robust contrastive learning methods (Kim et al., 2020;Jiang et al., 2020) focus on adversarial robustness while this paper focuses on distributional robustness."}
{"paper_id": 348, "abstract": "In the ever-evolving realm of fluid dynamics, we embark on a daring quest: to unravel the intricate dance of 3D flow and volumetric densities from the fleeting glimpses captured in a single monocular video. This endeavor, fraught with complexity, might seem insurmountable at first glance. Yet, we unveil a path forward\u2014one that eschews the need for any 3D ground truth during training. Instead of relying on synthetic constructs that often fall short, we harness the power of real-world observations, drawing from the richness of actual capture setups.  Our journey begins with the creation of an initial prototype volume, a nascent form that we then guide and manipulate through the passage of time, all without the crutch of volumetric supervision. This innovative approach thrives on the synergy of image-based losses, the keen insight of an adversarial discriminator network, and the stability offered by regularization techniques. The result? A method capable of estimating long-term sequences with remarkable stability, deftly capturing the elegance of phenomena like rising smoke plumes, and achieving an astonishing fidelity to the targets we seek. In this work, we forge a new frontier in unsupervised training, illuminating the path for future explorations in the fluidic arts.", "introduction": "Estimating motion is a fundamental problem, and is studied for a variety of settings in two and three dimensions (Ranjan & Black, 2017;Hur & Roth, 2021;Gregson et al., 2014).It is also a highly challenging problem, since the motion u is a secondary quantity that typically can't be measured directly and has to be recovered from changes observed in transported markers \u03c1.We focus on volumetric, momentum-driven materials like fluids, where in contrast to the single-step estimation in optical flow (OF), motion estimation typically considers multiple coupled steps to achieve a stable global transport.Furthermore, in this setting the volume distribution of markers \u03c1 is usually unknown and needs to be reconstructed from the observations in parallel to the motion estimation.So far, most research is focused on the reconstruction of single scenes.Classic methods use an optimization process working with an explicit volumetric representation (Eckert et al., 2019;Zang et al., 2020;Franz et al., 2021) while some more recent approaches optimize single scenes with neural fields (Mildenhall et al., 2020;Chu et al., 2022).As such an optimization is typically extremely costly, and has to be redone for each new scene, training a neural network to infer an estimate of the motion in a single pass is very appealing.Similar to most direct optimizations, existing neural network methods rely on multiple input views to simplify the reconstruction (Qiu et al., 2021).However, this severely limits the settings in which inputs can be captured, as a fully calibrated lab environment is often the only place where such input sequences can be recorded.The flexibility of motion estimation from single views makes them a highly attractive direction, and physical priors in the form of governing equations make this possible in the context of fluids (Eckert et al., 2018;Franz et al., 2021).Nonetheless, despite using strong priors, the single viewpoint makes it challenging to adequately handle the otherwise fully unconstrained depth dimension.We target a deep learning-based approach where a neural network learns to represent the underlying motion structures, such that almost instantaneous, single-pass motion inference is made possible without relying on ground truth motion data.The latter is especially important for complex volumetric motions, as reference motions of real fluids can not be acquired directly.Instead, one has to work with reconstructions or even simulated data, suffering from a mismatch between the observations and the synthetic motion data.While obtaining multiple calibrated captures for training is feasible, using additional views only for losses results in issues with the depth ambiguity of single-view inputs.In this work, we address the challenging problem of training neural networks to infer 3D motions from monocular videos in scenarios where no 3D reference data is available.To the best of our knowledge, we are to first to propose an end-to-end approach, denoted by Neural Global Transport (NGT) in the following, which (i) yields a neural network to estimate a global, dense 3D velocity from a single-view image sequence without requiring any 3D ground truth as targets.Among others, this is made possible by a custom 2D-to-3D UNet architecture.(ii) We address the resulting depth-ambiguity problem using a new approach with differentiable rendering and an adversarial technique.(iii) A single network trained with the proposed approach generalizes across a range of different inputs, vastly outperforming optimization-based approaches in terms of performance."}
{"paper_id": 349, "abstract": "In the realm of machine learning, where the balance of privacy and performance hangs delicately in the balance, the advent of Differential Private Stochastic Gradient Descent with Gradient Clipping (DP-SGD-GC) heralds a promising path forward. This algorithm, a beacon of hope for those seeking to train models while safeguarding sensitive information, has garnered significant attention. Yet, lurking in the shadows of its popularity lies a conundrum: its convergence in unbounded domains, particularly when the Lipschitz continuity condition is absent, remains shrouded in uncertainty. Previous analyses have either tethered themselves to additional assumptions or yielded utility bounds marred by persistent, non-vanishing bias terms.  In this work, we embark on a journey to illuminate this obscured territory. For smooth, unconstrained optimization problems, we refine the existing frameworks and unveil a compelling revelation: DP-SGD-GC can indeed achieve a vanishing utility bound devoid of any bias. Moreover, when the noise from subsampled gradients exhibits light-tailed behavior, we demonstrate that DP-SGD-GC can rival the utility bounds of its Lipschitz-continuous counterparts. As a noteworthy extension of our findings, we introduce a novel clipping technique\u2014value clipping\u2014designed to alleviate the computational burdens traditionally imposed by classic gradient clipping methods. To substantiate our theoretical advancements, we present a series of experiments conducted on standard benchmark datasets, providing a robust foundation for our claims. In this intersection of privacy and performance, we forge a new path, one that promises not only to protect but also to empower.", "introduction": "Training machine learning models that can achieve decent prediction accuracy while preserving data privacy is fundamental in many modern machine learning applications.The concept of differential privacy (DP) from Dwork (2006); Dwork & Roth (2014) offers an elegant mathematical framework to characterize the privacy-preserving ability of randomized algorithms, which has been widely applied to tasks including clustering, regression, principle component analysis, empirical-risk minimization, matrix completion, graph distance estimation, optimization and deep learning (Chaudhuri & Monteleoni, 2008;Chaudhuri et al., 2011;Agarwal et al., 2018;Ge et al., 2018;Jain et al., 2018;Fan & Li, 2022;Fan et al., 2022).For the empirical-risk minimization (ERM) problem, among many proposed methods, differential private stochastic gradient descent (DP-SGD) is an effective algorithm that can solve the ERM problem with a privacy guarantee and achieve a reasonable utility bound.DP-SGD has received substantial interest in recent years due to its simplicity and effectiveness (Song et al., 2013;Bassily et al., 2014;Abadi et al., 2016;Wang et al., 2017;Bassily et al., 2019;Feldman et al., 2020;Asi et al., 2021).In the classic analysis of DP-SGD, the variance of the Gaussian noise used in each iteration of DP-SGD relies crucially on the \u2113 2 -sensitivity of the loss function.Therefore most early works on DP-SGD assume each individual loss function to be Lipschitz continuous in its domain (Song et al., 2013;Bassily et al., 2014).However, many real-world problems are only smooth but not globally Lipschitz continuous; for example, the unconstrained linear regression problem.There are two techniques to circumvent the Lipschitz continuous assumption: (i) imposing an additional bounded domain constraint to the original problem; (ii) clipping gradients in their 2-norm and using the clipped gradients to update the model (Abadi et al., 2016).In practice, the gradient clipping technique is usually more preferred than imposing a bounded domain constraint because the latter requires prior knowledge of the distance between initialization and solution, which is typically unavailable for unconstrained problems.In summary, the state-of-the-art implementations of DP-SGD all advocate the gradient clipping technique.Table 1: The utility bound and assumptions needed by different algorithms for convex problems, where d is the problem size, n is the number of data points and \u03f5 measures the privacy-preserving ability; see Section 3 for more details.\" \u2020\" is based on a trivial extension of Bassily et al. (2014).\u2022 This work is theoretical in essence but also includes a practical contribution (Section 5).We develop a novel value clipping technique for problems that satisfy the weak growth condition (Definition 3.1).The proposed value clipping technique can be implemented within one forwardbackward propagation on existing learning platforms and can alleviate the computation overhead caused by gradient clipping.The efficiency of value clipping is demonstrated on real datasets."}
{"paper_id": 350, "abstract": "In the realm of learning, a multitude of challenges can be distilled to the fundamental act of sampling \\( k \\) elements from a vast universe of \\( n \\) elements. Yet, this seemingly straightforward operation poses a conundrum when it comes to integration within differentiable models, necessitating clever approximations. The prevailing strategies have leaned heavily on an \\emph{order sampling} framework, relying on differentiable approximations of the Top-\\( k \\) operator to sift through and select the largest \\( k \\) elements from a given set.   However, we propose a refreshing alternative: a method rooted in \\emph{conditional Poisson sampling}. This innovative approach diverges from the constraints of order sampling, boasting a parallel complexity that remains unaffected by the size of the subset. This scalability opens the door to handling larger subsets with ease. Our adaptation not only enhances efficiency but also aligns seamlessly with discrete gradient approximations, making it suitable for integration into differentiable models.   Moreover, our method introduces a novel twist: the subset size parameter \\( k \\) can now be treated as differentiable, adding a layer of flexibility previously unattainable. We put our approach to the test across various applications, including model explanation, image sub-sampling, and stochastic \\( k \\)-nearest neighbor tasks. The results speak volumes, showcasing our method's superiority in accuracy, efficiency, and scalability when compared to existing alternatives.", "introduction": "The fundamental combinatorial operation of selecting subsets of elements from a given universe is ever increasingly being incorporated in differentiable neural models due to its range of applicability.Example applications include model explanations (Chen et al., 2018), sequence modeling (Kool et al., 2019), point cloud modeling (Yang et al., 2019), and nearest neighbor networks (Grover et al., 2018).Current neural network approaches for sampling subsets generally fall in the class of order sampling methods.In the order sampling scheme, each element in the universe is assigned an independent ranking random variable.To obtain a subset sample of size k, the largest (or smallest) k elements are chosen.Thereby, the ranking variable distribution induces a probability distribution over the possible subsets.However, the operation of choosing the largest k elements (Top-k) is naturally not differentiable, since it is a discrete operation.This means that the Top-k procedure cannot be directly used in gradient learning models.This has led to a number of proposals of relaxed and differentiable versions of the Top-k operator (Goyal et al., 2018;Pietruszka et al., 2021;Pl\u00f6tz & Roth, 2018).Building on Top-k approaches several methods of sampling subsets as k-hot vectors have appeared in the literature (Paulus et al., 2020;Xie & Ermon, 2019).In this paper, we explore Poisson sampling (Till\u00e9, 2006) and conditional Poisson sampling (H\u00e1jek & Dupa\u010d, 1981) as an alternative to order sampling for subsets.With Poisson sampling, each element in the set is independently drawn to be selected for the subset or not.As these independent trials cannot guarantee a fixed size for subsets, with conditional Poisson sampling, the Poisson sampling procedure is conditioned to return subsets of exactly k elements.In practice, the conditioning amounts to repeating the Poisson sampling procedure until a subset of size k is obtained.The general (conditional) Poisson sampling approach has a number of features which make it an attractive alternative to Top-k-based order sampling methods.Firstly, the sampling is done independently in Poisson sampling methods, which makes the procedure very efficient for sampling subsets with large values of k.By contrast, current Top-k methods (Goyal et al., 2018;Pl\u00f6tz & Roth, 2018) often have an inner loop depending on k, which makes them expensive for sampling large subsets in terms of both time and memory.Furthermore, computations in modern neural network models are vectorized.This makes sampling different subset size k for different elements in a batch difficult for current Top-k procedures, since the number of sampling iterations to obtain the Top-k elements depend on k.With Poisson sampling, it is trivial to sample different subset sizes for batched inputs, making it ideally suited to vectorized computation.Finally, with Top-k, the subset size k itself is not differentiable.With Poisson sampling, k appears as a scaling parameter for the probabilities of the individual elements in the universe.Therefore, the subset size parameter k can easily be incorporated in differentiable computations when having a differentiable sampling procedure.Despite the aforementioned advantages, there are two difficulties with Poisson sampling.The first is that vanilla Poisson sampling can lead to large variance in the sampled subset size.This can be resolved with conditional Poisson sampling to obtain exact samples, but only at the cost of high computational complexity.The second (and main) difficulty is that neither Poisson sampling nor its conditional variant are differentiable and cannot be directly included in differentiable models.In this paper, and in the context of differentiable subset sampling with neural networks, we propose neural conditional Poisson subset sampling.We note that often we do not need subsets of k elements exactly, as conditional Poisson sampling would have us do, and instead sampling k-sized subsets in expectation is enough.With neural conditional Poisson subset sampling, we relax the constraint of sampling exactly k elements, thus allowing to trade off accuracy in the subset size for efficiency of sampling large subsets.Compared to Top-k approaches for sampling subsets (Xie & Ermon, 2019), neural conditional Poisson subset sampling allows for efficient sampling of large subsets, easy choice of per-instance subset sizes, and differentiable subset sizes for a small loss in subset size accuracy, when an exact number of elements in the sampled subsets is not a necessity.Secondly, we adapt the sampling procedure so that gradient approximations for discrete variables are applicable.The resulting method is scalable and can be used to sample large subsets even from image-size domains in full resolution -a task that is to date infeasible for current subset sampling methods."}
{"paper_id": 351, "abstract": "In the realm of object-centric representation learning, we unveil a novel approach: topology-aware feature partitioning into \\( k \\) distinct segments, tailored specifically for the intricate tapestry of scene features. Imagine a method that employs minimum \\( s \\)-\\( t \\) graph cuts, elegantly framed as a linear programming challenge. This method is not merely an abstract construct; it is imbued with a profound awareness of topology, meticulously encoding the neighborhood relationships that weave through the image graph.  To navigate the complexities of these graph cuts, we harness an efficient, scalable, and differentiable approximation to quadratic programming. By leveraging optimizations tailored to the unique challenges of cut problems, we significantly enhance the efficiency of our solution, allowing us to compute gradients with a speed and precision that eclipses traditional quadratic programming methods.  Our empirical results are nothing short of compelling. They demonstrate that our approach is not only scalable but also surpasses existing techniques in the realm of object discovery, particularly within richly textured scenes and objects. In this way, we stand at the forefront of a new horizon in representation learning, where the intricate dance of topology and efficiency leads to breakthroughs in understanding the visual world.", "introduction": "Object-centric representation learning aims to learn representations of individual objects in scenes given as static images or video.Object-centric representations can potentially generalize across a range of computer vision tasks by embracing the compositionality inherent in visual scenes arising from the interaction of mostly independent entites.(Burgess et al., 2019;Locatello et al., 2020;Elsayed et al., 2022).One way to formalize object-centric representation learning is to consider it as an input partitioning problem.Here we are given a set of spatial scene features, and we want to partition the given features into k per-object features, or slots, for some given number of objects k.A useful requirement for a partitioning scheme is that it should be topology-aware.For example, the partitioning scheme should be aware that points close together in space are often related and may form part of the same object.A related problem is to match object representations in two closely related scenes, such as frames in video, to learn object permanence across space and time.In this paper we focus on differentiable solutions for the partitioning and matching problems that are also efficient and scalable for object-centric learning.We formulate the topology-aware k-part partitioning problem as the problem of solving k minimum s-t cuts in the image graph (see Figure 1) and the problem of matching as a bipartite matching problem.An interesting feature of the minimum s-t cut and bipartite matching problems is that they can both be formulated as linear programs.We can include such programs as layers in a neural network by parameterizing the coefficients of the objective function of the linear program with neural networks.However, linear programs by themselves are not continuously differentiable with respect to the objective function coefficients (Wilder et al., 2019).A greater problem is that batch solution of linear programs using existing solvers is too inefficient for neural network models, especially when the programs have a large number of variables and constraints.We solve these problems by 1) approximating linear programs by regularized equality constrained quadratic programs, and 2) precomputing the optimality condition (KKT matrix) factorizations so that optimality equations can be quickly solved during training.The advantage of using equality constrained quadratic programs is that they can be solved simply from the optimality conditions.Combined with the appropriate precomputed factorizations for the task of object-centric learning, the optimality conditions can be solved very efficiently during training.Algorithm 1 Feature k-Part Partitioning Require: Input features x of dimension C \u00d7 H \u00d7 W with C channels, height H and width W 1: Compute quadratic program parameters y i = f i y (x), for i \u2208 {1, . . .k} where f i q are CNNs.2: Optionally transform spatial features x f = f x (x), where f x is an MLP transform acting on the channel dimension.x f has dimension D \u00d7 H \u00d7 W . 3: Solve regularized quadratic programs for minimum s-t cut and extract vertex variables z i = qsolve(y i ) for each y i .Each z i has dimension H \u00d7 W . 4: Normalize z i across cuts i = 1, ..., k for each pixel with a temperature-scaled softmax.5: Multiply z i with x f along H, W for each i to obtain K masked features maps r i .6: Return r i as the k-partition.A second advantage of using quadratic programming approximations is that quadratic programs can be differentiated relative to program parameters using the implicit function theorem as shown in prior literature (Barratt, 2018;Amos and Kolter, 2017).To learn the objective coefficients of the cut problem by a neural network, the linear program needs to be solved differentiably, like a hidden layer.For this, we can relax the linear program to a quadratic program and employ techniques from differentiable mathematical programming (Wilder et al., 2019;Barratt, 2018) to obtain gradients.This amounts to solving the KKT optimality conditions which then result in the gradients relative to the parameters of the quadratic program (Amos and Kolter, 2017;Barratt, 2018).However, with a naive relaxation, the required computations for both the forward and backward pass are still too expensive for use in object-centric representation learning applications.Given that the techniques generally employed for differentiably solving quadratic programs are limited to smaller program sizes (Amos and Kolter, 2017), we introduce optimizations in the gradient computation specific to the problem of solving graph cuts for image data.For instance, we note that the underlying s-t flow graph remains unchanged across equally-sized images, allowing us to pre-compute large matrix factorization.Furthermore, we replace the forward pass by a regularized equality constrained quadratic program constructed from the linear programming formulation of the minimum s-t cut problem.When combined with these task specific optimizations, equality constrained quadratic programs can be solved significantly more efficiently than general quadratic programs with mixed equality and inequality constraints (Wright and Nocedal, 1999).The regularization of slack variables ensures that the output of the new quadratic program can still be interpreted as an s-t cut solution.The use of sparse matrix computations in the forward and backward passes ensures that time and memory usage is significantly reduced.To summarize, we make the following contributions in this paper.1. We formulate object-centric representation learning in terms of partitioning and matching.2. We propose s-t cuts in graphs for topology-aware partitioning with neural networks.3. We propose to use regularized equality constrained quadratic programs as a differentiable, general, efficient and scalable scheme for solving partitioning and matching problems with neural networks."}
{"paper_id": 352, "abstract": "In the realm of training deep neural networks, a formidable foe lurks in the shadows: the ever-diminishing GPU memory. This precious resource is often consumed by the insatiable need to cache intermediate tensors, vital for the backward pass and gradient computation. Yet, in the face of this challenge, we present a groundbreaking solution\u2014Dropping Intermediate Tensors, or DropIT for short.   Imagine a method that skillfully discards the least significant elements of these intermediate tensors, a clever maneuver that allows us to approximate gradients from a sparser representation during the backward pass. Theoretically, this approach not only slashes the memory footprint but also diminishes the noise in gradient estimation, leading to a convergence rate that outpaces traditional stochastic gradient descent.   Our experiments reveal a remarkable truth: we can eliminate up to 90% of the intermediate tensor elements in both fully-connected and convolutional layers, all while boosting testing accuracy across a variety of tasks\u2014from classification to object detection and instance segmentation\u2014on Visual Transformers and Convolutional Neural Networks alike.   For those eager to delve deeper into this innovative technique, our code and models await you at https://github.com/chenjoya/dropit. Join us in this quest to conquer the hardware bottleneck and unlock the full potential of deep learning!", "introduction": "The training of state-of-the-art deep neural networks (DNNs) (Krizhevsky et al., 2017;Simonyan & Zisserman, 2015;He et al., 2016;Vaswani et al., 2017;Dosovitskiy et al., 2021) for computer vision often requires a large GPU memory.For example, training a simple visual transformer detection model ViTDet-B (Li et al., 2022), with its required input image size of 1024\u00d71024 and batch size of 64, requires \u223c700 GB GPU memory.Such a high memory requirement makes the training of DNNs out of reach for the average academic or practitioner without access to high-end GPU resources.When training DNNs, the GPU memory has six primary uses (Rajbhandari et al., 2020): network parameters, parameter gradients, optimizer states (Kingma & Ba, 2015), intermediate tensors (also called activations), temporary buffers, and memory fragmentation.Vision tasks often require training with large batches of high-resolution images or videos, which can lead to a significant memory cost for intermediate tensors.In the instance of ViTDet-B, approximately 70% GPU memory cost (\u223c470 GB) is assigned to the intermediate tensor cache.Similarly, for NLP, approximately 50% of GPU memory is consumed by caching intermediate tensors for training the language model GPT-2 (Radford et al., 2019;Rajbhandari et al., 2020).As such, previous studies (Gruslys et al., 2016;Chen et al., 2016;Rajbhandari et al., 2020;Feng & Huang, 2021) treat the intermediate tensor cache as the largest consumer of GPU memory.For differentiable layers, standard implementations store the intermediate tensors for computing the gradients during back-propagation.One option to reduce storage is to cache tensors from only some layers.Uncached tensors are recomputed on the fly during the backward pass -this is the strategy of gradient checkpointing (Gruslys et al., 2016;Chen et al., 2016;Bulo et al., 2018;Feng & Huang, 2021).Another option is to quantize the tensors after the forward computation and use the quantized values for gradient computation during the backward pass (Jain et al., 2018;Chakrabarti & Moseley, 2019;Fu et al., 2020;Evans & Aamodt, 2021;Liu et al., 2022) -this is known as activation compression training (ACT).Quantization can reduce memory considerably, but also brings inevitable performance drops.Accuracy drops can be mitigated by bounding the error at each layer through adaptive quantization (Evans & Aamodt, 2021;Liu et al., 2022), i.e. adaptive ACT.However, training time consequently suffers as extensive tensor profiling is necessary during training.In this paper, we propose to reduce the memory usage of intermediate tensors by simply dropping elements from the tensor.We call our method Dropping Intermediate Tensors (DropIT).In the most basic setting, dropping indices can be selected randomly, though dropping based on a min-k ranking on the element magnitude is more effective.Both strategies are much simpler than the sensitivity checking and other profiling strategies, making DropIT much faster than adaptive ACT.During training, the intermediate tensor is transformed over to a sparse format after the forward computation is complete.The sparse tensor is then recovered to a general tensor during backward gradient computation with dropped indices filled with zero.Curiously, with the right dropping strategy and ratio, DropIT has improved convergence properties compared to SGD.We attribute this to the fact that DropIT can, theoretically, reduce noise on the gradients.In general, reducing noise will result in more precise and stable gradients.Experimentally, this strategy exhibits consistent performance improvements on various network architectures and different tasks.To the best of our knowledge, we are the first to propose activation sparsification.The closest related line of existing work is ACT, but unlike ACT, DropIT leaves key elements untouched, which is crucial for ensuring accuracy.Nevertheless, DropIT is orthogonal to activation quantization, and the two can be combined for additional memory reduction with higher final accuracy.The key contributions of our work are summarized as follows:\u2022 We propose DropIT, a novel strategy to reduce the activation memory by dropping the elements of the intermediate tensor.\u2022 We theoretically and experimentally show that DropIT can be seen as a noise reduction on stochastic gradients, which leads to better convergence.\u2022 DropIT can work for various settings: training from scratch, fine-tuning on classification, object detection, etc.Our experiments demonstrate that DropIT can drop up to 90% of the intermediate tensor elements in fully-connected and convolutional layers with a testing accuracy higher than the baseline for CNNs and ViTs.We also show that DropIT is much better regarding accuracy and speed compared to SOTA activation quantization methods, and it can be combined with them to pursue higher memory efficiency."}
{"paper_id": 353, "abstract": "In the ever-evolving landscape of machine learning, one of the most insidious challenges we face is the vulnerability of our models to Out-Of-Distribution (OOD) examples. This issue, while garnering significant attention, remains shrouded in complexity. Not all OOD data is created equal; some represent benign anomalies that, when harnessed, can bolster our models' learning capabilities. Others, however, are malign\u2014capable of leading our models astray, causing catastrophic failures in classification.  In this paper, we unveil the HOOD method, a groundbreaking approach that skillfully navigates the nuances of OOD data. By leveraging the intricate interplay of content and style inherent in each image instance, HOOD adeptly categorizes OOD data into benign and malign. At the heart of our approach lies a sophisticated variational inference framework, meticulously designed to disentangle content and style features through a structural causal model.  Our methodology employs an intervention process to augment both content and style, yielding distinct sets of OOD data. The benign OOD examples, rich in novel styles yet grounded in relevant content, serve as invaluable assets for training style-invariant models. Conversely, the malign OOD instances\u2014imbued with familiar styles but tainted by unknown content\u2014pose a threat that we can now detect and mitigate, enhancing our models' robustness against deceptive anomalies.  With innovative disentanglement and data augmentation techniques, HOOD rises to the occasion, effectively addressing OOD challenges in unpredictable and open environments. Our empirical validation across three pivotal applications\u2014OOD detection, open-set semi-supervised learning, and open-set domain adaptation\u2014demonstrates the efficacy of our approach, paving the way for a more resilient future in machine learning.", "introduction": "Learning in the presence of Out-Of-Distribution (OOD) data has been a challenging task in machine learning, as the deployed classifier tends to fail if the unseen data drawn from unknown distributions are not properly handled (Hendrycks & Gimpel, 2017;Pan & Yang, 2009).Such a critical problem ubiquitously exists when deep models meet domain shift (Ganin et al., 2016;Tzeng et al., 2017) and unseen-class data (Hendrycks & Gimpel, 2017;Scheirer et al., 2012), which has drawn a lot of attention in some important fields such as OOD detection (Hein et al., 2019;Hendrycks & Gimpel, 2017;Lee et al., 2018;Liang et al., 2018;Liu et al., 2020;Wang et al., 2022a;2023;2022b), Open-Set Domain Adaptation (DA) (Liu et al., 2019;Saito et al., 2018), and Open-Set Semi-Supervised Learning (SSL) (Huang et al., 2021b;2022b;a;Oliver et al., 2018;Saito et al., 2021;Yu et al., 2020).In the above fields, OOD data can be divided into two types, namely benign OOD data 1 and malign OOD data.The benign OOD data can boost the learning performance on the target distribution through DA techniques (Ganin & Lempitsky, 2015;Tzeng et al., 2017), but they can be misleading if not being properly exploited.To improve model generalization, many positive data augmentation techniques (Cubuk et al., 2018;Xie et al., 2020) have been proposed.For instance, the performance of SSL (Berthelot et al., 2019;Sohn et al., 2020) has been greatly improved thanks to the augmented benign OOD data.On the contrary, malign OOD data with unknown classes can damage the The green lines and red lines denote the augmentation of benign OOD data X and malign OOD data X, respectively.In all figures, the blank variables are observable and the shaded variables are latent.classification results, but they are deceiving and hard to detect (Hendrycks & Gimpel, 2017;Liang et al., 2018;Wei et al., 2022b;a).To train a robust model against malign OOD data, some works (Kong & Ramanan, 2021;Sinha et al., 2020) conduct negative data augmentation to generate \"hard\" malign data which resemble in-distribution (ID) data.By separating such \"hard\" data from ID data, the OOD detection performance can be improved.When presented with both malign and benign OOD data, it is more challenging to decide which to separate and which to exploit.As a consequence, the performance of existing open-set methods could be sub-optimal due to two drawbacks: 1) radically exploiting too much malign OOD data, and 2) conservatively denying too much benign OOD data.In this paper, we propose a HOOD framework (see Fig. 2) to properly harness OOD data in several OOD problems.To distinguish benign and malign OOD data, we model the data generating process by following the structural causal model (SCM) (Glymour et al., 2016;Pearl, 2009;Gao et al., 2022) in Fig. 1 (a).Particularly, we decompose an image instance X into two latent components: 1) content variable C which denotes the interested object, and 2) style variable S which contains other influential factors such as brightness, orientation, and color.The content C can indicate its true class Y , and the style S is decisive for the environmental condition, which is termed as domain D. Intuitively, malign OOD data cannot be incorporated into network training, because they contain unseen contents, thus their true classes are different from any known class; and benign OOD data can be adapted because they only have novel styles but contain the same contents as ID data.Therefore, we can distinguish the benign and malign OOD data based on the extracted the content and style features.In addition, we conduct causal disentanglement through maximizing an approximated evidence lower-bound (ELBO) (Blei et al., 2017;Yao et al., 2021;Xia et al., 2022b) of joint distribution P (X, Y, D).As a result, we can effectively break the spurious correlation (Pearl, 2009;Glymour et al., 2016;Hermann et al., 2020;Li et al., 2021b;Zhang et al., 2022) between content and style which commonly occurs during network training (Arjovsky et al., 2019), as shown by the dashed lines in Fig. 1 (b).In the ablation study, we find that HOOD can correctly disentangle content and style, which can correspondingly benefit generalization tasks (such as open-set DA and open-set SSL) and detection task (such as OOD detection).To further improve the learning performance, we conduct both positive and negative data augmentation by solely intervening the style and content, respectively, as shown by the blue and red lines in Fig. 1 (c).Such process is achieved through backpropagating the gradient computed from an intervention objective.As a result, style-changed data X must be identified as benign OOD data, and contentchanged data X should be recognized as malign OOD data.Without including any bias, the benign OOD data can be easily harnessed to improve model generalization, and the malign OOD data can be directly recognized as harmful ones which benefits the detection of unknown anomalies.By conducting extensive experiments on several OOD applications, including OOD detection, open-set SSL, and open-set DA, we validate the effectiveness of our method on typical benchmark datasets.To sum up, our contributions are three-fold:\u2022 We propose a unified framework dubbed HOOD which can effectively disentangle the content and style features to break the spurious correlation.As a result, benign OOD data and malign OOD data can be correctly identified based on the disentangled features.\u2022 We design a novel data augmentation method which correspondingly augments the content and style features to produce benign and malign OOD data, and further leverage them to enhance the learning performance.\u2022 We experimentally validate the effectiveness of HOOD on various OOD applications, including OOD detection, open-set SSL, and open-set DA.                            , ,  , , "}
{"paper_id": 354, "abstract": "In the expansive realm of machine learning, recent strides in multi-modal contrastive learning have unveiled a remarkable potential: the ability to craft an embedding space that serves as a foundation for robust vision classifiers. This is achieved by tapping into the vast reservoirs of information found within large-scale image-caption datasets. Our exploration sheds light on a unique advantage of this multi-modal embedding space: it empowers us to diagnose vision classifiers using the very language that humans understand.   Traditionally, the arduous task of diagnosing model behavior in real-world applications has been fraught with challenges, demanding extensive data collection and meticulous annotation. However, our innovative approach circumvents these hurdles. We unveil a method capable of pinpointing high-error data slices, uncovering influential attributes, and rectifying undesirable model behaviors\u2014all without the need for visual data.   Through a careful blend of theoretical insights and empirical validation, we delineate the conditions under which classifiers trained on one modality's embeddings can seamlessly transition to another modality's embeddings. Our findings, drawn from a diverse array of image datasets with known error slices, demonstrate that our method not only identifies these troublesome segments and the attributes that drive them but also harnesses the power of natural language to mend the classifier's failure modes. In this way, we bridge the gap between human understanding and machine learning, paving the way for more intuitive and effective model diagnostics.", "introduction": "Recent models trained using multi-modal contrastive learning have leveraged large-scale datasets of aligned image-caption pairs to obtain shared embedding spaces that capture rich visual and textual features.The learned image and text encoders resulting from multi-modal contrastive learning have been demonstrated to be effective feature extractors that can be used to train strong single-modality classifiers (Radford et al., 2021;Jia et al., 2021;Yuan et al., 2021).In this work, we show how visual classification models obtained through multi-modal contrastive learning, as described above, offer a significant additional advantage: the ability to use language to probe and diagnose the behavior of the vision models.Model diagnosis aims to gain a systematic and comprehensive understanding of when and why models fail.This is a critical quality assurance process to prevent unexpected and catastrophic failures of models in high-stake settings.A growing body of work has proposed methods for addressing this need.For example, error slice discovery methods aim to find subsets of inputs with similar characteristics where the model performs significantly worse (d'Eon et al., 2022;Eyuboglu et al., 2022).Interpretability methods aim to understand the black-box process of model prediction and thus the reasons why models fail for certain inputs (Ribeiro et al., 2016;Lundberg & Lee, 2017;Koh et al., 2020).In addition, model diagnosis is relevant to model auditing, an important topic that also deals with identifying model failures and sensitive attributes (Raji et al., 2020), and has a broad societal impact in terms of AI accountability and integration (Buolamwini & Gebru, 2018;Mitchell et al., 2019;Gebru et al., 2021).While these prior efforts have made progress in vision model diagnosis, they all suffer from a critical Achilles' heel -susceptibility to lack of visual data.Curated training and test sets from the same data distribution are typically used to develop vision models.Even if models achieve perfect performance on these datasets, their performance can degrade drastically when deployed in-the-wild, due to distribution shifts (Koh et al., 2021;Wiles et al., 2022).Yet most existing model diagnosis methods require visual examples of failure modes (e.g., present in the test set) to discover them.As Text Embedding Image Embedding ClassifierFigure 1: Overview of our approach, DrML, that diagnoses and rectifies vision models using language.Our approach leverages the shared image and text representation space learned by multimodal contrastive learning.We find that classifiers trained on embeddings from one modality can be equivalently applied to embeddings from another modality, despite the fact that embeddings from these two modalities are distinctly separated.This cross-modal transferability phenomenon enables us to diagnose a vision model by training it on the image embedding space and probing it with text embeddings.The use of language allows us to generate a large set of diverse and novel inputs to discover error slices, identify influential attributes, and rectify model misbehaviors.a result, using these methods is reliant on efforts to collect large-enough datasets to cover all data distributions and potential failure modes of interest, which is often impractical or infeasible.The goal of our work is to circumvent this need to collect test data representing all data distributions of interest, and instead use natural language input to diagnose vision classifiers.It is often easier to generate a set of diverse natural language inputs by combining known attributes and prompt generators than to collect a set of image inputs representing the same desired concepts.We observe that vision classifiers trained on image embeddings from a shared image-text embedding space suggest the possibility of leveraging text embeddings as a proxy for image embeddings.Multi-modal contrastive losses are frequently used to learn such shared embedding spaces.However, while these losses encourage image and text embeddings to be closer for aligned pairs than for mismatched pairs, there is no guarantee that in practice, using text embeddings as input into a vision classifier trained on the image embeddings will result in the same predictions.In this work, we first verify that text inputs can indeed work as good proxies to image inputs trained on a shared image-text embedding space obtained through contrastive learning.We refer to this as cross-modal transferability.Based on the phenomenon of cross-modal transferability, we then present DrML for Diagnosing and Rectifying Vision Models using Language.We show that DrML can use language to diagnose vision models in two different ways: discovering error slices including concepts for which we have no visual data, and identifying attributes that have the greatest impact on model predictions.Finally, we present a method that uses language to rectify undesirable behaviors without requiring the collection of more visual data.Figure 1 illustrates our framework for diagnosing and rectifying vision models using language.On three image datasets representing the three most common types of model failure modes, we demonstrate that DrML can effectively identify error slices and influential attributes, and can further rectify these model failure modes using language.In summary, our contributions are:1. We present a theoretical explanation of when cross-modal transferability happens (Section 2.1), and empirically verify that the assumptions required by the analysis is true in practice across a range of multi-modal contrastive models and datasets (Section 3.2).2. We propose DrML, a framework for diagnosing vision models using natural language, including error slice discovery and influential attribute identification.We empirically validate DrML by simulating common types of failure modes using the Waterbirds (Sagawa et al., 2020), Fair-Face (Karkkainen & Joo, 2021), and dSpitesV (Matthey et al., 2017) datasets, and show the effectiveness of our method in identifying known error slices and influential attributes.3. We further demonstrate that DrML can rectify undesirable model behaviors and improve model performance with respect to the identified error slices and influential attributes, by fine-tuning the vision classifier using text embeddings constructed from the diagnosis process.We first define basic notations used in this paper.Given a pre-trained multi-modal contrastive model, along with an image X \u2208 X or text Y \u2208 Y as input, we can obtain their l 2 -normalized embeddings x or y from the image encoder, where d is the dimension of the shared multi-modal embedding space.We can build classifiers h \u2236 R d \u21a6 C such as a linear layer or multi-layer perception on the shared embedding space to predict the label c \u2208 C given an image embedding or text embedding.We focus on the case of vision classifiers trained using image embeddings."}
{"paper_id": 355, "abstract": "In the ever-evolving realm of graph neural networks (GNNs), the quest to unveil the reasoning behind their predictions has garnered significant focus. Traditionally, researchers have sought to provide clear explanations by selecting subgraphs through intricate combinatorial optimization. Yet, the sheer exponential growth of potential subgraphs poses a formidable challenge, restricting the effectiveness of these advanced methods when applied to larger GNNs.  In a bold departure from the norm, we introduce a novel approach: the GFlowNets-based GNN Explainer, affectionately dubbed GFlowExplainer. This innovative framework transforms the optimization dilemma into a dynamic, step-by-step generative challenge. Our GFlowExplainer aspires to master a policy that crafts a distribution of subgraphs, where each subgraph's probability aligns with its corresponding reward, illuminating the path to clarity.  Moreover, we unveil a cutting-edge cut vertex matrix, a strategic tool designed to deftly navigate parent states within the GFlowNets architecture. This enhancement not only streamlines our methodology but also broadens its applicability to large-scale scenarios, freeing it from the constraints of node sequence and the burdens of pre-training.  Through rigorous experimentation across both synthetic and real-world datasets, we demonstrate the prowess of our GFlowExplainer. The results, both qualitative and quantitative, resoundingly affirm its superiority, marking a significant advancement in the quest for comprehensible and scalable explanations in the world of GNNs.", "introduction": "Graph Neural Networks (GNNs) have received widespread attention due to the springing up of graph-structured data in real-world applications, such as social networks and chemical molecules Zhang et al. (2020).Various graph related task are widely studied including node classification Henaff et al. (2015); Liu et al. (2020) and graph classification Zhang et al. (2018).However, uncovering rationales behind predictions of graph neural networks (GNNs) is relatively less explored.Recently, some explanation approaches for GNNs have gradually stepped into the public eye.There are two major branches of them: instance-level explanations and model-level explanations Yuan et al. (2022).In this paper, we mainly focus on instance-level explanations.Instance-level approaches explain models by identifying the most critical input features for their predictions.They have four sub-branches: Gradients/Features-based Zhou et al. (2016); Baldassarre & Azizpour (2019); Pope et al. (2019), Perturbation-based Ying et al. (2019); Luo et al. (2020); Schlichtkrull et al. (2020); Wang et al. (2020), Decompose-based Baldassarre & Azizpour (2019); Schnake et al. (2020); Feng et al. (2021) and Surrogate-based Vu & Thai (2020); Huang et al. (2022); Yuan et al. (2022).Some works such as XGNN Yuan et al. (2020) and RGExplainer Shan et al. (2021) apply reinforcement learning (RL) to model-level and instance-level explanations.However, the pioneering works have some drawbacks.Perturbation-based approaches return the discrete edges for explanations, which are not as intuitive as graph generation-based approach, which could provide connected graphs.However, the task of searching connected subgraphs is a combinatorial problem, and the potential candidates increase exponentially, making most current approaches inefficient and intractable in large-scale settings.In addition, current research consider Monte-Carlo tree search, which has high variance and ignores the fact that graph is an unordered set.This could lead to a loss of sampling efficiency and effectiveness, i.e., the approaches fail to consolidate information of sampled trajectories that form the same subgraph with different sequences.To address the above issues, we take advantage of the strong generation property of Generative Flow Networks (GFlowNets) Bengio et al. (2021b) and cast the combinatorial optimization problem as a generation problem.Unlike the previous work, which focus on the maximization of mutual information, our insight is to learn a generative policy that generates a distribution of connected subgraphs with probabilities proportional to their mutual information.We called this approach GFlowExplainer, which could overcome the current predicament for the following reasons.First, it has a stronger exploration ability due to its flow matching condition, helping us to avoid the trap of suboptimal solutions.Second, in contrast to previous tree search or node sequence modeling, GFlowExplainer consolidate information from sampled trajectories generating the same subgraph with different sequences.This critical difference could largely increase the utilization of generated samples, and hence improve the performance.Moreover, by introducing a cut vertex matrix, GFlow-Explainer could be applied in large-scale settings and achieve better performance with fewer training epochs.We summarize the main contributions as follows.Main Contributions: 1) We propose a new hand-crafted method for GNN explanation via GFlowNet frameworks to sample from a target distribution with the energy proportional to the predefined score function; 2) We take advantage of the DAG structure in GFlowNets to connect the trajectories of outputting the same graph but different node sequences.Therefore, without any pre-training strategies, we can significantly improve the effectiveness of our GNN explanations; 3) Considering relatively cumbersome valid parent state explorations in GFlowNets because of the connectivity constraint of the graph, we introduce the concept of cut vertex and propose a more efficient cut vertex criteria for dynamic graphs, thus speeding up the whole process; 4) We conduct extensive experiments to show that GFlowExplainer can outperform current state-of-the-art approaches."}
{"paper_id": 356, "abstract": "What hidden truths about the visual realm can a neural network uncover when fed but a solitary image? While it\u2019s clear that a single image cannot encapsulate the vast tapestry of all existing objects, scenes, and lighting conditions, it may still serve as a powerful anchor in the ocean of possibilities represented by $256^{3\\cdot224\\cdot224}$ potential $224$-sized square images. To explore this intriguing notion of the \"augmented image prior,\" we crafted a straightforward framework to train neural networks from the ground up, leveraging a single image alongside augmentations, all while harnessing the wisdom of a supervised pretrained teacher through knowledge distillation. The results? They reveal a surprising truth: the network learns an astonishing amount. In concrete terms, we achieved remarkable accuracies of $94\\%$ and $74\\%$ on CIFAR-10 and CIFAR-100, respectively, and $69\\%$ on ImageNet. By extending our approach to the realms of video and audio, we also recorded $51\\%$ on Kinetics-400 and $84\\%$ on SpeechCommands. Our extensive investigations, spanning 13 diverse datasets, allowed us to unravel the intricate effects of augmentations, data selection, and network architectures. We even uncovered fascinating qualitative insights, including the emergence of distinct \"panda neurons\" within networks that had never encountered a panda before.", "introduction": "Deep learning has both relied and improved significantly with the increase in dataset sizes.In turn, there are many works that show the benefits of dataset scale in terms of data points and modalities used.Within computer vision, these models trained on ever larger datasets, such as Instagram-1B (Mahajan et al., 2018) or JFT-3B (Dosovitskiy et al., 2021), have been shown to successfully distinguish between semantic categories at high accuracies.In stark contrast to this, there is little research on understanding neural networks trained on very small datasets.Why would this be of any interest?While smaller dataset sizes allow for better understanding and control of what the model is being trained with, we are most interested in its ability to provide insights into fundamental aspects of learning: For example, it is an open question as to what exactly is required for arriving at semantic visual representations from random weights, and also of how well neural networks can extrapolate from their training distribution.While for visual models it has been established that few or no real images are required for arriving at basic features, like edges and color-contrasts (Asano et al., 2020;Kataoka et al., 2020;Bruna & Mallat, 2013;Olshausen & Field, 1996), we go far beyond these and instead ask what the minimal data requirements are for neural networks to learn semantic categories, such as those of ImageNet.This approach is also motivated by studies that investigate the early visual development in infants, which have shown how little visual diversity babies are exposed to in the first few months whilst developing generalizeable visual systems (Orhan et al., 2020;Bambach et al., 2018).In this paper, we study this question in its purest form, by analyzing whether neural networks can learn to extrapolate from a single datum.However, addressing this question na\u00efvely runs into the difficulties of i) current deep learning methods, such as SGD or BatchNorm being tailored to large datasets and not working with a single datum and ii) extrapolating to semantic categories requiring information about the space of natural images beyond the single datum.In this paper, we address these issues by developing a simple framework that recombines augmentations and knowledge distillation.First, augmentations can be used to generate a large number of variations from a single image.This can effectively address issue i) and allow for evaluating the research question on standard architectures and datasets.This use of data augmentations to generate variety is drastically different to their usual use-case in which transformations are generated to implicitly encode desirable invariances during training.Second, to tackle the difficulty of providing information about semantic categories in the single datum setting, we opt to use the outputs of a supervisedly trained model in a knowledge distillation (KD) fashion.While KD (Hinton et al., 2015) is originally proposed for improving small models' performance by leveraging what larger models have learned, we re-purpose this as a simple way to provide a supervisory signal about semantic classes into the training process.We combine the above two ideas and provide both student and teacher only with augmented versions of a single datum, and train the student to match the teacher's imagined class-predictions of classes -almost all of which are not contained in the single datum, see Fig. 1.While practical applications do result from our method -for example we provide results on single image dataset based model compression in the Appendix -our goal in this paper is analyzing the fundamental question of how well neural networks trained from a single datum can extrapolate to semantic classes, like those of CIFAR, SpeechCommands, ImageNet or even Kinetics.What we find is that despite the fact that the resulting model has only seen a single datum plus augmentations, surprisingly high quantitative performances are achieved: e.g.74% on CIFAR-100, 84% on SpeechCommands and 69% top-1, single-crop accuracy on ImageNet-12.We further make the novel observations that our method benefits from high-capacity student and low-capacity teacher models, and that the source datum's characteristics matter -random noise or less dense images yield much lower performances than dense pictures like the one shown in Figure 1.In summary, in this paper we make these four main contributions:1.A minimal framework for training neural networks with a single datum using distillation.2. Extensive ablations of the proposed method, such as the dependency on the source image, augmentations and network architectures.3. Large scale empirical evidence of neural networks' ability to extrapolate on > 12 vision and audio datasets.4. Qualitative insights on what and how neural networks trained with a single image learn."}
{"paper_id": 357, "abstract": "In the realm of domain generalization (DG), many existing strategies focus on the fine-tuning of a singular pretrained model, weaving intricate algorithms into the fabric of their approach. However, we propose a fresh path, one that harnesses the collective strength of a diverse pool of pretrained models without the burden of fine-tuning. Through a blend of rigorous empirical studies and theoretical insights, we uncover two pivotal truths: first, that pretrained models inherently possess a degree of generalization, yet no single model reigns supreme across all distribution shifts; second, that the out-of-distribution (OOD) generalization error is intricately tied to the compatibility between a pretrained model and the unseen test distributions. This understanding drives us to embrace a multitude of pretrained models and to intelligently select the most suitable ones for each OOD sample using recommendation techniques. Enter SIMPLE\u2014a novel approach to model-sample matching tailored for domain generalization. Our method begins by adapting the predictions of the pretrained models to the target domain through a linear transformation of the label space. Next, we introduce a matching network that is attuned to the specialties of each model, dynamically recommending the most appropriate pretrained models for predicting each test sample. Experiments conducted on DomainBed reveal that our method delivers remarkable performance enhancements, achieving gains of up to 12.2% on individual datasets and an average improvement of 3.9% over state-of-the-art methods. Additionally, by expanding the pretrained model pool, we realize an impressive 6.1% increase in performance. Notably, our approach is not just effective but also remarkably efficient, boasting a training speedup of over 1000 times compared to traditional DG methods that rely on fine-tuning. For those interested, our code and supplementary materials can be found at https://seqml.github.io/simple.", "introduction": "Distribution shift is a common problem in real-world applications, which breaks the independent and identically distributional (i.i.d.) assumption of machine learning algorithms (Wang et al., 2022).Mismatches between training and test distributions, which are quite common in reality, can largely deteriorate model performance and make machine learning models infeasible for practical applications (Gonz\u00e1lez & Abu-Mostafa, 2015).Therefore, enhancing the generalization ability of models has attracted increasing attention (Cha et al., 2021;Zhang et al., 2022).For its practical significance, various methods have been proposed, e.g., domain alignment (Ganin et al., 2016;Gong et al., 2019;Arjovsky et al., 2019), meta-learning (Finn et al., 2017;Dou et al., 2019;Du et al., 2020), and ensemble learning (Mancini et al., 2018;Cha et al., 2021;Arpit et al., 2021).The effectiveness of DG algorithms is generally verified by fine-tuning a pre-trained ResNet (He et al., 2016) model with these algorithms (Gulrajani & Lopez-Paz, 2020).It has demonstrated that these algorithms improve upon empirical risk minimization (ERM) baseline on ResNet-50 backbone (Arpit et al., 2021;Wiles et al., 2021).Meanwhile, recent studies show that neural architectures and pretraining methods have a large impact on the model robustness to distribution shifts (Radford et al., 2021;Wiles et al., 2021).For example, vision transformers are more robust to texture and style shifts compared with ResNet-based models (Zhang et al., 2022), which are instead superior to transformer-based models on dense image classification tasks (Liu et al., 2022).In terms of pretraining, using pretraining datasets other than ImageNet-1k improves the generalization performance in one test domain, yet leads to performance degradation in another (Kim et al., 2022).These findings are in line with the No Free Lunch (NFL) Theorem (Wolpert, 1996), which suggests that no single model can always perform better than any other model without having substantive information about the targeted problem.In DG, we usually have very limited information about the test domain, so we are more likely to encounter the above challenge.Inspired by these attempts, in this paper, we conduct a fine-grained study on the relationship between pretrained models and distribution shifts.From both empirical and theoretical evidence, we show that no free lunch in terms of pretraining for domain generalization, i.e., there is no single best pretrained model across shifting test distributions.Specifically, 283 pretrained models with different network architectures, pretraining datasets, and learning objectives are compared for their generalization performance under different distributional shifts.The results reveal that the pretrained models without fine-tuning generalize well to some unseen domains, but none of these models dominate in all unseen distributions.Furthermore, the theoretical analysis indicates that OOD generalization error is determined by the fitness between model (varying w.r.t. the network architectures and model weights) and test distribution.For any network architecture with fixed training distributions, such as pretrained models (Iandola et al., 2014;He et al., 2016;2021a), it is always possible to find a beneficial or detrimental test distribution with a small or large generalization error.Motivated by these findings, we propose an alternative DG paradigm that leverages pretrained models with different network architectures and shifting training distributions, upon which we match the most suitable pretrained models for each OOD sample.As shown in Figure 3, our paradigm (specialized model-sample matching for domain generalization, SIMPLE) first adopts a simple label adapter that projects the label space of the pretraining domainfoot_0 to that of unseen domainsfoot_1 , where the adapter is shared by pretrained models from the same pretraining domain.Then, a matching network, which is aware of model specialty, selects a set of proper pretrained models and aggregates them together to conduct the prediction for each OOD sample.Notably, this promising alternative exhibits significant performance improvements, averaging 3.9% over the existing SOTA results, with gains of up to 12.2% on single datasets, and a significant increase in training efficiency.To summarize, this work has made the following contributions:\u2022 We theoretically and empirically analyze the generalization of pretrained models on shifting unseen test distribution, revealing no free lunch hypothesis exists that motivates our solution of modelsample matching.\u2022 In complementary to traditional DG solutions, we propose a novel DG paradigm which directly leverages pretrained models without fine-tuning, and it has significantly improved the DG performance in the mainstream benchmark upon other strong baselines.\u2022 Besides the performance gain, our method is even more efficient since it does not follow the common fine-tuning approach, shedding new light on using pretrained models in DG tasks."}
{"paper_id": 358, "abstract": "In the vast and intricate realm of molecular representation, where the quest for new drugs and materials unfolds like an epic saga, the challenge of limited labeled molecules looms large. Most scholars have turned their gaze toward the familiar territory of 2D molecular graphs, yet the untapped potential of 3D geometric structures remains a tantalizing mystery, waiting to be unraveled. The obstacle lies in the elusive search for a proxy task capable of harnessing the full power of pretraining, enabling the extraction of vital features from these complex geometric forms.  Inspired by the dynamic dance of 3D molecules, where their continuous motion weaves a tapestry of smooth potential energy surfaces in the vast expanse of Euclidean space, we unveil our innovative framework: GeoSSL. This 3D coordinate denoising pretraining approach is designed to model the intricate landscape of molecular energy. But we do not stop there. By employing an SE(3)-invariant score matching method, we introduce GeoSSL-DDM, where the challenge of coordinate denoising is distilled into the elegant task of refining the pairwise atomic distances within a molecule.  Through a series of rigorous and comprehensive experiments, we validate the effectiveness and robustness of our method, illuminating the path forward in the exploration of 3D molecular representations. In this journey, we unlock new possibilities for discovery, paving the way for a future where the secrets of molecular design are laid bare before us.", "introduction": "Learning effective molecular representations is critical in a variety of tasks in drug and material discovery, such as molecular property prediction [14,20,21,74], de novo molecular design and optimization [7,36,37,39,53,77], and retrosynthesis and reaction planning [4,22,52,64].Recent work based on graph neural networks (GNNs) [20] has shown superior performance thanks to the simplicity and effectiveness of GNNs in modeling graph-structured data.However, the problem remains challenging due to the limited number of labeled molecules as it is in general expensive and time-consuming to label molecules, which usually requires expensive physics simulations or wet-lab experiments.As a result, recently, there has been growing interest in developing pretraining or self-supervised learning methods for learning molecular representations by leveraging the huge amount of unlabeled molecule data [28,35,63,75].These methods have shown superior performance on many tasks, especially when the number of labeled molecules is insufficient.However, one limitation of these approaches is that they represent molecules as topological graphs, and molecular representations are learned through pretraining 2D topological structures (i.e., based on the covalent bonds).But intrinsically, for molecules, a more natural representation is based on their 3D geometric structures, which largely determine the corresponding physical and chemical properties.Indeed, recent works [20,38] have empirically verified the importance of applying 3D geometric information for molecular property prediction tasks.Therefore, a more promising direction is to pretrain molecular representations based on their 3D geometric structures, which is the main focus of this paper.The main challenge for molecule geometric pretraining arises from discovering an effective proxy task to empower the pretraining to extract essential features from the 3D geometric structures.Our proxy task is motivated by the following observations.Studies [48] have shown that molecules are not static but in a continuous motion in the 3D Euclidean space, forming a potential energy surface (PES).As shown in Figure 1, it is desirable to study the molecule in the local minima of the PES, called conformer.However, such stable state conformer often comes with different noises for the following reasons.First, the statistical and systematic errors in conformation estimation are unavoidable [11].Second, it has been well-acknowledged that a conformer can have vibrations around the local minima in PES.Such characteristics of the molecular geometry motivate us to attempt to denoise the molecular coordinates around the local minima, to mimic the computation errors and conformation vibration within the corresponding local region.The denoising goal is to learn molecular representations that are robust to such noises and effectively capture the energy surface around the local minima.Figure 1: Illustration on coordinate geometry of molecules.The molecule is in a continuous motion, forming a potential energy surface (PES), where each 3D coordinate (x-axis) corresponds to an energy value (y-axis).The provided molecules, i.e., conformers, are in the local minima (g1).It often comes with noises around the minima (e.g., statistical and systematic errors or vibrations), which can be captured using the perturbed geometry (g2).To achieve the aforementioned goal, we first introduce a general geometric self-supervised learning framework called GeoSSL.Based on this, we further propose an SE(3)-invariant denoising distance matching pretraining algorithm, GeoSSL-DDM.In a nutshell, to capture the smooth energy surface around the local minima, we aim to maximize the mutual information (MI) between a given stable geometry and its perturbed version (i.e., g 1 and g 2 in Figure 1).In practice, it is difficult to directly maximize the mutual information between two random variables.Thus, we propose to maximize an equivalent lower bound of the above mutual information, which amounts to a pretraining framework on denoising a geometric structure, coined GeoSSL.Moreover, directly denoising such noisy coordinates remains challenging because one may need to effectively constrain the pairwise atomic distances while changing the atomic coordinates.To cope with this obstacle, we further leverage an SE(3)-invariant score matching method, GeoSSL-DDM, to successfully transform the coordinate denoising desire to the denoising of pairwise atomic distances, which then can be effectively computed.In other words, our pretraining proxy task, namely mutual information maximization, effectively boils down to achieving an intuitive learning objective: denoising a molecule's pairwise atomic distances.Using 22 downstream geometric molecular prediction tasks, we empirically verify that our method outperforms nine pretraining baselines.Our main contributions are summarized as follows.(1) We propose a novel geometric self-supervised learning framework, GeoSSL.To the best of our knowledge, it is the first pretraining framework focusing on the pure 3D molecular data 1 .(2) To overcome the challenge of attaining the coordinate denoising objective in GeoSSL, we propose GeoSSL-DDM, an SE(3)-invariant score matching strategy to successfully transform such objective into the denoising of pairwise atomic distances.(3) We empirically demonstrate the effectiveness and robustness of GeoSSL-DDM on 22 downstream tasks."}
{"paper_id": 359, "abstract": "In the ever-evolving realm of home-assistant robotics, the ability to perceive and interact with the myriad of 3D objects that populate our daily lives stands as both a vital goal and a formidable challenge. To forge ahead in creating scalable systems capable of executing a wide array of manipulation tasks across diverse shapes, recent advancements have illuminated the path of visual actionable affordance. This innovative approach assigns an action likelihood to every point on a given 3D geometry, guiding robots in tasks such as pushing or picking up objects. Yet, a significant gap remains; previous research has largely focused on single-gripper manipulation, overlooking the collaborative nature of many real-world tasks that demand the coordination of two hands.  In response to this challenge, we introduce DualAfford\u2014a groundbreaking framework designed to learn collaborative affordance specifically for dual-gripper manipulation tasks. Our approach elegantly transforms the complex quadratic problem associated with two grippers into two distinct yet interwoven subtasks, paving the way for more efficient learning. Leveraging the expansive PartNet-Mobility and ShapeNet datasets, we establish four benchmark tasks tailored for dual-gripper manipulation. Our experimental results not only validate the effectiveness of DualAfford but also demonstrate its superiority over three existing baselines. Upon acceptance, we are excited to share our code and data, inviting the community to explore the potential of collaborative robotic manipulation.", "introduction": "We, humans, spend little or no effort perceiving and interacting with diverse 3D objects to accomplish everyday tasks in our daily lives.It is, however, an extremely challenging task for developing artificial intelligent robots to achieve similar capabilities due to the exceptionally rich 3D object space and high complexity manipulating with diverse 3D geometry for different downstream tasks.While researchers have recently made many great advances in 3D shape recognition (Chang et al., 2015;Wu et al., 2015), pose estimation (Wang et al., 2019;Xiang et al., 2017), and semantic understandings (Hu et al., 2018;Mo et al., 2019;Savva et al., 2015) from the vision community, as well as grasping (Mahler et al., 2019;Pinto & Gupta, 2016) and manipulating 3D objects (Chen et al., 2021;Xu et al., 2020) on the robotic fronts, there are still huge perception-interaction gaps (Batra et al., 2020;Gadre et al., 2021;Shen et al., 2021;Xiang et al., 2020) to close for enabling future home-assistant autonomous systems in the unstructured and complicated human environments.One of the core challenges in bridging the gaps is figuring out good visual representations of 3D objects that are generalizable across diverse 3D shapes at a large scale and directly consumable by downstream planners and controllers for robotic manipulation.Recent works (Mo et al., 2021;Wu et al., 2022) have proposed a novel perception-interaction handshaking representation for 3D objects visual actionable affordance, which essentially predicts an action likelihood for accomplishing the given downstream manipulation task at each point on the 3D input geometry.Such visual actionable affordance, trained across diverse 3D shape geometry (e.g., refrigerators, microwaves) and for a specific downstream manipulation task (e.g., pushing), is proven to generalize to novel unseen objects (e.g., tables) and benefits downstream robotic executions (e.g., more efficient exploration).Though showing promising results, past works (Mo et al., 2021;Wu et al., 2022) are limited to single-gripper manipulation tasks.However, future home-assistant robots shall have two hands just like us humans, if not more, and many real-world tasks require two hands to achieve collaboratively.For example (Figure 1), to steadily pick up a heavy bucket, two grippers need to grasp it at two top edges and move in the same direction; to rotate a display anticlockwise, one gripper points downward to hold it and the other gripper moves to the other side.Different manipulation patterns naturally emerge when the two grippers collaboratively attempt to accomplish different downstream tasks.In this paper, we study the dual-gripper manipulation tasks and investigate learning collaborative visual actionable affordance.It is much more challenging to tackle dual-gripper manipulation tasks than single-gripper ones as the degree-of-freedom in action spaces is doubled and two affordance predictions are required due to the addition of the second gripper.Besides, the pair of affordance maps for the two grippers needs to be learned collaboratively.As we can observe from Figure 1, the affordance for the second gripper is dependent on the choice of the first gripper action.How to design the learning framework to learn such collaborative affordance is a non-trivial question.We propose a novel method DualAfford to tackle the problem.At the core of our design, DualAfford disentangles the affordance learning problem of two grippers into two separate yet highly coupled subtasks, reducing the complexity of the intrinsically quadratic problem.More concretely, the first part of the network infers actionable locations for the first gripper where there exist second-gripper actions to cooperate, while the second part predicts the affordance for the second gripper conditioned on a given first-gripper action.The two parts of the system are trained as a holistic pipeline using the interaction data collected by manipulating diverse 3D shapes in a physical simulator.We evaluate the proposed method on four diverse dual-gripper manipulation tasks: pushing, rotating, toppling and picking-up.We set up a benchmark for experiments using shapes from PartNet-Mobility dataset (Mo et al., 2019;Xiang et al., 2020) and ShapeNet dataset (Chang et al., 2015).Quantitative comparisons against baseline methods prove the effectiveness of the proposed framework.Qualitative results further show that our method successfully learns interesting and reasonable dual-gripper collaborative manipulation patterns when solving different tasks.To summarize, in this paper,\u2022 We propose a novel architecture DualAfford to learn collaborative visual actionable affordance for dual-gripper manipulation tasks over diverse 3D objects; \u2022 We set up a benchmark built upon SAPIEN physical simulator (Xiang et al., 2020) using the PartNet-Mobility and ShapeNet datasets (Chang et al., 2015;Mo et al., 2019;Xiang et al., 2020) for four dual-gripper manipulation tasks; \u2022 We show qualitative results and quantitative comparisons against three baselines to validate the effectiveness and superiority of the proposed approach."}
{"paper_id": 360, "abstract": "In the realm of optimal transport, where the flow of probability takes on a life of its own, the challenge of computing the convex conjugate\u2014known also as the Legendre-Fenchel conjugate or c-transform\u2014looms like a shadow over the Euclidean Wasserstein-2 problems. This operation, intricate and elusive, has long been a barrier, hampering the ability to accurately conjugate dual potentials in continuous space. Yet, hope glimmers on the horizon. Through the power of amortized optimization, we can approximate this formidable conjugation, crafting a model that learns to predict the conjugate with a deftness that belies its complexity.  In this paper, I unveil a method that marries these amortized approximations with a fine-tuning solver, resulting in a remarkable enhancement in the quality of transport maps. The improvements shine particularly bright within the Wasserstein-2 benchmark established by Korotin et al. (2021a), allowing us to adeptly model a plethora of two-dimensional couplings and flows that have been explored in the annals of our field. For those eager to delve deeper into this journey, all methodologies, baselines, and solvers are readily accessible at http://github.com/facebookresearch/w2ot, inviting fellow scholars to join in the quest for understanding the intricate dance of transport in a world governed by Wasserstein geometry.", "introduction": "Optimal transportation (Villani, 2009;Ambrosio, 2003;Santambrogio, 2015;Peyr\u00e9 et al., 2019) is a thriving area of research that provides a way of connecting and transporting between probability measures.While optimal transport between discrete measures is well-understood, e.g. with Sinkhorn distances (Cuturi, 2013), optimal transport between continuous measures is an open research topic actively being investigated (Genevay et al., 2016;Seguy et al., 2017;Taghvaei and Jalali, 2019;Korotin et al., 2019;Makkuva et al., 2020;Fan et al., 2021;Asadulaev et al., 2022).Continuous OT has applications in generative modeling (Arjovsky et al., 2017;Petzka et al., 2017;Wu et al., 2018;Liu et al., 2019;Cao et al., 2019;Leygonie et al., 2019), domain adaptation (Luo et al., 2018;Shen et al., 2018;Xie et al., 2019), barycenter computation (Li et al., 2020;Fan et al., 2020;Korotin et al., 2021b), and biology (Bunne et al., 2021;2022;L\u00fcbeck et al., 2022).This paper focuses on estimating the Wasserstein-2 transport map between measures \u03b1 and \u03b2 in Euclidean space, i.e. supp(\u03b1) = supp(\u03b2) = R n with the Euclidean distance as the transport cost.The Wasserstein-2 transport map, T : R n \u2192 R n , is the solution to Monge's primal formulation:T \u2208 arg infwhere T (\u03b1, \u03b2) := {T : T # \u03b1 = \u03b2} is the set of admissible couplings and the push-forward operator # is defined by T # \u03b1(B) := \u03b1(T -1 (B)) for a measure \u03b1, measurable map T , and all measurable sets B. T exists and is unique under general settings, e.g. as in Santambrogio (2015, Theorem 1.17), and is often difficult to solve because of the coupling constraints T .Almost every computational method instead solves the Kantorovich dual, e.g. as formulated in Villani (2009, \u00a75) and Peyr\u00e9 et al. (2019, \u00a72.5).This paper focuses on the dual associated with the negative inner product cost (Villani, 2009, eq. 5.12), which introduces a dual potential function f : R n \u2192 R and solves:where L 1 (\u03b1) is the space of measurable functions that are Lebesgue-integrable over \u03b1 and f is the convex conjugate, or Legendre-Fenchel transform, of a function f defined by: f (y) := -inf x\u2208X J f (x; y) with objective J f (x; y) := f (x) -x, y .(3)1 Published as a conference paper at ICLR 2023x(y) denotes an optimal solution to eq. ( 3).Even though the eq.( 2) searches over functions in L 1 (\u03b1), the optimal dual potential \" f is convex (Villani, 2009, theorem 5.10).When one of the measures has a density, Brenier (1991, theorem 3.1) and McCann (1995) relate \" f to an optimal transport map T for the primal problem in eq.(1) with T (x) = \u2207 x \" f (x), and the inverse to the transport map is given by T -1 (y) = \u2207 y \" f (y).A stream of foundational papers have proposed methods to approximate the dual potential f with a neural network and learn it by optimizing eq. ( 2): Taghvaei and Jalali (2019); Korotin et al. (2019); Makkuva et al. (2020) parameterize f as an input-convex neural network (Amos et al., 2017), which can universally represent any convex function with enough capacity (Huang et al., 2020).Other works explore parameterizing f as a non-convex neural network (Nhan Dam et al., 2019;Korotin et al., 2021a;Rout et al., 2021).Efficiently solving the conjugation operation in eq. ( 3) is the key computational challenge to solving the Kantorovich dual in eq. ( 2) and is an important design choice.Exactly computing the conjugate as done in Taghvaei and Jalali (2019) is considered computationally challenging and approximating it as in Korotin et al. (2019); Makkuva et al. (2020); Nhan Dam et al. (2019); Korotin et al. (2021a); Rout et al. (2021) may be instable.Korotin et al. (2021a) fortifies this observation:The [exact conjugate] solver is slow since each optimization step solves a hard subproblem for computing [the conjugate].[Solvers that approximate the conjugate] are also hard to optimize: they either diverge from the start or diverge after converging to nearly-optimal saddle point.In contrast to these statements on the difficulty of exactly estimating the conjugate operation, I will show in this paper that computing the (near-)exact conjugate is easy.My key insight is that the approximate, i.e. amortized, conjugation methods can be combined with a fine-tuning procedure using the approximate solution as a starting point.Sect. 3 discusses the amortization design choices and sect.3.2.2presents a new amortization perspective on the cycle consistency term used in Wasserstein-2 generative networks (Korotin et al., 2019), which was previously not seen in this way.Sect. 5 shows that amortizing and fine-tuning the conjugate results in state-of-the-art performance in all of the tasks proposed in the Wasserstein-2 benchmark by Korotin et al. (2021a).Amortization with fine-tuning also nicely models synthetic settings (sect.6), including for learning a single-block potential flow without using the likelihood."}
{"paper_id": 361, "abstract": "In the realm of machine learning, the journey from source to target is fraught with peril, as the very essence of the data often shifts beneath our feet. The source data, while seemingly rich, may not fully encapsulate the intricacies required to navigate the uncharted waters of target inputs. This divergence gives rise to a cacophony of potential functions, each achieving near-perfect accuracy on the source yet diverging wildly in their predictions for the target. Such uncertainty poses a formidable challenge to the robustness we seek.  Enter DivDis, a novel two-stage framework crafted to pierce through this fog of ambiguity. In the first stage, DivDis embarks on a quest to uncover a diverse array of hypotheses\u2014each one achieving low loss on the source while offering differing visions for the target. With this collection of insights in hand, we then move to the second stage: disambiguation. Here, we wield additional information, perhaps a handful of target labels, to select the most fitting function from our diverse assembly.  Our rigorous experimental evaluations reveal that DivDis not only thrives in the face of subpopulation shifts but also excels in the challenging domain of generalization. The results speak for themselves, showcasing DivDis's remarkable ability to adapt to the shifting sands of distribution in both image and text classification benchmarks. In a world where ambiguity reigns, DivDis stands as a beacon of clarity and adaptability.", "introduction": "Datasets are often underspecified: multiple plausible hypotheses each describe the data equally well (D'Amour et al., 2020), and the data offers no further evidence to prefer one over another.Despite such ambiguity, machine learning models typically choose only one of the possible explanations of given data.Such choices can be suboptimal, causing these models to fail when the data distribution is shifted, as common in real-world applications.For example, examination of a chest X-ray dataset (Oakden-Rayner et al., 2020) has shown that many images of patients with pneumothorax include a thin drain used for treating the disease.A standard classifier trained on this dataset can erroneously identify such drains as a predictive feature of the disease, exhibiting degraded accuracy on the intended distribution of patients not yet being treated.To not suffer from such failures, it is desirable to have a model that can discover a diverse collection of alternate plausible hypotheses.The standard empirical risk minimization (Vapnik, 1992, ERM) paradigm performs poorly on underspecified data, because ERM tends to select the solution based on the most salient features without considering alternatives (Geirhos et al., 2020;Shah et al., 2020;Scimeca et al., 2021).This simplicity bias occurs even when training an ensemble (Hansen & Salamon, 1990;Lakshminarayanan et al., 2017) because each model is still biased towards simple functions.While many recent methods (Ganin et al., 2016;Sagawa et al., 2020;Liu et al., 2021) improve robustness in distribution shift settings, we find that they fail on data with more severe underspecification.This is because, similarly to ERM, these methods only consider a single solution even in situations where multiple explanations exist.We propose Diversify and Disambiguate (DivDis), a two-stage framework for learning from underspecified data.Our key idea is to learn a collection of diverse functions that are consistent with the training data but make differing predictions on unlabeled test datapoints.DivDis operates as follows.We train a neural network consisting of a shared backbone feature extractor with multiple heads, each representing a different function.As in regular training, each head is trained to predict labels for training data, but the heads are additionally encouraged to represent different functions from each other.More specifically, the heads are trained to make disagreeing predictions on a separate unlabeled dataset from the test distribution, a setting close to transductive learning.At test time, we select one member of the diversified functions by querying labels for the datapoints most informative for disambiguation.We visually summarize this framework in Fig. 1.DivDis is designed for Figure 1: Our two-stage framework for learning from underspecified data.In the DIVERSIFY stage, we train each head in a multi-headed neural network to accurately predict the labels of source data while also outputting differing predictions for unlabeled target data.In the DISAMBIGUATE stage, we choose one of the heads by observing labels for an informative subset of the target data.scenarios with underspecified data and distribution shift, and its heads will not yield a set of diverse functions in settings where only one function can achieve low training loss.We evaluate DivDis in several settings in which underspecification limits the performance of prior methods, such as standard subpopulation shift benchmarks (Sagawa et al., 2020) or the large-scale CXR and Camelyon17 datasets (Wang et al., 2017;Sagawa et al., 2022).DivDis achieves an over 15% improvement in worst-group accuracy on the Waterbirds task when tuning hyperparameters without any spurious attribute annotations, and outperforms existing semi-supervised methods on the Camelyon17 task.We also consider challenging problem settings in which labels are completely correlated with spurious attributes, so a classifier based on the spurious feature can achieve zero loss.In these completely correlated settings, our experiments find that DivDis is substantially more sample-efficient: DivDis with 4 target domain labels outperforms two fine-tuning methods that use 128 labels."}
{"paper_id": 362, "abstract": "In the ever-evolving landscape of machine learning, where the winds of distribution shift can easily scatter the carefully cultivated knowledge of pre-trained models, a new strategy emerges\u2014one that offers a deft hand in the art of adaptation. This paper introduces a method we call \"surgical fine-tuning,\" where we selectively refine a chosen subset of layers in a pre-trained model, striking a delicate balance between retaining the wisdom of learned features and embracing the nuances of a new task.   Our exploration reveals that this approach not only rivals but often surpasses traditional fine-tuning techniques, revealing a surprising truth: the nature of the distribution shift itself dictates the most effective layers to adjust. For instance, when faced with the challenges of image corruption, the most potent results arise from fine-tuning only the initial layers, a revelation that underscores the intricate dance between model architecture and data characteristics.  Through rigorous validation across seven diverse real-world tasks, each grappling with three distinct types of distribution shifts, we unveil the robustness of our findings. Furthermore, we delve into the theoretical underpinnings of our approach, proving that in the realm of two-layer neural networks, tuning the first layer alone can yield superior outcomes compared to fine-tuning all layers in an idealized setting. This insight aligns with a deeper intuition: over-tuning on a small dataset can lead to the erosion of pre-trained knowledge, especially as the relevant information shifts with the nature of the task at hand. Thus, we stand at the precipice of a new understanding, where precision in tuning can unlock the full potential of our models, even amidst the chaos of distribution shifts.", "introduction": "While deep neural networks have achieved impressive results in many domains, they are often brittle to even small distribution shifts between the source and target domains (Recht et al., 2019;Hendrycks & Dietterich, 2019;Koh et al., 2021).While many approaches to robustness attempt to directly generalize to the target distribution after training on source data (Peters et al., 2016;Arjovsky et al., 2019), an alternative approach is to fine-tune on a small amount of labeled target datapoints.Collecting such small labeled datasets can improve downstream performance in a cost-effective manner while substantially outperforming domain generalization and unsupervised adaptation methods (Rosenfeld et al., 2022;Kirichenko et al., 2022).We therefore focus on settings where we first train a model on a relatively large source dataset and then fine-tune the pre-trained model on a small target dataset, as a means of adapting to distribution shifts.The motivation behind existing fine-tuning methods is to fit the new data while also preserving the information obtained during the pre-training phase.Such information preservation is critical for successful transfer learning, especially in scenarios where the source and target distributions share a lot of information despite the distribution shift.To reduce overfitting during fine-tuning, existing works have proposed using a smaller learning rate compared to initial pretraining (Kornblith et al., 2019;Li et al., 2020), freezing the early backbone layers and gradually unfreezing (Howard & Ruder, 2018;Mukherjee & Awadallah, 2019;Romero et al., 2020), or using a different learning rate for each layer (Ro & Choi, 2021;Shen et al., 2021).We present a result in which preserving information in a non-standard way results in better performance.Contrary to conventional wisdom that one should fine-tune the last few layers to re-use the learned features, we observe that fine-tuning only the early layers of the network results in better performance on image corruption datasets such as CIFAR-10-C (Hendrycks & Dietterich, 2019).More specifically, as an initial finding, when transferring a model pretrained on CIFAR-10 to CIFAR-10-C by fine-tuning on a small amount of labeled corrupted images, fine-tuning only the first block of layers and freezing the others outperforms full fine-tuning on all parameters by almost 3% on average on unseen corrupted images.81.2% 79.3% 86.2% 82.2%Figure 1: Surgical fine-tuning, where we tune only one block of parameters and freeze the remaining parameters, outperforms full fine-tuning on a range of distribution shifts.Moreover, we find that tuning different blocks performs best for different types of distribution shifts.Fine-tuning the first block works best for input-level shifts such as CIFAR-C (image corruption), later blocks work best for feature-level shifts such as Entity-30 (shift in entity subgroup), and tuning the last layer works best for output-level shifts such as CelebA (spurious correlation between gender and hair color).To better understand this counterintuitive result, we study a general class of fine-tuning algorithms which we call surgical fine-tuning, defined as fine-tuning only a small contiguous subset of all layers in the pre-trained neural network.Equivalently, we could define surgical fine-tuning as freezing all but a few layers during fine-tuning.Parameter freezing can be beneficial because, depending on the relationship between the source and target tasks, some layer parameters trained on the source task may be close to a minima for the target distribution.Therefore, freezing these layers can facilitate generalization to the target distribution.We evaluate the performance of surgical fine-tuning with various layer choices on 7 different distribution shift scenarios, which we categorize into input-level, feature-level, and output-level shifts.As shown in Figure 1, fine-tuning only the first block of layers, the middle block, or the last layer can perform best in different distribution shift conditions, with the best such subset consistently outperforming fine-tuning all parameters.To support our empirical results, we theoretically analyze why different types of distribution shifts require fine-tuning different layers.For two-layer neural networks, we show why fine-tuning the first layer is better for input perturbations but fine-tuning the last layer is better for label perturbations.We then present a setting where surgical fine-tuning on the first layer provably outperforms fine-tuning all parameters.If the target distribution contains only a few new \"directions\" (inputs outside the span of the source distribution), we show that tuning only the first layer can learn these new directions with very few target examples, while preserving all the information learned from the source distribution.However, we show that full fine-tuning forgets information learned from the source distribution-the last layer changes to accommodate the new target directions, but now performs poorly on examples outside the span of the training data.Motivated by the theoretical insight that freezing some layers can help generalization, we empirically analyze two criteria for automatically selecting layers to tune based on loss gradients.Tuning the layers selected by such criteria can also outperform full fine-tuning, though this procedure does not outperform manually choosing the best layers to tune.Our main contribution is the empirical observation that fine-tuning only a small contiguous subset of layers can outperform full fine-tuning on a range of distribution shifts.Intriguingly, the best layers to tune differ for different distribution shift types (Figure 1).This finding is validated empirically across seven real-world datasets and three types of distribution shifts, and theoretically in an idealized two-layer neural network setup.We additionally empirically analyze two criteria for automatically selecting which layers to tune and find that fine-tuning only the layers with higher relative gradient norm outperforms full fine-tuning."}
{"paper_id": 363, "abstract": "In the intricate tapestry of life, proteins stand as the architects of biological function, their structures woven from the delicate interplay of amino acids. These proteins are defined by two distinct realms: the 3D geometry of their amino acid coordinates and the linear sequence of their peptide chains. The three-dimensional form is a chaotic dance, where amino acids are scattered unevenly across the vast expanse of Euclidean space, their coordinates flowing as continuous variables. In stark contrast, the one-dimensional sequence unfolds with a rhythmic regularity, each amino acid aligned in a precise order, their positions fixed as discrete variables. Yet, these two dimensions exist in separate domains, their units of measurement clashing, creating a formidable challenge in capturing the essence of both structures without allowing their modeling to interfere with one another.  To navigate this complexity, we introduce the Continuous-Discrete Convolution (CDConv)\u2014a novel approach that harmonizes the irregularity of geometric structures with the orderliness of sequential arrangements. CDConv employs independent learnable weights for various regular sequential displacements, while it directly encodes the erratic nature of geometric displacements, embracing their inherent unpredictability. This innovative method enhances protein modeling, deftly mitigating the disruptive influence of geometric irregularity on the sequence.  Through extensive experimentation across a spectrum of tasks\u2014including protein fold classification, enzyme reaction classification, gene ontology term prediction, and enzyme commission number prediction\u2014we demonstrate the remarkable efficacy of CDConv. Our findings illuminate a path forward in the quest to unravel the mysteries of protein structures, showcasing the power of this new approach in redefining our understanding of biological systems.", "introduction": "Proteins are large biomolecules and are essential for life.Understanding their function is significant for life sciences.However, it usually requires enormous experimental efforts (W\u00fcthrich, 2001;Jaskolski et al., 2014;Bai et al., 2015;Thompson et al., 2020) to find out their function.Recently, with the development of deep learning, emerging computational and data-driven approaches are particularly useful for efficient protein understanding (Derevyanko et al., 2018;Ingraham et al., 2019;Strokach et al., 2020;Cao et al., 2021;Jing et al., 2021;Jumper et al., 2021;Shanehsazzadeh et al., 2020), including protein design, structure classification, model quality assessment, function prediction, etc.Because the function of proteins is based on their structure, accurately modeling protein structures can facilitate a mechanistic understanding of their function to life.Proteins are made up of different amino acids.There are 20 types of amino acids (residues) commonly found in plants and animals and a typical protein is made up of 300 or more amino acids.Because these amino acids are linked by peptide bonds and form a chain (shown in Fig. 1), proteins exhibit a 1D sequence structure.Moreover, because amino acids are arranged uniformly in the chains and their orders are discrete, the sequence structure is regular.In this case, 1D Convolutional Neural Network (CNN) (Kulmanov et al., 2018;Hou et al., 2018;Kulmanov & Hoehndorf, 2021) and Long Short-Term Memory (LSTM) (Bepler & Berger, 2019;Rao et al., 2019;Alley et al., 2019;Strodthoff et al., 2020) are widely used to model the regular 1D sequence structure of proteins.In addition to the 1D sequential order in peptide chains, each amino acid is with a 3D coordinate that specifies its spatial position in a protein.As shown in Fig. 1, those 3D coordinates describe a geometry structure, which is also crucial for protein recognition.As mentioned by Alexander et al. (2009),  (18.09, 5.83, 18.78) (16.25, 9.24, 18.87) (-18.37, 8.50, -6.16) ) Amino acids are linked by peptide bonds and form a chain, which exhibits a regular 1D sequence structure because they are arranged uniformly and their sequential orders are discrete.2) In addition, amino acids are with 3D coordinates that determine a geometry structure, which exhibits irregularity because they are distributed unevenly in Euclidean space and their coordinates are continuous variables.proteins with similar peptide chains may fold into very different 3D geometry structures.Conversely, proteins with similar 3D geometry structures may have entirely different amino acid chains (Agrawal & Kishan, 2001).Therefore, it is necessary to consider both the 1D and 3D structures in protein modeling.However, different from the sequence structure, the geometry structure is irregular because amino acids are distributed unevenly in Euclidean space and their coordinates are continuous variables.This increases the challenge for neural networks to understand proteins.To model the 1D and 3D structures in proteins, Gligorijevi\u0107 et al. (2021) employed an LSTM and a Graph Convolutional Network (GCN) for sequence and geometry modeling, respectively.Because the two structures are processed separately, it may not properly understand proteins' local geometrysequence structure.In contrast, a few unified networks try to model the two types of structures together (Baldassarre et al., 2021;Jing et al., 2021;Zhang et al., 2022;Hermosilla & Ropinski, 2022).However, those methods process geometric and sequential displacements together or model the sequence structure in a similar way to geometry modeling, thus neglecting the regularity of the 1D structure.Moreover, because the length units of the sequential and geometric spaces in proteins are not compatible, treating their distances similarly may mislead deep neural networks.In this paper, we first propose and formulate a new class of convolution operation, named Continuous-Discrete Convolution (CDConv), to make the most of the dual discrete and continuous nature of the data to avoid the impact of regular and irregular modeling on each other.Specifically, CDConv employs independent learnable weights to reflect different regular and discrete displacements but directly encodes continuous displacements due to their irregularity.Then, we implement a (3+1)D CDConv and use this operation to construct a hierarchical (3+1)D CNN for geometry-sequence modeling in proteins.We apply our network to protein fold classification, enzyme reaction classification, gene ontology term prediction and enzyme commission number prediction.Experimental results demonstrate the effectiveness of the proposed method.The contributions of this paper are fourfold:\u2022 We propose a new class of convolution, i.e., CDConv, which unifies continuous and discrete convolutions and makes the most of the regularity and irregularity in data, respectively.\u2022 We implement a (3+1)D CDConv for geometry-sequence modeling in proteins.Based on the convolution, we construct deep neural networks for protein representation learning.\u2022 We conduct extensive experiments on four tasks and the proposed method surpasses the previous methods by large margins, resulting in the new state-of-the-art accuracy.\u2022 We find that amino acids in central regions may be more important than those in surface areas.This may need to be verified via biochemical experiments in the future."}
{"paper_id": 364, "abstract": "In the realm of multi-agent general-sum Markov games, where the dynamics weave a complex tapestry of interactions, we embark on a quest to unravel the mysteries of nonlinear function approximation. Our focus narrows to low-rank Markov games, where the transition matrix hides a low-rank structure beneath the surface of an enigmatic nonlinear representation. The challenge before us is twofold: first, to forge an algorithm capable of discovering an $\\varepsilon$-equilibrium policy sample with remarkable efficiency, all while navigating the uncharted waters of an unknown environment and representation; and second, to ensure that our creation is amenable to the powerful tools of deep learning.  Harnessing the power of representation learning, we unveil both model-based and model-free approaches that craft effective representations from the data we gather. In both paths, our algorithm achieves a sample complexity that scales as poly$(H,d,A,1/\\varepsilon)$\u2014where $H$ denotes the game horizon, $d$ the dimension of the feature vector, $A$ the size of the joint action space, and $\\varepsilon$ the optimality gap. Yet, as the number of players swells, the specter of exponential scaling looms large over our sample complexity, threatening to overwhelm our efforts.  To combat this formidable challenge, we delve into Markov Games characterized by a factorized transition structure, unveiling an algorithm that deftly sidesteps the exponential scaling that plagues its predecessors. To the best of our knowledge, this marks the first sample-efficient algorithm for multi-agent general-sum Markov games that embraces the intricacies of (non-linear) function approximation. We complement our theoretical findings with a neural network-based implementation of our algorithm, putting it to the test against the renowned deep reinforcement learning baseline, DQN with fictitious play. In this grand experiment, we seek to illuminate the path forward, forging new alliances between theory and practice in the ever-evolving landscape of multi-agent learning.", "introduction": "Multi-agent reinforcement learning (MARL) studies the problem where multiple agents learn to make sequential decisions in an unknown environment to maximize their (own) cumulative rewards.Recently, MARL has achieved remarkable empirical success, such as in traditional games like GO (Silver et al., 2016(Silver et al., , 2017) ) and Poker (Morav\u010d\u00edk et al., 2017), real-time video games such as Starcraft and Dota 2 (Vinyals et al., 2019;Berner et al., 2019), decentralized controls or multi-agent robotics systems (Brambilla et al., 2013) and autonomous driving (Shalev-Shwartz et al., 2016).On the theoretical front, however, provably sample-efficient algorithms for Markov games have been largely restricted to either two-player zero-sum games (Bai et al., 2020;Xie et al., 2020;Chen et al., 2021;Jin et al., 2021c) or general-sum games with small and finite state and action spaces (Bai and Jin, 2020;Liu et al., 2021;Jin et al., 2021b).These algorithms typically do not permit a scalable implementation applicable to real-world games, due to either (1) they only work for tabular or linear Markov games which are too restrictive to model real-world games, or (2) the ones that do handle rich non-linear function approximation (Jin et al., 2021c) are not computationally efficient.This motivates us to ask the following question:Can we design an efficient algorithm that (1) provably learns multi-player general-sum Markov games with rich nonlinear function approximation and (2) permits scalable implementations?This paper presents the first positive answer to the above question.In particular, we make the following contributions:1. We design a new centralized self-play meta algorithm for multi-agent low-rank Markov games:General Representation Learning for Multi-player General-sum Markov Game (GERL_MG2).We present a model-based and a model-free instantiation of GERL_MG2 which differ by the way function approximation is used, and a clean and unified analysis for both approaches.2. We show that the model-based variant requires access to an MLE oracle and a NE/CE/CCE oracle for matrix games, and enjoys a \u00d5 H 6 d 4 A 2 log(|\u03a6||\u03a8|)/\u03b5 2 sample complexity to learn an \u03b5-NE/CE/CCE equilibrium policy, where d is the dimension of the feature vector, A is the size of the joint action space, H is the game horizon, \u03a6 and \u03a8 are the function classes for the representation and emission process.The model-free variant replaces model-learning with solving a minimax optimization problem, and enjoys a sample complexity of \u00d5 H 6 d 4 A 3 M log(|\u03a6|)/\u03b5 2 for a slightly restricted class of Markov game with latent block structure.3.Both of the above algorithms have sample complexities scaling with the joint action space size, which is exponential in the number of players.This unfavorable scaling is referred to as the curse of multi-agent, and is unavoidable in the worst case under general function approximation.We consider a spatial factorization structure where the transition of each player's local state is directly affected only by at most L = O(1) players in its adjacency.Given this additional structure, we provide an algorithm that achieves \u00d5(M 4 H 6 d 2(L+1) 2 \u00c32(L+1) /\u03b5 2 ) sample complexity, where \u00c3 is the size of a single player's action space, thus escaping the exponential scaling to the number of agents.4. Finally, we provide an efficient implementation of our reward-free algorithm, and show that it achieves superior performance against traditional deep RL baselines without principled representation learning."}
{"paper_id": 365, "abstract": "In the realm of few-shot anomaly detection (FSAD), the efficiency of visual features serves as the lifeblood for memory bank $\\mathcal{M}$-based methods. Yet, a critical oversight persists: these methods often overlook the intricate relationship between visual features and their rotated counterparts, which significantly hampers their anomaly detection prowess. To transcend these limitations, we unveil a pivotal insight: the rotation-invariant properties of features wield considerable influence in industrial applications of FSAD.   In this endeavor, we harness the power of graph representation, introducing a groundbreaking concept\u2014the visual isometric invariant feature (VIIF)\u2014to serve as a robust metric for anomaly detection. This innovative approach not only enhances the ability to discriminate anomalies but also substantially trims the excess baggage of redundant features within the memory bank $\\mathcal{M}$.   Moreover, we present GraphCore, a novel model built upon the foundation of VIIFs, designed to expedite unsupervised FSAD training while amplifying the efficacy of anomaly detection. A thorough evaluation juxtaposes GraphCore against other state-of-the-art anomaly detection models within our newly established few-shot anomaly detection framework. The results are striking: GraphCore exhibits a remarkable increase in average AUC scores\u2014by 5.8%, 4.1%, 3.4%, and 1.6% on the MVTec AD dataset, and by an astonishing 25.5%, 22.0%, 16.9%, and 14.1% on the MPDD dataset for 1, 2, 4, and 8-shot scenarios, respectively. Thus, we stand on the precipice of a new era in anomaly detection, where the interplay of features and their invariances unlocks unprecedented potential.", "introduction": "With the rapid development of deep vision detection technology in artificial intelligence, detecting anomalies/defects on the surface of industrial products has received unprecedented attention.Changeover in manufacturing refers to converting a line or machine from processing one product to another.Since the equipment has not been completely fine-tuned after the start of the production line, changeover frequently results in unsatisfactory anomaly detection (AD) performance.How to achieve rapid training of industrial product models in the changeover scenario while assuring accurate anomaly detection is a critical issue in the actual production process.The current state of AD in the industry is as follows: (1) In terms of detection accuracy, the performance of state-ofthe-art (SOTA) AD models degrades dramatically during the changeover.Current mainstream work utilizes a considerable amount of training data as input to train the model, as shown in Fig. 1(a).However, this will make data collecting challenging, even for unsupervised learning.As a result, many approaches based on few-shot learning at the price of accuracy have been proposed.For instance, Huang et al. (2022) employ meta-learning, as shown in Fig. 1(b).While due to complicated settings, it is impossible to migrate to the new product during the changeover flexibly, and the detection accuracy cannot be guaranteed.(2) In terms of training speed, when a large amount of data is utilized for training, the training progress for new goods is slowed in the actual production line.As is well-known, vanilla unsupervised AD requires to collect a large amount of information.Even though meta-learning works in few-shot learning, as shown in Fig. 1(b), it is still necessary to train a massive portion of previously collected data.For our setting (c), there is no requirement to aggregate training categories in advance.The proposed model, vision isometric invariant GNN, can fast obtain the invariant feature within a few normal samples, and its accuracy outperforms models trained in a meta-learning context.We state that AD of industrial products requires just a small quantity of data to achieve performance comparable to a large amount of data, i.e., a small quantity of image data can contain sufficient information to represent a large number of data.Due to the fact that industrial products are manufactured with high stability (no evident distortion of shape and color cast), the taken images lack the diversity of natural images, and there is a problem with the shooting angle or rotation.Therefore, it is essential to extract rotation-invariant structural features.As graph neural networks (GNNs) are capable of robustly extracting non-serialized structural features (Han et al. (2022), Bruna et al. (2013), Hamilton et al. (2017), Xu et al. (2018)), and they integrate global information better and faster Wang et al. (2020); Li et al. (2020).They are more suited than convolution neural networks (CNNs) to handle the problem of extracting rotation-invariant features.For this reason, the core idea of the proposed GraphCore method in this paper is to use the visual isometric invariant features (VIIFs) as the anomaly measurement features.In the method using memory bank (M) as the AD paradigm, PatchCore (Roth et al. (2022)) uses ResNet (He et al. ( 2016)) as the feature extractor.However, since their features obtained by CNNs do not have rotation invariance (Dieleman et al. ( 2016)), a large number of redundant features are stored in M. Note that these redundant features maybe come from multiple rotation features of the same patch structure.It will hence require a huge quantity of training data to ensure the high accuracy of the test set.To avoid these redundant features, we propose VIIFs, which not only produce more robust visual features but also dramatically lower the size of M and accelerate detection.Based on the previous considerations, the goal of our work is to handle the cold start of the production line during the changeover.As shown in Fig. 1(c), a new FSAD method, called GraphCore, is developed that employs a small number of normal samples to accomplish fast training and competitive AD accuracy performance of the new product.On the one hand, by utilizing a small amount of data, we would rapidly train and accelerate the speed of anomaly inference.On the other hand, because we directly train new product samples, adaptation and migration of anomalies from the old product to the new product do not occur.Contributions.In summary, the main contributions of this work are as follows:\u2022 We present a feature-augmented method for FSAD in order to investigate the property of visual features generated by CNNs.\u2022 We propose a novel anomaly detection model, GraphCore, to add a new VIIF into the memory bank-based AD paradigm, which can drastically reduce the quantity of redundant visual features.\u2022 The experimental results show that the proposed VIIFs are effective and can significantly enhance the FSAD performance on MVTec AD and MPDD.Related Work.Few-shot anomaly detection (FSAD) is an attractive research topic.However, there are only a few papers devoted to the industrial image FSAD.Some works (Liznerski et al. (2020); Pang et al. (2021); Ding et al. (2022)) experiment with few-shot abnormal images in the test set, which contradicts our assumptions that no abnormal images existed.While others (Wu et al. (2021); Huang et al. (2022)) conduct experiments in a meta-learning setting.This configuration has the disadvantage of requiring a high number of base class images and being incapable of addressing the shortage of data under cold-start conditions in industrial applications.PatchCore (Roth et al. (2022)), SPADE (Cohen & Hoshen (2020)), and PaDiM (Defard et al. (2021)) investigated AD performance on MVTec AD in a few-shot setting.However, these approaches are not intended for changeover-based few-shot settings.Thus their performance cannot satisfy the requirements of manufacturing changeover.In this research, we propose a feature augmentation method for FSAD that can rapidly finish the training of anomaly detection models with a small quantity of data and meet manufacturing changeover requirements."}
{"paper_id": 366, "abstract": "In the ever-evolving landscape of machine learning, we have encountered and conquered two formidable challenges that stand in the way of harnessing the power of BERT-style pre-training, particularly in the realm of convolutional networks (convnets). The first obstacle arises from the convolution operation's inability to process irregular, randomly masked input images, while the second stems from the single-scale nature of BERT pre-training, which clashes with the inherently hierarchical structure of convnets.  To tackle the first issue, we reimagine unmasked pixels as sparse voxels within a 3D point cloud, employing sparse convolution for encoding\u2014marking a pioneering application of this technique in the realm of 2D masked modeling. For the second challenge, we introduce a hierarchical decoder designed to reconstruct images from features encoded at multiple scales. Our innovative approach, dubbed Sparse masKed modeling (SparK), is remarkably versatile, enabling seamless integration into any convolutional model without necessitating alterations to the backbone architecture.  We rigorously validate SparK on both classical architectures like ResNet and contemporary designs such as ConvNeXt. The results are compelling: across three downstream tasks, our method outperforms both state-of-the-art contrastive learning and transformer-based masked modeling by a substantial margin\u2014approximately +1.0%. Notably, the enhancements in object detection and instance segmentation are even more pronounced, reaching up to +3.5%, underscoring the robust transferability of the features learned through SparK. Furthermore, we observe a promising scaling behavior, with larger networks yielding even greater gains.  These findings illuminate a bright horizon for generative pre-training within the realm of convnets. For those eager to explore this frontier, both the code and pre-trained models are readily available at https://github.com/keyu-tian/SparK.", "introduction": "The pretrain-finetune paradigm in natural language processing (NLP), as exemplified by BERT and GPT (Devlin et al., 2018;Clark et al., 2020;Radford et al., 2019;Brown et al., 2020), is remarkably effective and thus long envied by our vision community.It is the emerging masked image modeling (Bao et al., 2021;He et al., 2021;Xie et al., 2021;Chen et al., 2022) initially extends the success of BERT from language transformers to vision transformers (ViTs).A bold move that increases the mask ratio to a staggering level (60~75%) is largely credited with this success (He et al., 2021;Xie et al., 2021).As a result, the field of visual self-supervised learning on ViTs (Dosovitskiy et al., 2020;Liu et al., 2021) has now shifted from contrastive learning (Grill et al., 2020;Chen et al., 2021;Caron et al., 2021) to BERT-style masked modeling or a fusion of the two (Zhou et al., 2021).Despite this progress, extending the success of BERT pre-training from transformers to convolutional networks (convnets) remains a desirable but unrealized goal.Early pioneering work (Pathak et al., 2016) predated BERT but performed much worse than supervised pre-training.Although there have been efforts over the past year to port BERT to convnets, they ultimately compromise by proposing a non-convolutional model (Gao et al., 2022) or non-masked modeling (Fang et al., 2022).One might therefore wonder: what exactly is impeding the application of BERT to convnets?We try to conclude that in essence, the difficulty is rooted in the fundamental differences in data processing between language and vision (Bateman, 2014;Cheng et al., 2022).While typical NLP models like recurrent networks or transformers process text as a variable-length sequence of words (well-defined semantic units), convnets have to recognize objects of different sizes from raw pixels (\"units\" at different scales).This large disparity rises two challenges: (i) Removing the information (a) illustrates MAE (He et al., 2021) that has no such side effect thanks to the transformer's ability to process variable-length input.We propose (c) to adapt convnets to irregular masked input without a distribution shift.of masked \"words\" is difficult for convnets.In ViTs, an input image is divided into non-overlapping patches.Simply dropping masked patches or replacing them with mask tokens can remove the information.This ease relies on transformer being able to handle irregular (variable-length) and nonoverlapping patches, thus cannot be achieved on convnets as they not only operate on regular grids, but also perform sliding window with overlapping.One may zero-out all masked pixels and feed this \"mosaic\" into a convnet, but this would lead to a significant distribution shift (in figure 1) and other issues (discussed further in section 3.1 and figure 3), thus cannot be an ideal solution.(ii) Single-scale algorithms are inadequate for learning multi-scale (hierarchical) features.Multi-scale structures have been a gold standard in computer vision, which allows visual processing systems like SIFT descriptors (Lowe, 1999;Bay et al., 2006) and pyramid networks (He et al., 2015;Lin et al., 2017) to handle variations in object scale.In contrast, the masked modeling approach from NLP operates in a single-scale manner.Applying it directly on convnets will miss the advantage of model hierarchy.In this work, we clear the hurdles above and make BERT suitable for convnet by proposing Sparse masKed modeling with hierarchy (SparK).We first randomly mask an image in a patch-wise manner.Observing the sparse nature of point clouds coincides with these unmasked patches, we treat them as a flatten point cloud and use sparse convolution for encoding.This enables convnets to handle irregular masked images.For decoding and reconstruction, the sparse features are filled with mask embeddings and fed into a multi-scale decoder, leveraging the hierarchical structure of convnets.SparK is a general method that does not limit the specific encoder to be pre-trained.We test it with two representative convnet famlies: classical ResNets (He et al., 2016) and modern ConvNeXts (Liu et al., 2022).All models benefit from SparK, with more gains on larger models that demonstrates its favorable scaling ability.On standard downstream tasks (classification, object detection and instance segmentation), convnet-based SparK outperforms both (i) state-of-the-art contrastive learning and (ii) transformer-based masked modeling by similarly large margins (around +1.0%).The improvements over COCO baselines are more significant than those on ImageNet (up to +3.5%), indicating the representations learned by SparK are highly transferable.To summarize, SparK provides:\u2022 The first pre-training method in the style of BERT that can be directly applied to any convnets without backbone modifications, overcoming their inability to handle irregular masked inputs.\u2022 The insights into the design of generative pre-training for convnets, e.g., the first use of sparse convolution for masked image modeling and a hierarchical design for BERT-style pre-training.\u2022 A leap in convnet's performance across downstream tasks with gains of up to 3.5 points, showing the promise of extending the success of transformer's pretrain-finetune paradigm to convnets.The recent surge of interest in vision transformers (Liu et al., 2021;He et al., 2021) has shifted the focus away from convnets in the computer vision community.However, convnets embody the core principles of many classical vision processing systems, such as scale-and translation-equivariance, locality, weight-sharing, and hardware-friendliness (Lowe, 1999;Csurka et al., 2004).These networks continue to be indispensable in addressing a variety of challenging and structural real-world tasks beyond classification (Jaderberg et al., 2015;Liu et al., 2017;2022).We hope SparK's inspiring performance will prompt us to revisit convnets as generic backbones for computer vision community, and motivate more future arts in exploiting their potential through generative pre-training."}
{"paper_id": 367, "abstract": "In the vast landscape of machine learning, the quest to unravel the mappings between infinite-dimensional function spaces has sparked remarkable advancements across a multitude of fields\u2014be it generative modeling, functional data analysis, causal inference, or the intricate dance of multi-agent reinforcement learning. In this exploration, we delve into the statistical foundations of learning a Hilbert-Schmidt operator that connects two infinite-dimensional Sobolev reproducing kernel Hilbert spaces.   Our journey begins with the establishment of an information-theoretic lower bound, intricately tied to the Sobolev Hilbert-Schmidt norm, illuminating the path ahead. We unveil a strategy that involves regularization: by honing in on spectral components nestled below the bias contour while disregarding those that soar above the variance contour, we unlock the potential for achieving the optimal learning rate.   Yet, it is within the spectral components that lie between these contours where we find our greatest flexibility\u2014a treasure trove for crafting computationally feasible machine learning algorithms. Drawing from this insight, we introduce a multilevel kernel operator learning algorithm, meticulously designed to be optimal for the task of learning linear operators within the realm of infinite-dimensional function spaces. With this, we not only push the boundaries of what is possible but also forge a new path forward in the intricate tapestry of machine learning.", "introduction": "Supervised learning of operators between two infinite-dimensional spaces has attracted attention in several areas of application of machine learning, including, scientific computing (Lu et al., 2019;Li et al., 2020;de Hoop et al., 2021;Li et al., 2018;2021b), functional data analysis (Crambes & Mas, 2013;H\u00f6rmann & Kidzi\u0144ski, 2015;Wang et al., 2020a), mean-field games (Guo et al., 2019;Wang et al., 2020b), conditional kernel mean embedding (Song et al., 2009;2013;Muandet et al., 2017) and econometrics (Singh et al., 2019;Muandet et al., 2020;Dikkala et al., 2020;Singh et al., 2020).Despite the empirical success of operator learning, the statistical limit of learning an infinite-dimensional operator has not been investigated studied.In this paper, we study the problem of learning Hilbert Schmidt operators between infinite-dimensional Sobolev RKHSs H \u03b2 K and H \u03b3 L with given kernels k and l, respectively with \u03b2, \u03b3 \u2208 [0, 1) (Adams & Fournier, 2003;Christmann & Steinwart, 2008;Fischer & Steinwart, 2020).Our goal is to derive the optimal sample complexity for linear operator learning, i.e. how much data is required to achieve a certain performance level.We first establish an information-theoretic lower bound for learning a Hilbert-Schmidt operator between Sobolev spaces with respect to a general Sobolev norm.Our information-theoretic lower bound indicates that the optimal learning rate is determined by the minimum of two polynomial rates: one is purely decided by the input Sobolev reproducing kernel Hilbert space and its evaluating norm, while the other one is purely determined by the output space along with its evaluating norm.The rate is novel in that all existing results (Fischer & Steinwart, 2020;Li et al., 2022;de Hoop et al., 2021) only establish rates that depend on the parameter of input space.The reason is all previous works (Talwai et al., 2022;Li et al., 2022;de Hoop et al., 2021) only consider the case of the output space as a subspace of a trace bounded reproducing kernel Hilbert space but not a general Sobolev space.We refer to Remark 2.1 for detailed comparisons.To design a learning algorithm for approximating an infinite-dimensional operator, we need to learn a finite-dimensional restriction instead of the whole operator, as the latter would result in infinite variance.The finite-dimensional selection leads to bias error but decreases the variance.A natural task is then to study the shape of regularization that can lead to the optimal bias-variance trade-off and achieve the optimal learning rate.In this paper, we consider the bias and variance contour at the scale of optimal learning.Once the regularization enables one to learn all the spectral parts above the bias contour and below the variance contour, the learning is optimal.Finally, utilizing the region between the bias contour and variance contour, we developed a multilevel training algorithm (Lye et al., 2021;Li et al., 2021a) which first learns the mapping on low frequency and then successively fine-tunes the machine learning models to fit the high-frequency output.The intuition of our algorithm aligns with the original motivation of multilevel Monte Carlo (Giles, 2008;2015): we use the next level to reduce bias while keeping the variance at the same scale.We demonstrate that such a multilevel algorithm can achieve an optimal non-parametric rate for linear operator learning."}
{"paper_id": 368, "abstract": "In the realm of Multiplayer Online Battle Arenas (MOBAs), where strategy and teamwork reign supreme, titles like Dota 2 and Honor of Kings have become fertile ground for the latest advancements in artificial intelligence. While many AI systems have reached a level of proficiency that rivals human players, the focus has predominantly been on competition rather than collaboration. This paper embarks on an ambitious quest to bridge that gap, forging a new path in the exploration of human-agent synergy within these dynamic battlegrounds.  We introduce a groundbreaking framework, the Meta-Command Communication (MCC), designed to facilitate seamless collaboration between humans and AI agents through explicit communication. At the heart of this framework lie two essential components: first, an interpretable communication protocol known as the Meta-Command, which serves as a vital link, enabling clear dialogue between human players and their AI counterparts; second, the Meta-Command Selector, a sophisticated value estimator that empowers each agent to choose the most impactful meta-command, ensuring effective teamwork.  Our experiments within Honor of Kings reveal that agents operating under the MCC framework not only collaborate effectively with human teammates but also demonstrate an impressive ability to adapt and work alongside players of varying skill levels and team compositions. For those eager to witness this innovative collaboration in action, videos showcasing our findings can be found at https://sites.google.com/view/mcc-demo. Join us as we venture into uncharted territory, redefining the boundaries of cooperation between humans and machines in the heat of battle.", "introduction": "Games, as the microcosm of real-world problems, have been widely used as testbeds to evaluate the performance of Artificial Intelligence (AI) techniques for decades.Recently, many researchers focus on developing various human-level AI systems for complex games, such as board games like Go (Silver et al., 2016;2017), Real-Time Strategy (RTS) games like StarCraft 2 (Vinyals et al., 2019), and Multi-player Online Battle Arena (MOBA) games like Dota 2 (OpenAI et al., 2019).However, these AI systems mainly focus on how to compete instead of collaborating with humans, leaving Human-Agent Collaboration (HAC) in complex environments still to be investigated.In this paper, we study the HAC problem in complex MOBA games (Silva & Chaimowicz, 2017), which is characterized by multi-agent cooperation and competition mechanisms, long time horizons, enormous state-action spaces (10 20000 ), and imperfect information (OpenAI et al., 2019;Ye et al., 2020a).HAC requires the agent to collaborate reasonably with various human partners (Dafoe et al., 2020).One straightforward approach is to improve the generalization of agents, that is, to collaborate with a sufficiently diverse population of teammates during training.Recently, some population-based methods proposed to improve the generalization of agents by constructing a diverse population of partners in different ways, succeeding in video games (Jaderberg et al., 2017;2019;Carroll et al., 2019;Strouse et al., 2021) and card games (Hu et al., 2020;Andrei et al., 2021).Furthermore, to better evaluate HAC agents, several objective as well as subjective metrics have been proposed (Du et al., 2020;Siu et al., 2021;McKee et al., 2022).However, the policy space in complex MOBA games is enormous (Gao et al., 2021) and requires massive computing resources to build a sufficiently diverse population of agents, posing a big obstacle to the scalability of these methods.The communication ability to explicitly share information with others is important for agents to collaborate effectively with humans (Dafoe et al., 2020).In Multi-Agent Reinforcement Learning (MARL), communication is often used to improve inter-agent collaboration.Previous work (Sukhbaatar et al., 2016;Foerster et al., 2016;Lazaridou et al., 2016;Peng et al., 2017;Mordatch & Abbeel, 2018;Singh et al., 2018;Das et al., 2019;Wang et al., 2020) mainly focused on exploring communication protocols between multiple agents.Other work (Ghavamzadeh & Mahadevan, 2004;Jiang & Lu, 2018;Kim et al., 2019) proposed to model the value of multi-agent communication for effective collaboration.However, these methods all model communication in latent spaces without considering the human-interpretable common ground (Clark & Brennan, 1991;Stalnaker, 2002) or lingua franca Kambhampati et al. (2022), making themselves less interpretable to humans.Explicit communication dominated by natural language is often considered in human-robot interaction (Kartoun et al., 2010;Liu et al., 2019;Shafti et al., 2020;Gupta et al., 2021).However, these studies are mainly limited to collaboration between a robot and a human through one-way communication, i.e., humans give robots orders.Therefore, there is still a large room to study RL with the participation of humans.Success in MOBA games requires subtle individual micro-operations and excellent communication and collaboration among teammates on macro-strategies, i.e., long-term intentions (Wu, 2019;Gao et al., 2021).The micro-operation ability of the existing State-Of-The-Art (SOTA) MOBA agents has exceeded the high-level (top 1%) humans (Ye et al., 2020a).However, these agents' macro-strategies are deterministic and quite different from those of humans (Ye et al., 2020a).Moreover, all existing SOTA MOBA AI systems lack bridges for explicit communication between agents and humans on macro-strategies.These result in the agent's behavior not being understood immediately by humans (Ye et al., 2020a) and not performing well when collaborating with humans (see Section 4.3).To this end, we propose an efficient and interpretable Meta-Command Communication-based humanagent collaboration framework, dubbed MCC, to achieve effective HAC in MOBA games through explicit communication.First, we design an interpretable communication protocol, i.e., the Meta-Command, as a general representation of macro-strategies to bridge the communication gap between agents and humans.Both macro-strategies sent by humans and messages outputted by agents can be converted into unified meta-commands (see Figure 1(b)).Second, following Gao et al. (2021), we construct a hierarchical model that includes the command encoding network (macro-strategy layer) and the meta-command conditioned action network (micro-action layer), used for agents to generate and execute meta-commands, respectively.Third, we propose a meta-command value estimator, i.e., the Meta-Command Selector, to select the optimal meta-command for each agent to execute.The training process of the MCC agent consists of three phases.We first train the command encoding network to ensure that the agent learns the distribution of meta-commands sent by humans.Afterward, we train the meta-command conditioned action network to ensure that the agent learns to execute meta-commands.Finally, we train the meta-command selector to ensure that the agent learns to select the optimal meta-commands to execute.We train and evaluate the agent in Honor of Kings 5v5 mode with a full hero pool (over 100 heroes).Experimental results demonstrate the effectiveness of the MCC framework.In general, our contributions are as follows:\u2022 To the best of our knowledge, we are the first to investigate the HAC problem in MOBA games.We propose the MCC framework to achieve effective HAC in MOBA games.\u2022 We design the Meta-Command to bridge the communication gap between humans and agents.We also propose the Meta-Command Selector to model the agent's value system for meta-commands.\u2022 We introduce the training process of the MCC agent in a typical MOBA game Honor of Kings and evaluate it in practical human-agent collaboration tests.Experimental results show that the MCC agent can reasonably collaborate with different levels and numbers of human teammates."}
{"paper_id": 369, "abstract": "In the realm of modern quantum annealers, the quest for high-quality solutions to the intricate puzzles of combinatorial optimization\u2014specifically those framed as quadratic unconstrained binary optimization (QUBO) problems\u2014has revealed itself to be a formidable challenge. The labyrinth of computer vision, in particular, poses significant hurdles, often demanding painstakingly crafted analytical derivations to arrive at suitable QUBO formulations. These explicit constructs, while functional, impose rigid constraints on how solutions can be encoded, limiting the creative potential of the algorithms.  In a departure from traditional approaches, this paper unveils a novel methodology: the art of learning QUBO forms directly from data through the elegant dance of gradient backpropagation, rather than relying on cumbersome derivations. This shift allows for a remarkable flexibility in solution encodings, enabling them to be both compact and adaptable. Our approach stands apart, offering a generality that remains largely agnostic to the specific nuances of any given problem type.  To illustrate the power of our learned QUBOs, we delve into a diverse array of challenges, including graph matching, 2D point cloud alignment, and 3D rotation estimation. The results speak for themselves, showcasing competitive performance against the previous quantum state of the art, all while utilizing significantly fewer logical and physical qubits. This efficiency not only enhances our method's scalability but also opens the door to tackling larger, more complex problems with ease.  For those eager to explore this frontier, the code and a new dataset await at https://4dqv.mpi-inf.mpg.de/QuAnt/. Join us as we venture into this brave new world of quantum optimization, where learning and innovation intertwine to forge paths previously unimagined.", "introduction": "Hybrid computer vision methods that can be executed partially on a quantum computer (QC) are an emerging research area (Boyda et al., 2017;Cavallaro et al., 2020;Seelbach Benkner et al., 2021;Yurtsever et al., 2022).Compared to classical methods, they promise to solve computationally demanding (e.g., combinatorial) sub-problems faster, with improved scaling, and without relaxations that often lead to approximate solutions.Although quantum primacy has not yet been demonstrated in remotely practical usages of quantum computing, all existing quantum computer vision (QCV) methods fundamentally assume that it will be achieved in the future.Thus, solving these suitable algorithmic parts on a QC has the potential to reshape the field.However, reformulating them for execution on a QC is often non-trivial.QCV continues building up momentum, fuelled by accessible experimental quantum annealers (QA) allowing to solve practical (N P-hard) optimisation problems.Existing QCV methods using QAs rely on analytically deriving QUBOs (both QUBO matrices and solution encodings) for a specific problem type, which is challenging, especially since solutions need to be encoded as binary vectors (Li & Ghosh, 2020;Seelbach Benkner et al., 2020;2021;Birdal et al., 2021).This often leads to larger encodings than necessary, severely impacting scalability.Alternatively, QUBO derivations with neural networks are conceivable but have not yet been scrutinised in the QA literature.In stark contrast to the state of the art, this paper proposes, for the first time, to learn QUBO forms from data for any problem type using backpropagation (see Fig. 1).Our framework captures, in the weights of a neural network, the entire subset of QUBOs belonging to a problem type; a single forward pass yields the QUBO form for a given problem instance.It is thus a meta-learning approach in the context of hybrid (quantum-classical) neural network training, in which the superordinate network instantiates the parameters of the QUBO form.We find that sampling instantiated QUBOs can be a reasonable alternative to non-quantum neural baselines that regress the solution directly.Figure 1: We propose QuAnt for QUBO learning, i.e., a quantum-classical meta-learning algorithm that avoids analytical QUBO derivations by learning to regress QUBOs to solve problems of a given type.We first represent a problem instance as a vector p and then feed it into an MLP that regresses the entries of the QUBO matrix A. We then initialise a quantum annealer with A and use quantum annealing to find a QUBO minimiser and extract it as the solution x * to the problem instance.We define losses involving x * that avoid backpropagation through the annealing and backpropagate gradients through the MLP to train it.We demonstrate the generalisability of QuAnt on graph matching, point set registration, and rotation estimation.In particular, we show how a (combinatorial) quantum annealing solver can be integrated into a vanilla neural network as a custom layer and be used in the forward and backward passes, which may be useful in other contexts.To that end, we introduce a contrastive loss that circumvents the inherently discontinuous and non-differentiable nature of QUBO solvers.Our method is compatible with any QUBO solver at training and test time-we consider parallelised exhaustive search, simulated annealing, and quantum annealing.QUBO learning, i.e., determining a function returning QUBO forms given a problem instance of some problem type as input, is a non-trivial and challenging task.In summary, this paper makes several technical contributions to enable QUBO learning:1. QuAnt, i.e., a new meta-learning approach to obtain QUBO forms executable on modern QAs for computer vision problems.While prior methods rely on analytical derivations, we learn QUBOs from data (Sec.3.1).2. A new training strategy for neural methods with backpropagation involving finding lowenergy solutions to instantaneous (optimised) QUBO forms, independent of the solver (Secs.3.2 and 3.3).3. Application of the new framework to several problems with solutions encoded by permutations and discretised rigid transformations (Secs.3.4 and 3.5).We show that our methodology is a standardised way of obtaining QUBOs independent of the target problem type.This paper focuses on three problem types already tackled by QCV methods relying on analytical QUBO derivations: graph matching and point set alignment (with and without known prior point matches in the 3D and 2D cases, respectively).We emphasise that we do not claim to outperform existing specialised methods for these problem types or that QA is particularly wellsuited for them.Rather, we show that this wide variety of problems can be tackled successfully and competitively by our general quantum approach already now, before quantum primacy.Thus, in the future, computer vision methods may readily benefit from the (widely expected) speed-up of QC through an easy and flexible re-formulation of algorithmic parts as QUBOs, thanks to our proposed method.We run our experiments on D-Wave Advantage5.1 (Dattani et al., 2019), an experimental realisation of AQC with remote access.This paper assumes familiarity with the basics of quantum computing.For convenience, we summarise several relevant definitions in the Appendix."}
{"paper_id": 370, "abstract": "In the ever-evolving landscape of sequence modeling, where the realms of natural language processing and computer vision converge, a new champion has emerged. The transformer-based models, heralded for their remarkable prowess, harness the power of attention to weave intricate relationships between tokens and employ position embeddings to anchor them in time. Yet, even these titans face a daunting challenge: the inefficiency that arises when scaling to lengthy input sequences, a consequence of the quadratic space-time complexity inherent in their attention mechanisms.  To confront this formidable obstacle, we present a novel approach that reimagines sequence modeling through the lens of a relative position encoded Toeplitz matrix. By employing a clever Toeplitz matrix-vector product trick, we elegantly reduce the space-time complexity to a more manageable log-linear scale. At the heart of our innovation lies a lightweight sub-network, aptly named the relative position encoder, which deftly generates relative position coefficients while adhering to a fixed parameter budget. This enables our Toeplitz neural network to gracefully adapt to sequences of varying lengths.  Remarkably, although our model is trained on sequences of 512 tokens, it possesses the uncanny ability to extrapolate input lengths up to an astonishing 14,000 tokens during inference, all while maintaining consistent performance. Through rigorous experimentation across autoregressive and bidirectional language modeling, image modeling, and the formidable Long-range Arena Benchmark, our method has consistently outperformed its rivals in most downstream tasks, all while delivering a significant boost in speed. In this brave new world of sequence modeling, we stand at the forefront, ready to redefine what is possible.", "introduction": "Sequence modeling is a fundamental problem in natural language processing, speech processing, and computer vision.Various sequence modeling methods have been proposed in the literature, including recurrent (Hochreiter & Schmidhuber, 1997), convolutional architectures (LeCun et al., 1989), and transformers (Vaswani et al., 2017).These models utilize various properties of sequential data for their modeling.For example, recurrent models (Hochreiter & Schmidhuber, 1997) mimic the sequential property by sequentially processing the input while maintaining hidden states through steps.Convolutional models (LeCun et al., 1989) enforce the locality bias sequentially and only interact elements within local patches.Transformers use attention matrices to model pairwise relations regardless of the distance between them.Recently, Transformers (Vaswani et al., 2017;Dosovitskiy et al., 2021) show strong performance on a wide range of applications across domains and become arguably one of the most successful architectures for sequence modeling in general.There are two main components in transformers: the attention mechanism that learns pairwise correlations of tokens from data, and the position embedding to introduce positional inductive biases.The vanilla attention mechanism requires quadratic space-time complexity, which precludes Transformers from handling long sequences.Numerous attention variants have been proposed recently to reduce the complexity, including linear transformers (Katharopoulos et al., 2020), and Performer (Choromanski et al., 2021).Although the types of attention vary, the position embedding remains in every method, which indicates the importance of position information in sequence modeling.This motivates us to ask the following question: since position information is important, can we design a model that relies entirely on the position information of its elements regardless of their content, thus alleviating the quadratic computation cost of the vanilla attention mechanism?In this paper, we give an affirmative answer to this question by introducing Toeplitz neural network, a new efficient architecture that solely exploits relative position relations for sequence modeling.In specific, instead of attention matrices, the Toeplitz neural network uses Toeplitz matrices to capture relations between each token pair.There are two motivations for selecting the Toeplitz matrix.One is that it compactly represents relative positional relations between tokens with much fewer parameters, i.e., 2n -1 parameters for an n \u00d7 n Toeplitz matrix.The other is that the Toeplitz matrix-vector production can be efficiently processed in O(n log n) complexity, which is exactly what we used in our token mixing operation.In this way, we avoid computing content similarities between tokens and effectively reduce the quadratic computation complexity of transformers to log linear, rendering a more efficient sequence modeling architecture.We further propose relative position encoder, a lightweight module that generates relative position parameters to assemble the Toeplitz matrices, so that the number of the TNN's parameters will no longer depend on the sequence length.Moreover, it allows TNN to deal with varying sequence lengths without retraining.In addition, the input sequence length extrapolation becomes an important ability in sequence modeling as training on longer sequences can be prohibitively expensive (Press et al., 2022).We propose an exponential decay bias that directly applies to the Toeplitz matrix.Our model achieves a consistent performance to a sequence length of 14K tokens in inference when training on sequences of 512 tokens.We also show analytically that the Toeplitz neural network represents a general form of sequence modeling methods, and derives transformers, CNNs, and the recently proposed State-space-based methods (Gu et al., 2022) as its special forms.We validate our model on a wide range of sequence modeling tasks and benchmarks.These include auto-regressive language modeling, text classification, image classification, and the Long-Range Arena benchmark.As illustrated in Fig. 1, our model achieves state-of-the-art performance on most tasks at a favorable log linear space-time complexity.It also demonstrates superior extrapolation capabilities when training on shorter sequences and evaluating on longer ones off-the-shelf."}
{"paper_id": 371, "abstract": "In the ever-evolving landscape of machine learning, the challenge of adapting classification models to embrace new classes while safeguarding the knowledge of old ones is akin to a hero balancing the weight of their past with the promise of the future. Enter Class-Incremental Learning (CIL), a realm where models are trained within the confines of limited memory, striving to meet this noble quest. Traditional CIL methods often rely on preserving a selection of representative exemplars from bygone classes, a strategy designed to stave off the treacherous grip of forgetting. However, recent revelations have uncovered a powerful ally: the historical models themselves, which can dramatically enhance performance. Yet, herein lies a paradox\u2014these stored models elude the constraints of the memory budget, leading to comparisons that are anything but fair.  Our exploration reveals that when we incorporate the model size into the total memory budget and align the scales of comparison, the efficacy of saving these models becomes inconsistent, particularly under the strain of limited memory resources. Thus, we are compelled to undertake a thorough evaluation of various CIL methodologies across diverse memory scales, weighing both accuracy and memory size with equal measure.  Moreover, we delve into the intricacies of constructing a memory buffer that champions efficiency. By scrutinizing the impact of different layers within the network, we uncover a dichotomy: shallow and deep layers exhibit distinct behaviors in the realm of CIL. Inspired by these insights, we propose a straightforward yet potent solution\u2014MEMO, short for Memory-efficient Expandable MOdel. MEMO ingeniously extends specialized layers rooted in shared generalized representations, adeptly extracting a rich tapestry of diverse representations while maintaining a modest cost and preserving vital exemplars.  Our extensive experiments on benchmark datasets demonstrate the competitive prowess of MEMO, affirming its place in the pantheon of CIL advancements. For those eager to embark on this journey with us, the code awaits at: https://github.com/wangkiw/ICLR23-MEMO.", "introduction": "In the open world, training data is often collected in stream format with new classes appearing (Gomes et al., 2017;Geng et al., 2020).Due to storage constraints (Krempl et al., 2014;Gaber, 2012) or privacy issues (Chamikara et al., 2018;Ning et al., 2021), a practical Class-Incremental Learning (CIL) (Rebuffi et al., 2017) model requires the ability to update with incoming instances from new classes without revisiting former data.The absence of previous training data results in catastrophic forgetting (French, 1999) in CIL -fitting the pattern of new classes will erase that of old ones and result in a performance decline.The research about CIL has attracted much interest (Zhou et al., 2021b;2022a;b;Liu et al., 2021b;2023;Zhao et al., 2021a;b) in the machine learning field.Saving all the streaming data for offline training is known as the performance upper bound of CIL algorithms, while it requires an unlimited memory budget for storage.Hence, in the early years, CIL algorithms are designed in a strict setting without retaining any instances from the former classes (Li & Hoiem, 2017;Kirkpatrick et al., 2017;Aljundi et al., 2017;Lee et al., 2017).It only keeps a classification model in the memory, which helps save the memory budget and meanwhile preserves privacy in the deployment.Afterward, some works noticed that saving limited exemplars from former classes can boost the performance of CIL models (Rebuffi et al., 2017;Chaudhry et al., 2019).Various exemplar-based methodologies have been proposed, aiming to prevent forgetting by revisiting the old during new class learning, which improves the performance of CIL tasks steadily (Rolnick et al., 2019;Castro et al., 2018;Wu et al., 2019;Isele & Cosgun, 2018).The utilization of exemplars has drawn the attention of the community from the strict setting to update the model with restricted memory Figure 1: The average accuracy of different methods by varying memory size from small to large.The start point corresponds to the memory size of exemplar-based methods with benchmark backbone (WA (Zhao et al., 2020), iCaRL (Rebuffi et al., 2017), Replay (Chaudhry et al., 2019)), and the endpoint corresponds to the memory cost of model-based methods with benchmark backbone (DER (Yan et al., 2021) and MEMO (our proposed method)).We align the memory cost by using the small model for model-based methods or adding exemplars for exemplar-based methods.'Base' stands for the number of classes in the first task, and 'Inc' represents the number of classes in each incremental new task.See Section 4.1 and 4.2 for more details.size (Castro et al., 2018;Rebuffi et al., 2017).Rather than storing exemplars, recent works (Yan et al., 2021;Wang et al., 2022;Li et al., 2021;Douillard et al., 2021) find that saving backbones from the history pushes the performance by one step towards the upper bound.These model-based methods propose to train multiple backbones continually and aggregate their representations as the feature representation for final prediction.Treating the backbones from history as 'unforgettable checkpoints,' this line of work suffers less forgetting with the help of these diverse representations.Model-based CIL methods push the performance towards the upper bound, but does that mean catastrophic forgetting is solved?Taking a close look at these methods, we find that they implicitly introduce an extra memory budget, namely model buffer for keeping old models.The additional buffer implicitly results in an unfair comparison to those methods without storing models.Take CIFAR100 (Krizhevsky et al., 2009) for an example; if we exchange the model buffer of ResNet32 (He et al., 2015) into exemplars of equal size and append them to iCaRL (Rebuffi et al., 2017) (a baseline without retaining models), the average accuracy drastically improves from 62% to 70%.How to fairly measure the performance of these methods remains a long-standing problem since saving exemplars or models will both consume the memory budget.In this paper, we introduce an extra dimension to evaluate CIL methods by considering both incremental performance and memory cost.For those methods with different memory costs, we need to align the performance measure at the same memory scale for a fair comparison.How to fairly compare different methods?There are two primary sources of memory cost in CIL, i.e., exemplar and model buffer.We can align the memory cost by switching the size of extra backbones into extra exemplars for a fair comparison.For example, a ResNet32 model has the same memory size with 603 images for CIFAR100, and 297 ImageNet (Deng et al., 2009) images have the same memory size with a ResNet18 backbone.Figure 1 shows the fair comparison on benchmark datasets, e.g., CIFAR100 and ImageNet100.We report the average accuracy of different models by varying the memory size from small to large.The memory size of the start point corresponds to the cost of an exemplar-based method with a single backbone, and the endpoint denotes the cost of a model-based method with multiple backbones.As we can infer from these figures, there is an intersection between these methods -saving models is less effective when the total budget is limited while more effective when the total budget is ample.In this paper, we dive deeply into the empirical evaluations of different CIL methods considering the incremental performance and memory budget.Towards a fair comparison between different approaches, we propose several new measures that simultaneously consider performance and memory size, e.g., area under the performance-memory curve and accuracy per model size.On the other hand, how to organize the memory buffer efficiently so that we can save more exemplars and meanwhile maintain diverse representations?We analyze the effect of different layers of the network by counting the gradients and shifting range in incremental learning, and find that shallow layers tend to learn generalized features.By contrast, deep layers fit specialized features for corresponding tasks and yield very different characteristics from task to task.As a result, sharing the shallow layers and only creating deep layers for new tasks helps save the memory budget in CIL.Furthermore, the spared space can be exchanged for an equal number of exemplars to further boost the performance.Intuitively, we propose a simple yet effective baseline MEMO to simultaneously consider extending diverse features with the most modest memory cost.MEMO shows competitive results against state-of-the-art methods under the fair comparison on vast benchmark datasets and various settings, which obtains the best performance in most cases of Figure 1."}
{"paper_id": 372, "abstract": "In this exploration of vectorized sketch generation, we uncover a fascinating truth: the art of creating sketches can be viewed as a reversal of the stroke deformation process. We achieved this revelation through a diffusion model, a powerful tool that learns the intricate dance of data distributions over the stroke-point locations and pen states found in authentic human sketches. When presented with a chaotic array of stroke points, our sketch generation transforms into a delicate dance of deformation-based denoising. Here, the generator meticulously adjusts the positions of these stroke points at each timestep, guiding them toward the emergence of a recognizable image.  At the heart of our innovation lies the embedding of recognizability within the reverse-time diffusion process. We discovered a compelling correlation between the estimated noise during this reversal and the accuracy of sketch classification. To harness this insight, we employed an auxiliary recurrent neural network (RNN), which quantifies recognizability during the sampling phase. This led us to a remarkable conclusion: by leveraging these recognizability scores, we could devise a sampling shortcut function that produces higher-quality sketches with fewer sampling steps.  Ultimately, our model extends its reach into the realm of conditional generation. When faced with incomplete or unfaithful sketches, it skillfully crafts a final piece that not only enhances visual appeal but also elevates recognizability. In this way, we blend artistry with technology, revealing the profound potential of our approach to sketch generation.", "introduction": "Free-hand human sketches are abstract concepts which can efficiently express ideas.Generative models for sketches have received increasing attentions in recent years.Compared with producing pixelated sketches (Ge et al., 2020;Chen et al., 2001;Liu et al., 2020), modeling sketches with point trajectories is more reasonable and appealing as it more closely resembles drawing process of humans.Sketch-RNN (Ha & Eck, 2018) utilizes a set of discrete stroke points and binary pen states as an approximation of the continuous drawing trajectory.B\u00e9zierSketch (Das et al., 2020) makes use of parametric representation, which fits the stroke trajectory by B\u00e9zier curves.Very recently, SketchODE (Das et al., 2021a) applies neural ordinary differential equations to representing stroke trajectory through continuous-time functions.All said approaches however suffer from the inability to model complex vectorized sketches.This is largely attributed to the de-facto RNN backbone that falls short in accommodating large stroke point numbers -rule of thumb is anything beyond 200 points will fail (Pascanu et al., 2013;Das et al., 2021b).In this paper, we attempt to change the status quo in how stroke-point trajectories are modeled.Instead of seeing sketch generation as a process of determining where the next stroke-point lies under each recurrent step (as per RNN), we attempt to estimate distributions of all stroke-points holistically at each time instance -as every knitting enthusiast will tell you, it is all about having a global plan, never just about the next thread! 1 .Our key novelty lies with the realization that sketch generation can be conceptualized as the reversal of a stroke deformation process.Through modeling a forward deformation process (i.e., sketch to noise), our diffusion model learns the stroke-point distributions of real human sketches, and thus able to reverse the process to generate novel sketches given noisy input.It follows that given this diffusion setup, the sequential information in sketches can be persevered by simply maintaining the temporal ordering of stroke-points during reverse-time diffusion.We further draw importance on the overall quality (recognizability) of the sketches generated.We show that the estimated noise in the sampling stage naturally can reflect the recognizability of the generated sketch at each timestep.It follows that a learnable RNN was devised to explicitly model the relation between estimated noise and recognizability.This is achieved by introducing an pretrained image classifier as supervision signal.Embedding recognizabilty into the sampling process also yields the added benefit of introducing skip steps that allows for more efficient and effective data generation.This is because early stages for generating sequential data is very inefficient using vanilla DDPMs sampling (Ho et al., 2020) as witnessed in Figure 1 (b), resulting in minor improvement of recognizability in a long period of sampling as unveiled in Figure 1 (c).Last but not least, we demonstrate the model (without retraining) can be readily used to remedy defects in sketches due to unfaithful or incomplete drawing, by incorporating instance-aware guidance into data sampling.Motivated by recent works on guided diffusion models (Dhariwal & Nichol, 2021;Ho & Salimans, 2022), gradients of perceptual similarity (Zhang et al., 2018) between the generated data and the conditional sketch were incorporated during sampling to guide the noise prediction, thereby influencing the obtained sample at each timestep.This was done with the goal of enforcing visual similarity to the conditional, flawed sketch, while also being more appealing and recognizable after reverse-time diffusion.Our contributions can be summarized as follows: (i) Denoising diffusion models are exploited for sketch generation in vector format.The generative model is to learn distribution over stroke points' locations, from a deformation-based denoising process which starts from noise.(ii) The quality, i.e., recognizablity, of the generated sketches is quantifiable by leveraging the knowledge of the estimated noises during sampling.This is achieved by devising an auxiliary RNN, which is trained supervised under a pre-trained image classifier, to predict the recognizability of a generated sketch at timestep t from the corresponding estimated noise.(iii) A shortcut sampling path can be discovered through a simple skip strategy based on the learned quality measurement net.This allows faster and more effective generation with little trade off in data quality.(iv) Instance-aware guidance built on perceptual metric is embedded into the reverse-time diffusion.It enables our model to recover distorted or corrupted sketches without retraining.with deep learning, much progress has been made recently.Particularly, generative adversarial networks (GANs) have motivated extensive works in sketch synthesis, involving doodle-sketch generation (Ge et al., 2020), image-to-sketch translation (Liu et al., 2020), colored sketch rendering (Rathod et al., 2021), pencil-shading sketch generation (Li et al., 2020b), and face sketch synthesis (Wang et al., 2020).However, those models are all pixel-based generation, which is fundamentally different from how humans sketch objects using pens or brushes.Towards modeling sketches like humans, sketches are preferred to be treated as sequential pen actions, and RNN-based variational autoencoder (VAE) (Ha & Eck, 2018;Zhang et al., 2017;Graves, 2013), reinforcement learning (RL) (Xie et al., 2013;Zheng et al., 2018;Ganin et al., 2018), Transformed-based sketch representation (Ribeiro et al., 2020;Lin et al., 2020), learning parametric B\u00e9zier curve (Das et al., 2020;2021b), and neural ODE (Das et al., 2021a) are explored for sketch generation.(Aksan et al., 2020) proposes a relational model built on auto-encoder, which can decompose sketch formed by a single temporal sequence into a group of disordered strokes.Particularly, promising generative results on complex structures have been witnessed.Other notable works include generating stylized line drawing from 3D shapes (Liu et al., 2021), and intent communication through sketching by referential communication game (Mihai & Hare, 2021).Diffusion Models Recently, approaches with diffusion models have delivered impressive results on several generative tasks, including image generation (Ho et al., 2020;Dhariwal & Nichol, 2021), shape generation (Cai et al., 2020), 3D shape modelling (Luo & Hu, 2021), audio synthesis (Kong et al., 2020), and cross-domain generation (Popov et al., 2021;Nichol et al., 2021).Different from likelihood-based models (variational auto-encoder (VAEs) (Kingma & Welling, 2013), normalizing flow models (Dinh et al., 2014;Papamakarios et al., 2021), energy-based models (EBMs) (LeCun et al., 2006)) and implicit generative models (GANs (Goodfellow et al., 2014)), diffusion models can be categorized into score-based generative modeling (SGM) (Song et al., 2020), which aims to model the gradient of the log probability density function by score matching (Hyv\u00e4rinen & Dayan, 2005).There are two popular sub-classes in SGMs, i.e., score matching with Langevin dynamics (SMLD) (Song & Ermon, 2019) and denoising diffusion probabilistic models (DDPM) (Sohl-Dickstein et al., 2015;Ho et al., 2020).Despite attractive, rare work with diffusion models targets at handling sketches.The most relevant work to ours is Diff-HW (Luhman & Luhman, 2020) which also applies diffusion models for sequential data generation, i.e., handwriting.However, Diff-HW adopts the vanilla DDPMs and focuses on text-to-sketch translation.On contrast, we offer (model built-in) quality quantifiable diffusion models, improved sampling strategy and gradients guided conditional sampling based on DDIM."}
{"paper_id": 373, "abstract": "In the ever-evolving landscape of robotics, the ability to navigate the complexities of the real world hinges not only on adept manipulation skills but also on a nuanced understanding of when to deploy those skills. Recent advancements have sought to merge the rich semantic representations gleaned from large-scale pretrained vision-language (VL) models with manipulation frameworks, bestowing these systems with enhanced reasoning capabilities. Yet, we uncover a critical flaw in the traditional pretraining-finetuning pipeline: it entangles the learning of specialized action information with overarching visual insights, resulting in inefficient training and a troubling lack of adaptability to novel objects and tasks.  To address this challenge, we introduce \\ours\u2014a modular approach designed to harness the full potential of pretrained VL models by leveraging the intricate syntactic and semantic structures inherent in language instructions. Our innovative framework employs a semantic parser to distill executable programs, each constructed from functional modules that are firmly grounded in both vision and action across diverse modalities. These functional modules blend deterministic computations with adaptable neural networks, allowing for a dynamic response to the environment. The execution of these programs generates parameters that drive general manipulation primitives for robotic end-effectors. Remarkably, the entire modular network can be trained through end-to-end imitation learning objectives.  Our experiments reveal that this model adeptly disentangles the realms of action and perception, leading to significant advancements in zero-shot and compositional generalization across a spectrum of manipulation tasks. For further exploration, visit our project webpage at: \\url{https://progport.github.io}.", "introduction": "Robotic manipulation models that map directly from raw pixels to actions are capable of learning diverse and complex behaviors through imitation.To enable more abstract goal specification, many such models also take as input natural language instructions.However, this vision-language manipulation setting introduces a new problem: the agent must jointly learn to ground language tokens to its perceptual inputs, and correspond this grounded understanding with the desired actions.Moreover, to fully leverage the flexibility of language, the agent must handle novel vocabulary and compositions not explicitly seen during training, but specified at test time (Fig. 1).To these ends, many recent works have relied on large pretrained vision-language (VL) models such as CLIP (Radford et al., 2021) to tackle both grounding and zero-shot generalization.As shown in Fig. 2a, these works generally treat the pretrained VL model as a semantic prior, for example, by partially initializing the weights of image and text encoders with a pretrained VL model (Ahn et al., 2022;Khandelwal et al., 2022;Shridhar et al., 2022a).These models are then updated via imitating expert demonstrations.However, this training scheme entangles the learning of domainspecific control policies and domain-independent vision-language grounding.Specifically, we find that these VL-enabled agents overfit to their task-specific data, leveraging shortcuts during training to successfully optimize their imitation learning (IL) objective, without learning a generalizable grounding of language to vision.This phenomenon is particularly apparent when the agent is given language goals with unknown concepts and objects, or novel compositions of known concepts and objects.For example, an agent that overfits to packing shapes into a box can fail to generalize to other manipulation behaviors (e.g.pushing) involving the same shapes.In this work, we introduce PROGRAMPORT, a program-based modular approach that enables more faithful vision-language grounding when incorporating pretrained VL models for robotic manipulation.Given the natural language instruction, we first use a Combinatory Categorial Grammar (CCG) (Steedman, 1996) to parse the sentence into a \"manipulation program,\" based on a compact but general domain-specific language (DSL).The program consists of functional modules that are either visual grounding modules (e.g., locate all objects of a given category) or action policies (e.g., produce a control parameter).This enables us to directly leverage a pretrained VL model to ground singular, independent categories or attribute descriptors to their corresponding pixels, and thus disentangles the learning of visual grounding and action policies (Fig. 2b).Our programmatically structured, modular design enables PROGRAMPORT to successfully learn more performant imitation policies with fewer data across 10 diverse tasks in a tabletop manipulation environment.We show that after training on a subset of objects, visual properties, and actions, our model can zero-shot generalize to completely different subsets and reason over novel compositions of language descriptors, without further finetuning (Fig. 1)."}
{"paper_id": 374, "abstract": "In the realm of autonomous driving, where precision and clarity are paramount, high-definition (HD) maps stand as the cornerstone of navigation and planning. They weave a tapestry of environmental detail that is both rich and exacting. Enter MapTR, a groundbreaking structured end-to-end Transformer that revolutionizes the online construction of vectorized HD maps.   At the heart of our innovation lies a unified modeling approach\u2014one that treats map elements as a dynamic point set, embracing a symphony of equivalent permutations. This method not only captures the intricate shapes of map elements but also fortifies the learning process against the chaos of data variability. To further enhance our system, we\u2019ve devised a hierarchical query embedding scheme that deftly encodes structured map information, allowing for seamless hierarchical bipartite matching in the learning of map elements.  The results are nothing short of extraordinary. MapTR outshines all existing vectorized map construction methods on the nuScenes dataset, achieving unparalleled performance and efficiency with mere camera inputs. Our compact variant, MapTR-nano, operates at a blistering real-time inference speed of 25.1 FPS on an RTX 3090\u2014an astonishing eight times faster than the leading camera-based approach, all while boasting a 5.0 mAP improvement. When stacked against the current multi-modality frontrunners, MapTR-nano still triumphs, surpassing them by 0.7 mAP and maintaining that remarkable speed. Meanwhile, MapTR-tiny elevates the game further, delivering a 13.5 mAP increase and tripling the inference speed.  Through a wealth of qualitative results, we demonstrate that MapTR consistently delivers stable and robust map construction, even in the most complex and varied driving environments. The implications for autonomous driving are profound. For those eager to explore this innovation further, our code and additional demonstrations await at https://github.com/hustvl/MapTR.", "introduction": "High-definition (HD) map is the high-precision map specifically designed for autonomous driving, composed of instance-level vectorized representation of map elements (pedestrian crossing, lane divider, road boundaries, etc.).HD map contains rich semantic information of road topology and traffic rules, which is vital for the navigation of self-driving vehicle.Conventionally HD map is constructed offline with SLAM-based methods (Zhang & Singh, 2014;Shan & Englot, 2018;Shan et al., 2020), incurring complicated pipeline and high maintaining cost.Recently, online HD map construction has attracted ever-increasing interests, which constructs map around ego-vehicle at runtime with vehicle-mounted sensors, getting rid of offline human efforts.Early works (Chen et al., 2022a;Liu et al., 2021a;Can et al., 2021) leverage line-shape priors to perceive open-shape lanes based on the front-view image.They are restricted to single-view perception and can not cope with other map elements with arbitrary shapes.With the development of bird's eye view (BEV) representation learning, recent works (Chen et al., 2022b;Zhou & Kr\u00e4henb\u00fchl, 2022;Hu et al., 2021;Li et al., 2022c) predict rasterized map by performing BEV semantic segmentation.However, the rasterized map lacks vectorized instance-level information, such as the lane structure, which is important for the downstream tasks (e.g., motion prediction and planning).To construct vectorized HD map, HDMapNet (Li et al., 2022a) groups pixel-wise segmentation results, which requires complicated and time-consuming post-processing.VectorMapNet (Liu et al., 2022a) represents each map element as a point sequence.It adopts a cascaded coarse-to-fine framework and utilizes auto-regressive decoder to predict points sequentially, leading to long inference time.Current online vectorized HD map construction methods are restricted by the efficiency and not applicable in real-time scenarios.Recently, DETR (Carion et al., 2020) employs a simple and efficient encoder-decoder Transformer architecture and realizes end-to-end object detection.It is natural to ask a question: Can we design a DETR-like paradigm for efficient end-to-end vectorized HD map construction?We show that the answer is affirmative with our proposed Map TRansformer (MapTR).Different from object detection in which objects can be easily geometrically abstracted as bounding box, vectorized map elements have more dynamic shapes.To accurately describe map elements, we propose a novel unified modeling method.We model each map element as a point set with a group of equivalent permutations.The point set determines the position of the map element.And the permutation group includes all the possible organization sequences of the point set corresponding to the same geometrical shape, avoiding the ambiguity of shape.Based on the permutation-equivalent modeling, we design a structured framework which takes as input images of vehicle-mounted cameras and outputs vectorized HD map.We streamline the online vectorized HD map construction as a parallel regression problem.Hierarchical query embed-dings are proposed to flexibly encode instance-level and point-level information.All instances and all points of instance are simultaneously predicted with a unified Transformer structure.And the training pipeline is formulated as a hierarchical set prediction task, where we perform hierarchical bipartite matching to assign instances and points in turn.And we supervise the geometrical shape in both point and edge levels with the proposed point2point loss and edge direction loss.With all the proposed designs, we present MapTR, an efficient end-to-end online vectorized HD map construction method with unified modeling and architecture.MapTR achieves the best performance and efficiency among existing vectorized map construction approaches on nuScenes (Caesar et al., 2020) dataset.In particular, MapTR-nano runs at real-time inference speed (25.1 FPS) on RTX 3090, 8\u00d7 faster than the existing state-of-the-art camera-based method while achieving 5.0 higher mAP.Even compared with the existing state-of-the-art multi-modality method, MapTR-nano achieves 0.7 higher mAP and 8\u00d7 faster inference speed, and MapTR-tiny achieves 13.5 higher mAP and 3\u00d7 faster inference speed.As the visualization shows (Fig. 1), MapTR maintains stable and robust map construction quality in complex and various driving scenes.Our contributions can be summarized as follows:\u2022 We propose a unified permutation-equivalent modeling approach for map elements, i.e., modeling map element as a point set with a group of equivalent permutations, which accurately describes the shape of map element and stabilizes the learning process.\u2022 Based on the novel modeling, we present MapTR, a structured end-to-end framework for efficient online vectorized HD map construction.We design a hierarchical query embedding scheme to flexibly encode instance-level and point-level information, perform hierarchical bipartite matching for map element learning, and supervise the geometrical shape in both point and edge levels with the proposed point2point loss and edge direction loss.\u2022 MapTR is the first real-time and SOTA vectorized HD map construction approach with stable and robust performance in complex and various driving scenes."}
{"paper_id": 375, "abstract": "In the ever-evolving realm of artificial intelligence, recent advancements in text-to-image generation have unveiled a new frontier of astonishingly high-fidelity, photo-realistic images. Yet, beneath this dazzling surface lies an intriguing question: how effective are these synthetic creations when it comes to the rigorous demands of image recognition tasks? In this exploration, we embark on a thorough investigation into the utility of images birthed from cutting-edge text-to-image models, scrutinizing their role through two distinct lenses. First, we delve into their potential for enhancing classification models in data-scarce environments\u2014those challenging realms of zero-shot and few-shot learning. Second, we explore their capacity for large-scale model pre-training, paving the way for effective transfer learning. Through our analysis, we illuminate both the remarkable strengths and notable limitations of synthetic data derived from current generative models. Moreover, we present a series of strategic recommendations to maximize the efficacy of synthetic data in the pursuit of image recognition excellence. For those eager to delve deeper, our findings and methodologies are encapsulated in the code available at: https://github.com/CVMI-Lab/SyntheticData.", "introduction": "Over the past decade, deep learning powered by large-scale annotated data has revolutionized the field of image recognition.However, it is costly and time-consuming to manually collect a largescale labeled dataset, and recent concerns about data privacy and usage rights further hinder this process.In parallel, generative models that aim to model real-data distributions can now produce high-fidelity photo-realistic images.In particular, recent text-to-image generation models (Nichol et al., 2021;Ramesh et al., 2022;Saharia et al., 2022b) have made major breakthroughs in synthesizing high-quality images from text descriptions.This promotes us to ask: is synthetic data from generative models ready for image recognition tasks?There are a few early attempts at exploring synthetic data from generative models for image recognition tasks.Besnier et al. (2020) use a class-conditional GAN (BigGAN (Brock et al., 2018) trained for ImageNet-1000 classes) to generate images for training image classifiers.Zhang et al. (2021) leverage StyleGAN (Karras et al., 2019) to produce synthetic labeled data for object-part segmentation.Jahanian et al. (2021) manipulate the latent space of a GAN model to produce multi-view images for contrastive learning.Albeit promising, early works either address tasks on a small scale or only for a specific setting.Plus, they all focus on GAN-based models and none explore the revolutionary text-to-image generation models, which hold more promises to benefit recognition tasks.In this paper, we present the first study on the state-of-the-art text-to-image generation models for image recognition.With the power of text-to-image generation, we could hopefully not only generate massive high-quality labeled data, but also achieve domain customization by generating synthetic data targeted for a specific label space, i.e. the label space of a downstream task.Our study is carried out on one open-sourced text-to-image generation model, GLIDE (Nichol et al., 2021) 1 .We attempt to uncover the benefits and pitfalls of synthetic data for image recognition through the lens of investigating the following two questions: 1) is synthetic data from generative models ready for improving classification models?2) whether synthetic data can be a feasible source for transfer learning (i.e.model pre-training)?It is worth noting that for 1), we only studied the zero-shot and few-shot settings because the positive impact of synthetic data diminishes as more shots are present.And, we build most of our investigations on the state-of-the-art method CLIP (Radford et al., 2021) with the feature extractor initialized with large-scale pre-trained weights frozen.Our Findings.First, in the zero-shot setting, i.e. no real-world data are available, we demonstrate that synthetic data can significantly improve classification results on 17 diverse datasets: the performance is increased by 4.31% in top-1 accuracy on average, and even improved by as much as 17.86% on the EuroSAT dataset.To better leverage synthetic data in this setting, we also investigate useful strategies to increase data diversity, reduce data noise, and enhance data reliability.This is achieved by designing diversified text prompts and measuring the correlation of text and synthesized data with CLIP features.Second, in the few-shot setting, i.e. a few real images are available, albeit not as significant as in the zero-shot task, synthetic data are also shown to be beneficial and help us achieve a new state of the art.Our observation shows that the domain gap between synthetic data and downstream task data is one challenge on further improving the effectiveness of synthetic data on classifier learning.Fortunately, in this setting, the accessibility of real data samples can provide useful information about the data distribution of the downstream task.We thus propose to use real images as guidance in the generation process to reduce domain gaps and improve effectiveness.Third, in large-scale model pre-training for transfer learning, our study shows that synthetic data are suitable and effective for model pre-training, delivering superior transfer learning performance and even outperforming ImageNet pre-training.Especially, synthetic data work surprisingly well in unsupervised model pre-training, and favor ViT-based backbones.We also demonstrate that by increasing the label space (i.e.text prompts) for data generation, the enlarged data amount and diversity could further bring performance boosts.Besides, synthetic data can work collaboratively with real data (i.e.ImageNet) where we obtain improved performance when the model is initialized with ImageNet pre-trained weights."}
{"paper_id": 376, "abstract": "In the realm of creativity, text-to-image models stand as gateways to boundless possibilities, allowing us to wield the power of language to shape our visions into reality. Yet, amidst this vast expanse of potential, a question lingers: how can we harness this freedom to breathe life into our unique ideas, to transform our beloved cat into a stunning work of art, or to conjure a brand-new invention inspired by our favorite childhood toy?  In this exploration, we unveil a straightforward yet profound approach that unlocks this creative potential. By simply providing a handful of images\u2014between three to five\u2014of a concept that resonates with you, whether it be an object or a stylistic nuance, we can learn to articulate it through novel \u201cwords\u201d within the embedding space of a pre-trained text-to-image model. These \u201cwords\u201d serve as the building blocks for crafting natural language sentences, enabling an intuitive and personalized creative journey.  Remarkably, our findings reveal that a single word embedding can encapsulate the essence of diverse and distinct concepts. Through rigorous comparisons with a variety of baseline methods, we demonstrate that our approach excels in faithfully representing these concepts across a spectrum of applications and tasks. We invite you to explore this innovative frontier with us; our code, data, and newly minted words will soon be at your disposal, ready to empower your own creative endeavors.", "introduction": "Large-scale text-to-image models (Rombach et al., 2021;Ramesh et al., 2021;2022;Nichol et al., 2021;Yu et al., 2022;Saharia et al., 2022) have demonstrated an unprecedented capability to reason over natural language descriptions.They allow users to synthesize novel scenes with unseen compositions and produce vivid pictures in a myriad of styles.These tools have been used for artistic creation, as sources of inspiration, and even to design new, physical products (Yacoubian, 2022).Their use, however, is constrained by the user's ability to describe the desired target through text.One can then ask: How could we instruct such models to mimic the likeness of a specific object?How could we ask them to craft a novel scene containing a cherished childhood toy?Or to pull our child's drawing from its place on the fridge, and turn it into an artistic showpiece?Introducing new concepts into large scale models is often difficult.Re-training a model with an expanded dataset for each new concept is prohibitively expensive, and fine-tuning on few exam-ples typically leads to catastrophic forgetting (Ding et al., 2022;Li et al., 2022).More measured approaches freeze the model and train transformation modules to adapt its output when faced with new concepts (Zhou et al., 2021;Gao et al., 2021;Skantze & Willemsen, 2022).However, these approaches are still prone to forgetting prior knowledge, or face difficulties in accessing it concurrently with newly learned concepts (Kumar et al., 2022;Cohen et al., 2022).We propose to overcome these challenges by finding new words in the textual embedding space of pre-trained text-to-image models.We consider the first stage of the text encoding process (Figure 2).Here, an input string is first converted to a set of tokens.Each token is then replaced with its own embedding vector, and these vectors are fed through the downstream model.Our goal is to find new embedding vectors that represent new, specific concepts.We represent a new embedding vector with a new pseudo-word (Rathvon, 2004) which we denote by S * .This pseudo-word is then treated like any other word, and can be used to compose novel textual queries for the generative models.One can therefore ask for \"a photograph of S * on the beach\", \"an oil painting of a S * hanging on the wall\", or even compose two concepts, such as \"a drawing of S 1 A string containing our placeholder word is first converted into tokens (i.e.word or sub-word indices in a dictionary).These tokens are converted to continuous vector representations (the \"embeddings\", v).Finally, the embedding vectors are transformed into a conditioning code c \u03b8 (y) that guides the generation.We optimize the embedding vector v * associated with our pseudo-word S * , using a reconstruction objective.GAN inversion.Manipulating images with generative networks often requires one to find a corresponding latent representation of the given image, a process referred to as inversion (Zhu et al., 2016;Xia et al., 2021).In the GAN literature, this inversion is done through either an optimizationbased technique (Abdal et al., 2019;2020;Zhu et al., 2020b;Gu et al., 2020) or by using an encoder (Richardson et al., 2020;Zhu et al., 2020a;Pidhorskyi et al., 2020;Tov et al., 2021).In our work, we follow the optimization approach, as it can better adapt to unseen concepts.Encoders face harsher generalization requirements, and would likely need to be trained on web-scale data to offer the same freedom.We further analyze our embedding space in light of the GANinversion literature, outlining the core principles that remain and those that do not.Diffusion-based inversion.Diffusion inversion can be performed na\u00efvely by adding noise to an image and then de-noising it through the network.However, this process tends to change the image content.Choi et al. (2021) improve inversion by conditioning the denoising process on low-pass filter data from the target image.(Dhariwal & Nichol, 2021) demonstrate that the DDIM (Song et al., 2020) sampling process can be inverted in a closed-form manner, extracting a latent noise map that will produce a given real image.DALL-E 2 (Ramesh et al., 2022) builds on this method and demonstrates that it can be used to facilitate cross-image interpolations or semantic editing.Whereas the above works invert a given image into the model's latent space, we invert a userprovided concept.Moreover, we represent this concept as a new pseudo-word in the model's vocabulary, allowing for more general and intuitive editing.Personalization.Recent work in graphics aims to adapt models to better represent specific individuals or objects.There, it is typical to delicately tune a model to better reconstruct specific faces or scenes (Bau et al., 2019;Roich et al., 2021;Alaluf et al., 2021;Nitzan et al., 2022).PALAVRA (Cohen et al., 2022) identifies pseudo-words in the textual embedding space of CLIP for personalized retrieval and segmentation.However, their task and losses are discriminative, aiming to separate an object from other candidates.As we later show (Figure 5), this approach fails to capture details required for plausible reconstructions or synthesis in new scenes."}
{"paper_id": 377, "abstract": "In the realm where deep learning meets the intricate dance of symmetry, we find a promising pathway to enhanced performance. Recently, the intriguing domain of equivariant partial differential operators (PDOs) has captured the imagination of researchers, serving as a compelling bridge between the principles of physics and the evolving landscape of deep learning. Yet, a challenge looms: prior approaches have tethered the translation equivariance of PDOs to the rigid constraints of constant, spatially shared coefficient matrices. This limitation often stifles optimal feature learning, leaving potential untapped at various positions.  In our exploration, we unveil a groundbreaking scheme for nonlinear PDOs that transcends these constraints, offering a spatially adaptive and translation-equivariant solution. Instead of relying on static coefficients, our innovative method derives them from local features through a generator, allowing for a more nuanced and responsive approach. Furthermore, we lay the groundwork for a new theoretical framework that incorporates additional forms of equivariance, such as rotations, into the fabric of these PDOs.  To bring our vision to life, we harness the power of an equivariant multilayer perceptron (EMLP) to implement our generator. Thus, we introduce Neural ePDOs\u2014equivariant PDOs birthed from the creativity of neural networks. Our experiments reveal a remarkable leap forward, showcasing significant performance enhancements over previous methodologies while maintaining a smaller model size across diverse datasets. Notably, we achieve state-of-the-art results on the MNIST-rot dataset, accomplishing this feat with merely half the parameters of the previous leading model. In this convergence of theory and practice, we forge a new path toward the future of deep learning, one that resonates with the elegance of symmetry.", "introduction": "In recent years, convolutional neural networks (CNNs) have achieved superior performance on various vision tasks (Szegedy et al., 2015;He et al., 2016;Chen et al., 2017).It is acknowledged that the success of CNNs is attributed to their ability to exploit the intrinsic translation-invariance symmetry of data to help downstream vision tasks.To incorporate other symmetries like rotation-invariance, various CNNs-based equivariant networks have been studied and carried out to enhance the performance of vision tasks (Cohen & Welling, 2016a;b;Weiler & Cesa, 2019).In another branch, some works (Osher & Rudin, 1990;Perona & Malik, 1990) adopted partial differential operators (PDOs) to process images in the early period.Recently, PDOs with learnable coefficients are adopted by Shen et al. (2020) to design equivariant networks which achieve competitive performance compared to previous equivariant networks.Jenner & Weiler (2021) further generalized this work to a unified framework on the equivariant linear PDOs on Euclidean spaces of various representation types.Actually, the coefficient matrices of the current PDOs works are spatially shared, e.g. the same PDOs are applied to process features at each position (see Figure.1(a)).However, such a coefficient sharing scheme of the PDOs is not the optimal pattern to extract features from input images (Wu et al., 2018;Su et al., 2019;Zhou et al., 2021;He et al., 2021a).To be specific, the contents of the input images vary according to positions, e.g.some pixels cover the background while some express texture, which would make coefficient-sharing PDOs inefficient to extract features at each position.In fact, Jenner & Weiler (2021) have proved that the linear PDOs layer is translation equivariant if and only if its coefficient matrices are spatially shared, so it seems impossible to ensure both the spatial adaptivity and translation equivariance for PDOs.In this work, to deal with the above issue, we think outside the box of the linear limitation and propose brand new nonlinear PDOs that are both spatially adaptive and translation equivariant.Compared with spatially shared PDOs, we construct a coefficient generator that inputs local features and outputs the coefficient matrices.Since different positions produce different coefficient matrices, the PDOs are essentially position-specific and can extract individual features according to the local content (see Figure 1(b)).In addition, the coefficient matrices generated by local features guarantee the translation equivariance for such PDOs naturally.However, such a nonlinear PDOs scheme is not intrinsically equivariant to rotations or reflections.To incorporate equivariance of these transformations, we establish a theory on the equivariant formulation of this nonlinear PDOs scheme under any given symmetry group.Specifically, the theory reveals that this type of PDOs is equivariant if and only if the coefficient generators are exactly equivariant maps of particular transformations.In practice, we choose a two-layer EMLP (Finzi et al., 2021) as the coefficient generator to satisfy the equivariance condition and provide an efficient implementation scheme.We name our model Neural ePDOs and evaluate its performance on MNIST-rot and ImageNet datasets.Extensive experiments show that our model can significantly improve accuracy with fewer parameters.Especially, we achieve the state-of-the-art results on MNIST-rot dataset with only a tenth of the parameters compared to previous best models.We summarize the main contributions as follows:\u2022 To our knowledge, we are the first one to propose the nonlinear form of PDOs that are both spatially adaptive and translation equivariant.The coefficient matrices of the novel PDOs are adaptive to local features, which could alleviate the sub-optimal feature learning problem at each position.\u2022 We develop a theory for such nonlinear PDOs that precisely characterize when it is equivariant under any given symmetry group.The theory reveals that the nonlinear PDOs are equivariant if and only if the coefficient generators are exactly equivariant maps of particular transformations.\u2022 We provide an efficient implementation which adopts a two-layer EMLP as the coefficient generator and could largely save parameters and computations.\u2022 Extensive experiments show that our method can significantly improve the results on MNIST-rot and ImageNet datasets with significantly fewer parameters.Especially, we achieve state-of-the-art results on the MNIST-rot dataset."}
{"paper_id": 378, "abstract": "In the intricate realm of autonomous driving, assessing the performance of perception modules stands as a pivotal challenge in the crafting of these sophisticated systems. While we can draw upon the unit testing methodologies from traditional computer vision, their application remains somewhat limited when it comes to understanding the profound ways in which alterations in a perception module ripple through to affect the planning capabilities of an autonomous vehicle. In this endeavor, we unveil a robust framework designed to illuminate the intricate interplay between perception and planning, akin to the way a masterful storyteller weaves together disparate threads into a cohesive narrative.   We frame the planning of an autonomous vehicle as a grand quest for expected utility maximization, where the myriad input signals from upstream modules converge to paint a comprehensive portrait of the world state. The planner, akin to a strategic hero, seeks the optimal action to undertake, striving to maximize the expected utility shaped by both the world state and the chosen action. Our findings reveal that, under certain gentle conditions, this objective can be elegantly expressed as an inner product within a Hilbert space\u2014a geometric revelation that offers a fresh lens through which to examine, analyze, and quantify the influence of noise in world state estimation on our quest for solutions.   This framework, inspired by the philosophical tenets of transcendental idealism, not only provides a novel approach to evaluating the impact of perception on planning but also introduces a universal quantitative metric that serves as a guiding star in this complex landscape. Thus, we embark on a journey to redefine our understanding of how perception and planning intertwine in the grand tapestry of autonomous driving.", "introduction": "Autonomous driving has recently risen as a fast-advancing realm in both industry and academia, and receives a surge of interest from engineering and scientific communities (Yurtsever et al., 2020;Sun et al., 2020).As an intricate system, an autonomous driving vehicle consists of numerous hardware components and interactive onboard modules.As one such core component, the onboard perception module serves as the major source of real-time characterisation of the dynamic environment an autonomous vehicle (AV) navigates through.To evaluate and improve the perception module, conventional perception tasks (such as detection, segmentation, tracking) have been well defined and corresponding performance measurements are established in computer vision to benchmark performance of perception algorithms (Lin et al., 2014).Despite their great success in driving the development of advanced perceptual information processing modules, almost all such metrics exclusively focus on the perception-level performance in a deployment-agnostic fashion, for instance, how close a detected object is to the ground truth, while ignoring the actual impact of the result to the entire AV system.Indeed, not all perception errors translate the same to the planning of an AV.Obviously, miss detecting a vehicle in front of an AV is far more serious than one behind far away.This problem is further compounded by the heterogeneity of perception errors that share little semantics in common (\"How dose an error of 5m/s in velocity compare to that of a size 25% larger?\"),where intuitive manual engineering is widely used (Caesar et al., 2020).Although these issues are typically addressed by integrating road test in the real world, the process is extremely costly and time-consuming (Wachenfeld and Winner, 2016;\u00c5sljung et al., 2017).In result, tools are in great demand to effectively and efficiently measure the impact of perception to the whole autonomous driving system before deployment on road.Unfortunately, these solutions still remain far less explored in the research literature.The change in AV behaviour due to perception error is not always correlated to the cost of consequence.In (a) the AV has to circumvent the erroneously perceived cone by making a large detour.While for (b) the AV only needs to make a slight detour to the right, yet it inevitably hits the cone.In this case, although the behaviour change is far less than that of (a), the consequence is significantly worse (\"hitting an object\" v.s.\"making a large detour\").In (c) the consequence of either way is indifferent to the AV in moving forward, yet the change in behaviour is considerable in terms of spatiotemporal motion.As for (d), if there are two falsely detected cones on both sides, which are close enough to the AV when passing by despite no collision, the AV still decides to maintain the same motion as in the ground truth case.Therefore, the final behaviour of the AV does not change given the perception error, but the cost of passing by two close objects already changes the planning process, which will be missed by the metrics that only look at the AV behaviour or planning result.Most recently, the community starts to approach this problem with some initial efforts (Sun et al., 2020;Philion et al., 2020;Ivanovic and Pavone, 2021;Deng et al., 2021).Despite some success, these preliminary solutions only exploit certain aspects of the problem, either implicitly relying on weak correlation between behaviour change and driving cost (Philion et al., 2020), inferring the holistic cost via local properties (Ivanovic and Pavone, 2021), or coarse levels (Sun et al., 2020).In this work, we propose a principled and universal framework to quantify how noise in perception input affects the AV planning.This is achieved by explicitly analysing the process of AV planning, in the context of expected utility maximisation (Osborne and Rubinstein, 1994), and evaluating the change of utility values critical to the AV reasoning subject to input perception errors.Under some mild conditions (Section 3.3), we show that this planning process can be formulated as an optimisation problem with linear objective function in a Hilbert space, where utility to optimise is the inner product of an action-wise utility function and the world state distribution represented by perception.This geometric interpretation reveals many natural and insightful properties of the problem, for example, any input error can be decomposed into two parts: one that does not affect the utility comparison (planning-invariant error) and the other one that directly changes the planning problem (planning-critical error).Based on this novel insight, we derive a metric that quantify how a perception error changes the planning process.We want to emphasise the necessity of understanding impacts of perception errors on an autonomous driving system via the process of planning, rather than purely from the final result (i.e., the AV behaviour, or the trajectory output from the planning module), as proposed by previous works (Philion et al., 2020).This results from the fact that, the final planning result does not necessarily reflect how AVs evaluate the situation, reason with the environment, and assess the costs of actions.In fact, the correlation between behaviour change and the actual consequence is weak, or even negative in many common cases, as illustrated in Figure 1.Actually, most works implicitly or explicitly integrate some priori knowledge of consequences of perception errors into metric design.The complexity of such impact on autonomous driving, however, is far beyond hand-crafted rules, defeating their purposes despite tremendous amounts of manual efforts, e.g., Deng et al. (2021) assumes that severity of an error should be weighted proportional to the reciprocal of its cubed Manhattan distance to the AV, regardless of its position relative to the AV (in front or behind the AV).In contrast, we make little such presumption and fully rely on the planning process to infer the error consequence in a fully transparent way, which enables our solution to capture many critical cases.In this regard, the core principle of our design resembles the idea in the philosophical system of transcendental idealism, proposed by Immanuel Kant in his classical work Critique of Pure Reason (Kant, 1998), which argues that, due to the limitation of the observer's sensibility, cognition of external objects is processed never as they are in themselves, but via the cognitive faculties and subject to the interpretation of the observer's experience.For the same reason, the consequence of environment misrepresentation for an AV due to perception errors is naturally reflected via the change in its planning (the core component of an AV that interprets its environment) and measured by the extra loss incurred, which gives the name to our framework: transcendental idealism of planner (TIP)."}
{"paper_id": 379, "abstract": "In the realm of object detection, the Detection Transformer (DETR) has carved out a unique niche with its end-to-end approach, yet it stumbles when faced with the intricacies of multiple positive object queries. To address this limitation, we unveil a groundbreaking training strategy: **Group DETR**. This innovative method introduces a one-to-many assignment paradigm, allowing for a more nuanced understanding of object relationships in a group-wise manner.  At the heart of Group DETR lies a series of straightforward yet powerful modifications to the training process. First, we harness the power of $K$ distinct groups of object queries, each designed to capture the multifaceted nature of the objects in our scenes. Next, we apply decoder self-attention uniformly across these groups, ensuring a cohesive approach that maintains the integrity of our model. Finally, we implement one-to-one assignments for each group, culminating in $K$ positive object queries for every ground-truth object.  Crucially, during inference, we streamline our process by utilizing only a single group of object queries, preserving the elegance of the original model architecture and inference workflow. Group DETR stands as a versatile training methodology, seamlessly adaptable to a variety of DETR variants. Our experiments reveal that this approach not only accelerates the convergence of training but also elevates the performance of multiple DETR-based methods, heralding a new chapter in the evolution of object detection.", "introduction": "Detection Transformer (DETR) (Carion et al., 2020) achieves end-to-end detection without the need of non-maximum suppression (NMS) (Hosang et al., 2017).There are several designs: (i) adopt an encoder-decoder architecture based on transformer layers (Vaswani et al., 2017), (ii) introduce object queries, and (iii) perform one-to-one assignmentfoot_0 by conducting bipartite matching (Kuhn, 1955) between object predictions and ground-truth objects.The original DETR suffers from the slow convergence issue and needs 500 training epochs to achieve good performance.Various solutions have been developed to accelerate the training from different aspects.For example, sparse transformers (Zhu et al., 2020b;Gao et al., 2021;Chen et al., 2022c;Roh et al., 2022) are adopted to replace dense transformers.Additional spatial modulations are introduced into object queries (Zhu et al., 2020b;Meng et al., 2021;Wang et al., 2022b;Yao et al., 2021;Liu et al., 2022a;Gao et al., 2022).Denoising modules are presented for stabilizing the object query and group-truth matching in the assignment process (Li et al., 2022;Zhang et al., 2022b).In this paper, we propose a novel training approach Group DETR to accelerate DETR training convergence.Group DETR introduces group-wise one-to-many assignment.It assigns each groundtruth object to many positive object queries (one-to-many assignmentfoot_1 ), and separate them into multiple independent groups, keeping only one positive object query per object (one-to-one assignment) in each group.To achieve it, we make simple modifications during training: (i) adopt K groups of object queries; (ii) conduct decoder self-attention on each group of object queries with the same parameters; (iii) perform one-to-one assignment in each group, leading to K positive object queries for each ground-truth object.The design achieves fast training convergence, maintaining the key DETR property: enabling end-to-end object detection without NMS.We only use one group of object queries in inference, and we do not modify either architectures or processes, bringing no extra cost compared with the original method.Group DETR is a versatile training method and can be applied to various DETR-based models.Extensive experiments prove that our method is effective in achieving fast training convergence  (Lin et al., 2014) with ResNet-50 (He et al., 2016) as the backbone.More results and comparisons can be found in Table 1 and Table 2. Here, we use different colors to distinguish different models in the figure and apply dashed curves and bold curves to highlight the comparisons between baseline models and their Group DETR counterparts.Best view in color.(convergence curves are shown in Figure 1).Group DETR obtains consistent improvements on various DETR-based methods (Meng et al., 2021;Liu et al., 2022a;Li et al., 2022;Zhang et al., 2022b).With a 12-epoch ( 1\u00d7) training schedule on MS COCO (Lin et al., 2014), Group DETR significantly improves Conditional DETR-C5 by 5.0 mAP.The non-trivial improvements hold when we adopt longer training schedules (e.g., 36 epochs or 50 epochs).Moreover, Group DETR can easily outperform baseline models when applied to multi-view 3D object detection (Liu et al., 2022b;c) and instance segmentation (Cheng et al., 2021)."}
{"paper_id": 380, "abstract": "In this paper, we embark on the quest of unsupervised object discovery within the realm of videos\u2014a challenge fraught with complexity and nuance. Previous endeavors have illuminated pathways through the use of optical flows to delineate objects, yet this approach carries with it two significant burdens. First, the reliance on flow alone falters when objects stand still or are shrouded in partial occlusion, rendering vital cues elusive. Second, the attempt to weave a tapestry of temporal coherence from flow data often unravels, hindered by the absence of rich texture information.  To surmount these obstacles, we propose a novel model that directly engages with consecutive RGB frames, weaving together their visual narratives. By employing a layered representation, we infer the optical flow between any two frames, treating the opacity channels as the very fabric of segmentation. But we do not stop there. To uphold the principle of object permanence, we introduce a temporal consistency loss that operates on inferred masks from randomly paired frames\u2014an acknowledgment of the varied rhythms of motion. This encourages our model to maintain its grasp on object identities, even when they remain motionless at a given moment.  Through rigorous experimentation, we unveil our model's superior prowess, outshining previous state-of-the-art methods across three esteemed public video segmentation datasets: DAVIS2016, SegTrackv2, and FBMS-59. Remarkably, our approach is not only effective but also computationally efficient, deftly sidestepping the overhead typically associated with optical flow calculations. In this way, we chart a new course in the landscape of video segmentation, illuminating the path forward with clarity and precision.", "introduction": "Representing the visual scene with objects as the basic elements has long been considered a fundamental cognitive ability of the intelligent agent, for it enables understanding and interaction with the world more efficiently, for example, combinatorial generalization in novel settings (Tenenbaum et al., 2011).Although it remains somewhat obscure at the level of neurophysiology on exactly how humans discover the objects in a visual scene in the first place, it is a consensus that motion seems to play an indispensable role in defining and discovering the objects from the scene.For example, in 1923, Wertheimer introduced the common fate principle that elements moving together tends to be perceived as a group (Wertheimer, 1923); while later Gibson claimed the independent motion has even been treated as one attribute to define an object visually (Gibson & Carmichael, 1966).Grounded on the above assumptions, the recent literature has witnessed numerous works with different models proposed for segmenting the moving objects via unsupervised learning (Yang et al., 2019;2021b;a;Liu et al., 2021).Exploiting optical flows for object discovery naturally incurs two critical limitations: First, objects in videos may stop moving or be partially occluded at any time point, leaving no effective cues for their existence in the flow field; Second, computing optical flow from a pair of frames refers to a lossy encoding procedure, that poses a significant challenge for establishing temporal coherence, due to the lack of effective texture information.In contrast, adopting RGB frame sequences poses a few clear advantages.The most obvious one is that, while objects do not necessarily move all the time, the property of temporal coherence in RGB space naturally guarantees a preliminary understanding of object permanence; Additionally, the rich textures in the appearance stream give more distinctive patterns than those in motion, allowing to better identify and distinguish the different objects.Last but not least, processing RGB streams still enables a faster processing speed than using optical flow.In this paper, our goal is to train a video segmentation model that can discover the moving objects within a sequence of RGB frames, in the form of segmentation.In specific, our proposed model first encodes consecutive frames independently, into a set of frame-wise visual features, that is followed by a temporal fusion with a Transformer encoder.To localise the moving objects, we randomly  (Li et al., 2013).The red boxes and the yellow boxes refer to the arm and the back of the player, respectively.Flowonly method (Yang et al., 2021a) fails to track the same region in a temporal consistent fashion since it derives the foreground region directly from current optical flow.However, our methodology of processing a RGB video clip develops a sense of object permanence and solves the issue.pair the visual features from two frames and pass them into a frame comparator module, effectively establishing the relative motion between frames.Inspired by Yang et al. (2021a), we decode the motion features into optical flows with a dual-layered representation, with the opacity weight of each layer treated as the segmentation mask.At training time, we exploit an off-the-shelf optical flow estimator, e.g., RAFT (Teed & Deng, 2020), as the induction for flow reconstruction.To develop the property of object permanence, we enforce a temporal consistency on the inferred segmentation masks, which encourages the model to mine effective texture information from the RGB sequence and keep track of the objects even if they may be static at the current time point.In short, we summarize the contributions in this paper: First, we introduce the Motion-inductive Object Discovery (MOD) model, a simple architecture for discovering the moving objects in videos, by directly processing a set of consecutive RGB frames.Second, we propose a self-supervised proxy task that is used to train the architecture without relying upon any manual annotation.To overcome the challenge from flow-based methods, i.e., objects may stay static or move slowly, we adopt a random-paired policy and restrain the temporal consistency.Third, we conduct a series of ablation studies to validate each key component of our method, such as the temporal consistency of randompaired flow.While evaluating three public benchmarks, we demonstrate superior performance over existing approaches on DAVIS2016 (Perazzi et al., 2016), SegTrackv2 (Li et al., 2013), and FBMS-59 (Ochs et al., 2013), with considerable speed-up during the inference procedure."}
{"paper_id": 381, "abstract": "In the realm of multivariate time series forecasting, the prevailing method for unraveling the intricate relationships among various time series involves the construction of a graph. In this graph, each time series is depicted as a node, with edges connecting related nodes, giving rise to what we call spatial-temporal graph neural networks. These graphs can be either predetermined or learned based on the similarities observed among nodes. Yet, the relationships between these time series are often far more complex than a simple connection; for instance, the total outflows from upstream nodes might precisely equal the inflows into downstream nodes. Such nuanced dynamics are prevalent in numerous real-world forecasting scenarios but have remained largely unexamined.  To address this complexity, we propose a novel framework designed to model inter-node relationships with greater fidelity. Our approach introduces an inductive bias for graphs termed the Functional Relation Field. Within this framework, we employ a set of functions, each parameterized by neural networks, to capture the dependencies among multiple time series. These learned functions exhibit remarkable versatility: they not only help unveil the underlying graph structure by pinpointing the most relevant neighbors of a target node but also establish a \u201cfield\u201d where the nodes within the backbone prediction networks are compelled to adhere to the constraints defined by these functions.  We validate our approach through experiments on a synthetic dataset, demonstrating its capability to accurately recover the true relational constraints between nodes. Additionally, we apply our framework to two real-world datasets\u2014MiniApp calling traffic and road network data\u2014utilizing various backbone networks. The results are compelling, revealing a significant reduction in prediction error when leveraging the proposed Functional Relation Field framework. Through this work, we aim to illuminate the intricate web of relationships that govern multivariate time series, paving the way for more precise forecasting in complex systems.", "introduction": "Multivariate time series forecasting has surged recently due to its strong expressiveness of the spatio-temporal dependence among the data and its enormous popularity in vast application areas, such as the prediction of urban traffic, computer network flow, cloud micro-services calling flow, and rigid body motion, to name a few (Li et al., 2018;Yu et al., 2018;Bai et al., 2020;Yan et al., 2018;Liu et al., 2020).The most popular and straightforward strategy for modeling the relationship between multiple time series is the introduction of graph, where each time series is represented as a node and related nodes are connected by edges.This particular inductive bias for multivariate time series prediction results in the so called spatial-temporal graph neural networks (Yu et al., 2018).The graph structure is either given apriori (e.g. in traffic flow prediction, each road as a node has connected roads forming the graph.)or learned based the similarity between nodes (Yu et al., 2019;Bai et al., 2020;Shang et al., 2021).However, in practice, the relationship between multiple time series is typically complicated.For instance, there often exist constraints among the nodes, ranging from the equality between the inflow and the outflow for a node in a traffic network to the geometric constraints of the rigid body motion.Such relations widely exist in many real-world multivariate time series forecasting scenarios, yet are far from well studied.In these cases, graph might not be sufficient for characterizing the dependency between nodes.As a remedy, in this work, we explore a new framework to model the inter-node relationship in a more precise manner than graph, Functional Relation Field (FRF), where a group of functions parameterized by neural networks are learned to characterize the dependency between multiple time series explicitly.These learned functions are versatile: first they can then be used to discover the underlying graph structure by identifying the most relevant neighbors of the target node; and on the other hand, the learned functions will form a \"field\" where the nodes in the backbone prediction networks are further enforced to satisfy the constraints defined by these functions.As illustrated in Fig. 1, the left panel shows the traditional graph neural networks assuming similar time series have edge connections, while our framework on the right panel models the dependency between nodes through a functional relationship, e.g. a linear form to enforce the constraints between the flows of target and dependent nodes.In our framework, we mainly solve the following two issues: (i) How to learn the functional field?We need to select the dependent nodes that have a relationship with the target node, and express the constraint in a functional form; (ii) How to guarantee the constraints satisfaction?The (functional) constraints relationship should be maintained in the predicted output in both training and test process.To address these issues, we propose a two-stage approach that can discover the functional relations (i.e.constraints) from data and further integrate the constraints seamlessly when forecasting the multivariate time series.Specifically, we first train a neural network with a selected target node as its output and all the other nodes as dependent variables (i.e. the input of this neural network), and identify the most relevant dependent nodes based on this trained network.We then re-train it to learn the relationship among the target and the discovered relevant nodes.Next, we incorporate these functional constraints into the network backbones by imposing them to the predicted output during both training and test process.More precisely, the output of the network could be guaranteed to satisfy the constraints by utilizing the constraint-satisfied transformation and loss minimization.We compare the proposed approach with SVM, fully connected networks, fully connected LSTM, and five backbone models (i.e., STGCN (Yu et al., 2018), AGCRN (Bai et al., 2020), Autoformer (Wu et al., 2021), FEDformer (Zhou et al., 2022), SCINet (Liu et al., 2022)).Experimental results show that our approach significantly improves the performance over the original network backbones and other baseline models.Univariate time series forecasting.Recently, much research focuses on time series forecasting with deep learning models due to their powerful representational capability and prediction performance, including feed-forward neural network, RNN (Rumelhart, 1986) and its variants LSTM (Hochreiter & Schmidhuber, 1997) and GRU (Cho et al., 2014).The transformer architecture and its variants (Vaswani et al., 2017;Simm et al., 2020;Zhou et al., 2021;Child et al., 2019;Lim et al., 2020;Li et al., 2019;Wu et al., 2021;Zhou et al., 2022) also made much progress on univariate time-series forecasting on learning long-range dependence.In order to model the trend and seasonality of time series in an interpretable way, N-beats (Oreshkin et al., 2020) network that stacked very deep fullconnection network based on backward and forward residual links has improved the multi-horizon prediction accuracy significantly.Moreover, DeepAR (Salinas et al., 2020) and Deep State-Space Model (DSSM) (Rangapuram et al., 2018) stack multi-layer LSTM network to generate parameters of one-step-ahead Gaussian predictive distributions for multi-horizon prediction.Multivariate time series forecasting.Spatio-temporal graph neural networks (Yu et al., 2018;Chen et al., 2019;Pan et al., 2021;Li et al., 2020) have been proposed to model the spatial correlation and temporal dependency in multivariate time-series.Apart from capturing the temporal dependence, these methods further model the spatial dependence among all time series via graph neural networks, leveraging the information from the neighboring time series to help forecasting the target one.It is well known that an informative graph structure is important to the graph time series forecasting.Therefore, many algorithms (Bai et al., 2020;Seo et al., 2016;Shang et al., 2021) were proposed to discovery the underlying graph structure.AGCRN (Bai et al., 2020) assumed the graph structure is unknown and adopted an adaptive approach to learn the embedding vectors for all nodes, and then replaced the adjacency matrix in graph convolutions with a function of the node embeddings.However, the similarity graph calculated with the learned node embedding is a dense and continuous graph instead of a sparse and discrete graph.Therefore, GTS (Shang et al., 2021) formulated the graph structure learning problem as a probabilistic graph model to learn the discrete graph through optimizing the mean performance over the graph distribution.Different from the existing multivariate time series prediction methods, AGCRN (Bai et al., 2020) (with a fully connected graph) and STGCN (Yu et al., 2018) (with a given graph), we consider a more precise way, i.e. functional relations as constraints, to learn the connection between time series.The new inductive bias expressed by these functional relations can be applied to different backbone networks to help recover the graph structure and act as regularization in both training and test process."}
{"paper_id": 382, "abstract": "In the ever-evolving landscape of artificial intelligence, the quest to glean reward functions from human feedback has predominantly relied on the enigmatic depths of deep neural networks. Yet, this opacity often obscures our understanding of agent behavior and complicates our efforts to ensure alignment with human values. In this study, we turn our attention to a more transparent alternative: intrinsically interpretable tree models. We harness a novel approach for constructing reward trees from preference labels, revealing that these models stand shoulder to shoulder with their neural counterparts, even in the face of daunting high-dimensional challenges. Remarkably, they exhibit resilience against limited or corrupted data, showcasing their robustness. With our findings firmly established, we delve into the compelling reasons for adopting reward tree learning in complex environments. The clarity of the interpretable reward structure not only enhances traceability but also empowers us with the tools for verification and explanation\u2014essential elements for fostering trust in intelligent systems.", "introduction": "For a reinforcement learning (RL) agent to reliably achieve a goal or desired behaviour, this objective must be encoded as a reward function.However, manual reward design is widely understood to be challenging, with risks of under-, over-, and mis-specification leading to undesirable, unsafe and variable outcomes (Pan et al., 2022).For this reason, there has been growing interest in enabling RL agents to learn reward functions from normative feedback provided by humans (Leike et al., 2018).These efforts have proven successful from a technical perspective, but an oft-unquestioned aspect of the approach creates a roadblock to practical applications: reward learning typically uses black-box neural networks (NNs), which resist human scrutiny and interpretation.For advocates of explainable AI (XAI), this is a problematic state of affairs.The XAI community is vocal about the safety and accountability risks of opaque learning algorithms (Rudin, 2019), but an inability to interpret even the objective that an agent is optimising places us in yet murkier epistemic territory, in which an understanding of the causal origins of learnt behaviour, and their alignment with human preferences, becomes virtually unattainable.Black-box reward learning could also be seen as a missed scientific opportunity.A learnt reward function is a tantalising object of study from an XAI perspective, due to its triple status as (1) an explanatory model of revealed human preferences, (2) a normative model of agent behaviour, and (3) a causal link between the two.The approach proposed by Bewley & Lecue (2022) provides a promising way forward.Here, human preference labels over pairs of agent behaviours are used to learn tree-structured reward functions (reward trees), which are hierarchies of local rules that admit visual and textual representation and can be leveraged to monitor and debug agent learning.In this paper, we adapt and extend the method (including by integrating it with model-based RL agents), and compare it to NN-based reward learning in a challenging aircraft handling domain.We find it to be broadly competitive on both quantitative metrics and qualitative assessments, with our new modification to tree growth yielding significant improvements.The resultant trees are small enough to be globally interpretable (\u2248 20 leaves), and we demonstrate how they can be analysed, verified, and used to generate explanations.The primary contribution of this paper is positive empirical evidence that reward learning can be done effectively using interpretable models such as trees, even in complex, high-dimensional continuous environments.We also make secondary methodological contributions: improvements to the originally-proposed learning algorithm, as well as metrics and methods for reward evaluation and interpretability that may be useful to others working in what remains a somewhat preparadigmatic field.After reviewing the necessary background and related work in Sections 2 and 3, we present our refinement of reward tree learning in Section 4, and describe how we deploy it online with a model-based agent in Section 5. Section 6 contains our experiments and results, which consider both quantitative and qualitative aspects of learning performance, and an illustrative analysis of learnt tree structures.Finally, Section 7 concludes and discusses avenues for future work.Markov Decision Processes (MDPs) In this formulation of sequential decision making, the state of a system at time t, s t \u2208 S, and the action of an agent, a t \u2208 A, condition the successor state s t+1 according to dynamics D : S \u00d7 A \u2192 \u2206(S) (\u2206(\u2022) denotes the set of all probability distributions over a set).A reward function R : S \u00d7A\u00d7S \u2192 R then outputs a scalar reward r t+1 given s t , a t and s t+1 .RL uses exploratory data collection to learn action-selection policies \u03c0 : S \u2192 \u2206(A), with the goal of maximising the expected discounted sum of future reward, E D,\u03c0 \u221e h=0 \u03b3 h r t+h+1 , \u03b3 \u2208 [0, 1].Reward Learning In the usual MDP framing, R is an immutable property of the environment, which belies the practical fact that AI objectives originate in the uncertain goals and preferences of fallible humans (Russell, 2019).Reward learning (or modelling) (Leike et al., 2018) replaces handspecified reward functions with models learnt from humans via revealed preference cues such as demonstrations (Ng et al., 2000), scalar evaluations (Knox & Stone, 2008), approval labels (Griffith et al., 2013), corrections (Bajcsy et al., 2017), and rankings (Christiano et al., 2017).The default use of NNs for reward learning severely limits interpretability; reward trees provide a possible solution.XAI for RL (XRL) Surveys of XAI for RL (Puiutta & Veith, 2020;Heuillet et al., 2021) divide between intrinsic approaches, which imbue agents with structure such as object-oriented representations (Zhu et al., 2018) or symbolic policy primitives (Verma et al., 2018), and post hoc analyses of learnt representations (Zahavy et al., 2016), including computing feature importance/saliency (Huber et al., 2019).Spatiotemporal scope varies from the local explanation of single actions (van der Waa et al., 2018) to the summary of entire policies via representative trajectories (Amir & Amir, 2018) or critical states (Huang et al., 2018).While most post hoc methods focus on fixed policies, some provide insight into the dynamics of agent learning (Dao et al., 2018;Bewley et al., 2022).Explainable Reward Functions At the intersection of reward learning and XRL lie efforts to improve human understanding of reward functions and their effects on action selection.While this area is \"less developed\" than other XRL sub-fields (Glanois et al., 2021), a distinction has again emerged between intrinsic approaches which create rewards that decompose into semantic components (Juozapaitis et al., 2019) or optimise for sparsity (Devidze et al., 2021), and post hoc approaches which apply feature importance analysis (Russell & Santos, 2019), counterfactual probing (Michaud et al., 2020), or simplifying transformations (Jenner & Gleave, 2022).Sanneman & Shah (2022) use human-oriented metrics to compare the efficacy of reward explanation techniques.In this taxonomy, reward tree learning is an intrinsic approach, as the rule structure is inherently readable.Trees in RL Tree models have a long history in RL (Chapman & Kaelbling, 1991;D\u017eeroski et al., 1998;Pyeatt, 2003).Their use is increasingly given an XRL motivation.Applications again divide into intrinsic methods, where an agent's policy (Silva et al., 2020), value function (Liu et al., 2018;Roth et al., 2019) or dynamics model (Jiang et al., 2019) is a tree, and post hoc tree approximations of an existing agent's policy (Bastani et al., 2018;Coppens et al., 2019) or transition statistics (Bewley et al., 2022).Related to our focus on human-centric learning, Cobo et al. (2012) learn tree-structured MDP abstractions from demonstrations and Tambwekar et al. (2021) distill a differentiable tree policy from natural language.While Sheikh et al. ( 2022) use tree evolution to learn dense intrinsic rewards from sparse environment ones, Bewley & Lecue (2022) are the first to learn and use reward trees in the absence of any ground-truth reward signal, and the first to do so from human feedback."}
{"paper_id": 383, "abstract": "In the realm of visual recognition, deep metric learning (DML) stands as a formidable ally, adept at tackling the challenge of vast intra-class variation while minimizing the subtle inter-class discrepancies. Yet, when thrust into the arena of generalized zero-shot learning (GZSL)\u2014where a target image might bear the mark of an unseen category\u2014DML can falter, often skewed towards the familiar seen classes. However, within the GZSL landscape lies a treasure: the semantic space. This invaluable resource forges connections between seen and unseen classes, serving as a guiding star for the learning of visual representations.  To harness the strengths of DML while sidestepping the pitfalls of overfitting to seen classes, we unveil a groundbreaking framework: Metric Learning with Implicit Semantics (MLIS). This innovative approach meticulously disentangles the influences of semantics on both the feature extraction and the classification processes of the model. In this way, semantics play a pivotal role solely in the realm of feature learning, while the classification mechanism relies exclusively on the refined visual features.  Moreover, we boldly relax the stringent requirement for visual-semantic alignment, liberating our model from the necessity of pair-wise comparisons between images and class embeddings. The results of our experiments illuminate the path forward, showcasing that the MLIS framework not only bridges the divide between DML and GZSL but also achieves state-of-the-art performance. Robust and adaptable, it seamlessly integrates with a variety of metric learning-based loss functions, heralding a new era in visual recognition.", "introduction": "With the consideration of real-world recognition problems that may not have defined all classes during training, generalized zero-shot learning (GZSL) aims to leverage third-party data (e.g., attributes, semantic descriptors) to recognize samples from both of the seen and unseen classes Socher et al. (2013); Chao et al. (2016); Pourpanah et al. (2022).Therefore, a typical dataset for studying this problem is divided into two class sets: seen and unseen, with no intersection in between Xian et al. (2018a).Only samples of the seen classes are available for training the image recognition model; however, samples of both seen and unseen classes may appear during inference.The third-party data involving class-level semantic descriptors such as attributes are important in GZSL to relate seen and unseen classes.The knowledge learned from the seen classes must be generalized to recognize an unseen class through semantic information, because the visual data of unseen classes are absent in the training stage.Whether a zero-shot setting is applied or not, an image recognition task can be categorized into fine-grained and coarse-grained, based on the amount of inter-class variation in visual appearance.Fine-grained recognition is considered more difficult than coarse-grained recognition due to subtle differences between classes.Nevertheless, the large intra-class variation in fine-grained recognition, often neglected in current studies, poses additional challenges to the task.Figure 1 displays a few samples in the CUB benchmark Wah et al. (2011).Some samples look quite differently to other samples in the same class.Such a large intra-class variation in appearance is inevitable because factors such as migration and molt may affect how birds change their colors.Deep metric learning (DML) offers a natural solution to address large intra-class and small interclass variance problem Hoffer & Ailon (2015); Wang & Chen (2017); Wang et al. (2019); Sun et al. (2020).It provides a flexible similarity measurement of data points.Each sample can have a different penalty in updating the model.By optimizing the contrastive loss from positive pairs (intra-class) and negative pairs (inter-class), a model can leverage class-wise supervision to learn embeddings and give more penalties to hard samples.Furthermore, this technique can be applied on large scale dynamic and open-ended image datasets, and can allow extensions with limited efforts to new classes.The semantic information in GZSL has been used to generate the visual features for unseen classes Xian et al. (2018b;2019), as well as to guide the learning of discriminative visual features Ji et al. (2018); Zhu et al. (2019); Li & Yeh (2021).However, when DML is applied, a model can be easily overfitted to the seen classes despite the merits of DML mentioned above Bucher et al. (2016).Furthermore, a broad family of GZSL methods learn a joint embedding space, in which the classification is performed by directly comparing the embedded data points with the class prototypes Xian et al. (2018a).Learning such embedding functions can be difficult, because image features are extracted by a visual model pre-trained on ImageNet and class prototypes are human annotated attributes or are from word embeddings learned from text corpus.The visual and the semantic feature vectors may reflect inconsistent inter-class and intra-class discrepancies.The difficulty is exacerbated with the integration of generative methods to create visual features for unseen classes because the distribution of synthesized features is less predictable.Delving into the image classification framework, it is composed of a feature extractor and a classifier Krizhevsky et al. (2012); He et al. (2016).Previous works typically use semantics in both feature extraction and classification Hu et al. (2020); Liu et al. (2021); Chen et al. (2021a); Chandhok & Balasubramanian (2021).As a result, the model is forced to align visual and semantic spaces, which may be difficult because of the modality gap mentioned above.Furthermore, semantics are used in both synthesizing visual features and learning embedding functions, which may introduce serious bias towards seen classes.To better leverage the semantic information, a viable solution is to refine visual embeddings by semantics, while the classification is performed only based on visual features.Therefore, we present a novel representation learning framework, named Metric Learning with Implicit Semantics (MLIS), for GZSL.It takes advantage of metric learning to refine discriminative visual features from the original image features, while avoiding overfitting by making good (but not too extensive) use of semantics.MLIS decouples the effect of semantics on feature extractor and image classification, so that semantics only participate in feature learning, and classification only uses the refined visual features.This decoupling facilitates the training of both tasks.In feature learning we further relax the visual-semantic alignment requirement, avoiding performing pair-wise comparisons between the image and the class embeddings.To summarize, MLIS has the following characteristics that distinguish itself from existing methods:\u2022 Semantic descriptors are given and fixed; they cannot be trained or fine-tuned.A GZSL model will reply on semantics to relate the seen and unseen classes; therefore fixing semantics reduces model complexity and thereby also reduces the chances of being overfitted.\u2022 Semantic descriptors are involved only in training the encoder; they are not used for downstream tasks (e.g., classification, segmentation).The downstream model utilizes only the visual features to perform the task.In this work semantic information is agnostic to the classification task.\u2022 The entire framework learns only to refine visual features, and semantic descriptors implicitly affect the learning of visual features.We only pair an input visual feature vector up with its semantic descriptor to compute the loss, not all semantic descriptors.\u2022 Visual-semantic alignment is not strictly enforced as we rely only on visual features to perform classification.The MLIS model is optimized to refine visual features so that when they are concatenated with the semantic descriptor of the target class, the metric learning based loss is minimized.We learn semantically meaningful visual features via metric learning from the semantics without aligning the visual and the semantic space.We conduct extensive experiments on five benchmark datasets, including CUB Wah et al. (2011), AWA2 Xian et al. (2018a), SUN Patterson & Hays (2012), FLO Nilsback & Zisserman (2008), and aPY Farhadi et al. (2009).We demonstrate the superiority of the proposed method with performance on par with the state of the art.The code will be available upon paper acceptance."}
{"paper_id": 384, "abstract": "In our latest exploration, we embark on a journey to elevate the concept of class representative vectors found in deep classification networks, transforming them into the realm of linear subspaces. This innovative approach not only enhances inter-class discrimination but also enriches intra-class feature variation, creating a harmonious balance that was previously elusive. Traditionally, the logit is derived from the inner product of a feature and its corresponding class vector. However, we propose a paradigm shift: envisioning classes as subspaces where the logit emerges from the norm of the projection of a feature onto these subspaces.  As we navigate the intricate landscape of Grassmann manifolds, our task becomes one of optimizing the loss on a Grassmannian\u2014a challenge that we tackle with the integration of Riemannian Stochastic Gradient Descent (SGD) into established deep learning frameworks. This allows for a seamless joint optimization of class subspaces within the Grassmannian alongside the model parameters residing in Euclidean space.   The beauty of subspaces lies in their dual nature: they possess the capacity for multi-dimensional representation while remaining scaleless. Our empirical investigations reveal the profound impact of these properties across various tasks.   1. **Image Classification**: Our novel formulation propels the top-1 accuracy of ResNet50-D on ImageNet-1K from 78.04% to an impressive 79.37% over 100 training epochs, showcasing the superior representative power of subspaces compared to traditional vectors.  2. **Feature Transfer**: The flexibility afforded by subspaces fosters greater intra-class variability, particularly as subspace dimensions expand. This leads to a marked improvement in feature quality for downstream tasks, with average transfer accuracy across six datasets rising from 77.98% to 80.12% against a robust baseline of vanilla softmax.  3. **Long-Tail Classification**: The inherent scaleless nature of subspaces proves advantageous in long-tail scenarios, boosting the accuracy of ImageNet-LT from 46.83% to 48.94% when juxtaposed with standard formulations.  With these promising results, we are excited about the potential for broader applications stemming from our Grassmannian class representation. Stay tuned, as we will be releasing our code for the community to explore further.", "introduction": "The idea of representing classes as linear subspaces in machine learning can be dated back, at least, to 1973 (Watanabe & Pakvasa (1973)), yet it is mostly ignored in the current deep learning literature.In this paper, we revisit the scheme of representing classes as linear subspaces in the deep learning context.To be specific, each class i is associated with a linear subspace S i , and for any feature vector x, the i-th class logit is defined as the norm of projection l i := proj Si x .(1)Since a subspace is a point in the Grassmann manifold (Absil et al. (2009)), we call this formulation the Grassmannian class representation.In the following, we answer the two critical questions, 1.Is Grassmannian class representation useful in real applications?2. How to optimize the subspaces in training?The procedure fully-connected layer \u2192 softmax \u2192 cross-entropy loss is the standard practice in deep classification networks.Each column of the weight matrix of the fullyconnected layer is called the class representative vector and serves as a prototype for one class.This representation of class has achieved huge success, yet it is not without imperfections.In the study of transferable features, researchers noticed a dilemma that representations with higher classification accuracy on the original task lead to less transferable features for downstream tasks (Kornblith et al. (2021); M\u00fcller et al. (2019)).This is connected to the fact that they tend to collapse intra-class variability of representations, resulting in loss of information in the logits about the resemblances between instances of different classes.Furthermore, the neural collapse phenomenon (Papyan et al. (2020)) indicates that as training progresses, the intra-class variation becomes negligible, and features collapse to their class-means.So this dilemma inherently originates from the practice of representing classes by a single vector.The Grassmannian class representation shed light on this issue as features of each class are allowed to vary in a high-dimensional subspace without incurring losses in classification.In the study of the long-tail classification, researchers found that the norm of class representative vectors is highly related to the number of training instances in the corresponding class (Kang et al. (2019)) and the recognition accuracy is affected.To counter this effect, the class representative vector is typically been rescaled to unit length during training (Liu et al. (2019)) or re-calibrated in an extra post-processing step (Kang et al. (2019)).In addition to these techniques, the Grassmannian class representation provides a natural and elegant solution for this as subspace is scaleless.It is well known that the set of k-dimensional linear subspaces form a Grassmann manifold, so finding the optimal subspace representation for classes is to optimize on the Grassmann manifold.Thus for the second question, the natural solution is to use the geometric optimization (Edelman et al. (1998)), which optimizes an objective function under the constraint of a given manifold.Points being optimized are moving along geodesics instead of following the direction of Euclidean gradients.The preliminary concepts of geometric optimization are reviewed in Section 3, and the technical details of subspace learning are presented in Section 4. We implemented an efficient Riemannian SGD for optimization in Grassmann manifold as shown in Algorithm 1, which integrates the geometric optimization algorithms to deep learning frameworks so that both the linear subspaces in Grassmannian and model weights in Euclidean are jointly optimized.Going back to the first question, we experiment on three concrete tasks in Section 5 to demonstrate the practicality and effectiveness of Grassmannian class representation.We find that (1) Grassmannian class representation improves large-scale image classification accuracy.(2) Grassmannian class representation produces high-quality features that can better transfer to downstream tasks.(3) Grassmannian class representation improves the long-tail classification accuracy.With these encouraging results, we believe that Grassmannian class representation is a promising formulation and more applications may benefit from its attractive features."}
{"paper_id": 385, "abstract": "In the realm of Federated Recommender Systems (FRSs), the quest for delivering personalized recommendations while safeguarding user privacy unfolds like a delicate dance between efficiency and accuracy. These systems face the daunting challenge of high communication costs, stemming from the intricate exchanges between a central server and a multitude of clients. Previous explorations in federated supervised learning reveal that randomly sampling clients can enhance communication efficiency without sacrificing precision. Yet, in the world of FRSs, each user stands as an individual client, communicating solely item gradients. This complicates the integration of random sampling and the determination of the optimal number of clients to sample in each round, all while striving to maintain the integrity of the model\u2019s accuracy.  In this paper, we delineate sample complexity bounds that illuminate the number of clients necessary to sample in an FRS to uphold accuracy. We then confront a critical issue: demographic bias, measured by the disparity in average error rates across various groups. Traditional supervised learning techniques tackle this bias by imposing fairness constraints within the training loss, a strategy that necessitates sharing sensitive attributes with the server\u2014a practice strictly forbidden in federated environments to protect client privacy.  To navigate this conundrum, we introduce \\ouralgo, a Random Sampling based Fair Federated Recommender System, crafted to cultivate a global model that embodies fairness. Moreover, it simultaneously guides local clients toward a fair global model, effectively mitigating demographic bias at the client level without the need to disclose their protected attributes. Our empirical findings, drawn from two prominent real-world datasets (ML1M and ML100k) and varying sensitive features (age and gender), compellingly illustrate that RS-FairFRS not only curtails communication costs and demographic bias but also enhances model accuracy, paving the way for a more equitable and efficient future in federated recommendation systems.", "introduction": "Recommender systems (RSs) have a wide variety of applications in online platforms like e-business, e-commerce, e-learning, e-tourism, music and movie recommendation engines (Lu et al. (2015)).Traditional RSs require gathering clients' private information at the central server, leading to serious privacy and security risks.ML models can train locally due to edge devices' increased storage and processing power.This has led to Federated learning (FL) (McMahan et al. (2017)), which allows clients to share their updates with the server without any data transfer.The server proposes a common model which is communicated with all clients.Using their data and the global model, clients train locally and communicate the updated model to the server.FL has found many applications in the past few years, e.g., Google keyboard query suggestion (Yang et al. (2018)), smartphone voice assistant, mobile edge computing, and visual object detection (Aledhari et al. (2020)).These applications face numerous challenges including communication efficiency (Smith et al. (2018)), statistical heterogeneity (Smith et al. (2017)), systems heterogeneity (Bonawitz et al. (2019)), privacy, personalization, fairness (Kairouz et al. (2021)), and many more.This paper focuses on two primary issues: communication efficiency and demographic bias in FRSs.Unlike other applications of FL, where one client has data of many users, in FRS, each user acts as one client constituting a user's profile.FedRec (Lin et al. (2021)), an FRS, expects all the clients to train parallelly using matrix factorization (MF).In each communication round, the server aggregates the model updates from a huge number of local clients to obtain a global model, and this global model is then sent back to all the clients.This whole procedure increases the communication cost.We show that random sampling of clients in each communication round reduces the communication cost even when only item gradients of sampled users are communicated.Theoretically, we provide bounds on an ideal fraction of clients to be sampled to maintain the model's accuracy.Proving sample complexity bounds is non-trivial as the clients may possess non-IID data.To circumvent this issue, we assume an underlying clustering structure on the clients such that clients within a cluster share similar item vectors.The main novelty lies in proving that the random sampling will fetch enough representation from each cluster and the predicted ratings obtained after sampling small number of clients will not be far (with high probability) from that of predicted ratings obtained after communicating with all clients in all the rounds.Fairness in FRSs is a critical yet under-investigated area.Empirically, we prove that FedRec offers better recommendations to a particular group of clients.This unfair treatment can fortify the social stereotypes based on gender, race, or age, resulting in significant repercussions.So far, researchers have studied fairness in the domain of centralized RSs (Li et al. (2022)).Many past works (Islam et al. (2019); Yao & Huang (2017); Li et al. (2021); Yang et al. (2020a)) develop bias mitigation strategies in traditional RSs, which require sharing sensitive attributes with the server, causing privacy leakage in the federated setting.In FL framework, Yue et al. (2021); Kanaparthy et al. (2021); Du et al. (2020); Zhang et al. (2020) ameliorate bias in classification setting where each client possesses data of many users and thus can train for fairness in each communication round.As opposed to this, in FRS, each user acts as one client that sends its item gradients to the server after updating the user vectors and item gradients locally.This makes it extremely difficult to train locally towards fairness.We propose a dual-fair vector update technique with two phases.In Phase 1, the server aggregates the received item vectors and trains them towards fairness on a small fraction of data.Even if the global model is fair, local client updates may result in a heavily biased model.Thus in Phase 2, the clients minimize local error and learn item vectors closer to the globally fair vectors.In summary, our work aims at mitigating the issues of reducing the communication bottleneck and group bias in Federated Recommendation system (FedRec) (Yang et al. (2020b)) for the first time.We list down our main contributions below:1. We provide sample complexity bounds on the fraction of clients required for maintaining accuracy within the desirable limit in Theorem4.1.Our experiments prove that sampling these many clients improve communication costs in FRS without affecting accuracy even when the clients do not disclose their user vectors and share only updated item gradients.2. We show the existence of group bias in FRS quantified by evaluating discrepancies in the average group losses for each sensitive attribute.To mitigate this issue, we propose a novel dual-fairness vector update technique that tackles the issue of group fairness at local as well as global level.3. Combining the ideas of random-sampling and dual-fairness vector update, we propose RS-FAIRFRS, a novel FRS model which provides communication efficiency and improved fairness as well as accuracy .4. We show that RS-FAIRFRS mitigates demographic bias and improves accuracy via extensive experimentation on the two most popular datasets of ML1M and ML100K, with different demographics (age and gender)."}
{"paper_id": 386, "abstract": "In the ever-evolving landscape of machine learning, self-supervised learning has made remarkable strides, particularly in the realm of large model pre-training. Yet, when it comes to smaller models, the journey has been fraught with challenges. Traditionally, the path to improvement has relied heavily on the cumbersome process of knowledge distillation\u2014a two-stage endeavor where a robust teacher model is first crafted, only to be distilled into a smaller counterpart, enhancing its ability to generalize.  In this work, we unveil a groundbreaking approach: SlimCLR, a one-stage solution that empowers small models to achieve pre-training without the need for an external teacher. At the heart of SlimCLR lies the concept of slimmable networks, which integrate a full network alongside several weight-sharing sub-networks. This architecture allows us to conduct a singular pre-training session, yielding a variety of models\u2014including those that are compact and computationally efficient.  However, the self-supervised learning landscape presents unique hurdles. The interference caused by weight-sharing networks often leads to a significant drop in performance. A key indicator of this interference is the phenomenon of gradient imbalance, where a mere fraction of parameters dominate the gradient flow during backpropagation, leaving the primary parameters under-optimized. Additionally, the divergent gradient directions across various networks further exacerbate this issue.  To counteract these challenges, we implement a trio of innovative techniques: a gradual ramp-up in the training of sub-networks, an online distillation process that fosters collaboration among models, and a strategic loss re-weighting based on model sizes. Furthermore, we introduce a switchable linear probe layer during linear evaluation to mitigate the interference from weight-sharing linear layers.  By embedding SlimCLR within established contrastive learning frameworks, we achieve a remarkable breakthrough\u2014delivering superior performance while utilizing fewer parameters and reduced FLOPs than previous methods. With SlimCLR, we not only streamline the pre-training process but also pave the way for a new era of efficient model development.", "introduction": "In the past decade, deep learning achieves great success in different fields of artificial intelligence.A large amount of manually labeled data is the fuel behind such success.However, manually labeled data is expensive and far less than unlabeled data in practice.To relieve the constraint of costly annotations, self-supervised learning (Dosovitskiy et al., 2016;Wu et al., 2018;van den Oord et al., 2018;He et al., 2020;Chen et al., 2020a) aims to learn transferable representations for downstream tasks by training networks on unlabeled data.Great progress is made in large models, i.e., models bigger than ResNet-50 (He et al., 2016) that has roughly 25M parameters.For example, ReLICv2 (Tomasev et al., 2022) achieves 77.1% accuracy on ImageNet (Russakovsky et al., 2015) under linear evaluation protocol with ResNet-50, outperforming the supervised baseline 76.5%.In contrast to the success of the large model pre-training, self-supervised learning with small models lags behind.For instance, supervised ResNet-18 with 12M parameters achieves 72.1% accuracy on ImageNet, but its self-supervised result with MoCov2 (Chen et al., 2020c) is only 52.5% (Fang et al., 2021).The gap is nearly 20%.To fulfill the large performance gap between supervised and self-supervised small models, previous methods (Fang et al., 2021;Gao et al., 2022;Xu et al., 2022) mainly focus on knowledge distillation, namely, they try to transfer the knowledge of a selfsupervised large model into small ones.Nevertheless, such methodology actually has a two-stage procedure: first train an additional large model, then train a small model to mimic the large one.Besides, one-time distillation only produces a single small model for a specific computation scenario.An interesting question naturally arises: can we obtain different small models through one time pre-training to meet various computation scenarios without extra teachers?Inspired by the success of slimmable networks (Yu et al., 2019) in supervised learning, we present a novel one-stage Top-1 Accuracy on ImageNet (%) [1.0] [1.0, 0.5]Widths of a slimmable 200 epochs) [1.0, 0.5, 0.25] [1.0,0.75, 0.5, 0.25] The width 0.25 represents that the number of channels is scaled by 0.25 of the full network.method to obtain pre-trained small models without adding large models: slimmable networks for contrastive self-supervised learning (SlimCLR).A slimmable network consists of a full network and some weight-sharing sub-networks with different widths.The width denotes the number of channels in a network.Slimmable networks can execute at various widths, permitting flexible deployment on different computing devices.We can thus obtain multiple networks including small ones meeting low computing cases via one-time pre-training.Weight-sharing networks can also inherit knowledge from the large ones via the sharing parameters to achieve better generalization performance.Weight-sharing networks in a slimmbale network cause interference to each other when training simultaneously, and the situation is worse in self-supervised cases.As shown in Figure 1, with supervision, weight-sharing networks only have a slight impact on each other, e.g., the full model achieves 76.6% vs. 76.0%accuracy in  and 0.75,0.5,0.25] .Without supervision, the corresponding numbers become 67.2% vs. 64.8%.One observed phenomenon of the interference is gradient imbalance: a small proportion of parameters produces dominant gradients during backpropagation.The imbalance occurs because the sharing parameters receive gradients from multiple losses of different networks during optimization.The main parameters may not be fully optimized due to gradient imbalance.Besides, the conflicts in gradient directions of weight-sharing networks also cause gradient direction divergence of the full network.Please refer to Appendix A.3 for detailed explanations and visualizations.To relieve the gradient imbalance, the main parameters should produce dominant gradients during the optimization process.To avoid conflicts in gradient directions of various networks, sub-networks should have consistent guidance.Following these principles, we introduce three simple yet effective techniques during pre-training to relieve the interference of networks.1) We adopt a slow start strategy for sub-networks.The networks and pseudo supervision of contrastive learning are both unstable and fast updating at the start of training.To avoid interference making the situation worse, we only train the full model at first.After the full model becomes relatively stable, sub-networks can inherit the knowledge via sharing parameters and start with better initialization.2) We apply online distillation to make all sub-networks consistent with the full model to eliminate divergence of networks.The predictions of the full model will serve as global guidance for all sub-networks.3) We re-weight the losses of networks according to their widths to ensure that the full model dominates the optimization process.Besides, we adopt a switchable linear probe layer to avoid interference of weight-sharing linear layers during evaluation.A single slimmable linear layer cannot achieve several complex mappings simultaneously when the data distribution is complicated.We instantiate two algorithms for SlimCLR with typical contrastive learning frameworks, i.e., Mo-Cov2 and MoCov3 (Chen et al., 2020c;2021).Extensive experiments are done on ImageNet (Russakovsky et al., 2015) dataset, and the results show that our methods achieve significant performance improvements compared to previous arts with fewer parameters and FLOPs."}
{"paper_id": 387, "abstract": "In the realm of cross-modal models, the art of aligning image-text pairs through contrastive learning has revealed a potent yet intricate challenge. Traditionally, researchers have relied on cosine similarity to gauge the distance between the feature representations of these pairs, viewing it through the lens of negative inner product distance on a spherical surface. However, this spherical approach is fraught with vulnerabilities, particularly when faced with the murky waters of semantic ambiguity that arise from the noise inherent in pre-training datasets.  When the training data is tainted with noise, the model often finds itself not achieving the ideal alignment-uniformity but rather settling into an uneasy equilibrium\u2014a state where the distances between positive and negative pairs become neutralized, leading to gaps in expected performance. While one might assume that with sufficient training time, the model would navigate towards this equilibrium, the numerical outputs can often stray beyond the acceptable bounds (like the [-1, 1] range for cosine similarity).  Previous studies have attempted to address this conundrum by introducing a learnable softmax temperature parameter, effectively scaling the distance function's range. However, in this work, we take a different path. Inspired by the elegant principles of Riemannian geometry applied to visual tasks, we propose a novel solution: we reshape the embedding space's topology and redefine its distance function. By mapping feature representations onto the oblique manifold and employing the negative inner product as our distance measure, we tackle the equilibrium problem head-on.  Our experimental results speak volumes, demonstrating a significant leap in baseline performance\u2014an impressive 4% improvement in zero-shot image-to-text retrieval tasks\u2014achieved by altering merely two lines of code in the training process. This work not only advances the state of the art in cross-modal alignment but also highlights the transformative power of thoughtful geometric considerations in machine learning.", "introduction": "Learning visual and textual feature representations that are semantically aligned in their embedding space is an ordinary problem in the vision-language cross-modal tasks (Frome et al., 2013;Karpathy & Fei-Fei, 2015;Romera-Paredes & Torr, 2015;Wang et al., 2016;Faghri et al., 2017;Xian et al., 2016).In early works that employ feature representations from deep neural networks, e.g.Frome et al. (2013), the alignment is often achieved by a fundamental metric learning approach with the hinge rank loss.That is, the similarity between a visual feature vector u and a textual feature vector v is calculated as u T W v, where W are the learnable weight parameters.Thanks to the revolutionary advances in computational power, we can now achieve this in a more effective and practical approach termed contrastive learning, where we align quantities of positive samples and push their negative samples away simultaneously in a large mini-batch (Radford et al., 2021;Singh et al., 2022;Jia et al., 2021;Pham et al., 2021;Yuan et al., 2021).The standard choice of the distance measure between an image-text pair for the contrastive learning algorithm is the Cosine Similarity (in both uni-modal Chen et al. (2020a); Caron et al. (2020); Chen et al. (2020b) and cross-modal Radford et al. (2021); Jia et al. (2021); Singh et al. (2022) scenarios).Mathematically, the Cosine Similarity computes the negative inner product value between feature representation vectors mapped onto the unit sphere embedding space.The spherical embedding space is advantageous in aligning visual and textual feature representations in the following two aspects.First, calculat-ing the inner product consumes low computational resources during both forward and backward propagation.Second, we have a proper definition of uniformity on the sphere.The uniformity is a demanded property in optimizing the contrastive loss, by which the feature representations would preserve maximal information of the data.However, since the data for large-scale contrastive alignment are internet-collected noisy image-text pairs, we often find pairs of semantically related images and texts labeled as \"negative\" and verse visa, which we term \"semantic ambiguity\".Because of the ambiguity, it is impossible to achieve the perfect alignment and uniformity conditions of sample embeddings for the system.More specifically, during the training, the false negative samples are pushed away from each other (repulsion), while the false positive samples are pulled together (attraction).As a consequence, the system will gradually find an equilibrium when the noisy samples' gradients for attraction and repulsion are neutralized.In other words, we say the training progress is converged under the given hyper-parameters.To be more concrete, owing to the fact that the gradient is eventually back-propagated from the difference between the positive and negative distances.Given sufficient model capacity, the numerical values between the distances of positive and negative pairs of samples will be optimized to fit the noisy level of the dataset.For instance, if there is a reasonable amount of false negative samples, then the model would learn a smaller positive distance for not being punished too hard, when encountering false negative samples in another mini-batch.Furthermore, the triangular inequality (or a \"relaxed\" version) of the distance within a group of semantically similar samples will pull the positive pairs of samples away from each other (See Section 3.2).Finally, the model reaches the equilibrium of compromised positive and negative distances, which minimizes the contrastive loss regardless of the semantic ambiguity.Therefore, we say the equilibrium is essentially a property of the dataset, referring to its noise level, under a certain embedding space and its distance function.Here, the problem is that, since we implement the contrastive loss with the combination of softmax and cross-entropy, which makes the required numerical values for equilibrium exponentially larger.The numerical values of distances required by softmax at equilibrium might be out of the distance range (e.g.[-1, 1] for the cosine similarity).Therefore, in the practice of former studies (Wu et al., 2018;Radford et al., 2021), researchers have to scale the range with a learnable softmax temperature parameter.Although the learnable temperature somehow fixes the range problem, it still has two drawbacks that need to be addressed.Firstly, the learnable temperature delays the learning progress.Empirically, we observe that the model trends to acquire a proper scaling for the distance range earlier than achieving a good alignment.Secondly, a large temperature is numerically unstable for the back-propagation, especially for the low-bit precision computation.In this work, we alternatively design the topology for embedding vectors and its endowed distance function.Motivated by the utilization of Riemannian geometry for visual tasks, we propose a relatively simple solution to address the aforementioned out-of-range equilibrium problem.Our contributions can be summarized as follows:1. We reveal that the learnable softmax temperature is essentially a scaling factor for the distance range, which indicates the noise level of the dataset in the contrastive visual-textual alignment.We also observe that the model learns a suitable temperature before representations, which degrades the performance.2. We tackle the out-of-range equilibrium problem resulting from the softmax cross-entropy loss, by designing the topology of embedding space.That is, we employ the oblique manifold endowed with the negative inner product as distance functions.3. We demonstrate that the proposed approach can be non-painfully implemented by changing only two lines of the training code, whilst improving the baseline performance in the zeroshot image-text retrieval tasks.In the larger scale experiment, we have learned a ViT-B/16 model that outperforms the officially released ViT-L/14 model."}
{"paper_id": 388, "abstract": "In this exploration, we delve into the intricate challenge of gauging a model's performance on out-of-distribution (OOD) datasets, all without the crutch of labels. Our guiding principle is simple yet profound: a model that excels should not only deliver predictions with unwavering confidence but also exhibit a rich tapestry of dispersity. While contemporary approaches have honed in on the aspect of prediction confidence, we unveil a fresh perspective\u2014dispersity emerges as a crucial signal in this complex interplay.  Confidence serves as a beacon of certainty for individual predictions, illuminating the model's assurance in its choices. In contrast, dispersity paints a broader picture, revealing how predictions are scattered across the vast landscape of categories. To forge a more precise estimation, we propose a novel framework that harmoniously integrates these two dimensions, leveraging the nuclear norm of the prediction matrix as our guiding metric.  Through rigorous experimentation, we validate the potency of the nuclear norm across a spectrum of models\u2014ranging from Vision Transformers (ViT) to ConvNeXt\u2014and diverse datasets, including ImageNet and CUB-200. We also tackle various forms of distribution shifts, such as style shifts and reproduction shifts. Our findings demonstrate that the nuclear norm outstrips existing methodologies, offering a more accurate and resilient predictor of OOD accuracy.  In conclusion, we also reflect on the limitations inherent to the nuclear norm and chart potential avenues for future inquiry, inviting fellow researchers to join us in this quest for deeper understanding in the realm of model evaluation.", "introduction": "Model evaluation is critical in both machine learning research and practice.The standard evaluation protocol is to evaluate a model on a held-out test set that is 1) fully labeled and 2) drawn from the same distribution as the training set.However, this way of evaluation is often infeasible for realworld deployment, where the test environments undergo distribution shifts and ground truths are not provided.In presence of a distribution shift, in-distribution accuracy may only be a weak predictor of model performance (Deng & Zheng, 2021;Garg et al., 2022).Moreover, annotating data itself is a laborious task, let alone it is impractical to label every new test distribution.Hence, a way to predict a classifier accuracy using unlabelled test data only has recently received much attention (Chuang et al., 2020;Deng & Zheng, 2021;Guillory et al., 2021;Garg et al., 2022).In the task of accuracy estimation, existing methods typically derive model-based distribution statistics of test sets (Deng & Zheng, 2021;Guillory et al., 2021;Deng et al., 2021;Garg et al., 2022;Baek et al., 2022).Recent works develop methods based on prediction matrix on unlabeled data (Guillory et al., 2021;Garg et al., 2022).They focus on the overall confidence of the prediction matrix.Confidence refers to whether the model gives a confident prediction on an individual test data.It can be measured by entropy or maximum softmax probability.Guillory et al. (2021) show that the average of maximum softmax scores on a test set is useful for accuracy estimation.Garg et al. (2022) predict accuracy as the fraction of test data with maximum softmax scores above a threshold.In this work, we newly consider another property of prediction matrix: dispersity.It measures how spread out the predictions are across classes.When testing a source-trained classifier on a target (out-of-distribution) dataset, target features may exhibit degenerate structures due to the distribution shift, where many target features are distributed in a few clusters .As a result, their corresponding class predictions would also be degenerate rather than diverse: the classifier predicts test features into specific classes and few into others.There are existing works that encourages the cluster sizes in the target data to be balanced (Shi & Sha, 2012;Liang et al., 2020;Yang et al., 2021;Tang et al., 2020), thereby increasing the prediction dispersity.In contrast, this work does not aim to improve cluster structures and instead studies the prediction dispersity to predict model performance on various test sets without ground truths.To illustrate that dispersity is useful for accuracy estimation, we report our empirical observation in Fig. 1.We compute the predicted dispersity score by measuring whether the frequency of predicted class is uniform.Specifically, we use entropy to quantify the frequency distribution, with higher scores indicating that the overall predictions are well-balanced.We show that the dispersity score exhibits a very strong correlation (Spearman's rank correlation \u03c1 > 0.950) with classifier performance when testing on various test sets.This implies that when the classifier does not generalize well on the test set, it tends to give degenerate predictions (i.e., low prediction dispersity), where the test samples are mainly assigned to some specific classes.Based on the above observation, we propose to use nuclear norm, known to be effective in measuring both prediction dispersity and confidence (Cui et al., 2020;2021), towards accurate estimation.Other measurements can also be used, such as mutual information maximizing (Bridle et al., 1991;Krause et al., 2010;Shi & Sha, 2012).Across various model architectures on a range of datasets, we show that the nuclear norm is more effective than state-of-the-art methods (e.g., ATC (Garg et al., 2022) and DoC (Guillory et al., 2021)) in predicting OOD performance.Using uncontrollable and severe synthetic corruptions, we show that nuclear norm is again superior.Finally, we demonstrate that the nuclear norm still makes reasonably accurate estimations for test sets with moderate imbalances of classes.We additionally discuss potential solutions under strong label shifts."}
{"paper_id": 389, "abstract": "In the ever-evolving realm of artificial intelligence, large language models (LLMs) stand as titans of high-level analogical reasoning. They weave patterns from the vast tapestry of their training data, displaying remarkable prowess in zero-shot evaluations and few-shot in-context learning. Yet, as recent explorations reveal, even the mightiest of these models stumble when faced with the intricate dance of reasoning across multiple objects or facts, or when tasked with the sequential logic of deduction.   In response to these challenges, we unveil a groundbreaking two-stage probabilistic inference paradigm known as ThinkSum. This innovative approach elegantly navigates the complexities of reasoning over sets of objects or facts. The first stage, aptly named Think, engages in a swift retrieval of associations, querying the LLM in parallel across a spectrum of phrases drawn from the prompt or an auxiliary model call. The second stage, Sum, delves into the depths of 'slow' probabilistic inference, aggregating the results of these queries to forge a final prediction.  Our experiments showcase the formidable advantages of ThinkSum, particularly within the demanding landscape of the BIG-bench suite of evaluation tasks. Here, we achieve notable enhancements over the current state of the art, utilizing GPT-family models to conquer ten challenging tasks\u2014often with smaller model variants that defy expectations.   Moreover, we position ThinkSum against other strategies for prompting LLMs, including various iterations of chain-of-thought prompting. We contend that the probabilistic inference of ThinkSum, conducted outside the direct calls to the LLM, renders it less vulnerable to the whims of prompt design. This not only results in predictions that are more interpretable but also allows for a seamless integration with latent variable models, unlocking the potential to extract structured knowledge from the depths of these powerful language constructs.", "introduction": "Large language models (LLMs) (Brown et al., 2020;Rae et al., 2021;Chowdhery et al., 2022) can recall a broad range of basic facts, recognize and mimic various forms in language, and efficiently extrapolate analogies in structure and meaning.These abilities allow LLMs to excel in zero-shot and few-shot tasks that are formulated as generation or selection of a likely completion of a prompt.This formulation requires LLMs to perform associative fast thinking, in which each token of text in the sequence making up the answer is generated or scored in one pass through the model and, other than that, no intermediate information is created or retained.Fast thinking is made possible by the compression in the LLM weights of information that is repeated in a variety of ways in large training datasets.However, it is increasingly evident that when reasoning, or slow thinking, is required, failure modes of LLMs are revealed.In our usage, reasoning is sequential manipulation of concepts that can be expressed in language.Tasks that require iterative retrieval of rarely stated knowledge, uncertainties over multiple objects or facts, or multiple steps of deduction are difficult even for the most advanced LLMs.In a recently designed suite of evaluations, BIG-bench (Srivastava et al., 2022), some of the tasks where the gap between machine and human performance is large involve inference sequences with nested counterfactuals (LOGICAL DEDUCTION), concepts introduced though definitions (CONCEPTUAL COMBINATIONS), etc. (see Fig. A.1).These are tasks where a human solver's intuitive feeling of '(in)coherence' is not sufficient to produce the right answer: the solution is obtained by a sequence of thoughts that can be explained in words and may even require writing down intermediate results if working memory is insufficient.We show on several examples in BIG-bench that such problems can be addressed by a twocomponent mechanism, which we name THINKSUM:\u2022 THINK (fast thinking / association / knowledge retrieval step): creating an association of spans of text with sets of strings.This process may involve generation from a language model, as is the case in Fig. 1, where the novel word 'binne' is associated with the set of strings {'cat', 'mink', . . .}A binne is any furry four-legged creature, and a bam is a simple dwelling.A binne bam is a place for people (55%) animals (44%) birds (0.87%) researchers (0.022%)A binne is any furry four-legged creature, and a bam is a simple dwelling.Examples of binnes: cat, mink, ferret, guinea pig, rabbit.Examples of bams: hut, cabin, cottage, shelter, shack.A binne bam is a place for people (51%) animals (48%) birds (0.76%) researchers (0.011%)A binne is any furry four-legged creature, and a bam is a simple dwelling.binne = {cat, mink, ferret, guinea pig, rabbit} bam = {hut, cabin, cottage, shelter, shack} A binne bam is a place for animals (65%) people (34%) birds (1.5%) researchers (0.056%)\u2309 \u230b THINK (auxiliary LM calls to define sets)A cat cottage is a place for A rabbit cabin is a place for A mink shelter is a place forSUM (aggregate LM likelihoods) Figure 1: An example adapted from the CONCEPTUAL COMBINATIONS (INVENTED WORDS) task, in which models must select the most likely completion of a phrase that includes nonce words whose definitions are given.Direct prompting evaluates completion likelihoods normalized over the four answer choices ('people', 'animals', 'birds', 'researchers').Chain-of-thought-like or auxiliary knowledge approaches would query a LLM or knowledge base for additional context.Our THINKSUM approach to this task queries a LLM (GPT-2 XL) to produce sets of examples defining the nonce words, then marginalizes over substitutions of these examples into the target phrase.by prompting GPT-3 with the definition and asking for examples.However, it may also consist of scoring alone, in order to form a matrix of probabilities over which probabilistic inference is performed.\u2022 SUM (slow thinking / SUMmarization / reasoning step): probabilistic inference that aggregates generated strings or probabilities to produce the final answer.The summarization typically involves, and often entirely consists of, summing of probabilities of strings (computed in the THINK step), as in Fig. 1, where the final word is assumed to be sampled from a mixture of possible substitutions of 'binne' and 'bam' words into the input.THINKSUM is named by analogy with other algorithms with two basic operations that 'expand' and 'aggregate', like MapReduce in distributed computing and sum-product in graphical models.We discuss different ways to THINK and to SUM in section \u00a72, but we start with one example, illustrated in Fig. 1, motivated by the CONCEPTUAL COMBINATIONS (INVENTED WORDS) task in BIG-bench (Srivastava et al., 2022).In this task, the LLM is provided with two invented words and their definitions in the input.The LLM is then asked to infer the most plausible sentence that uses a combination of the invented words.As the words are invented, they are not common or consistently used in the training set, and the LLM needs to understand and combine the definitions of the invented words to reason about the meaning of the combination.The LLM is queried to produce example instances of the invented words with the help of the definitions.These example instances can be substituted into the query in place of the invented words.In this way, by mapping individual spans of the text of interest to sets we arrive at a mixture model (in this example, a mixture with 25 components, for 5 possible replacements of each of the words), which can be used in the same manner the original LLM is used, either to score text or to generate it token by token.In this case, when we score all candidate completions using this mixture model and normalize over the four choices, the correct answer -that 'binne bams' are for animals and not people -becomes most likely.An important difference between THINKSUM and existing chain-of-thought-like prompt engineering methods (Wei et al., 2022;Kojima et al., 2022), is that the reasoning step is not reduced to a generation problem for the LLM, but is performed as a probabilistic inference external to the LLM.This reduces its vulnerability to features of the prompt, such as accidental distraction of the LLM by spurious patterns.Instead, we engineer the slow thinking process to make parallel calls to the LLM to query for intermediate information, then possibly perform programmatic recombination of strings (THINK).The final reasoning step -in which likelihoods obtained from the LLM for the recombinations derived from earlier steps of the reasoning process are combined to make the final prediction -is left to classical probabilistic reasoning (SUM).In a sense, SUM replaces the self-attention mechanism over linear text, which is used as the sole 'reasoning' mechanism in chain-of-thought-like approaches that expect the intermediate 'thoughts' to take the form of generated tokens intervening between the input and output.Fig. 1 shows the potential brittleness of such 'reasoning', especially in smaller models, which have stronger recency bias (Malkin et al., 2022): if we simply list generated examples as additional context in the prompt, the recency bias causes the LLM to still give higher probability to 'people' than to 'animals', simply because 'bam' (simple dwelling) examples are given after the 'binne' examples.Imposing an alternative reasoning system over an associative \"knee-jerk reaction\" system has an analogy with models of human cognitive processes (Tversky & Kahneman, 1974;Kahneman, 2011) that separate System 1 (fast thinking) and System 2 (slow thinking).System 2 acts as a 'controller' that can prime System 1 to appropriately bias its fast thinking, and, in the context of reasoning with deep learning models, has been interpreted as operating with sparse concepts that can be described in language (Bengio, 2017;Goyal & Bengio, 2020).Through repeated usage, the slow-thinking functions of System 2 can become efficiently compressed into System 1 intuitions, in the same manner that iterative 'reasoning' functions of which smaller LLMs are not capable become zeroshot generation capacities for large LLMs.As is the case with humans, there is always the next frontier of problems where a trained model with remarkable 'intuition' needs to be slowed down.The main claim of this paper is that more is possible with LLMs of existing scale when they are used in concert with a wise System 2 controller that allows for probabilistic inference."}
{"paper_id": 390, "abstract": "In the realm of Deep Neural Networks (DNNs), their true potential lies not merely in their architecture, but in the rich tapestry of training data that fuels them\u2014data that must be abundant, high in quality, and diverse. Yet, the quest for such data often proves to be a formidable challenge, fraught with the burdens of time and expense. This dilemma has cast a long shadow over the deployment of DNNs in real-world applications.   To illuminate a path forward, we embark on an innovative venture into the realm of dataset expansion\u2014a task dedicated to the automatic generation of new labeled samples, thereby breathing life into the confines of small datasets. Enter our Guided Imagination Framework (GIF), a novel approach that harnesses the power of cutting-edge generative models, such as DALL-E2, to \u201cimagine\u201d and conjure new, informative data from existing seed samples.  At the heart of GIF lies a sophisticated process of imagination, wherein we optimize the latent features of our seed data within a semantically rich space. These optimized features serve as the catalyst for our generative models, which then produce photo-realistic images imbued with fresh content. To ensure that our imaginative creations are not just random musings, but rather valuable assets for model training, we leverage the zero-shot recognition capabilities of CLIP. We introduce three guiding principles to foster the generation of informative samples: prediction consistency, entropy maximization, and diversity promotion.  With these guiding tenets steering our creative endeavor, GIF proves itself a formidable ally across various domains, achieving an impressive average accuracy gain of 29.9% across six natural image datasets and 10.4% across three medical image datasets. We are excited to share our source code with the community, inviting others to join us on this journey of discovery and innovation.", "introduction": "Having a sufficient amount of training data is crucial for unleashing the power of deep neural networks (DNNs) (Deng et al., 2009;Qi & Luo, 2020).However, in many fields, collecting large-scale datasets is expensive and time-consuming (Qi & Luo, 2020;Zhang et al., 2020), resulting in limited dataset sizes which make it difficult to fully utilize DNNs.To address this data limitation issue and reduce the cost of manual data collection/annotation, we explore dataset expansion in this work, which seeks to build an automatic data generation pipeline for expanding a small dataset into a larger and more informative one, as illustrated in Figure 1There are some research attempts that could be applied to dataset expansion.Among them, data augmentation (DeVries & Taylor, 2017;Cubuk et al., 2020;Zhong et al., 2020) applies pre-defined transformations to each image for enriching datasets.However, these transformations mostly affect the surface visual characteristics of an image, but have a minimal effect on the actual image content.Therefore, the brought new information is limited, and cannot sufficiently address the limited-data issue in small datasets.Besides, some recent studies (Zhang et al., 2021c;Li et al., 2022) utilize generative adversarial networks (GANs) (Goodfellow et al., 2014;Brock et al., 2018) to synthesize images for model training.They, however, require a sufficiently large dataset for in-domain GAN training, which is not feasible in the small-data scenario.Moreover, the generated images are often not well-annotated, limiting their utility for DNN training.Therefore, both of them are unable to effectively resolve the dataset expansion problem.For an observed object, humans can easily imagine its different variants in various shapes, colors or contexts, relying on their accumulated prior understanding of the world (Warnock & Sartre, 2013;Vyshedskiy, 2019).Such an imagination process is highly useful for dataset expansion, since it does not simply perturb the object's appearance but applies rich prior knowledge to create object variants with new information.Meanwhile, recent breakthroughs in large-scale generative models (e.g., DALL-E2 (Ramesh et al., 2022)) have demonstrated that generative models can effectively capture the sample distribution of extremely large datasets (Schuhmann et al., 2021;Byeon et al., 2022) and show encouraging abilities in generating photo-realistic images with a rich variety of contents.This motivates us to explore their capabilities as prior models to develop a computational data imagination pipeline for dataset expansion, by imagining different sample variants from seed data.However, deploying big generative models for dataset expansion is highly non-trivial, complicated by several key challenges, including how to generate samples with correct labels, and how to make sure the created samples are useful for model training.To handle these challenges, we conduct a series of studies (cf.Section 3), from which we make two important findings.First, the CLIP model (Radford et al., 2021), which offers excellent zero-shot classification abilities, can map latent features of category-agnostic generative models to the specific label space of the target small dataset.This is helpful for generating samples with correct labels.Second, we empirically find three informativeness criteria crucial for generating effective training data: 1) zero-shot prediction consistency to ensure that the imagined image is class-consistent with the seed image; 2) entropy maximization to encourage the imagined images to bring more information; 3) diversity promotion to encourage the imagined images to have diversified contents.In light of the above findings, we propose the Guided Imagination Framework (GIF) for dataset expansion.Specifically, given a seed image, GIF first extracts its latent feature with the prior generative model.Different from data augmentation that imposes variation over the raw image, GIF optimizes the variation over the latent feature.Thanks to the guidance carefully designed by our discovered criteria, the latent feature is optimized to provide more information while maintaining its class semantics.This enables GIF to create informative new samples, with class-consistent semantics yet higher content diversity, to expand small datasets for model training.Considering that DALL-E2 have been shown to be powerful in generating images and MAE (He et al., 2022) is excellent at reconstructing images, we explore their use as prior models for imagination in this work.We evaluate the proposed method on both small-scale natural and medical image datasets.As shown in Figure 1 (right), compared to the ResNet50 model trained on the original dataset, our method improves the model performance by a large margin across a variety of visual tasks, including finegrained object classification, texture classification, cancer pathology detection, and ultrasound image classification.More specifically, GIF obtains 29.9% accuracy gain on average over six natural image datasets, and 10.4% accuracy gain on average over three medical image datasets.Moreover, we show that the expansion efficiency of our method is much higher than expansion with existing augmentation methods.For example, 5\u00d7 expansion by our GIF-DALLE method already outperforms 20\u00d7 expansion by Cutout, GridMask and RandAugment on the Cars and DTD datasets.In addition, the expanded datasets can be directly used to train different model architectures (e.g., ResNeXt, WideResNet and MobileNet), leading to consistent performance improvement."}
{"paper_id": 391, "abstract": "In this paper, we unveil a groundbreaking technique known as \"Succinct Compression,\" designed to achieve lossless compression of Deep Neural Network (DNN) models, facilitating rapid and memory-efficient inference. At the heart of our approach lies the innovative concept of *Succinct Data Structures*, which empowers swift queries without the need to decompress the compressed representations. Our method is built upon three pivotal insights. First, we introduce two foundational building blocks for constructing DNN models, illustrating how they can be harmoniously integrated with compressed architectures, including pruned or quantized models. Next, we propose a mixed-formulation inference scheme that allows different layers to leverage their unique strengths, maximizing the overall efficiency. Finally, we detail a specialized execution pipeline engineered to seamlessly incorporate various model formulations, ensuring rapid inference. Our extensive quantitative analysis reveals that our method can: (1) accelerate inference and enhance memory efficiency on uncompressed models; (2) synergize with a diverse array of compression techniques\u2014both structure-altered and unaltered\u2014yielding superior speedups and compression ratios while maintaining accuracy; and (3) surpass all existing state-of-the-art Model Coding approaches.", "introduction": "Recent efforts on Pareto improvements of compressed Deep Neural Network (DNN) models, on inference time, space consumption and the accuracy, have recently bloomed due to the great success of DNNs in practice.Prior works either aggressively simplify/optimize the structure of DNN models (e.g.Pruning and Neural Architecture Search) or retrench the representation of model parameters (e.g.Quantization and Model Coding), with a major focus on the compression ratio and the accuracy.Given a variety of methodologies for efficient compression, there still lacks a general method to further optimize the inference performance and compression ratio, without affecting the accuracy of both uncompressed and compressed models.This paper introduces \"Succinct Compression\", a method to provide lossless compression of Deep Neural Network (DNN) models for fast and memory-efficient inference.The emphasis of our method is to enhance the inference performance and compression ratio without affecting the accuracy at the same time, for a variety classes of uncompressed and compressed models.The unique characteristic of our method is to exploit Succinct Data Structures, which enables fast queries without decompressing the compressed representations.We consolidate three new insights to better incorporate Succinct Data Structures.\u278a we propose two semi-structured formulations to represent DNN models in element-wise or block-wise manners, and provide simple extensions to allow them for the combinations of other compression techniques.\u278b we enable mixed formulations of different layers in the model, to better extract the potential of Succinct Data Structures.\u278c we design a specialized execution pipeline to perform the inference on different formulations, by carefully engineering the inner operators of Succinct Data Structures.Our evaluation shows that our method can be very effective for the inference efficiency, and generally applicable for uncompressed and compressed models (including for .For uncompressed models, our method can achieves most 1.07\u00d7 speedup and 1.17\u00d7 compression ratio at the same time, without affecting the accuracy.We then show that our method can bring significantly more benefits by combining other compression schemes, where all models are pre-processed via other compression methods.For instance, by combining structure-altered compression (such as pruning), our method enables the at most 8.8\u00d7 acceleration of inference on ResNet-101, with 39.90\u00d7 compression ratio meanwhile.Similarly, the speedup can be further enhanced to reach 9.3\u00d7 by incorporating structure-unaltered method (such as quantization).We also compare our method with a variety of the state-of-the-art Model Coding schemes, and show that our method outperforms all of them."}
{"paper_id": 392, "abstract": "In this paper, we unveil a groundbreaking exploration into the realm of Multi-Player Games through the lens of \\feint, marking the first formalization, implementation, and quantitative evaluation of its principles. Our journey begins with a rigorous formalization of \\feint, dissecting its nuances within the context of Multi-Player Games, focusing on the intricate interplay of temporal and spatial dimensions, as well as their combined effects. This formal framework is constructed upon the robust foundation of the Non-transitive Active Markov Game Model, revealing the significant influence \\feint can wield within these dynamic environments.  Next, we delve into the practicalities of bringing \\feint to life in the vibrant world of Multi-Player Games, leveraging the cutting-edge advancements in multi-agent modeling, particularly through the lens of Multi-Agent Reinforcement Learning. Our exploration does not stop at implementation; we rigorously evaluate the effectiveness of our design through quantitative measures. The results are compelling: our version of \\feint not only enhances reward gains significantly but also enriches the diversity of gameplay experiences, all while incurring minimal overhead in terms of time.  In conclusion, we assert that our design of \\feint is not merely theoretical but a practical tool that breathes new life into Multi-Player Games, making them more engaging and multifaceted for players.", "introduction": "Game simulations, which only use Markov Game Model (Filar (1976)) or its variants (Wampler et al. (2010); Kim et al. (2022)), breed the needs for the diversity and the randomness to improve the game experiences.The trends of evolving more details into simulated games demand: \u278a the need for non-transitivity (i.e.there are no dominant gaming strategies), which allow players to dynamically change game strategies.In this way, the newly-incorporated strategies can maintain a high level of the diversity, which guarantee a high extent of unexploitability (Liu et al. (2021)); and \u278b the strict requirements on temporal impacts (and its implications on spatial and collective impacts), since modern game simulations are highly time-sensitive ( Nota & Thomas (2020)).Therefore, new optimizations on these game models are expected to be elegant and easy-to-implement, to preserve the original spirits of these games.Our work first builds upon representative examples from the above two trends, by unifying two stateof-the-art progress of Multi-Player Games: \u278a we use Unified Behavioral and Response Diversity (described in Liu et al. (2021)), which exploits non-transitivity (i.e.no single dominant strategy in many complex games), to highlight the importance of the diversity in game policies.Moreover, we address the issue from their work, which fails to consider the intensity and future impacts from complex interactions among agents; and \u278b we incorporate Long-Term Behavior Learning (described in Kim et al. (2022)), which proposes Active Markov Game Model to emphasize the convoluted future impacts from complex interactions among agents.Based on the above two results, we unify them as a new model called Non-transitive Active Markov Game Model (NTAMGM), and use it throughout this work.This unification satisfies the need for a game model where (A) agents have intense and time-critical interactions; and (B) the design space of game policies is highly diverse.The definition of NTAMGM is described below.\u2022 Non-transitive Active Markov Game Model: We define a K-agent Non-transitive Active Markov Game Model as a tuple \u27e8K, S, A, P, R, \u0398, U \u27e9: K = {1, ..., k} is the set of k agents; S is the state space; A = {A i } K i=1 is the set of action space for each agent, where there are no dominant actions; P performs state transitions of current state by agents' actions: P : S \u00d7A 1 \u00d7A 2 \u00d7...\u00d7A K \u2192 P (S), where P (S) denotes the set of probability distribution over state space S; R = {R i } K i=1 is the set of reward functions for each agent; \u0398 = {\u0398 i } K i=1 is the set of policy parameters for each agent; and U = {U i } K i=1 is the set of policy update functions for each agent.Based on the above assumption of Multi-Player Games, our goal is to incorporate Feint, a set of actions to mislead opponents, for strategic advantages in Multi-Player Games.Prior works simply incorporate Feint in the context of Two-Player Games (e.g.Wampler et al. (2010); Won et al. (2021a)), and our works begins by addressing the limitations of the derived version (denoted as the basic formalization of Feint) from these works.We find that: the basic formalization of Feint overlooks the complexity of potential impacts in Multi-Player Games, and therefore can not be generalized for Multi-Player Games.To this end, we deliver the first comprehensive formalization of Feint, by separating the complex impacts into \u278a the temporal dimension; \u278b the spatial dimension; and \u278c the collective impacts from these two dimensions.We also show that how the above components of our formalization can be synergistically put together.Based on the proposed formalization, we clear the implementation roadmap, under both Inference Learning and Reinforcement Learning models, to justify the applicability of our proposed formalization.To properly examine the benefits of our method, we first extensively build two complex scenarios, using Multi-Agent Deep Deterministic Policy Gradient (MADDPG Lowe et al. ( 2017)) and Multi-Agent Actor-attention Critic (MAAC Iqbal & Sha ( 2019)), with six agents in total.Then, we implement our formalization upon these two extensively-engineered scenarios.Our quantitative evaluations show that our formalization and implementations have great potential in practice.We first show that our work can make the game more interesting, via the following two metrics: for the Diversity Gain, our method can increase the exploitation of the search space by 1.98X, measured by the Exploitability metric; and for Gaming Reward Gain, our method can achieve 1.90X and 2.86X gains, when using MADDPG and MAAC respectively.We then show that our method only incur negligible overheads, by using per-episode execution time as the metric: our method only introduces less than 5% more for the time consumption.We conclude that our design of Feint is effective and practical, to make Multi-Player Games more interesting."}
{"paper_id": 393, "abstract": "In the ever-evolving landscape of artificial intelligence, one of the most daunting quests is the creation of prediction models that can adeptly traverse the boundaries of related domains. A burgeoning body of literature champions the idea of harnessing invariant associations gleaned from a multitude of source domains. Yet, the pivotal question remains: do these invariant predictors hold their ground when faced with a specific target domain? The answer hinges on the structural shifts that exist between these domains.   Through the lens of transportability theory, we unveil that the pursuit of invariance learning\u2014and the contexts in which these invariant predictors shine in the face of worst-case losses\u2014represents a unique subset of a broader challenge known as partial transportability. This intricate task seeks to pinpoint or constrain a conditional expectation, denoted as \\(\\mathbb{E}_{P_{\\pi^*}}[y \\mid \\mathbf{x}]\\), within an uncharted domain \\(\\pi^*\\). It does so by leveraging insights into qualitative changes across domains, encapsulated in causal graphs, alongside data harvested from source domains \\(\\pi^1, \\dots, \\pi^k\\).   Our findings reveal that the solutions to this intricate problem offer a much more expansive guarantee of generalization, encompassing the principles of invariance learning and other robust optimization strategies inspired by the tenets of causality. To bring our theoretical insights into the realm of practicality, we present an algorithm designed to yield tight bounds asymptotically as the number of samples from the source domains increases. This algorithm is tailored for any partial transportability challenge involving discrete observables, and we showcase its efficacy through illustrative examples drawn from synthetic datasets.", "introduction": "Generalization guarantees are central to the design of reliable machine learning models as the predictions and conclusions obtained in one or several source domains \u03c0 1 , . . ., \u03c0 k (e.g. in controlled laboratory circumstances, from a specific study or population, etc.) are transported and applied elsewhere, in a domain \u03c0 \u02dathat may differ in several aspects from that of source domains.It is apparent that what structure and what assumptions are imposed on the relationship between domains determines whether a model will generalize as intended.For example, if the target environment is arbitrary, or substantially different from the study environment, transporting predictions is difficult or even impossible.A structural account of causation provides suitable semantics for reasoning about the structural invariances across different domains, and has been studied under the umbrella of transportability theory (Pearl & Bareinboim, 2011;Bareinboim et al., 2013;Bareinboim & Pearl, 2016).Each domain \u03c0 i is associated with a different structural causal model (SCM) M i that differs in one or more of its component parts with respect to other domains and defines different distributions over the observed variables.In practice, the SCMs are usually not fully observable, which leads to the transportability challenge of using data from one (or more) SCMs to make inference about distributions from another SCM.A query, e.g.E P \u02darY | xs, is said to be point identified if it can be uniquely computed given available data (from one or more domains) and qualitative knowledge about the causal changes between domains in the form of selection diagrams.However, in problems of transportability, especially when no data in the target domain can be collected, the combination of qualitative assumptions and data often does not permit one to uniquely determine a given query, which is said to be non-identifiable.In such cases, partial identification methods deal with bounding a given query e.g.l \u0103 E P \u02darY | xs \u0103 u in non-identifiable problems and may still serve an informative purpose for decision-making if 0 \u0103 l \u0103 u \u0103 1.Both settings have been studied in the literature.In particular, there exists an extensive set of graphical conditions and algorithms for the identifiability of observational, interventional, and counterfactuals distributions across domains from a combination of datasets in various settings (Pearl & Bareinboim, 2011;Bareinboim et al., 2013;Bareinboim & Pearl, 2014;2016;Lee et al., 2020;Correa & Bareinboim, 2019).For example, Lee et al. (2020) investigate the transportability of conditional causal effects, while Correa & Bareinboim (2020) investigate the transportability of soft interventions or policies, from an arbitrary combination of datasets collected under different conditions.Several methods exist also for partial identification of causal effects and counterfactuals (Balke & Pearl, 1997;Chickering & Pearl, 1996;Zhang et al., 2021) that aim at bounding insead of point-identifying a particular causal effect.Despite the generality of these results, there is still no treatment or algorithms for the partial identification of transportability queries.In the machine learning literature, notably, a version of the transportability task is also widely studied as the problem of domain generalization (Wang et al., 2022).The objective is to learn a prediction function with a minimum performance guarantee on any distribution in some uncertainty set that includes potential test / target distributions (Ben-Tal et al., 2009;Gulrajani & Lopez-Paz, 2020).This problem has implicit connections to causality and SCMs if uncertainty sets of distributions are defined on the basis of \"invariant correlations\", such as stable conditional expectations E P 1 rY | xs \" \u00a8\u00a8\u00a8\" E P k rY | xs across training domains \u03c0 1 , . . ., \u03c0 k , to be used for prediction in a target domain \u03c0 \u02daand that may be learned from data sampled across sufficiently many different domains with statistical tests (Peters et al., 2016;Subbaswamy et al., 2019;Subbaswamy & Saria, 2020) or custom loss functions (Magliacane et al., 2018;Arjovsky et al., 2019;Rojas-Carulla et al., 2018;Bellot & van der Schaar, 2020).For instance, Arjovsky et al. (2019) argue for learning representations that define an invariant optimal classifier across several training datasets.Subbaswamy et al. (2019); Subbaswamy & Saria (2020) use causal graphs and identifiable interventional distributions to define invariant prediction rules across domains.Notwithstanding their wide applicability, there is little theoretical understanding of the extrapolation guarantees that can be expected from invariant prediction rules given a finite set of domains.Correlations invariant across source domains need not be invariant in a target domain; and performance guarantees, in general, depend on the structural invariances assumed for their respective SCMs.In this paper, we start by describing the conditions under which invariant prediction rules can be expected to perform well in an arbitrary target domain from first principles using the semantics of structural causal models (Pearl, 2009;Pearl & Bareinboim, 2011).We then introduce a broader optimization problem -the task of partial transportability -whose objective is to bound, instead of point estimate, a query in an arbitrary target domain of interest, such as E P \u02darY | xs, given data from one or more source domains and qualitative knowledge about the causal changes between domains in the form of selection diagrams.We demonstrate that solutions to this problem subsume various instantiations of invariant predictors (in the conditions where these are adequate) and have a wider distributional robustness guarantee to any distribution in the target domain that is compatible with the assumed selection diagrams.For computations in practice, we show that the partial transportability task can be solved approximately for systems of variables with finite domains with a Markov Chain Monte Carlo sampling approach.The resulting bounds are sound and tight, and provide the most informative inference on a target query given the available information."}
{"paper_id": 394, "abstract": "In the ever-evolving realm of deep learning, the quest for more compact and efficient representations of neural fields has become a vibrant frontier. In this paper, we unveil an innovative low-rank representation known as Tensor Train Neural Fields (TT-NF). This approach is designed to masterfully learn neural fields on dense, regular grids while also offering streamlined methods for sampling from them. At its core, our representation employs a Tensor Train parameterization of the neural field, which we diligently train using backpropagation to tackle a non-convex objective.  We delve into the ramifications of low-rank compression, scrutinizing its impact on the quality metrics of downstream tasks across two distinct scenarios. First, we showcase the prowess of our method within a controlled environment focused on tensor denoising, where it stands shoulder to shoulder with SVD-based techniques aimed at minimizing reconstruction errors. Furthermore, we extend our exploration to the realm of Neural Radiance Fields, revealing how the low-rank structure that yields optimal quality can only be uncovered through the learning process itself. Join us as we navigate this intricate tapestry of neural field representation, pushing the boundaries of what is possible in the world of deep learning.", "introduction": "Following the growing interest in deep neural networks, learning neural fields has become a promising research direction in areas concerned with structured representations.However, precision is usually at odds with the computational complexity of these representations, which makes training them and sampling from them a challenge.In this paper, we investigate interpretable low-rank neural fields defined on dense regular grids and efficient methods for learning them.Since, in extreme cases, the dimensionality of such fields can exceed the memory size of a typical computer by several orders of magnitude, we look at the problem of learning such fields from the angle of stochastic methods.Tensor decompositions have become a ubiquitous tool for dealing with structured sparsity of intractable volumes of data.Within the large family of tensor decompositions, we focus on the Tensor Train (TT) (Oseledets, 2011), also known as the Matrix Product State in physics.TT is notable for its high-capacity representation, efficient algebraic operations in the low-rank space, and support of SVD-based algorithms for data approximation.As such, we consider TT-SVD (Oseledets, 2011) and TT-cross (Oseledets & Tyrtyshnikov, 2010) methods for obtaining a low-rank representation of the full tensor.While TT-SVD requires access to the full tensor at once (which might already be problematic in specific scenarios), TT-cross requires access to data through a black-box function, computing (or looking up) elements by their coordinates on demand.Both methods operate under the assumption of noise-free data and are not guaranteed to output sufficiently good approximations in the presence of noise.While noise in observations is challenging for SVD-based schemes and requires devising tailored approaches to different noise types and magnitude (Zhou et al., 2022), exploiting the low-rank structure of the field driven by data is even more challenging (Novikov et al., 2014;Boyko et al., 2020) and typically resorts to the paradigm of data updates through algebraic operations on TT.In this work, we take a step back and leverage the modern deep learning paradigm to parameterize neural fields as TT, coined TT-NF.Through deep learning tooling with support for automatic differentiation and our novel sampling methods, we obtain mini-batches of samples from the parameterized neural field and perform optimization of a non-convex objective defined by a downstream task.The optimization comprises the computation of parameter gradients with backpropagation and parameter updates with any suitable technique, such as SGD.We analyze TT-NF and several sampling techniques on a range of problem sizes and provide reference charts for choosing a sampling method based on memory and computational constraints.Next, we define a synthetic task of low-rank tensor denoising and demonstrate the superiority of the proposed optimization scheme over several SVD-based schemes.Finally, we consider the formulation of Neural Radiance Fields (NeRF) introduced in Mildenhall et al. (2020), and propose a simple modification to TT-NF, termed QTT-NF, for dealing with hierarchical spaces.Our contributions in this paper:1. TT-NF -compressed low-rank neural field representation defined on a dense grid; 2. QTT-NF -a modification of TT-NF for learning neural fields defined on hierarchical spaces, such as 3D voxel grids seen in neural rendering; 3. Efficient algorithms for sampling from (Q)TT-NF and learning it from samples, designed for deep learning tooling.The rest of the paper is organized as follows: Sec. 2 discusses the related work; Sec. 3 introduces notations from the relevant domains; Sec. 4 presents the proposed contributions; Sec. 5 demonstrates the practical use of the proposed methods; Sec.6 concludes the paper.Many relevant details pertaining to our method, experiments, and extra discussion can be found in Appendix sections A, B, and C."}
{"paper_id": 395, "abstract": "In the realm of mathematical sorcery, a neural ordinary differential equation (ODE) emerges as a bridge between an elusive function and its derivatives, woven together by the intricate threads of a neural network. To unveil the secrets of a neural ODE, one must summon a solver capable of performing the delicate art of numerical integration. Enter Dopri5, a renowned champion among neural ODE solvers and the trusted default within the torchdiffeq library of PyTorch\u2014a toolkit for those daring enough to traverse the landscape of ODEs.   Dopri5 wields the power of adaptive step size, harnessing the venerable Runge-Kutta (RK) numerical methods. These methods, much like a seasoned mage, rely on the estimation of local truncation error to deftly select and adjust the integration step size, a critical factor that dictates the numerical stability of the solution. A step size too grand can plunge the solver into chaos, while one too minuscule may lead to a tedious journey of excessive steps, draining precious computational resources and risking the insidious accumulation of rounding errors. Thus, the quest for an accurate local truncation error estimation becomes paramount, guiding the choice of an appropriate step size to achieve a solution that is not only precise but also swift and stable.  In this paper, we unveil a groundbreaking approach to local truncation error approximation, the first of its kind to draw upon the wisdom of four distinct RK orders, crafting a more reliable error estimate. This innovation births a new contender in the arena of solvers: S-SOLVER (Stable Solver). With its enhanced numerical stability, S-SOLVER stands as a beacon of accuracy. We showcase its prowess through rigorous experiments, tackling challenges in image recognition with ODE-Net, mastering Hamiltonian dynamics with Symplectic ODE-Net, and navigating the complex terrain of continuous normalizing flows (CNF). The dawn of S-SOLVER heralds a new era in the pursuit of numerical excellence.", "introduction": "Neural ODEs are continuous depth deep learning models that combine neural networks and ODEs.Since their first introduction in (Chen et al., 2018), they have been used in many applications such as: stochastic differential equations (Li et al., 2020), physically informed modeling (Sanchez-Gonzalez et al., 2019;Zhong et al., 2020), free-form continuous generative models (Grathwohl et al., 2019;Finlay et al., 2020), mean-field games (Ruthotto et al., 2020), and irregularly sampled time-series (Rubanova et al., 2019).Neural ODEs parameterize the derivative of the hidden state using a neural network; and therefore, learn non-linear mappings via differential equations.A differential equation is a relation between an unknown function and its derivatives.Ordinary differential equations describe the change of only one variable (as opposed to multiple) with respect to time, i.e.: dx/dt = f (t, x).Typically, an ODE is formulated as an initial value problem (IVP), which has the following form.Given a function derivative dx/dt, a time interval t = (a, b) and an initial value (e.i.: x at time t = a), the solution to the IVP yields x evaluated at time t = b.The method for approximating x(b) is numerical integration; therefore, all the various ODE solvers include different methods for performing integration.Adaptive step size solvers are amongst the most popular solvers for neural ODEs.In fact, the default solver in torchdiffeq (a library of ODE solvers implemented in PyTorch) is Dopri5, the Dormand-Prince 5(4) embedded adaptive step size method of the Runge-Kutta (RK) family.Adaptive step size RK solvers perform two approximations: one of order p and another of p -1 and compare them to obtain the local truncation error, which is used to determine the integration step size.Specifically, the error is used to make a decision whether to accept or reject the solution step under the current step size and to decide how to modify the step size for the next step.A step size that is too large leads to numerical instability, while a step size that is too small may cause the solver to take unnecessarily many steps, which is computationally expensive and may even cause the rounding error to build up.Therefore, accurate local estimation is paramount for choosing an appropriate step size to obtain an accurate, numerically stable, and fast solution to the ODE.The local truncation error is defined as the difference between the exact and approximate solution obtained at a given time step.All currently available adaptive step neural ODE solvers rely on estimating the local error as the difference between order p and p -1 solutions, which assumes that the order p solution is exact.This is not necessarily true and if the p solution is far from the exact one, the local error estimate is inaccurate, which results in the solver making poor decisions regarding its step size.In this paper we propose a novel local truncation error estimation that takes into account multiple orders of the RK method as opposed to just order p and p -1 to obtain a more accurate estimate of the local truncation error that guides the integration step size.Specifically, we modify the local truncation error estimation of Dopri8, the Dormand-Prince 8(7) embedded adaptive step size method.Dopri8 calculates the local truncation error as the difference between its 8th and 7th order solution.Our modification computes this error as the average of the difference between both its 8th and 7th, and also 4th and 5th order solution.This leads to a new ODE solver, S-SOLVER (Stable Solver), a modified Dopri8 integrator with more accurate local truncation error estimation that provides more reliable information for step size calculations; and therefore, more numerically stable solution.To our best knowledge, S-SOLVER is the first solver that uses a multiple solution orders to estimate local truncation error for adjusting its step size."}
{"paper_id": 396, "abstract": "In the realm of Deep Neural Networks (DNNs), where precision is paramount and computational resources are often scarce, a powerful technique known as quantization emerges as a beacon of efficiency. By reducing the bit-widths used for computations and tensor storage\u2014shifting from the standard f32 floating-point precision to lower representations\u2014quantization not only slashes model size but also accelerates inference latency. This transformation opens the door for deploying DNNs in environments where computational power is limited, such as real-time systems.   Yet, this journey through the realm of quantization is fraught with peril. The reduction in precision can introduce numerical instability, manifesting as roundoff errors that threaten the accuracy of our models. In this paper, we delve into the concept of simulated quantized inference, where the model parameters are confined to low-precision formats, such as int4, while the heavy lifting of mathematical operations\u2014matrix multiplications and additions\u2014remains in the familiar territory of floating-point arithmetic. This duality creates a roundtrip of quantization and dequantization, a process that, while necessary, can exacerbate roundoff errors and lead to instability.  Building upon the insights of previous works, which have illuminated the heightened sensitivity of biases and activations to quantization, we extend this understanding to the realm of weights. Our findings reveal that not all weights are created equal; some bear a greater burden of sensitivity that should be reflected in their quantization bit-width. To address this challenge, we introduce MixQuant, a search algorithm designed to discover the optimal custom quantization bit-width for each layer's weights, guided by the specter of roundoff error. This approach serves as a pre-processing optimization that can seamlessly integrate with any existing quantization method.  Through rigorous experimentation, we demonstrate that combining MixQuant with BRECQ\u2014a state-of-the-art quantization technique\u2014produces quantized models with superior accuracy compared to BRECQ alone. Furthermore, we showcase MixQuant's versatility by pairing it with vanilla asymmetric quantization, underscoring its potential to elevate the performance of any quantization strategy. In this way, we forge a path toward more robust and efficient DNNs, ensuring that even in the face of quantization's inherent challenges, we can emerge victorious.", "introduction": "Quantization is a method for mapping continuous values to a set of discrete values.The goal of neural network quantization is to perform computations and store tensors at lower bit-widths than floating point precision to reduce model size and inference latency while maintaining model accuracy, which allows for deploying DNNs on platforms with constrained computational resources, e.g.: real time inference on mobile devices.Quantization can be performed during training or inference.In this paper we focus on quantized inference, specifically post-training quantization, which quantizes a full precision trained model without the need for re-training or fine-tuning.Quantized inference can be either simulated or integer-only, and in this paper we focus on simulated quantization, where the quantized model parameters are stored in low-precision, but the mathematical operations on them (e.g.matrix multiplications and additions) are performed with floating point arithmetic (Gholami et al., 2022).In Tensorflow, PyTorch, and HuggingFace (QDQBERT model), simulated quantization is referred to as fake quantization.This means that the DNN parameters are first quantized from f32 to, for example, int4, and then dequantized back to f32 to perform the forward pass executed during inference.We show that the roundtrip process of quantizing and dequantizing the model parameters leads to roundoff error, which may lead to numerical instability.Similarly to prior works, which have shown that both biases and activations are more sensitive to quantization and are best kept in full precision or quantized with higher bit-widths (Zhou et al., 2016), we show that some weights are more sensitive than others which should be reflected on their quantization bit-width.To that end we propose MixQuant, a search algorithm that finds the optimal quantization bit-width from int2, int3, int4, int5, int6, int7, and int8 for each layer weight based on roundoff error and can be combined with any quantization method as a form of pre-processing optimization.We show that combining MixQuant with BRECQ (Li et al., 2021), a state-of-the-art quantization method, yields better quantized model accuracy than BRECQ alone.Additionally, we combine MixQuant with vanilla asymmetric quantization to show that MixQuant has the potential to optimize the performance of any quantization technique.MixQuant has three main benefits.First, MixQuant is a component of the quantization process, which can be leveraged to find optimal quantization mixed precision bit-widths that can be plugged into any quantization method to optimize its performance.Second, MixQuant is linear and runs in a matter of seconds, which makes it practical.Third, combining MixQuant with BRECQ, a stateof-the-art quantization method yields better quantized model accuracy than BRECQ alone, OMSE (Choukroun et al., 2019), AdaRound (Nagel et al., 2020), AdaQuant (Hubara et al., 2020), and Bit-Split (Wang et al., 2020)."}
{"paper_id": 397, "abstract": "In the ever-evolving landscape of machine learning, the challenge of Out-of-Distribution (OOD) detection looms large, akin to a shadowy figure lurking just beyond the campfire's glow. This task\u2014distinguishing whether an input emerges from a novel distribution rather than the familiar training grounds\u2014is vital for the safe deployment of our algorithms in the unpredictable wilderness of the real world. Recently, a beacon of hope has emerged in the form of post hoc detection, harnessing the power of pre-trained models to yield promising results, scalable to vast and complex challenges. This leads us to a pivotal question: could we harness the diverse strengths of multiple pre-trained models to elevate the performance of our post hoc detection methods?  In this endeavor, we unveil a novel detection enhancement strategy that orchestrates the wisdom of a multitude of models, each a unique voice in a grand chorus of detection decisions. By employing the p-value\u2014a more nuanced measure than the traditional hard threshold\u2014we tap into the rich framework of multiple hypothesis testing, ensuring that our true positive rate for In-Distribution (ID) data remains steadfast. Our exploration emphasizes the potential of model zoos, providing rigorous empirical comparisons against the current state-of-the-art on a variety of OOD detection benchmarks.  The results of our ensemble approach are nothing short of remarkable, consistently surpassing the capabilities of single-model detectors and decisively outpacing existing competitive methods. On the CIFAR10 and ImageNet benchmarks, our method achieves a staggering relative performance boost of 65.40% and 26.96%, respectively. With this work, we not only illuminate the path forward but also set a new standard for OOD detection in the realm of machine learning.", "introduction": "Deep neural networks have achieved empirical success in many applications, but generalization robustness has always been a thorny problem in deep learning.A sophisticated and well-trained deep neural network can provide excellent test performance on identically distributed (ID) test data but may fail to make accurate predictions on inputs from outside the training distribution Nguyen et al. (2015).This poses a big obstacle to the generalization of deep neural network models.Especially in safety-critical applications, it is better to identify out-of-distribution (OOD) inputs ahead of time rather than letting the model make predictions that may be unreliable.On the basis of pre-trained deep neural networks, many recent works on post hoc OOD detection have proposed diverse score functions to distinguish OOD samples utilizing the output probability, logits, gradients, and features of the pre-trained classifier.At the same time, some works also propose new training strategies to encourage the network to learn more features that may not be relevant to the OOD classification task.For example, MSP (Hendrycks & Gimpel, 2017) uses the maximum softmax probability, Energy score (Liu et al., 2020) considers the logits, and GradNorm (Huang et al., 2021) employs the vector norm of gradients.Based on these frameworks, several improved methods such as ODIN (Liang et al., 2018), Adjusted Energy Score (Lin et al., 2021), ReAct (Sun et al., 2021) are proposed to enhance the performance of OOD detection.These score functions above measure the similarity between a test input and the training (ID) data through pretrained feature extractors or classifiers.There are also many distance-based algorithms that directly quantify the distance of samples in the embedding space extracted from a pre-trained model and regard a test input as an OOD sample when it is far from the ID data.Lee et al. (2018) assumes the conditional distribution of features given the class label is a Gaussian distribution and derives a confidence score based on the Mahalanobis distance.SSD (Sehwag et al., 2020) considers selfsupervised pre-training and a Mahalanobis distance.Tack et al. (2020) uses contrastive learning with distributionally-shifted augmentations for pre-training and proposes a detection score specific to their training scheme.Sun et al. (2022) studies the nearest-neighbor distance and demonstrates the efficacy of non-parametric modeling of the feature distribution for OOD detection tasks.The performance of post hoc detection highly depends on the quality of pre-training.The most commonly used model architectures in OOD detection include convolutional networks such as ResNet (He et al., 2016), DenseNet (Huang et al., 2017) and Wide-ResNet (Zagoruyko & Komodakis, 2016), and of course Transformer models such as Swin (Liu et al., 2022) or ViT (Dosovitskiy et al., 2021).In general, the pre-trained models focus on the features related to classification tasks and the learnt representation may be insufficiently rich for OOD detection.Therefore, researchers have proposed ideas such as contrastive learning (Winkens et al., 2020;Tack et al., 2020), adversarial training Biggio & Roli (2018);Miller et al. (2020); Chalapathy & Chawla (2019) , outlier exposure (Hendrycks et al., 2018;Papadopoulos et al., 2021) or other auxiliary artificially synthesized data (Lee et al., 2017) and auxiliary loss function (Vyas et al., 2018) to encourage models to learn high-level, taskagnostic and comprehensive features, which makes the model more robust and efficient in the downstream detection task.These models trained with different architectures and training strategies can extract diverse features that may complement each other well.So, a natural question is raised:Can we leverage the diversity of multiple pre-trained models to improve the performance of post hoc OOD detectors?To answer this question, we first build a model zoo that captures as many properties of the input as possible and remains sensitive to distributional changes.Then we reformulate the OOD detection task to check whether there exists a model in the model zoo that can identify the test input as an OOD sample.Section 3.1 shows that the naive ensemble of multiple OOD detection decisions cannot maintain the true positive rate of the ID data (TPR).Therefore, we propose an ensemble scheme to integrate the results of multiple OOD detectors and provide theoretical guarantees that our method can keep TPR at the target level.In Section 4, we also report the empirical TPR of our method, which is close to the target TPR level.Ensembling is not new to OOD detection.Morningstar et al. (2021) combines multiple test statistics from generative models to differentiate ID and OOD data.Haroush et al. (2022) uses both the Simes' method and Fisher's method to summarize p-values computed for each channel and layer of a deep neural network.Bergamin et al. (2022) shows that combining different types of test statistics using Fisher's method overall leads to a more accurate out-of-distribution test.Recently, Magesh et al. (2022) proposes an ensemble framework that combines any number of different test statistics using the Benjamini-Yekutieli procedure (Benjamini & Yekutieli, 2001) and a conformal p-value estimator (Vovk et al., 1999).In this work, we develop a simple and fundamental ensemble scheme for using model zoos in OOD detection and name our method Zoo-based OOD Detection Enhancement (ZODE).Our method directly estimates the p-values according to its definition and employs the Benjamini-Hochberg procedure (Benjamini & Hochberg, 1995) to control TPR.Then, we provide theoretical guarantees and empirical validation to show that ZODE can maintain the TPR close to its target level.On the other hand, we focus on the settings of the model zoo and conduct systematic experiments to demonstrate the superiority of our approach.First, we show that ZODE can consistently improve current OOD detectors.Second, by comparing single-model detectors with the ZODE-ensembled detector, we find that ZODE can exploit the diversity of multiple pretrained models and leverage complementarity among single-model detectors.Finally, our approach significantly improves current SOTA performance.We summarize our contributions as follows:\u2022 We provide novel insights into OOD detection from the perspective of the model zoo.We propose an enhancement scheme, ZODE, for OOD detection by exploiting the diversity of pre-trained models.The proposed method is inspired by a simple and fundamental framework of multiple hypothesis testing.Our theoretical results and experiments clearly show that ZODE can leverage the complementarity among single-model detectors to improve performance.\u2022 We point out that the naive ensemble of multiple OOD detectors leads to lower TPR.Then we provide theoretical analysis and empirical validation to demonstrate that our proposed method can maintain TPR well under the settings of the model zoo.\u2022 Extensive experiments show that our method can effectively and consistently improve the power of identifying OOD samples.On a commonly used CIFAR10 benchmark, our method significantly improves the SOTA result of the average false positive rate from 11.07% to 3.83%.For a challenging OOD detection task based on ImageNet, we show that our method is scalable to large-scale problems and significantly improves the SOTA result of the average false positive rate from 38.47% to 28.10%."}
{"paper_id": 398, "abstract": "In this endeavor, we unveil Di-SSL, a groundbreaking Diversity-inducing Self-Supervised Learning technique crafted specifically for the intricate realm of histopathology image analysis. Self-Supervised Learning (SSL) methods, whether contrastive or non-contrastive, have proven adept at forging rich, effective representations without the guiding hand of human supervision. Recently, the field of computational pathology has harnessed the remarkable potential of SSL to propel its advancements.   Within this work, we introduce an innovative domain-aware pretext task aimed at amplifying representation learning within digital pathology. Our meticulous examination of the attention distribution in conventional SSL-pretrained models yields a compelling insight: these models often exhibit a tendency toward attention sparsity, fixating on a select few prominent patterns within the image. While this focus can be advantageous in natural images\u2014where these patterns often represent the objects of interest\u2014such an approach falls short in the complex landscape of digital pathology. Here, the scans are not merely object-centric; they encapsulate a rich tapestry of spatially interwoven biological components. The failure to diversify attention adequately in these multifaceted images risks the loss of critical information.  To counter this challenge, we first harness cell segmentation to extract a wealth of histopathology-specific representations. We then propose a dense pretext task for SSL, meticulously designed to align multiple corresponding representations across different views. This approach compels the model to distribute its attention more evenly and comprehensively, fostering a necessary diversification that captures the context-rich nature of the images.   Through rigorous quantitative and qualitative analyses across a spectrum of slide-level tasks pertaining to various cancer types, as well as patch-level classification tasks, we substantiate the effectiveness of our method. Notably, we observe a more globally distributed attention pattern. Our results reveal a remarkable relative accuracy improvement of up to 6.9% in slide-level tasks and 2% in patch-level classification tasks, corresponding to AUC enhancements of up to 7.9% and 0.7%, respectively, when juxtaposed against a baseline SSL model.", "introduction": "Computational pathology is a rapidly emerging field that aims at analyzing high resolution images of biopsied or resected tissue samples.Advancements in computer vision and deep learning has enabled learning of the rich phenotypic information from whole slides images (WSIs) to understand mechanisms contributing to disease progression and patient outcomes.Acquiring crop-level localized annotations for WSIs is expensive and often not feasible; only slide-level pathologist labels are usually available.In such a scenario, weak supervision is a commonly utilized strategy, where crops are embedded into representations in the first stage, followed by considering these WSI-crops' representation as a bag for multiple instance learning (MIL).Now the question remains, how do we learn a model to effectively encode the crops into rich representations?Traditionally, ImageNet (Krizhevsky et al., 2017) pre-trained neural networks are utilized to extract the representations (Lu et al., 2021b;Lerousseau et al., 2021;Shao et al., 2021).However ImageNet and pathology datasets are composed of different semantics; while the former contains object-centric natural images, the later consists of images with spatially distributed biological components such as cells, glands, stroma, etc.Therefore, to learn domain-specific features of WSI-crops in the absence of localized annotations, various self-supervised learning (SSL) techniques are recently gaining traction (Ciga et al., 2022;Stacke  In both natural imaging and digital pathology, vanilla SSL pre-training creates sparse attention maps, i.e., it attends largely to only some prominent patterns.Although attention sparsification can be beneficial in natural image tasks such as object classification, this could be sub-optimal for encoding representations in digital pathology as it leads to loss of important contextual information.Through a more diversified attention mechanism, DiRL encodes dense information critical to non object-centric tasks.et al., 2021;Boyd et al., 2021).SSL models pre-trained on histopathology datasets have been shown to be effective in downstream classification tasks when compared to those trained on ImageNet.To further analyze the role of SSL in computational pathology, we pre-trained a vision transformer (Dosovitskiy et al., 2020) on various WSI datasets using vanilla SSL (Caron et al., 2021).In-depth analysis of the pre-trained models' attention maps on WSI-crops led us to a striking observation: sparsity in attention maps.The model tends to localize most of its attention to a small fraction of regions, leading to sub-optimal representation learning.To further validate our observation, we visualized the attention maps of a self-supervised ImageNet pre-trained model on natural images (see Fig. 1).Similar observations led us to conclude that this is a property of SSL rather than of data.We believe that sparsity in attention might potentially benefit the performance in some natural imaging tasks such as object classification.This stems from the fact that during SSL, the model is tasked to match the two views, optimizing which leads the model to focus on the prominent patterns.For example, in Fig. 1(a), for an object-centric ImageNet example, since the prominent pattern is the object (eg.bird) itself (Yun et al., 2022), the model tends to center its attention towards the object, thus benefiting numerous downstream applications (for eg., bird classification).In contrast, WSI-crops are not object-centric, rather they constitute a spatial distribution of complex structures such as cells, glands, their clusters and organizations, etc, see Fig. 1(b).Encoding this dense information available into a holistic representation demands the model to focus more diversely to various histopathology primitives and not just to specific ones.Conversely, the vanilla SSL model pre-trained on histopathology only sparsely attends to the important regions (Fig. 1(b)), i.e., there is inadequate diversity in attention.We hypothesize that this sparsely attending model could result in encoding sub-optimal representations, as fine-grained context-rich details are often ignored.To address this issue of inadequate attention diversity, we propose DiRL, a diversity-inducing pretraining technique, tailored to enhance representation learning in digital pathology.Each WSI-crop consists of two regions: cellular regions (one containing cells) and non-cellular regions (containing no cells).We leverage an off-the-shelf cell segmentation pipeline to identify these regions.This domain-specific knowledge is then utilized to extract region-level representations separately for the cellular and non-cellular regions.We further propose to encode the inter-and intra-spatial interplay of two regions.This biologically-inspired step (Saltz et al., 2018;Fassler et al., 2022) is achieved through a transformer-based disentangle block to encode the self-interaction within the regions, and cross-interaction between both the regions, termed as disentangled representations.In contrast to vanilla SSL frameworks that leverage one image-level representation for a WSI-crop, our prior-guided representation learning framework leverages histology-specific domain knowledge to densely extract a set of region-level and disentangled representations.We then task our framework to match all the corresponding representations between the views.We hypothesize that optimizing this dense matching objective between the views would encourage the model to diversify its attention to various regions; matching assorted representations would then enforce the model to explore diverse image-regions relevant for each such representations.We validate this hypothesis through consistent improvements in performance on multiple downstream tasks such as slide-level and patch-level classifications.Our qualitative analysis on attention distribution of the pre-trained models reveals that our DiRL framework can effectively de-sparsify attention, thereby learning global context-rich representations, unlike existing methods.To summarize our main contributions, we:\u2022 Demonstrate that attention sparsification in self-supervised learning may lead to learning sub-optimal representations in digital pathology classification tasks.\u2022 Design a novel domain-aware pretext task to de-sparsify attention maps and achieve enhanced representations for digital pathology.\u2022 Demonstrate the efficacy of our DiRL through slide-level and patch-level classification tasks on four WSI datasets and two patch datasets."}
{"paper_id": 399, "abstract": "In the realm of machine learning, where the intricacies of data often resemble the tangled threads of a grand tapestry, we introduce the Class-Informed Variational Autoencoder (CI-VAE). This innovative architecture is designed to weave together observations of the same class, allowing for seamless interpolation between any two points within that category. At its core, CI-VAE melds the foundational structure of the traditional Variational Autoencoder (VAE) with a linear discriminator layer, crafting a latent space where the boundaries between different classes are not just suggested, but sharply defined.  In the conventional landscape of VAEs, one often encounters the vexing issue of class overlap within the latent space\u2014a muddled realm where distinctions blur. CI-VAE, however, boldly confronts this challenge, imposing a linear separability that transforms the latent space into a well-ordered realm. This clarity not only facilitates robust linear traversals but also empowers the generation of new data points nestled between any two observations of the same class.  The implications of class-specific data interpolation are vast, particularly in scientific fields such as biology, where it could illuminate the intricate pathways of diseases, including cancer. To illustrate the prowess of our CI-VAE, we conducted a case study using the MNIST dataset of handwritten digits, rigorously comparing its performance against that of the traditional VAE in the realm of class-specific data augmentation. Our findings revealed a striking enhancement in linear traversal and data generation capabilities with CI-VAE, all while maintaining a reconstruction error that remains on par with its predecessor.  Furthermore, we ventured into the complex world of colon cancer genomics, demonstrating how CI-VAE facilitates interpolation between normal and tumor cells. This capability not only enriches our comprehension of cancer development but also opens new avenues for exploration in the biological sciences. In essence, CI-VAE stands as a beacon of innovation, illuminating paths previously shrouded in ambiguity and propelling us toward a deeper understanding of the data that shapes our world.", "introduction": "Variational Autoencoders (VAEs) have emerged as a popular unsupervised probabilistic neural network models Kingma & Welling (2013); Dilokthanakul et al. (2016); Hsu et al. (2017) with a variety of applications in computer vision Hsieh et al. (2018); Vahdat & Kautz (2020); Tabacof et al. (2016); Huang et al. (2018), natural language processing Wu et al. (2019); Bahuleyan et al. (2017); Semeniuta et al. (2017), genomics and precision medicine Gr\u00f8nbech et al. (2020); Minoura et al. (2021) and many other domains.In VAEs, through the encoder, a probabilistic latent representation of data in lower dimensional space is inferred.Besides many applications of dimensionality reduction using VAEs, these probabilistic models have proven to be very effective in synthetic data generation.Although VAEs are initially designed for unsupervised learning, several variants of VAEs have been designed in other domains such supervised learning and semi-supervised learning Kameoka et al. (2019); G\u00f3mez-Bombarelli et al. (2018); Ehsan Abbasnejad et al. (2017); Xu et al. (2017); Wu et al. (2019); Sohn et al. (2015); Higgins et al. (2016); Zhao et al. (2019).Suppose we would like to know how two observations transform from one to another.Through linear traversal on the latent space in VAEs, we can generate a trajectory and observe how this transformation may take place.A wide variety of applications can be benefited from this type of solution.For instance, one may attempt to uncover the disease mechanism of Parkinson's through understanding how a neuronal cell in a healthy brain tissue transformed to a neural cell with Parkinson's disease Hook et al. (2018); Blauwendraat et al. (2020).Such investigations are often intended for a specific subset/class of the data.In a Parkinson's disease study, for example, we may intend to investigate the neuronal cells as a cell-type/class of the entire population of cells.Therefore, to perform linear traversal within neuronal cells, we need a latent space that is linearly separable among classes to ensure that, during linear traversal, there is no overlapping of classes (cell types in this example).With this motivation, we proposed Class-Informed Variational AutoEncoders (CI-VAE), a novel deep learning model architecture that includes an additional linear discriminator applied on the latent space, extending the capabilities of VAEs to form a latent space where observations from different"}
{"paper_id": 400, "abstract": "In the realm of private deep learning, Differentially Private Stochastic Gradient Descent (DP-SGD) stands as a stalwart champion, diligently providing a singular privacy shield for each data point within the vast expanse of a dataset. Yet, what if we could peer beneath this protective veil and unveil the individual privacy guarantees afforded to each example? In our quest for clarity, we present an innovative algorithm that meticulously computes these guarantees, allowing us to dissect the privacy landscape across a variety of datasets.   Our findings reveal a fascinating truth: the majority of examples bask in stronger privacy assurances than the direst worst-case scenarios suggest. Moreover, we uncover a compelling correlation between the training loss of an example and its corresponding privacy parameter. This correlation hints at a troubling reality: groups that suffer from a lack of model utility are also deprived of robust privacy protections. For instance, in the CIFAR-10 dataset, we observe a staggering 43.6% disparity in the average privacy parameter, \u03b5, between the class with the lowest test accuracy and that with the highest. To further substantiate our claims, we conduct membership inference attacks, illuminating the stark differences in empirical privacy risks that accompany these disparities. In this intricate dance of data and privacy, our work seeks to shed light on the hidden inequities that lie beneath the surface.", "introduction": "Differential privacy is a strong notion of data privacy, enabling rich forms of privacy-preserving data analysis (Dwork & Roth, 2014).Informally speaking, it quantitatively bounds the maximum influence of any datapoint using a privacy parameter \u03b5, where a small value of \u03b5 corresponds to stronger privacy guarantees.Training deep models with differential privacy is an active research area (Papernot et al., 2017;Bu et al., 2020;Yu et al., 2022;Anil et al., 2021;Li et al., 2022;Golatkar et al., 2022;Mehta et al., 2022;De et al., 2022;Bu et al., 2022).Models trained with differential privacy not only provide theoretical privacy guarantee to their data but also are more robust against empirical attacks (Bernau et al., 2019;Carlini et al., 2019;Jagielski et al., 2020;Nasr et al., 2021).Differentially private stochastic gradient descent (DP-SGD) is the de-facto choice for differentially private deep learning (Song et al., 2013;Bassily et al., 2014;Abadi et al., 2016).DP-SGD first clips individual gradients and then adds Gaussian noise to the average of clipped gradients.Standard privacy accounting takes a worst-case approach, and provides all examples with the same privacy parameter \u03b5.However, from the perspective of machine learning, different examples can have very different impacts on a learning algorithm (Koh & Liang, 2017;Feldman & Zhang, 2020).For example, consider support vector machines: removing a non-support vector has no effect on the resulting model, and hence that example would have perfect privacy.In this paper, we give an efficient algorithm to accurately estimate individual privacy parameters of models trained by DP-SGD.Our privacy guarantee adapts to the training trajectory of one execution of DP-SGD to provide a precise characterization of privacy cost (see Section 2.1 for more details).Inspecting individual privacy parameters allows us to better understand example-wise impacts.It turns out that, for common benchmarks, many examples experience much stronger privacy guarantee than the worst-case bound.To illustrate this, we plot the individual privacy parameters of MNIST (LeCun et al., 1998), CIFAR-10 (Krizhevsky, 2009), and UTKFace (Zhang et al., 2017) in Figure 1.Experimental details, as well as more results, can be found in Section 4 and 5.The disparity in individual privacy guarantees naturally arises when running DP-SGD.To the best of our knowledge, our investigation is the first to explicitly reveal such disparity.We propose two techniques to make individual privacy accounting viable for DP-SGD.First, we maintain estimates of the gradient norms for all examples so the individual privacy costs can be computed accurately at every update.Second, we round the gradient norms with a small precision r to control the number of different privacy costs, which need to be computed numerically.We explain Figure 1: Individual privacy parameters of models trained by DP-SGD.The value of \u03b4 is 1 \u00d7 10 -5 .The dashed lines indicate 10%, 30%, and 50% of datapoints.The black solid line shows the privacy parameter of the original analysis.why these two techniques are necessary in Section 2.More details of the proposed algorithm, as well as methods to release individual privacy parameters, are in Section 3.We further demonstrate a strong correlation between the privacy parameter of an example and its final training loss.We find that examples with higher training loss also have higher privacy parameters in general.This suggests that the same examples suffer a simultaneous unfairness in terms of worse privacy and worse utility.While prior works have shown that underrepresented groups experience worse utility (Buolamwini & Gebru, 2018), and that these disparities are amplified when models are trained privately Bagdasaryan et al. (2019);Suriyakumar et al. (2021);Hansen et al. (2022); Noe et al. (2022), we are the first to show that the privacy guarantee and utility are negatively impacted concurrently.This is in comparison to prior work in the differentially private setting which took a worst-case perspective for privacy accounting, resulting in a uniform privacy guarantee for all training examples.For instance, when running gender classification on UTKFace, the average \u03b5 of the race with the lowest test accuracy is 25% higher than that of the race with the highest accuracy.We also study the disparity in privacy when models are trained without differential privacy, which may be of independent interest to the community.We use the success rates of membership inference attacks to measure privacy in this case and show groups with worse accuracy suffer from higher privacy risks.1.1 RELATED WORK Several works have explored example-wise privacy guarantees in differentially private learning.Jorgensen et al. (2015) propose personalized differential privacy that provides pre-specified individual privacy parameters which are independent of the learning algorithm, e.g., users can choose different levels of privacy guarantees based on their sensitivities to privacy leakage (M\u00fchl & Boenisch, 2022).A recent line of works also uses the variation in example-wise sensitivities that naturally arise in learning to study example-wise privacy.Per-instance differential privacy captures the privacy parameter of a target example with respect to a fixed training set (Wang, 2019;Redberg & Wang, 2021;Golatkar et al., 2022).Feldman & Zrnic (2021) design an individual R\u00e9nyi differential privacy filter.The filter stops when the accumulated cost reaches a target budget that is defined before training.It allows examples with smaller per-step privacy costs to run for more steps.The final privacy guarantee offered by the filter is still the worst-case over all possible outputs as the predefined budget has to be independent of the algorithm outcomes.In this work, we propose output-specific differential privacy and give an efficient algorithm to compute individual guarantees of DP-SGD.We further discover that the disparity in individual privacy parameters correlates well with the disparity in utility."}
{"paper_id": 401, "abstract": "In the realm of offline goal-conditioned supervised learning (GCSL), a powerful tool emerges: the ability to learn how to achieve a multitude of goals from datasets devoid of reward signals. This capability offers a tantalizing promise of enhanced control over policy execution. Yet, we posit that a crucial component remains unaddressed\u2014the seamless learning of a composite policy that can switch effortlessly between various goals. Such a skill should be attainable, provided the dataset encompasses sufficient examples of these transitions. Regrettably, many existing datasets either lack these vital demonstrations or contain them only in fragments.  Current GCSL methodologies, which harness hindsight information, tend to focus on the reachability of states or returns. However, they often falter when confronted with the challenge of goal-switching within a single episode. To tackle this limitation, we introduce Goal-Masked Transformers (GMT)\u2014a cutting-edge GCSL algorithm that leverages the power of transformers and the concept of goal masking. GMT ingeniously utilizes trajectory-level hindsight information, which is automatically collected and can be tailored to various statistical needs. Thanks to GMT's autoregressive design, we can alter goals and exert control over the policy at will.  We rigorously evaluate GMT against established baselines across the MuJoCo continuous control benchmarks and Atari discrete control games, utilizing image states for our assessments. Our findings reveal that GMT possesses a remarkable ability to infer the elusive switching processes absent from the dataset, enabling it to navigate smoothly between different goals. Consequently, GMT not only showcases its prowess in policy control but also achieves success across all tasks with impressively low variance\u2014an accomplishment that existing GCSL approaches struggle to replicate when faced with the intricacies of goal-switching.", "introduction": "Runners can control and adjust their pace in a marathon by switching comfortably between various poses for different goals.Similarly, agents can also acquire such switching ability through reinforcement learning (RL) or imitation learning (IL).This process generally requires environments that can start with arbitrary pose states, carefully tuned rewards, or massive offline demonstrations.However, these critical things are notoriously challenging to obtain.In comparison, by knowing the pace of each running stance, a human can easily switch between various poses to control speed without learning such switching processes intentionally.From another perspective, we try to formulate this problem as goal-conditioned supervised learning (GCSL) (Ghosh et al., 2019) problem given a fixed amount of offline dataset: Considering pose or pace as a goal, can agents learn a composite policy that can switch among these goals interchangeably over the dataset?We refer to this problem as the goal-switching problem.Since the distribution of initial states shifts between the training and evaluation, this problem might face the covariate shift issue.Instead of a fixed set of states in the training set, any state might be the start of the switched goal during evaluation, resulting in agents not knowing how to achieve the goal.The goal-switching has widespread adoption in practical applications.The control of robots to transfer to a different skill while performing another skill is essential in robotics.In the game field, it can induce immersive experiences by managing the performance and strategies of AI bots according to the game progress.as goals can be called state-conditioned models.These models generate actions based on targeting future states or slices of successful demonstration.There are also models conditioning on return-togo -the total rewards an agent can receive from the current step until the end of an episode.These methods typically use the \"relabeling\" strategy to excavate a variety of goal signals by bootstrapping any of the aforementioned goals from a fixed dataset.However, each method from above has some fatal flaws that make them result in poor performance on the goal-switching problem.By setting return-to-go as goals, switching can only happen by tweaking a normalized return-to-go, which is neither intuitive nor efficient.State-conditioned methods can reach an arbitrary state in theory.However, they tend to model the problem as a pure MDP, which leads to poor performance or failure when demonstrations of transiting between two states are missing from the dataset.We argue that the essence of the goal-switching problem is to enable the model to learn unseen transitions.Modelbased approaches may be one remedy, which allows for unseen transitions by planning across a latent space (Jiang et al., 2022) or an explicitly learned world model (Micheli et al., 2022).These methods require additional environmental model learning with higher learning complexity.This paper presents Goal-Masked Transformers (GMT), an efficient GCSL algorithm that neither necessitates explicit world model learning nor successful demonstrations to solve the goal-switching problem.In particular, we employ a causal transformer (Radford et al., 2019) to autoregressively describe trajectories with transitions consisting of goals, states, and actions.With such a setting, the goal can be changed at any moment, releasing the full potential of the policy control.However, since the goals are the same in each transition during training, the model tends to neglect the change of goals, leading to a goal-switching failure during evaluation.In order to compel the model to put more effort into goals, we introduce a masking mechanism with a probability of replacing the goal information with a [mask] token.As a result, we observe the promising outcome that agents can smoothly switch from one goal to another.Figure 1 presents an overview of GMT.Similar to previous GCSL works, we apply the \"relabeling\" strategy to increase the diversity and coverage of goals.Additionally, there are various expressions of goals and numerous approaches to achieving them.Thus, we propose a simple yet effective approach that automatically aggregates and clusters the offline data into several goals in accordance with the statistics of interests.In summary, our main contributions are as follows:\u2022 We draw attention to the goal-switching problem and argue that doing so is an essential step toward controllable policy.The goal-switching problem requires that agents should adapt to goal changes within one episode.It is an exceedingly challenging generalization problem, especially when training on limited datasets.\u2022 We propose Goal-Masked Transformers (GMT), a family of goal-conditioned algorithms based on causal transformers with a goal-masking mechanism and hindsight information to achieve controllable policy.Through experiments, we demonstrate that GMT possesses goal-switching capabilities that are not present in the current GCSL algorithms.\u2022 We introduce an unsupervised approach to cluster trajectories into multiple goals from the datasets without any goal information.Empirically, we find that this approach improves the stability and efficiency of the switching process."}
{"paper_id": 402, "abstract": "In the realm of digital imagery, shadows often cloak the finer details, leaving images marred by underexposure\u2014be it due to the capriciousness of scene lighting or the limitations of hardware. Such obscurity not only diminishes visibility but also hampers subsequent tasks that rely on clarity and aesthetics. Thus, the quest for enhancing low-light images emerges as a matter of practical significance. Among the myriad of techniques available, retinex-based methods have taken center stage in this pursuit. Yet, a significant flaw plagues many of these approaches: they either neglect the insidious presence of noise or handle it with a clumsy touch, resulting in visual artifacts that can tarnish both the enhancement process and higher-level analytical tasks.  In response to this challenge, we unveil RetinexUTV\u2014a robust method crafted to breathe new life into low-light images while deftly suppressing noise. At the heart of RetinexUTV lies an innovative adaptive illumination estimation mechanism, embodied in an unfolded total variational network. This clever architecture learns to gauge the noise level inherent in the original image by fine-tuning the balance parameter of the total variation regularization term. From this, it generates both a noise level map and a smooth, noise-free sub-map of the image.   With the illumination information gleaned from the smooth sub-map, we derive an initial illumination map, which, when combined with the original image, yields the initial reflection map. Guided by the noise level map, we then meticulously suppress the noise within the reflection map. The culmination of this process is a final enhancement result, achieved by multiplying the refined reflection with the adjusted illumination map.  Our method has been rigorously tested against real low-light datasets, including LOL and VELOL, and the results speak volumes\u2014demonstrating a clear superiority over existing state-of-the-art techniques. In the ever-evolving landscape of image enhancement, RetinexUTV stands as a beacon of clarity in the darkness.", "introduction": "Recording people's lives by taking pictures or videos is becoming more and more popular.However, since most users lack professional shooting skills, many photos are captured in less than ideal lighting conditions such as night time and backlight.Such images have low contrast, strong noise, and unclear details, which not only affect the human visual experience, but also limit the application of many computer vision algorithms, such as object recognition and object detection.There are several approaches to enhance low-light images, including histogram equalization (Abdullah-Al-Wadud et al., 2007;Pizer, 1990), inverse domain operations (Li et al., 2015;Zhang et al., 2016), Retinex decomposition (Xiao & Shi, 2013;Jobson et al., 1997a;b;Herscovitz & Yadid-Pecht, 2004) and deep learning (Yang et al., 2016;Lore et al., 2017).Histogram equalization-based methods flatten the histogram and stretch the dynamic range of intensity, thereby amplifying the illumination of low-light images.If noise is not specifically considered, noise and artifacts will be amplified in its results.Some researchers noticed the similarity between haze images and inverted low-light images.Therefore, these inverse domain-based methods apply de-overlapping methods to enhance low-light images.To jointly adjust illumination and suppress noise, a method based on Retinex theory is proposed.Methods based on Retinex decomposition treat the scene in the human eye as the product of the reflection layer and the illumination layer.Enhanced results are produced by adjusting the corresponding layers.The earliest methods directly regard the decomposed reflection layer as the enhancement result (Jobson et al., 1997b;a;Xiao & Shi, 2013;Herscovitz & Yadid-Pecht, 2004).Single-scale Retinex (SSR) (Jobson et al., 1997b) and Multi-scale Retinex (MSR) (Jobson et al., 1997a) utilize Gaussian filters to build Retinex representations.In (Xiao & Shi, 2013), a bilateral filter is used to remove halo artifacts.Later methods adjust the illumination and reflection layers and reconstruct the enhanced result by combining them.In (Kimmel et al., Figure 1: The proposed framework.Our sequential processing is divided into two stages.In the first stage, the illumination map and noise level map are obtained by using the unfolding total variation.In the second stage, the reflection map is obtained by calculating the illumination map and the original image.Input the reflection map and the noise level map into the non-blind denoising subnetwork to suppress the noise to obtain reflection map without noise, then adjusted with the illumination map to obtain an enhanced denoised image.& Wang, 2011;Fu et al., 2014) Due to the interpretability of the Retinex theory and the ease of modeling, more and more methods have been proposed to combine deep learning with Retinex.The first one to combine deep learning with Retinex is RetinexNet (Wei et al., 2018), which uses a decomposition network and a brightness adjustment network.The two sub-networks complete the enhancement of the dark light image, and the subsequent methods are improved on this basis, such as adding a denoising network to the decomposed reflection image, and adding a brightness correction network to the decomposed lighting network, such as KinD (Zhang et al., 2019).These methods are all trained under the expected noise model, but still lack robustness in real dark-light environments because the noise exhibits different noise levels.In our proposed method, we use the unfolding total variational model to estimate the noise level map and the illumination map, then the reflection map is obtained through the illumination map, and the reflection map is denoised through the guidance of the noise level map.In the Retinex theory, the anticipate illumination map needs to be smooth enough in space, while the reflection map needs edge details to represent the essence of the object.In this paper, we consider noise as a non-negligible factor in Retinex-based decomposition.The proposed model and method are noiseaware throughout the process, rather than in the form of individual ad-hoc operations.Compared with previous methods that only consider light noise modeling, this paper also aims to model and remove strong noise in low-light images (Wang et al., 2020).Firstly, we build a unfolding total variational model to estimate the noise level map and noise-free smooth map, and then obtain the illumination information of the noise-free smooth map to obtain the original illumination map.Then according to the retienex theory, the original illumination map and the original image are calculated to obtain the original reflection map, and the reflection map is denoised by the guidance of the noise level map.The illumination map is adjusted by the light adjustment network to obtain the adjusted illumination map.Finally, multiply them to get the enhanced image.Specific process is shown in Figure 1.The contributions of this paper are mainly reflected in the following aspects:\u2022 We propose a robust low-light enhancement method RetinexUTV for joint denoising.Previous Retinex decompositions often ignored noise as a pre-or post-processing term, which compromised the overall visual quality.The variational model we built estimates both the noise level map and the illumination map, resulting in a noise-free enhancement result.\u2022 We introduce noise levels into Retinex enhancements.Most of the existing enhancement algorithms only focus on low visibility problems or suppress noise under assumed noise levels, resulting in a lack of robustness.We learn a noise level map by learning a balance parameter in a model-based total variation regularization denoising method to approximate the noise level of real low-light images.\u2022 Our proposed method achieves excellent results on real captured low-light images with various noise levels."}
{"paper_id": 403, "abstract": "In the ever-evolving realm of deep learning, a recent wave of research (Li et al., 2020; Wan et al., 2021) has unveiled a fascinating mechanism at play within normalized models trained using Stochastic Gradient Descent (SGD) and Weight Decay (WD). This mechanism, which we shall refer to as Spherical Motion Dynamics (SMD), has proven to wield significant influence in practical applications. Yet, amidst these revelations, a gap remains\u2014a theoretical exploration of SMD\u2019s impact on the training process of normalized models is conspicuously absent from the literature.  In this study, we embark on a quest to unravel the mysteries surrounding SMD, turning our focus to a simplified normalized model we have dubbed the Noisy Rayleigh Quotient (NRQ). Through rigorous theoretical analysis, we demonstrate that SMD exerts a commanding influence over the entire training process by governing the evolution of angular updates (AU), a pivotal characteristic of SMD. Our findings reveal two critical insights: First, within the equilibrium state of SMD, both the convergence rate and the limiting risk of the NRQ are predominantly dictated by the theoretical value of the AU. Second, when we venture beyond this equilibrium, the evolution of the AU disrupts the optimization trajectory, leading to peculiar behaviors\u2014what we describe as \u201cescape\u201d phenomena.  Moreover, we establish that the insights gleaned from our analysis of the NRQ align harmoniously with empirical observations derived from experiments conducted on real-world datasets. We believe that our theoretical contributions illuminate the intricate role of normalization techniques in the training of contemporary deep learning models, paving the way for deeper understanding and future advancements in the field.", "introduction": "Normalization (Ioffe & Szegedy, 2015;Wu & He, 2018) is one of the most widely used deep learning techniques, and has become an indispensable part in almost all popular architectures of deep neural networks.Though the success of normalization techniques is indubitable, its underlying mechanism still remains mysterious, and has become a hot topic in the realm of deep learning.Many works have contributed in figuring out the mechanism of normalization from different aspects.While some works (Ioffe & Szegedy, 2015;Santurkar et al., 2018;Hoffer et al., 2018;Bjorck et al., 2018;Summers & Dinneen, 2019;De & Smith, 2020) focus on intuitive reasoning or empirical study, others (Dukler et al., 2020;Kohler et al., 2019;Cai et al., 2019;Arora et al., 2018;Yang et al., 2018;Wu et al., 2020) focus on establishing theoretical foundation.A series of works (Van Laarhoven, 2017;Chiley et al., 2019;Kunin et al., 2021;Li et al., 2020;Wan et al., 2021;Lobacheva et al., 2021;Li & Arora, 2019) have noted that, in practical implementation, the gradient of normalized models is usually computed in a straightforward manner which results in its scale-invariant property during training.The gradient of a scale-invariant weight is always orthogonal to the weight, and thus makes the training trajectory behave as motion on a sphere.Besides, in practice, many models are trained using SGD with Weight Decay (WD), hence normalization and WD in SGD can cause a so-called \"equilibrium\" state, in which the effect of gradient and WD on weight norm cancel out (see Fig. 1(a)).It has been a long time since the concept of equilibrium was first proposed (Van Laarhoven, 2017) while either theoretical justification or experimental evidence had still been lacking until recently.Recent works (Li et al., 2020;Wan et al., 2021) theoretically justify the existence of equilibrium in both theoretical and empirical aspects, and characterize the underlying mechanism that yields equilibrium, named as \"Spherical Motion Dynamics\".In Wan et al. (2021) the authors further show SMD exists in a wide range of computer vision tasks, including ImageNet Deng et al. (2009) and MSCOCO (Lin et al., 2014).More detailed review can be seen in appendix A. Though the existence of SMD has been confirmed both theoretically and empirically, as well as some of its characteristics, we notice that so far no previous work has ever theoretically justified how SMD can affect the evolution of the loss of normalized models.Although some attempts have been made in Li et al. (2020); Wan et al. (2021) to explore the role of SMD in the training by conjectures and empirical studies, they still lack theoretical justification on their findings.In hindsight, the main challenge to theoretically analyze the effect of SMD is that SMD comes from the joint effect of normalization and WD which can significantly distort the loss landscape (see Figure 1(b)), and thus dramatically weaken some commonly used assumptions such as (locally) convexity, Lipschitz continuity, etc. Exploring the optimization trajectory on such distorted loss landscape is very challenging, much less that taking in addition SMD into account in the consideration.In this paper, as the first significant attempt to overcome the challenge on studying the effect of SMD towards evolution of the loss, we propose a simple yet representative normalized model, and theoretically analyze how SMD influences the optimization trajectory.We adopt the SDE framework of Li et al. (2020) to derive the analytical results on the evolution of NRQ, and concepts of Wan et al. (2021) to interpret the theoretical results we obtain in this paper.Our contributions are\u2022 We design a simple normalized model, named as Noisy Rayleigh Quotient (NRQ).NRQ possesses all the necessary properties to induce SMD, consistent with those in real neural networks.NRQ contributes a powerful tool for analyzing how normalization affects firstorder optimization algorithms;\u2022 We derive the analytical results on the limiting dynamics and the stationary distribution of NRQ.Our results show the influence of SMD is mainly reflected on how angular update (AU), a crucial feature of SMD, affects the convergence rate and limiting risk.We discuss the influence of AU within equilibrium and beyond equilibrium respectively, figuring out the association between the evolution of AU and the evolution of the optimization trajectory in NRQ;\u2022 We show that the insights drawn from the theoretical results on NRQ can adequately interpret typical observations in deep learning experiments.Specifically, we confirm the role of learning rate and WD is equivalent to that of scale-invariant weight in SGD.We show the Gaussian type initialization strategy can affect the training process only because it can change the evolution of AU at the beginning.We also confirm that under certain condition, SMD may induce \"escape\" behavior of optimization trajectory, resulting in \"pseudo overfitting\" phenomenon in practice.2 NOISY RAYLEIGH QUOTIENT"}
{"paper_id": 404, "abstract": "In the ever-evolving realm of neural networks, we unveil a groundbreaking method that empowers anytime networks to deftly control their depths in real-time, crafting a spectrum of accuracy-efficiency trade-offs. While adjusting network depth has proven to be a potent strategy for accelerating inference, previous adaptive depth networks have been encumbered by the need for additional intermediate classifiers or decision networks\u2014elements that often prove cumbersome to train effectively.   In contrast to these prior approaches, our adaptive depth networks demand virtually no alterations to the foundational architecture. Instead, we introduce a revolutionary training technique that imbues select sub-paths of the baseline networks with a unique property: these sub-paths maintain the semantic integrity of the input features while refining them to minimize prediction errors. This clever specialization allows for the option to bypass certain sub-paths during testing, thus conserving computational resources with only a marginal sacrifice in accuracy.  We begin by elucidating the theoretical underpinnings of this sub-path specialization, laying the groundwork for a straightforward and pragmatic training method that enhances adaptive depth networks. Remarkably, our strategy requires minimal architectural modifications and training overhead, yet it demonstrably eclipses non-adaptive baselines across a variety of tasks, including ImageNet classification, COCO object detection, and instance segmentation. Furthermore, we reveal that the most compact sub-networks within our adaptive depth framework achieve a competitive model compression effect, rivaling the latest state-of-the-art techniques. In essence, our work not only advances the capabilities of neural networks but also redefines the boundaries of efficiency in machine learning.", "introduction": "Modern deep neural networks provide state-of-the-art performance at high computational costs, and, hence, lots of efforts have been made to leverage those inference capabilities in resource-constrained systems, such as autonomous vehicles.Those efforts include compact architectures (Howard et al., 2017;Zhang et al., 2018;Han et al., 2020), network pruning (Han et al., 2016;Liu et al., 2019), weight/activation quantization (Jacob et al., 2018), knowledge distillation (Hinton et al., 2015), to name a few.However, those approaches provide static accuracy-efficiency trade-offs that are often tailored for worst-case scenarios, and, hence, the lost accuracy cannot be recovered even if more resources become available.Adaptive networks such as anytime networks (Huang et al., 2018;Yu et al., 2018;Wan et al., 2020) attempt to provide runtime adaptability to deep neural networks by exploiting the redundancy in either depths or widths, as shown in Figure 1, or resolutions (Yang et al., 2020a).Dynamic networks (Wu et al., 2018;Li et al., 2021;2020;Zhu et al., 2021) add additional control logic to the backbone network for input-dependent adaptation.However, these adaptive networks usually require auxiliary networks, such as intermediate classifiers or decision networks, which are challenging to train properly.Further, since adaptive networks have multiple sub-networks, embedded in a single neural network, training them incurs potentially conflicting training objectives for the sub-networks, resulting in worse performance than non-adaptive networks (Li et al., 2019).In this work, we introduce a novel approach to anytime networks that is executable in multiple depths to provide instant runtime accuracy-efficiency trade-offs.Unlike previous adaptive depth networks, our approach does not require additional add-on networks or classifiers, and, hence, it can be applied to modern residual networks easily.While maintaining the structure of original networks, we train several sub-paths, or a sequence of residual blocks, of the network to have a special property, that preserves the level of input features, and only refines them to reduce prediction errors.At test time, these specialized sub-paths can be skipped, if needed, for efficiency at marginal loss of accuracy as shown in .Figure 1: Anytime networks with (left) early-exit branches, (middle) adaptive widths, or channels, and (right) specialized sub-paths (ours).Dashed layers (or blocks) and channels can be skipped for instant accuracy-efficiency trade-offs at runtime.The proposed sub-paths specialization is achieved by enforcing sub-networks with different depths to produce features with similar distributions for every spatial dimension.In Section 3, we formally discuss the rationale behind the sub-paths specialization and introduce a simple and practical training method for sub-paths specialization.In most previous adaptive networks, the total training time is linearly proportional to the number of supported sub-networks, and resolving potential conflicts between sub-networks is an important problem.In contrast, our approach does not try to resolve potential conflicts while jointly training many sub-networks.Instead, our training method exploits only two sub-networks for sub-paths specialization, and, at test time, those specialized sub-paths are exploited selectively to build many sub-networks of various depths.Therefore, the total training time is no greater than training two individual networks.Further, our approach with sub-paths specialization do not exploit specific properties of convolution neural networks (CNNs) or vision transformers, and, hence, is generally applicable to residual networks, including both CNNs and recent vision transformers.In Section 4, we demonstrate that our adaptive depth networks with sub-paths specialization outperform counterpart individual networks, both in CNNs and vision transformers, and achieve actual inference acceleration in multiple tasks including ImageNet classification, COCO object detection and instance segmentation.To the best of authors' knowledge, this work is the first general approach to adaptive networks demonstrating consistent performance improvements for both CNNs and vision transformers."}
{"paper_id": 405, "abstract": "In the ever-evolving landscape of natural language generation, large-scale pre-trained language models have emerged as powerful allies, wielding the ability to craft sentences with remarkable fluency. Yet, a persistent challenge looms: the quest for precise control over the attributes of generated text, such as topic and sentiment. Recent strides in this realm have birthed a new breed of models known as \"classifier-guided language models\" (CGLMs), which harness the power of an auxiliary attribute classifier to steer the generative process.  However, a troubling phenomenon arises within these models\u2014the probabilities output by the attribute classifiers often gravitate towards extremes, hovering near 0 or 1. This tendency obscures the nuanced spectrum of sentence alignment with desired attributes, a conundrum we term the \"biased probability distribution\" (BPD) problem.   To combat this issue, we delve into innovative strategies for recalibrating probability distributions and introduce a novel regularization technique, dubbed \"Teaching Others is Teaching Yourself\" (TOTY), designed to smooth out these erratic probabilities. Our empirical investigations into sentiment and topic control tasks reveal that CGLMs, when guided by classifiers honed with TOTY, achieve a remarkable enhancement in performance. Through this work, we illuminate a path toward more refined and controllable language generation, paving the way for a future where our words can be shaped with precision and intent.", "introduction": "Recently, with the advances in large-scale pre-trained language model (PLM) (Radford et al., 2017;2018;2019;Brown et al., 2020), great progress has been made on natural language generation tasks.With billions or even trillions of parameters, and abundant unlabeled training data, PLMs can generate diverse and realistic sentences.Formally, autoregressive PLM models the probability distribution of text X = {x 1 , x 2 , ..., x T } with the chain rule: p(X) = T i=1 p(x i |x 1 , x 2 , ..., x i-1 ).(1)However, those models are usually trained on general purpose corpus and the sentences generated by those PLMs are usually inconsistent with task requirements.Therefore, Controllable Language Generation (CLG), which aims to generate sentences that meet the requirements, has become more important in natural language generation.Controllable language generation attempts to model p(X|a) where a is a desired attribute (e.g.topic, length and sentiment):To simplify the expression, we use X 1:i to denote the sequence {x 1 , x 2 , ..., x i }.It has been found that using an attribute classifier to guide the generation of PLMs was an efficient approach to control the PLMs to generate sentences with expected attributes (Dathathri et al., 2019;Krause et al., 2021;Yang & Klein, 2021).These methods are called classifier-guided language models (CGLMs).In CGLMs, the conditional probability at each generation step is calculated by the Bayes Rule: p(x i |X 1:i-1 , a) \u221d p(a|X 1:i )p(x i |X 1:i-1 ).(3) In the formula above, p(x i |X 1:i-1 ) is the unconditional probability of generating x i at step i, which is usually instantiated by a large-scale language model such as GPT.p(a|X 1:i ) is the attribute probability that the generation result contains the attribute a when starting with the X 1:i .CGLMs usually use the output an attribute classifier, known as guiding classifier, to model p(a|X 1:i ).However, in experiments, we found that the probability distribution of p(a|X 1:i ) predicted by guiding classifiers was usually very biased.To be specific, for most of sentences X 1:i , the probability predicted by guiding classifiers either approaches 0 or approaches 1.We call this phenomenon the biased probability distribution (BPD) problem.In classification tasks, the BPD problem has little influence since these tasks only need to pick out the class with the highest probability.In other words, the concrete value of probability does not have a direct impact on classification accuracy.However, in CGLMs, the attribute of many sentences could be ambiguous.Especially for autoregressive models, the tokens are generated one after another, so the classifiers need to predict the attribute probability of many incomplete sentences.Obviously assigning a probability approaching to 0 or 1 to these sentences is unreasonable.For example, for the following sentences, the probabilities of positive sentiment predicted by the GRU classifier trained on the IMDB dataset (Maas et al., 2011)  A good guiding classifier for CGLMs should distinguish sentences with different matching degrees to the expected attribute, meaning that we should smooth the probability distribution predicted by the classifier.The existing methods (Wang et al., 2021;Gupta & Ramdas, 2021;Platt, 2000;Wei et al., 2022;Zadrozny & Elkan, 2001;Szegedy et al., 2016;M\u00fcller et al., 2019) for adjusting the probability distribution predicted by classifiers are usually used to address the mismatch between a model's confidence and its correctness.It will be shown that these methods does not significantly smooth the probability distribution.In this work, we propose a simple regularization method named Teaching Others is Teaching Yourself (TOTY) to address the BPD problem.In TOTY, we have two classifiers for the same classification task, one is named the \"teacher\" and the other is the \"student\".The teacher classifier learns from the ground truth and teaches the student classifier.Different from knowledge distillation in which knowledge only flows from the teacher to the student, in TOTY, we align the teacher and the student together, such that they can learn from each other.Figure 1 demonstrates the probability distribution of positive sentiment predicted by classifiers trained on the IMDB dataset (Maas et al., 2011) without TOTY regularization and with TOTY regularization.Experiments show that TOTY works well on smoothing the probability distribution of classifiers, and significantly improves the performance of CGLMs.Moreover, TOTY is an easily applicable method since it does not require complicated designing of model structure or training scheme."}
{"paper_id": 406, "abstract": "In the realm of machine learning, where the convergence of decentralized clients and collective intelligence beckons, federated learning (FL) stands as a beacon of potential. It offers a path for clients to unite in training a server model while safeguarding their local data from prying eyes. Yet, this noble endeavor is not without its trials. The traditional FL paradigm demands that clients engage in the intricate dance of backpropagation to derive gradients. However, these clients\u2014often mere edge devices\u2014are not bastions of trust. To burden them with the weight of backpropagation is to invite computational strain, storage woes, and the specter of white-box vulnerabilities.  In response to this challenge, we unveil BAFFLE: a transformative approach to federated learning that eschews backpropagation in favor of a series of forward processes to estimate gradients. This innovative method is not merely a workaround; it is a strategic reimagining of the FL landscape. BAFFLE is designed to be memory-efficient, seamlessly fitting within the constraints of uploading bandwidth. It harmonizes beautifully with inference-only hardware optimizations, and it embraces the art of model quantization and pruning. Moreover, it finds a natural home in trusted execution environments, as clients in BAFFLE only engage in forward propagation, returning a simple set of scalars to the server.  Our experiments reveal a promising truth: models trained through BAFFLE can achieve performance levels that stand shoulder to shoulder with their conventional FL counterparts. In this new frontier of federated learning, we invite you to explore the possibilities that BAFFLE unlocks, where efficiency and security walk hand in hand.", "introduction": "Federated learning (FL) allows decentralized clients to collaboratively train a server model (Kone\u010dn\u1ef3 et al., 2016;McMahan et al., 2017).In each training round, the selected clients compute model gradients or updates on their local private datasets, without explicitly exchanging sample points to the server.While FL describes a promising blueprint and has several applications (Yang et al., 2018;Hard et al., 2018;Li et al., 2020b), the mainstream training paradigm of FL is still gradient-based that requires the clients to locally execute backpropagation, which leads to two practical limitations:(i) Overhead for edge devices.The clients in FL are usually edge devices, such as mobile phones and IoT sensors, whose hardware is primarily optimized for inference-only purposes (Sharma et al., 2018;Umuroglu et al., 2018), rather than for backpropagation.Due to the limited resources, computationally affordable models running on edge devices are typically quantized and pruned (Wang et al., 2019a), making exact backpropagation difficult.In addition, standard implementations of backpropagation rely on either forward-mode or reverse-mode auto-differentiation in contemporary machine learning packages (Bradbury et al., 2018;Paszke et al., 2019b), which increases storage requirements.(ii) White-box vulnerability.To facilitate gradient computing, the server regularly distributes its model status to the clients, but this white-box exposure of the model renders the server vulnerable to, e.g., poisoning or inversion attacks from malicious clients (Shokri et al., 2017;Xie et al., 2020;Zhang et al., 2020;Geiping et al., 2020).With that, recent attempts are made to exploit trusted execution environments (TEEs) in FL, which can isolate the model status within a black-box secure area and significantly reduce the success rate of malicious evasion (Chen et al., 2020;Mo et al., 2021;Zhang et al., 2021;Mondal et al., 2021).However, TEEs are highly memory-constrained (Truong et al., 2021), while backpropagation is memory-consuming to restore intermediate states.While numerous solutions have been proposed to alleviate these limitations (discussed in Appendix B), in this paper, we raise an essential question: does FL really need backpropagation?Inspired by the literature on zero-order optimization (Stein, 1981), we intend to substitute backpropagation with multiple forward or inference processes to estimate the gradients.Technically speaking, we propose the framework of BAckpropagation-Free Federated LEarning (BAFFLE).As illustrated in Figure 1, BAFFLE consists of three conceptual steps: (1) each client locally perturbs the model parameters 2K times as W \u00b1 \u03b4 k (the server sends the random seed to clients for generating {\u03b4 k } K k=1 ); (2) each client executes forward processes on the perturbed models using its private dataset D c and obtains K loss differences {\u2206L(W, \u03b4 k ; D c )} K k=1 ;(3) the server aggregates loss differences to estimate gradients.BAFFLE's defining characteristic is that it only utilizes forward propagation, which is memoryefficient and does not require auto-differentiation.It is well-adapted to model quantization and pruning as well as inference-only hardware optimization on edge devices.Compared to backpropagation, the computation graph of forward propagation in BAFFLE may be more easily optimized, such as by slicing it into per-layer calculation (Kim et al., 2020).Since each loss difference \u2206L(W, \u03b4 k ; D c ) is a scalar, BAFFLE can easily accommodate the uploading bandwidth of clients by adjusting the value of K as opposed to using, e.g., gradient compression (Suresh et al., 2017).BAFFLE is also compatible with recent advances in inference approaches for TEE (Tramer & Boneh, 2019;Truong et al., 2021), providing an efficient solution for combining TEE into FL and preventing white-box evasion.Base on our convergence analyses, we adapt secure aggregation (Bonawitz et al., 2017a) to zeroorder optimization and investigate ways to improve gradient estimation in BAFFLE.In our experiments, BAFFLE is used to train models from scratch on MNIST (LeCun et al., 1998) and CIFAR-10/100 (Krizhevsky & Hinton, 2009), and finetune ImageNet-pretrained models to transfer to OfficeHome (Venkateswara et al., 2017).Compared to conventional FL, BAFFLE achieves suboptimal but acceptable performance.These results shed light on the potential of BAFFLE and the effectiveness of backpropagation-free methods in FL."}
{"paper_id": 407, "abstract": "In the realm of deep learning, where shadows of adversarial attacks loom large, adversarial training has emerged as a beacon of hope\u2014one of the most potent defenses against these insidious threats. Yet, within this promising landscape lies a formidable challenge: robust overfitting, a treacherous pitfall that often ensnares even the most carefully constructed deep networks.   Traditionally, researchers have viewed the deep neural network (DNN) as a monolithic entity, overlooking the nuanced interplay of its layers. It is widely acknowledged that each layer possesses unique characteristics, yet the intricate relationship between these layers and robust overfitting has remained shrouded in mystery. In this work, we embark on a journey of discovery, dissecting the DNN into its constituent layers to illuminate their distinct roles in the dance of robust overfitting.  Our exploration reveals a fascinating truth: different layers exhibit varied responses to the specter of overfitting, with the latter layers of the network bearing the brunt of its effects. Armed with this insight, we introduce a novel approach\u2014Robust Adversarial Training (RAT). In our prototype, we maintain the conventional optimization of the network's front layers while implementing strategic measures to regularize the latter layers' optimization.   From this foundation, we craft two distinct realizations of RAT, and the results are nothing short of transformative. Through rigorous experimentation, we demonstrate that RAT not only vanquishes robust overfitting but also significantly enhances adversarial robustness, surpassing the efficacy of standard adversarial training. In this unfolding narrative, we stand at the precipice of a new era in adversarial defense, one where the layered complexities of deep networks are finally understood and harnessed for greater resilience.", "introduction": "Deep neural networks (DNNs) have been widely applied in multiple fields, such as computer vision (He et al., 2016) and natural language processing (Devlin et al., 2018).Despite its achieved success, recent studies show that DNNs are vulnerable to adversarial examples.Well-constructed perturbations on the input images that are imperceptible to human's eyes can make DNNs lead to a completely different prediction (Szegedy et al., 2013).The security concern due to this weakness of DNNs has led to various works in the study of improving DNNs robustness against adversarial examples.Across existing defense techniques thus far, Adversarial Training (AT) (Goodfellow et al., 2014;Madry et al., 2017), which optimizes DNNs with adversarially perturbed data instead of natural data, is the most effective approach (Athalye et al., 2018).However, it has been shown that networks trained by AT technique do not generalize well (Rice et al., 2020).After a certain point in AT, immediately after the first learning rate decay, the robust test accuracy continues to decrease with further training.Typical regularization practices to mitigate overfitting such as l1 & l2 regularization, weight decay, data augmentation, etc. are reported to be as inefficient compared to simple early stopping (Rice et al., 2020).Many studies have attempted to improve the robust generalization gap in AT, and most have generally investigated robust overfitting by considering DNNs as whole.However, DNNs trained on natural images exhibit a common phenomenon: features obtained in the first layers appear to be general and applicable widespread, while features computed by the last layers are dependent on a particular dataset and task (Yosinski et al., 2014).Such behavior of DNNs sparks a question: Do different layers contribute differently to robust overfitting?Intuitively, robust overfitting acts as an unexpected optimization state in adversarial training, and its occurrence may be closely related to the entire network.Nevertheless, the unique effect of different network layers on robust overfitting is still unclear.Without a detailed understanding of the layer-wise mechanism of robust overfitting, it is difficult to completely demystify the exact underlying cause of the robust overfitting phenomenon.In this paper, we provide the first layer-wise diagnosis of robust overfitting.Specifically, instead of considering the network as a whole, we treat the network as a composition of layers and sys-tematically investigate the impact of robust overfitting phenomenon on different layers.To do this, we first fix the parameters for the selected layers, leaving them unoptimized during AT, and then normally optimize other layer parameters.We discovered that robust overfitting is always mitigated in the case where the latter layers are left unoptimized, and applying the same effect to other layers is futile for robust overfitting, suggesting a strong connection between the optimization of the latter layers and the overfitting phenomenon.Based upon the observed effect, we propose a robust adversarial training (RAT) prototype to relieve the issue of robust overfitting.Specifically, RAT works in each mini-batch: it optimizes the front layers as usual, and for the latter layers, it implements additional measures on these parameters to regularize their optimization.It is a general adversarial training prototype, where the front and latter network layers can be separated by some simple test experiments, and the implementation of additional measures to regularize network layer optimization can be versatile.For instance, we designed two representative methods for the realizations of RAT: RAT LR and RAT WP .They adopt different strategies to hinder weight update, e.g., enlarging the learning rate and weight perturbation, respectively.Extensive experiments show that the proposed RAT prototype effectively eliminates robust overfitting.The contributions of this work are summarized as follows:\u2022 We provide the first diagnosis of robust overfitting on different network layers, and find that there is a strong connection between the optimization of the latter layers and the robust overfitting phenomenon.\u2022 Based on the observed properties of robust overfitting, we propose the RAT prototype, which adopts additional measures to regularize the optimization of the latter layers and is tailored to prevent robust overfitting.\u2022 We design two different realizations of RAT, with extensive experiments on a number of standard benchmarks, verifying its effectiveness."}
